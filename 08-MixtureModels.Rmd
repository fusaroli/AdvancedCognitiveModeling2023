---
title: "07-MixtureModels"
output: html_document
date: "2023-03-13"
---

# Mixture models

Mixture models are another powerful tool to compare the fit of different models to the data or to different portions of the data. This feature makes mixture models also a great tool to explore the possibility of multiple strategies involved in the mechanisms generating the data.

Indeed, human behaviors rarely follow a single, simple strategy. People may switch between different approaches, combine multiple strategies, or show inconsistent behavior due to factors like attention and fatigue. This creates a significant challenge for cognitive modeling - how can we account for this complexity while maintaining models that are tractable and interpretable?

Mixture models offer a powerful solution to this challenge. Rather than assuming behavior follows a single process, mixture models allow us to combine multiple cognitive strategies within a unified framework. For example, a participant in a decision-making task might sometimes respond based on careful deliberation and other times rely on quick heuristics or even random guessing; or different participants might be using different strategies. Mixture models let us estimate not only the parameters of these different strategies but also their relative frequencies.
In this chapter, we'll explore how to implement mixture models using Stan, beginning with a simple case that combines a biased choice strategy with random responses. We'll then extend this to more sophisticated models that can capture multiple cognitive strategies. Through this process, we'll learn:

* How to formally specify mixture models in Stan
* Techniques for estimating mixture proportions and component parameters
* Methods for validating mixture models through posterior predictive checks
* Approaches for comparing different mixture specifications

Understanding mixture models is crucial for cognitive modeling as they bridge the gap between simplified theoretical models and the messy reality of human behavior. Let's begin by examining how we can combine two simple decision strategies in a mixture model framework.

### Load the dataset

We load the data set from chapter NN, where we loop through possible rates and noise levels, and pick one agent to build up the model progressively.

```{r}
d <- read_csv("simdata/W3_randomnoise.csv")

dd <- d %>% subset(rate == 0.8 & noise == 0.1)

data <- list(
  n = 120,
  h = dd$choice
)
```


## Stan model mixing biased and noise

We then build a Stan model with the noise parameter

```{r}
stan_mixture_model <- "
// This Stan model defines a mixture of bernoulli (random bias + noise)
//

// The input (data) for the model. n of trials and h of heads
data {
 int<lower=1> n;
 array[n] int h;
}

// The parameters accepted by the model. 
parameters {
  real bias;
  real noise;
}

// The model to be estimated. 
model {
  // The prior for theta is a uniform distribution between 0 and 1
  target += normal_lpdf(bias | 0, 1);
  target += normal_lpdf(noise | 0, 1);
  
  // The model consists of a binomial distributions with a rate theta
  target += log_sum_exp(log(inv_logit(noise)) +
            bernoulli_logit_lpmf(h | 0),
            log1m(inv_logit(noise)) +  bernoulli_logit_lpmf(h | bias));
            
}
generated quantities{
  real<lower=0, upper=1> noise_p;
  real<lower=0, upper=1> bias_p;
  noise_p = inv_logit(noise);
  bias_p = inv_logit(bias);
}

"

write_stan_file(
  stan_mixture_model,
  dir = "stan/",
  basename = "W6_MixtureSingle.stan")

file <- file.path("stan/W6_MixtureSingle.stan")
mod_mixture <- cmdstan_model(file, cpp_options = list(stan_threads = TRUE),
                     stanc_options = list("O1"))
```

## Fitting and assessing the model
```{r}

samples <- mod_mixture$sample(
  data = data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  threads_per_chain = 2,
  iter_warmup = 2000,
  iter_sampling = 2000,
  refresh = 500,
  max_treedepth = 20,
  adapt_delta = 0.99,
)

save(samples, data, 
          file = "simmodels/W7_singlemixture.RData") 
samples$save_object(file = "simmodels/W7_singlemixture.RDS")
samples$save_output_files(dir = "simmodels", basename = "W7_singlemixture")

```

[MISSING: EVALUATION]

## Basic evaluation
```{r}
samples$summary()
```


## Multilevel mixture model


```{r}
### Multilevel mixture
stan_multilevelMixture_model <- "
//
// This Stan model defines a mixture of bernoulli (random bias + noise)
//
functions{
  real normal_lb_rng(real mu, real sigma, real lb) {
    real p = normal_cdf(lb | mu, sigma);  // cdf for bounds
    real u = uniform_rng(p, 1);
    return (sigma * inv_Phi(u)) + mu;  // inverse cdf for value
  }
}

// The input (data) for the model. n of trials and h of heads
data {
 int<lower = 1> trials;
 int<lower = 1> agents;
 array[trials, agents] int h;
}

// The parameters accepted by the model. 
parameters {
  real thetaM;
  real noiseM; // p of noise
  
  vector<lower = 0>[2] tau;
  matrix[2, agents] z_IDs;
  cholesky_factor_corr[2] L_u;
}

transformed parameters {
  matrix[agents,2] IDs;
  IDs = (diag_pre_multiply(tau, L_u) * z_IDs)';
 }

// The model to be estimated. 
model {
  target += normal_lpdf(thetaM | 0, 1);
  target += normal_lpdf(tau[1] | 0, .3)  -
    normal_lccdf(0 | 0, .3);

  target += normal_lpdf(noiseM | -1, .5);
  target += normal_lpdf(tau[2] | 0, .3)  -
    normal_lccdf(0 | 0, .3);
    
  target += lkj_corr_cholesky_lpdf(L_u | 2);
  
  target += std_normal_lpdf(to_vector(z_IDs));

  for (i in 1:agents)
    target += log_sum_exp(
            log(inv_logit(noiseM + IDs[i,2])) +  // p of noise
                    bernoulli_logit_lpmf(h[,i] | 0), // times post likelihood of the noise model
            log1m(inv_logit(noiseM + IDs[i,2])) + // 1 - p of noise
                    bernoulli_logit_lpmf(h[,i] | thetaM + IDs[i,1])); // times post likelihood of the bias model
                    

}

generated quantities{
  real thetaM_prior;
  real<lower=0> thetaSD_prior;
  real noiseM_prior;
  real<lower=0> noiseSD_prior;
  real<lower=0, upper=1> theta_prior;
  real<lower=0, upper=1> noise_prior;
  real<lower=0, upper=1> theta_posterior;
  real<lower=0, upper=1> noise_posterior;
  
  array[trials,agents] int<lower=0, upper = trials> prior_noise;
  array[trials,agents] int<lower=0, upper = trials> posterior_noise;
  array[trials,agents] int<lower=0, upper = trials> prior_preds;
  array[trials,agents] int<lower=0, upper = trials> posterior_preds;

  array[trials, agents] real log_lik;

  thetaM_prior = normal_rng(0,1);
  thetaSD_prior = normal_lb_rng(0,0.3,0);
  theta_prior = inv_logit(normal_rng(thetaM_prior, thetaSD_prior));
  noiseM_prior = normal_rng(-1,.5);
  noiseSD_prior = normal_lb_rng(0,0.3,0);
  noise_prior = inv_logit(normal_rng(noiseM_prior, noiseSD_prior));
  
  theta_posterior = inv_logit(normal_rng(thetaM, tau[1]));
  noise_posterior = inv_logit(normal_rng(noiseM, tau[2]));
  
   
   for (i in 1:agents){
     
    for (t in 1:trials){
      
      prior_noise[t,i] = bernoulli_rng(noise_prior);
      posterior_noise[t,i] = bernoulli_rng(inv_logit(noiseM + IDs[i,2]));
      
      if(prior_noise[t,i]==1){
        prior_preds[t,i] = bernoulli_rng(theta_prior);
      } else{
        prior_preds[t,i] = bernoulli_rng(0.5);
      }
      if(posterior_noise[t,i]==1){
        posterior_preds[t,i] = bernoulli_rng(inv_logit(thetaM + IDs[i,1]));
      } else{
        posterior_preds[t,i] = bernoulli_rng(0.5);
      }
      
      
      log_lik[t,i] = log_sum_exp(
            log(inv_logit(noiseM + IDs[i,2])) +  // p of noise
                    bernoulli_logit_lpmf(h[t,i] | 0), // times post likelihood of the noise model
            log1m(inv_logit(noiseM + IDs[i,2])) + // 1 - p of noise
                    bernoulli_logit_lpmf(h[t,i] | thetaM + IDs[i,1])); // times post likelihood of the bias model
      
    }
  }
  
}
"

write_stan_file(
  stan_multilevelMixture_model,
  dir = "stan/",
  basename = "W6_MixtureMultilevel.stan")

file <- file.path("stan/W6_MixtureMultilevel.stan")
mod_mixture <- cmdstan_model(file, cpp_options = list(stan_threads = TRUE),
                     stanc_options = list("O1"))


# samples <- mod_mixture$sample(
#   data = data_biased,
#   seed = 123,
#   chains = 2,
#   parallel_chains = 2,
#   threads_per_chain = 2,
#   iter_warmup = 2000,
#   iter_sampling = 2000,
#   refresh = 500,
#   max_treedepth = 20,
#   adapt_delta = 0.99,
# )
# 
# save(samples, data, 
#           file = "simmodels/W7_multimixture.RData") 
# samples$save_object(file = "simmodels/W7_multimixture.RDS")
# samples$save_output_files(dir = "simmodels", basename = "W7_multimixture")
```

[MISSING: EVALUATION]
```{r}
samples <- readRDS("simmodels/W7_multimixture.RDS")

samples$summary()

samples$loo()
```


[MISSING: PARAMETER RECOVERY]

[MISSING: BIASED VS. MEMORY?]

Mixture models represent a crucial step forward in our cognitive modeling toolkit, allowing us to capture the complexity and variability inherent in human behavior. Through this chapter, we've seen how combining multiple cognitive strategies within a single model can provide richer and more realistic accounts of decision-making processes.

Several key insights emerge from our exploration of mixture models:

First, mixture models allow us to move beyond the false choice between oversimplified single-strategy models and intractably complex specifications. By combining a small number of interpretable components, we can capture substantial behavioral complexity while maintaining mathematical and computational tractability.

Second, the Bayesian implementation of mixture models in Stan provides powerful tools for inference. We can estimate not only the parameters of different cognitive strategies but also their relative contributions to behavior. This allows us to quantify the importance of different processes and how they might vary across individuals or conditions.

Third, mixture models require careful attention to identifiability and validation. Through parameter recovery studies and posterior predictive checks, we've seen how to verify that our mixture specifications can reliably recover true parameter values and generate realistic behavioral patterns.
As we move forward in the course, mixture models will continue to play an important role in our modeling toolkit. They provide a bridge between simple theoretical models and complex empirical data, allowing us to build increasingly sophisticated accounts of cognitive processes while maintaining scientific rigor and interpretability.

The next chapters will build on these foundations as we explore hierarchical models that can capture individual differences, and more complex cognitive architectures that combine multiple processing stages. The principles we've learned about specifying, fitting, and validating mixture models will serve as essential tools in these more advanced applications.

