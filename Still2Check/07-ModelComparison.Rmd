---
title: "Chapter 7 (Revised): Cognitive Model Comparison with K-Fold CV and PSIS-LOO"
output: html_document
date: "2025-03-08" # Updated date
---

```{r ch7_setup_kfold, include=FALSE}
# --- Setup Chunk ---
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.width=10, fig.height=6)

# Flags to control regeneration of results (set globally or here)
regenerate_simulations <- FALSE # Set TRUE to re-run Stan fits
regenerate_kfold <- FALSE       # Set TRUE to re-run K-fold CV

# Create directories if they don't exist
if (!dir.exists("stan")) dir.create("stan")
if (!dir.exists("simdata")) dir.create("simdata")
if (!dir.exists("simmodels")) dir.create("simmodels")

# --- Load Packages ---
pacman::p_load(
  tidyverse,    # Core suite for data manipulation (dplyr) and plotting (ggplot2)
  here,         # For robust file paths (optional, assumes project structure)
  posterior,    # Tools for working with posterior distributions
  cmdstanr,     # R interface to Stan
  brms,         # High-level interface for Bayesian models (used conceptually)
  tidybayes,    # Tidy manipulation of Bayesian model output
  loo,          # PSIS-LOO cross-validation and model comparison
  patchwork,    # Combining ggplot plots
  bayesplot,    # Specialized plotting functions for Bayesian models
  priorsense,   # Prior sensitivity analysis
  sbc,          # Simulation-Based Calibration
  groupdata2,   # For creating grouped folds (used in K-Fold)
  future,       # Backend for parallel processing
  furrr         # Parallel processing with purrr/map functions
)

# Set a default ggplot theme for consistency
theme_set(theme_classic())

# Set up parallel processing plan (adjust workers as needed)
cores_to_use <- max(1, parallel::detectCores() - 2)
plan(multisession, workers = cores_to_use) # Use future for parallel K-fold
```

# Model Comparison in Cognitive Science

Cognitive science aims to understand the processes that give rise to human thought and behavior. To do so we (also) need to create formal computational models (Chapters 3, 6) representing hypotheses about these underlying processes. However, human cognition is complex, and multiple theories might plausibly explain the same observed behaviors (as discussed in Chapter 2). How do we decide which theory, and thus which model, provides a better account?

This is where model comparison becomes essential. It provides a principled framework for evaluating competing models based on how well they explain observed data, while accounting for model complexity. In the Bayesian context, we often focus on a model's predictive performance – its ability to generalize and predict new, unseen data. Good predictive performance suggests a model has captured meaningful regularities rather than just fitting noise in the specific data sample it was trained on (a problem known as overfitting).

Remember, model comparison is not a fail-safe procedure to determine the absolute "truth". All models are simplifications. Instead, it helps us identify which of our candidate models provides the most useful or most plausible account of the data, given our current understanding and the available evidence.

## Learning Objectives 

After completing this chapter, you will be able to:

* **Explain Overfitting**: Understand why simply choosing the model that fits the training data best can be misleading.

* **Understand Cross-Validation**: Grasp the core idea of evaluating models based on their ability to predict unseen data, including K-Fold CV.

* **Explain ELPD**: Understand Expected Log Predictive Density (ELPD) as the key metric for Bayesian predictive model comparison.

* **Implement K-Fold CV**: Understand the process of splitting data, fitting models iteratively, and calculating predictive densities for held-out data in K-Fold CV.

* **Explain PSIS-LOO**: Understand how Pareto-Smoothed Importance Sampling Leave-One-Out Cross-Validation (PSIS-LOO) efficiently approximates ELPD using a single model fit.

* **Interpret LOO Diagnostics**: Use Pareto k values to assess the reliability of PSIS-LOO estimates.

* **Implement Model Comparison**: Use the loo package and cmdstanr output to calculate LOOIC/ELPD (from K-Fold or PSIS-LOO) and compare models using loo_compare.

* **Interpret Comparison Results**: Understand elpd_diff, se_diff, and model weights.

* **Perform Model Recovery**: Use simulation studies (where the true generating model is known) to validate model comparison procedures.

* **Integrate Comparison into Workflow**: Understand that model comparison follows model fitting and comprehensive quality checking.

## Why Compare Models?

Model comparison serves multiple purposes in cognitive science:

1. **Theory Testing**: Different models often represent competing theoretical accounts of cognitive processes (Chapter 2). Comparing their predictive fit helps evaluate the empirical support for these theories.

2. **Generalization**: By assessing how well different models predict new data and/or different tasks, we can evaluate their ability to capture general patterns rather than just fitting to specific samples, tasks or domains.

3. **Individual Differences**: Model comparison can reveal whether different individuals or groups are better described by different cognitive strategies.

4. **Model Selection/Averaging**: It can guide the selection of a single "best" model or be used for Bayesian model averaging, where predictions are combined across models weighted by their evidence.

This chapter demonstrates these principles by comparing simple random choice models against more sophisticated memory-based approaches, showing how to rigorously evaluate which better explains observed behavior.

### The Challenge of Model Selection

Imagine having several models of what might be going on and wanting to know which is the best explanation of the data. For example:

* Are people more likely to use a memory strategy or a win-stay-lose-shift strategy? 
* Are we justified in assuming that people react differently to losses than to wins?
* Would we be justified in assuming that capuchin monkeys and cognitive science students use the same model?

Model comparison defines a broad range of practices aimed at identifying the best model for a given dataset. What "best" means is, however, a non-trivial question. Ideally, "best" would mean the model describing the mechanism that actually generated the data.  However, knowing the truth is a tricky proposition and we need to use proxies. There are many of such proxies in the literature, for instance Bayes Factors (see Nicenboim et al 2023, https://vasishth.github.io/bayescogsci/book/ch-comparison.html). In this course, we rely on predictive performance - this helps combat overfitting, but has limitations we'll discuss at the end.

In other words, this chapter will assess models in terms of their (estimated) ability to predict new (test) data. Remember that predictive performance is a very useful tool, but not a magical solution. It allows us to combat overfitting to the training sample (your model snuggling to your data so much that it fits both signal and noise), but it has key limitations, which we will discuss at the end of the chapter. 

To learn how to make model comparison, in this chapter, we rely on our usual simulation based approach to ensure that the method is doing what we want. We simulate the behavior of biased agents playing against the memory agents. This provides us with data generated according to two different mechanisms: biased agents and memory agents. We can fit both models separately on each of the two sets of agents, so we can compare the relative performance of the two models: can we identify the true model generating the data (in a setup where truth is known)? This is what is usually called "model recovery" and complements nicely "parameter recovery". In model recovery we assess whether we can identify the correct model, in parameter recovery we assess whether - once we know the correct model - we can identify the correct parameter values.

## The Problem of Overfitting

A fundamental challenge in modeling is overfitting. This occurs when a model becomes too complex and learns the specific noise and idiosyncrasies of the training dataset, rather than the underlying generative process. An overfit model performs well on the data it was trained on but poorly on new, unseen data.

Imagine fitting polynomial curves to noisy data:

```{r}
# Simulate some noisy data with an underlying quadratic trend
set.seed(101)
x <- seq(-3, 3, length.out = 30)
y_true <- 1 + 0.5*x + 0.8*x^2
y_obs <- y_true + rnorm(30, 0, 2.5)
df <- tibble(x = x, y = y_obs)

# Fit models of varying complexity
fit_lin <- lm(y ~ x, data = df)
fit_quad <- lm(y ~ poly(x, 2), data = df)
fit_high <- lm(y ~ poly(x, 10), data = df) # High-degree polynomial

# Generate predictions
x_pred <- seq(-3, 3, length.out = 100)
preds <- tibble(
  x = x_pred,
  y_lin = predict(fit_lin, newdata = data.frame(x = x_pred)),
  y_quad = predict(fit_quad, newdata = data.frame(x = x_pred)),
  y_high = predict(fit_high, newdata = data.frame(x = x_pred))
)

# Plot
ggplot(df, aes(x = x, y = y)) +
  geom_point(size = 2, alpha = 0.8) +
  geom_line(data = preds, aes(y = y_lin, color = "Linear (Underfit)"), size = 1) +
  geom_line(data = preds, aes(y = y_quad, color = "Quadratic (Good Fit)"), size = 1) +
  geom_line(data = preds, aes(y = y_high, color = "High Poly (Overfit)"), size = 1) +
  scale_color_manual(values = c("Linear (Underfit)" = "blue",
                               "Quadratic (Good Fit)" = "darkgreen",
                               "High Poly (Overfit)" = "red")) +
  labs(title = "Model Complexity and Overfitting",
       x = "Predictor", y = "Outcome", color = "Model") +
  theme_classic() + theme(legend.position="bottom")
```

The high-degree polynomial (red) fits the observed points very closely but wildly deviates between them – it has learned the noise. It would likely make poor predictions for new data points. The quadratic model (green) captures the underlying trend without fitting every wiggle, representing a better balance.

Model comparison techniques aim to identify models like the green curve – those that capture the signal without overfitting the noise.

## Simulation Setup: Biased vs. Memory Agents

We will use the simulation data generated in Chapter 6. 

In this example, we have:

* Model 1 (Biased Agent): Makes choices with a consistent 80% bias toward the right option

* Model 2 (Memory Agent): Adjusts choices based on memory of previous patterns

The critical insight is that both models can produce similar-looking data, making it difficult to determine which cognitive process generated the observed behavior by simple visual inspection. Formal model comparison techniques give us principled ways to evaluate which model better explains the data while accounting for model complexity and generalization ability.

We know the ground truth for each agent. Our goal is to see if model comparison can correctly identify which model (Multilevel Biased vs. Multilevel Memory) better describes the data generated by each agent type. This process is often called model recovery.

### Load Simulation Data and Models
First, we load the simulation data and the compiled Stan models (NCP versions) from Chapter 6.

```{r}
# --- Load Simulation Data ---
simulation_data_file <- file.path("simdata", "ch6_simulation_data.rds")
if (!file.exists(simulation_data_file)) {
  stop("Simulation data file not found. Please run Chapter 6 simulations first.")
}
simulation_data <- readRDS(simulation_data_file)
# Ensure agent IDs are factors for grouping
simulation_data <- simulation_data %>% mutate(agent = factor(agent))

# --- Load Compiled Stan Models (NCP versions) ---
# Biased Model
stan_file_biased_ml_ncp <- file.path("stan", "ch6_multilevel_biased_ncp.stan") # Assumes file exists from Ch6
if (!exists("mod_biased_ml_ncp") || !cmdstanr::is_cmdstan_model(mod_biased_ml_ncp)) {
  if(file.exists(stan_file_biased_ml_ncp)) {
      mod_biased_ml_ncp <- cmdstan_model(stan_file_biased_ml_ncp, cpp_options = list(stan_threads = TRUE))
      message("Compiled biased NCP model.")
  } else {
      # Fallback: Define and compile if file missing (copy code from Ch6 Canvas)
      stop("Stan file for biased NCP model not found. Please ensure ch6_multilevel_biased_ncp.stan exists.")
  }
} else {
  message("Using already compiled biased NCP multilevel model.")
}

# Memory Model
stan_file_memory_ml_ncp <- file.path("stan", "ch6_multilevel_memory_ncp.stan") # Assumes file exists from Ch6
if (!exists("mod_memory_ml_ncp") || !cmdstanr::is_cmdstan_model(mod_memory_ml_ncp)) {
   if(file.exists(stan_file_memory_ml_ncp)) {
      mod_memory_ml_ncp <- cmdstan_model(stan_file_memory_ml_ncp, cpp_options = list(stan_threads = TRUE))
      message("Compiled memory NCP model.")
   } else {
      # Fallback: Define and compile if file missing (copy code from Ch6 Canvas)
       stop("Stan file for memory NCP model not found. Please ensure ch6_multilevel_memory_ncp.stan exists.")
   }
} else {
  message("Using already compiled memory NCP multilevel model.")
}

# --- Prepare Data Subsets for Stan Fitting ---
# We need lists formatted for Stan, separating data by generating agent type.

# 1. Data generated by BIASED agents
biased_agent_data_sim <- simulation_data %>%
  filter(agent_type == "Biased") %>%
  mutate(agent_id_stan = as.numeric(factor(agent))) # Remap IDs 1:J

stan_data_bias_gen <- list(
  N = nrow(biased_agent_data_sim),
  J = length(unique(biased_agent_data_sim$agent_id_stan)),
  agent_id = biased_agent_data_sim$agent_id_stan,
  h = biased_agent_data_sim$agent_choice
)

# 2. Data generated by MEMORY agents
memory_agent_data_sim <- simulation_data %>%
  filter(agent_type == "Memory") %>%
  mutate(agent_id_stan = as.numeric(factor(agent))) # Remap IDs 1:J

# Pre-calculate opponent history feature for memory model fitting
memory_agent_data_sim <- memory_agent_data_sim %>%
  arrange(agent_id_stan, trial) %>%
  group_by(agent_id_stan) %>%
  mutate(
    # Calculate running mean of opponent choices *lagged* by one trial
    opponent_rate_history = lag(cumsum(opponent_choice) / row_number(), default = 0.5)
  ) %>%
  ungroup() %>%
  # Clip for numerical stability before applying logit in Stan
  mutate(opponent_rate_history = pmax(0.01, pmin(0.99, opponent_rate_history)))

stan_data_mem_gen <- list(
  N = nrow(memory_agent_data_sim),
  J = length(unique(memory_agent_data_sim$agent_id_stan)),
  agent_id = memory_agent_data_sim$agent_id_stan,
  h = memory_agent_data_sim$agent_choice,
  # Pass the pre-calculated opponent rate history
  opponent_rate_prob = memory_agent_data_sim$opponent_rate_history
  # Note: The memory Stan model expects 'opponent_rate_prob' as input name
)

cat("Data prepared for fitting.\n")
```

### Fitting Models to Simulated Data (Full Data Fits)
Before cross-validation or comparison, we fit both models to both datasets using the full data. These fits are essential for PSIS-LOO and for initial quality checks.

```{r}
# Define file paths for saved fits
fit_files <- list(
  bias_on_bias = file.path("simmodels", "ch7_fit_bias_on_bias.rds"),
  mem_on_bias = file.path("simmodels", "ch7_fit_mem_on_bias.rds"),
  bias_on_mem = file.path("simmodels", "ch7_fit_bias_on_mem.rds"),
  mem_on_mem = file.path("simmodels", "ch7_fit_mem_on_mem.rds")
)

# Function to fit or load a model
fit_or_load <- function(model_name, model_obj, data_list, file_path, seed) {
  if (regenerate_simulations || !file.exists(file_path)) {
    cat("Fitting", model_name, "...\n")
    # Use slightly more robust settings for fitting
    fit <- model_obj$sample(
      data = data_list,
      seed = seed,
      chains = 4, parallel_chains = min(4, cores_to_use),
      iter_warmup = 1000, iter_sampling = 1500, # More samples
      refresh = 500,
      adapt_delta = 0.95, # Increase adapt_delta
      max_treedepth = 12  # Increase max_treedepth
    )
    fit$save_object(file = file_path)
    cat("Fit saved to:", file_path, "\n")
    # Check diagnostics immediately after fitting
    cat("Diagnostics for", model_name, ":\n")
    fit$cmdstan_diagnose()
    num_divergences <- sum(fit$sampler_diagnostics()[, "divergent__"])
     if(num_divergences > 0) {
        warning(paste("Divergences detected in", model_name, ":", num_divergences))
    }
    return(fit)
  } else {
    cat("Loading existing fit:", file_path, "\n")
    return(readRDS(file_path))
  }
}

# Fit/Load all four models
# IMPORTANT: Ensure the correct data list is used for each fit!
fit_bias_on_bias <- fit_or_load("Bias Model on Bias Data", mod_biased_ml_ncp, stan_data_bias_gen, fit_files$bias_on_bias, 101)

# For fitting memory model to bias data, need opponent history calculated from bias data
bias_agent_data_sim_for_mem <- simulation_data %>%
    filter(agent_type == "Biased") %>%
    mutate(agent_id_stan = as.numeric(factor(agent))) %>%
    arrange(agent_id_stan, trial) %>%
    group_by(agent_id_stan) %>%
    # Find corresponding opponent choice (assuming Memory agent was opponent)
    left_join(simulation_data %>% filter(agent_type=="Memory") %>% select(agent, trial, opponent_choice_hist = agent_choice),
              by=c("agent", "trial")) %>%
    mutate(opponent_rate_history = lag(cumsum(opponent_choice_hist) / row_number(), default = 0.5)) %>%
    ungroup() %>%
    mutate(opponent_rate_history = pmax(0.01, pmin(0.99, opponent_rate_history)))

stan_data_bias_gen_for_mem <- list(
    N = stan_data_bias_gen$N, J = stan_data_bias_gen$J,
    agent_id = stan_data_bias_gen$agent_id, h = stan_data_bias_gen$h,
    opponent_rate_prob = bias_agent_data_sim_for_mem$opponent_rate_history
)
fit_mem_on_bias <- fit_or_load("Memory Model on Bias Data", mod_memory_ml_ncp, stan_data_bias_gen_for_mem, fit_files$mem_on_bias, 102)


fit_bias_on_mem <- fit_or_load("Bias Model on Memory Data", mod_biased_ml_ncp, stan_data_mem_gen, fit_files$bias_on_mem, 103) # Use h from memory data
fit_mem_on_mem <- fit_or_load("Memory Model on Memory Data", mod_memory_ml_ncp, stan_data_mem_gen, fit_files$mem_on_mem, 104)

cat("All models fitted or loaded.\n")
```

### Model Quality Checks (Abbreviated)

**Crucially, before comparing these models, we must perform the full suite of quality checks from Chapter 5 & 6 on EACH of these four fits**. This includes:

* Convergence Diagnostics: Rhat, ESS, divergences, trace plots.

* Parameter Recovery: Check if models fit to their "own" data recover sensible parameters.

* Predictive Checks (Prior & Posterior): Check if the models generate reasonable data patterns.

* Sensitivity Analysis: Check robustness to priors.

* SBC: Check overall calibration.

**Comparing models that haven't passed these checks can lead to meaningless or misleading conclusions**.

```{r}
# Abbreviated Checks (Illustrative - Replace with full checks in practice)

check_fit_quality <- function(fit, fit_name) {
  cat("\n--- Quality Checks for:", fit_name, "---\n")
  # 1. Convergence
  diag_summary <- fit$diagnostic_summary()
  num_divergences <- diag_summary$num_divergent
  num_max_treedepth <- diag_summary$num_max_treedepth
  rhat_high <- sum(fit$summary()$rhat > 1.05, na.rm=TRUE)
  ess_low <- sum(fit$summary()$ess_bulk < 400, na.rm=TRUE) + sum(fit$summary()$ess_tail < 400, na.rm=TRUE)
  cat(" Divergences:", num_divergences, "\n")
  cat(" Max Treedepth Hits:", num_max_treedepth, "\n")
  cat(" High Rhats (>1.05):", rhat_high, "\n")
  cat(" Low ESS (<400):", ess_low, "\n")
  if(num_divergences > 0 || rhat_high > 0 || ess_low > 0) {
      warning("Potential convergence/sampling issues detected for ", fit_name)
  }

  # 2. Posterior Predictive Check (Example: Mean)
  if("h" %in% names(fit$data)) { # Check if 'h' exists in data used for fit
      y_obs <- fit$data$h
      y_rep <- fit$draws("h_post_rep", format="matrix") # Assumes h_post_rep exists
      if(!is.null(y_rep) && ncol(y_rep) == length(y_obs)) {
          p <- ppc_stat(y=y_obs, yrep=y_rep, stat="mean") + labs(title=fit_name) + theme_classic()
          print(p)
      } else {
          cat(" Could not perform PPC check (missing h or h_post_rep).\n")
      }
  } else {
       cat(" Could not perform PPC check (missing h in fit data).\n")
  }
  cat("---------------------------------------\n")
}

# Run checks on all fits
check_fit_quality(fit_bias_on_bias, "Bias Model on Bias Data")
check_fit_quality(fit_mem_on_bias, "Memory Model on Bias Data")
check_fit_quality(fit_bias_on_mem, "Bias Model on Memory Data")
check_fit_quality(fit_mem_on_mem, "Memory Model on Memory Data")

cat("\nNOTE: These are abbreviated checks. Full checks (parameter recovery, sensitivity, SBC) are essential before relying on model comparisons.\n")


```

## Cross-Validation: The Foundation of Model Comparison

Cross-validation is a fundamental technique for comparing models based on their predictive performance. The core idea is simple: a good model should not only fit the observed data but also generalize well to new, unseen data.

When we fit a model to data, there's always a risk of overfitting - capturing noise or idiosyncrasies in the particular sample rather than the underlying pattern we care about. 

Cross-validation helps us find the optimal balance between fitting the training data and generalizing to new data.

When the datasets are small, as it is often the case in cognitive science, keeping a substantial portion of the data out - substantial enough to be representative of a more general population - is problematic as it risks starving the model of data: there might not be enough data for reliable estimation of the parameter values. This is where the notion of cross-validation comes in: we can split the dataset in k folds, let's say k = 10. Then each fold is in turn kept aside as validation set, the model is fitted on the other folds, and its predictive performance tested on the validation set. Repeat this operation of each of the folds. This operation ensures that all the data can be used for training as well as for validation, and is in its own terms quite genial. However, this does not mean it is free of shortcomings. First, small validation folds might not be representative of the diversity of true out-of-sample populations - and there is a tendency to set k equal to the number of datapoints (leave-one-out cross validation). Second, there are many ways in which information could leak or contaminate across folds if the pipeline is not very careful (e.g. via data preprocessing scaling the full dataset, or hyper-parameter estimation). Third, and crucial for our case here, cross validation implies refitting the model k times, which for Bayesian models might be very cumbersome (I once had a model that took 6 weeks to run).

### How Cross-Validation Works

The basic idea of cross-validation is to:

1. Split your data into training and test sets
2. Fit your model to the training data
3. Evaluate the model's performance on the test data (which it hasn't seen during training)
4. Repeat with different training/test splits and average the results


#### K-Fold Cross-Validation

In k-fold cross-validation, we:
1. Divide the data into k equally sized subsets (folds)
2. Use k-1 folds for training and the remaining fold for testing
3. Repeat k times, each time using a different fold as the test set
4. Average the k test performance metrics

K-Fold CV involves refitting models multiple times on subsets of the data. It's computationally intensive but provides a direct estimate of out-of-sample performance. This visualization shows how 5-fold cross-validation works:

```{r}
# Visualization of k-fold cross-validation
set.seed(123)
n_data <- 20

# K-Fold CV (k=5)
cv_data <- tibble(
  index = 1:n_data,
  value = rnorm(n_data)
)
set.seed(456)
cv_data$fold <- sample(rep(1:5, length.out = n_data))

# Create visualization data for k-fold CV
cv_viz_data <- tibble(
  iteration = rep(1:5, each = n_data),
  data_point = rep(1:n_data, 5),
  role = ifelse(cv_data$fold[data_point] == iteration, "test", "train"),
  approach = "5-Fold CV"
)

# Define a consistent color scheme
role_colors <- c("train" = "steelblue", "test" = "firebrick")

# Create the plot
ggplot(cv_viz_data, aes(x = data_point, y = iteration, fill = role)) +
  geom_tile(color = "white", size = 0.5) +
  scale_fill_manual(values = role_colors, name = "Data Role") +
  labs(
    title = "K-Fold Cross-Validation (k=5)",
    subtitle = "Each row shows one iteration, columns represent data points",
    x = "Data Point",
    y = "Iteration",
    fill = "Data Usage"
  ) +
  theme_minimal() +
  theme(
    panel.grid = element_blank(),
    legend.position = "bottom"
  )
```

##### Stan Models for K-Fold CV
We need Stan models that calculate the log-likelihood only for the held-out test data in each fold. The model is trained only on the training data. Predictions for test agents (which might be unseen in the training fold) are typically based on the estimated population parameters.

```{r}
# Stan model for multilevel biased agent - CV Version
# (Code identical to stan_biased_ml_cv_code in previous response)
stan_biased_ml_cv_code <- "
// Multilevel Biased Agent Model (NCP) - CV Version
// Calculates log-likelihood ONLY for test data

data {
  // Training Data
  int<lower=1> N_train;
  int<lower=1> J_train;
  array[N_train] int<lower=1, upper=J_train> agent_id_train; // IDs mapped to 1..J_train
  array[N_train] int<lower=0, upper=1> h_train;

  // Test Data
  int<lower=1> N_test;
  array[N_test] int<lower=0, upper=1> h_test;
  // Info needed to predict test data (e.g., just use population mean)
}

parameters {
  real mu_theta;
  real<lower=0> sigma_theta;
  vector[J_train] z_theta_train; // Effects for training agents ONLY
}

transformed parameters {
  vector[J_train] theta_logit_train = mu_theta + z_theta_train * sigma_theta;
}

model {
  // Priors
  mu_theta ~ normal(0, 1);
  sigma_theta ~ exponential(1);
  z_theta_train ~ std_normal();

  // Likelihood for TRAINING data ONLY
  h_train ~ bernoulli_logit(theta_logit_train[agent_id_train]);
}

generated quantities {
  // Log-likelihood for TEST data ONLY
  vector[N_test] log_lik;
  // Predict test data using the estimated population mean (mu_theta)
  // This assumes test agents are new draws from the same population
  for (i in 1:N_test) {
     log_lik[i] = bernoulli_logit_lpmf(h_test[i] | mu_theta);
  }
}
"
stan_file_biased_ml_cv <- file.path("stan", "ch6_multilevel_biased_cv.stan")
write_stan_file(stan_biased_ml_cv_code, stan_file_biased_ml_cv)

# Stan model for multilevel memory agent - CV Version
# (Code identical to stan_memory_ml_cv_code in previous response)
stan_memory_ml_cv_code <- "
// Multilevel Memory Agent Model (NCP, Correlated) - CV Version
// Calculates log-likelihood ONLY for test data

data {
  // Training Data
  int<lower=1> N_train;
  int<lower=1> J_train;
  array[N_train] int<lower=1, upper=J_train> agent_id_train; // IDs mapped to 1..J_train
  array[N_train] int<lower=0, upper=1> h_train;
  vector[N_train] opponent_rate_prob_train; // History feature for train

  // Test Data
  int<lower=1> N_test;
  array[N_test] int<lower=0, upper=1> h_test;
  vector[N_test] opponent_rate_prob_test; // History feature for test
}

parameters {
  // Population parameters (estimated from training data)
  real mu_bias;
  real mu_beta;
  vector<lower=0>[2] sigma;
  cholesky_factor_corr[2] L_Omega;

  // Individual effects for training agents
  matrix[2, J_train] z_train;
}

transformed parameters {
  // Individual parameters for training agents
  matrix[J_train, 2] indiv_params_train;
  indiv_params_train = rep_matrix(vector({mu_bias, mu_beta})', J_train) + (diag_pre_multiply(sigma, L_Omega) * z_train)';
}

model {
  // Priors
  mu_bias ~ normal(0, 1);
  mu_beta ~ normal(0, 1.5);
  sigma ~ exponential(1);
  L_Omega ~ lkj_corr_cholesky(2);
  to_vector(z_train) ~ std_normal();

  // Likelihood for TRAINING data ONLY
  vector[N_train] logit_p_train;
  for (i in 1:N_train) {
    int j = agent_id_train[i];
    logit_p_train[i] = indiv_params_train[j, 1] + indiv_params_train[j, 2] * logit(opponent_rate_prob_train[i]);
  }
  h_train ~ bernoulli_logit(logit_p_train);
}

generated quantities {
  // Log-likelihood for TEST data ONLY
  vector[N_test] log_lik;
  // Predict test data using population parameters estimated from training data
  for (i in 1:N_test) {
     log_lik[i] = bernoulli_logit_lpmf(h_test[i] | mu_bias + mu_beta * logit(opponent_rate_prob_test[i]));
  }
}
"
stan_file_memory_ml_cv <- file.path("stan", "ch6_multilevel_memory_cv.stan")
write_stan_file(stan_memory_ml_cv_code, stan_file_memory_ml_cv)

# Compile CV models
if (!exists("mod_biased_ml_cv") || !cmdstanr::is_cmdstan_model(mod_biased_ml_cv)) {
  mod_biased_ml_cv <- cmdstan_model(stan_file_biased_ml_cv, cpp_options = list(stan_threads = TRUE))
}
if (!exists("mod_memory_ml_cv") || !cmdstanr::is_cmdstan_model(mod_memory_ml_cv)) {
  mod_memory_ml_cv <- cmdstan_model(stan_file_memory_ml_cv, cpp_options = list(stan_threads = TRUE))
}
cat("CV Stan models compiled.\n")
```

##### Running K-Fold CV (Example on Memory Data)
We perform 10-fold CV, fitting both models in each fold to the memory-generated data. Warning: This is computationally intensive and may take significant time.

```{r}
# NOTE: This chunk is set to eval=FALSE by default.
# Set eval=TRUE and ensure regenerate_kfold=TRUE to run it.

kfold_results_file <- file.path("simmodels", "ch7_kfold_results_memdata.rds")
K <- 10 # Number of folds

if (regenerate_kfold || !file.exists(kfold_results_file)) {
  cat("Running K-Fold Cross-Validation (K=", K, ") on Memory Agent Data...\n")

  # Data to use: memory_agent_data_sim
  data_for_kfold <- memory_agent_data_sim %>%
      mutate(obs_idx = 1:n()) # Add observation index

  # Create folds grouped by the original agent ID
  set.seed(1234) # for reproducible folds
  data_for_kfold <- groupdata2::fold(
      data_for_kfold,
      k = K,
      id_col = "agent" # Group by original agent ID
  )

  # Initialize matrices to store log-likelihoods from test sets
  # Determine posterior draws size from a quick fit or set manually
  # Example: assuming 1500 sampling iterations * 4 chains
  n_draws_per_fit <- 1500 * 4
  log_lik_bias_kfold <- matrix(NA_real_, nrow = n_draws_per_fit, ncol = nrow(data_for_kfold))
  log_lik_mem_kfold <- matrix(NA_real_, nrow = n_draws_per_fit, ncol = nrow(data_for_kfold))

  # --- K-Fold Loop Function ---
  run_fold <- function(k, full_data) {
      cat("  Processing fold", k, "...\n")
      train_data <- full_data %>% filter(.folds != k)
      test_data <- full_data %>% filter(.folds == k)

      # Prepare training data for Stan (remap agent IDs 1..J_train)
      train_agents <- unique(train_data$agent)
      J_train <- length(train_agents)
      train_data <- train_data %>% mutate(agent_id_train = as.numeric(factor(agent)))

      # Prepare test data (keep original agent IDs for potential mapping if needed)
      test_agents <- unique(test_data$agent)
      J_test <- length(test_agents) # Not directly used by CV models here

      # Stan data for Bias CV model
      stan_data_train_bias_cv <- list(
          N_train = nrow(train_data), J_train = J_train,
          agent_id_train = train_data$agent_id_train, h_train = train_data$agent_choice,
          N_test = nrow(test_data),
          h_test = test_data$agent_choice,
          # Need original J if predicting based on sampled individual params
          J_orig = length(unique(full_data$agent)),
          agent_id_test_orig = as.numeric(factor(test_data$agent, levels=unique(full_data$agent)))
      )

      # Stan data for Memory CV model
      stan_data_train_mem_cv <- list(
          N_train = nrow(train_data), J_train = J_train,
          agent_id_train = train_data$agent_id_train, h_train = train_data$agent_choice,
          opponent_rate_prob_train = train_data$opponent_rate_history,
          N_test = nrow(test_data),
          h_test = test_data$agent_choice,
          opponent_rate_prob_test = test_data$opponent_rate_history,
          J_orig = length(unique(full_data$agent)),
          agent_id_test_orig = as.numeric(factor(test_data$agent, levels=unique(full_data$agent)))
      )

      # Fit models (using fewer iterations for CV speed)
      fit_bias_cv <- mod_biased_ml_cv$sample(data = stan_data_train_bias_cv, seed = 100 + k, chains = 4, parallel_chains = 2, iter_warmup=500, iter_sampling=1500, refresh=0, adapt_delta=0.9)
      fit_mem_cv <- mod_memory_ml_cv$sample(data = stan_data_train_mem_cv, seed = 200 + k, chains = 4, parallel_chains = 2, iter_warmup=500, iter_sampling=1500, refresh=0, adapt_delta=0.95) # Higher adapt_delta

      # Extract log_lik for the test set
      ll_bias <- fit_bias_cv$draws("log_lik", format = "matrix")
      ll_mem <- fit_mem_cv$draws("log_lik", format = "matrix")

      # Get original observation indices for this test fold
      test_indices <- test_data$obs_idx

      # Return results
      list(test_indices = test_indices, ll_bias = ll_bias, ll_mem = ll_mem)
  }

  # Run folds in parallel
  plan(multisession, workers = cores_to_use) # Reset parallel plan if needed
  kfold_results_list <- future_map(1:K, ~run_fold(.x, data_for_kfold), .options = furrr_options(seed=TRUE, packages=c("dplyr", "cmdstanr")), .progress=TRUE)

  # Aggregate results into the pre-initialized matrices
  for(k in 1:K) {
      fold_res <- kfold_results_list[[k]]
      # Ensure dimensions match before assignment
      if(nrow(fold_res$ll_bias) == n_draws_per_fit && ncol(fold_res$ll_bias) == length(fold_res$test_indices)) {
         log_lik_bias_kfold[, fold_res$test_indices] <- fold_res$ll_bias
      } else { warning("Dimension mismatch for bias log_lik in fold ", k)}
      if(nrow(fold_res$ll_mem) == n_draws_per_fit && ncol(fold_res$ll_mem) == length(fold_res$test_indices)) {
         log_lik_mem_kfold[, fold_res$test_indices] <- fold_res$ll_mem
      } else { warning("Dimension mismatch for memory log_lik in fold ", k)}
  }

  # Check if aggregation was successful (any NAs left?)
  if(any(is.na(log_lik_bias_kfold)) || any(is.na(log_lik_mem_kfold))) {
      warning("NA values remain in aggregated K-fold log-likelihoods. Check loop logic and dimensions.")
  }

  # Save aggregated log-likelihoods
  kfold_logliks <- list(bias = log_lik_bias_kfold, memory = log_lik_mem_kfold)
  saveRDS(kfold_logliks, kfold_results_file)
  cat("K-Fold CV finished and results saved.\n")

} else {
  if(file.exists(kfold_results_file)) {
      kfold_logliks <- readRDS(kfold_results_file)
      cat("Loaded existing K-Fold CV results.\n")
  } else {
      kfold_logliks <- NULL
      cat("K-Fold CV results file not found. Set regenerate_kfold=TRUE to run.\n")
  }
}

# Calculate ELPD from K-fold log-likelihoods (if available)
if (!is.null(kfold_logliks) && !any(is.na(kfold_logliks$bias)) && !any(is.na(kfold_logliks$memory))) {
    loo_bias_kfold <- loo::loo(kfold_logliks$bias)
    loo_mem_kfold <- loo::loo(kfold_logliks$memory)

    cat("\n--- K-Fold CV Comparison (on Memory Data) ---\n")
    # Compare Memory vs Bias
    kfold_compare_memdata <- loo_compare(loo_mem_kfold, loo_bias_kfold)
    print(kfold_compare_memdata)
} else {
    cat("K-Fold results not available or contain NAs, cannot compare.\n")
    kfold_compare_memdata <- NULL # Ensure variable exists but is NULL
}

```

**Interpretation**: The K-Fold CV comparison (kfold_compare_memdata) should ideally favor the Memory model when run on data generated by memory agents. The elpd_diff indicates the difference in predictive performance. This process demonstrates the gold standard but highlights its significant computational expense (fitting 2 models * K times).


#### Leave-One-Out Cross-Validation (LOO-CV)

Leave-one-out is a special case of k-fold cross-validation where k equals the number of data points. In each iteration, we:
1. Hold out a single observation for testing
2. Train on all other observations
3. Repeat for every observation
4. Average the performance metrics

This approach can be very computationally intensive for large datasets or complex models.

```{r}
# Create visualization data for LOO CV
loo_viz_data <- tibble(
  iteration = rep(1:n_data, each = n_data),
  data_point = rep(1:n_data, n_data),
  role = ifelse(iteration == data_point, "test", "train"),
  approach = "LOO CV"
)

# Create the plot
ggplot(loo_viz_data, aes(x = data_point, y = iteration, fill = role)) +
  geom_tile(color = "white", size = 0.5) +
  scale_fill_manual(values = role_colors, name = "Data Role") +
  labs(
    title = "Leave-One-Out Cross-Validation",
    subtitle = "Each row shows one iteration, columns represent data points",
    x = "Data Point",
    y = "Iteration",
    fill = "Data Usage"
  ) +
  theme_minimal() +
  theme(
    panel.grid = element_blank(),
    legend.position = "bottom"
  )
```

### Cross-Validation in Bayesian Models

Cross-validation is especially important in Bayesian modeling for several reasons:

1. Bayesian models can be highly parameterized and this can make them prone to overfitting
2. Prior choices can significantly influence model performance
3. The complexity of hierarchical structures needs careful validation
4. We often need to compare competing theoretical accounts

However, cross-validation for Bayesian models presents two key challenges:

1. **Computational cost**: Bayesian models fitted with MCMC can take hours or days to run, making it impractical to refit them k times for cross-validation

2. **Proper scoring**: We need appropriate metrics for evaluating predictive performance in a Bayesian framework

Next, we'll see how these challenges are addressed.

### Expected Log Predictive Density (ELPD)

When comparing Bayesian models, we use the expected log predictive density (ELPD) as our metric. This measures how well the model predicts new data points on the log scale.

For a single observation, the log predictive density is:

$$\log p(y_i | y_{-i})$$

where $y_i$ is the observation we're trying to predict, and $y_{-i}$ represents all other observations that were used for training.

For the entire dataset, we sum across all observations:

$$\text{ELPD} = \sum_{i=1}^{n} \log p(y_i | y_{-i})$$

The ELPD has several desirable properties:

1. It accounts for the full predictive distribution, not just point estimates
2. It automatically penalizes overconfident models
3. Higher values indicate better predictive performance

Computing the ELPD exactly requires fitting the model n times (for n data points), which brings us back to the computational challenge.


## PSIS-LOO: An Efficient Approximation to LOO-CV

For complex Bayesian models, true leave-one-out cross-validation (LOO-CV) is often computationally infeasible. Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO) provides an elegant solution by approximating LOO-CV using just a single model fit.

### How Importance Sampling Works

When we fit a Bayesian model, we obtain samples from the posterior distribution $p(\theta|y_1,...,y_n)$ - the distribution of model parameters given all observations. For LOO-CV, we need $p(\theta|y_1,...,y_{i-1},y_{i+1},...,y_n)$ - the distribution without the i-th observation.

Importance sampling bridges this gap by reweighting the full posterior samples to approximate the LOO posterior. The importance weights for the i-th observation are:

$$w_i(\theta) = \frac{p(\theta|y_1,...,y_{i-1},y_{i+1},...,y_n)}{p(\theta|y_1,...,y_n)} \propto \frac{1}{p(y_i|\theta)}$$

By Bayes' theorem, this simplifies to:

w_i(θ) ∝ 1 / p(yᵢ|θ)

These weights effectively "undo" the influence of the i-th observation on the posterior.

However, standard importance sampling can be unstable when:
* The full posterior and LOO posterior differ substantially
* Some importance weights become extremely large
* The variance of the weights is high

### Pareto Smoothing for Stability

Pareto smoothing improves the reliability of importance sampling:

1. Fit a generalized Pareto distribution to the largest importance weights
2. Replace the largest weights with values from this smoothed distribution
3. Use the modified weights for more stable LOO estimation

The diagnostic parameter k from the Pareto fit helps assess reliability:
* k < 0.5: Reliable estimation
* 0.5 < k < 0.7: Somewhat reliable, proceed with caution
* k > 0.7: Unreliable, consider using other methods for this observation

These diagnostics help identify problematic observations that might require more attention or alternative methods.

### The Complete PSIS-LOO Process ###

The full PSIS-LOO method follows these steps:

Fit the Bayesian model once to all available data

For each observation i:

a. Calculate raw importance weights using the log-likelihood: w_i(θ) ∝ 1/p(yᵢ|θ)

b. Apply Pareto smoothing to stabilize the largest weights

c. Normalize the smoothed weights

d. Use the weights to compute the expected log predictive density (ELPD)

Sum the individual ELPD contributions to get the overall PSIS-LOO estimate

[I NEED TO FIND A BETTER WAY TO EXPLAIN AND VISUALIZE!]

```{r}
# Create improved visualizations of PSIS-LOO
set.seed(789)

# Generate some sample data
n <- 50
x <- seq(-3, 3, length.out = n)
y_true <- 2 + 1.5 * x + 0.5 * x^2
y <- y_true + rnorm(n, 0, 2)
data <- data.frame(x = x, y = y)

# Fit a simple model to all data (this will be our "Bayesian model")
full_model <- lm(y ~ x + I(x^2))
full_predictions <- predict(full_model)

# Generate posterior samples (simplified for illustration)
n_samples <- 1000
beta_samples <- mvrnorm(n_samples, 
                       coef(full_model), 
                       vcov(full_model))
sigma_samples <- rep(sigma(full_model), n_samples)

# Function to compute log-likelihood for each observation given parameters
log_lik <- function(beta, sigma, x, y) {
  mu <- beta[1] + beta[2] * x + beta[3] * x^2
  dnorm(y, mean = mu, sd = sigma, log = TRUE)
}

# Compute log-likelihood matrix (n_samples × n_observations)
log_lik_matrix <- matrix(NA, nrow = n_samples, ncol = n)
for (i in 1:n_samples) {
  for (j in 1:n) {
    log_lik_matrix[i, j] <- log_lik(beta_samples[i,], sigma_samples[i], x[j], y[j])
  }
}

# Select a point for demonstration
loo_idx <- 25  # This will be our "left out" point

# Calculate raw importance weights for the selected point
log_weights <- -log_lik_matrix[, loo_idx]  # Negative log-likelihood
weights <- exp(log_weights - max(log_weights))  # Stabilize by subtracting max

# Function to simulate Pareto smoothing (simplified)
pareto_smooth <- function(weights) {
  # Sort weights
  sorted_weights <- sort(weights, decreasing = TRUE)
  n_tail <- min(500, length(weights) / 5)  # Top 20% or 500, whichever is smaller
  
  # Fit generalized Pareto to the tail (simplified)
  # In practice, actual fitting would be used
  k <- 0.4  # Simulated Pareto shape parameter
  sigma <- mean(sorted_weights[1:n_tail]) * 0.8
  
  # Smooth the tail weights
  for (i in 1:n_tail) {
    q <- (i - 0.5) / n_tail
    sorted_weights[i] <- sigma/k * ((1 - q)^(-k) - 1)
  }
  
  # Rearrange to original order
  smoothed_weights <- weights
  smoothed_weights[order(weights, decreasing = TRUE)[1:n_tail]] <- sorted_weights[1:n_tail]
  
  return(smoothed_weights)
}

# Apply simulated Pareto smoothing
smoothed_weights <- pareto_smooth(weights)

# Normalize weights
raw_norm_weights <- weights / sum(weights)
smoothed_norm_weights <- smoothed_weights / sum(smoothed_weights)

# Calculate true LOO prediction for comparison
loo_model <- lm(y[-loo_idx] ~ x[-loo_idx] + I(x[-loo_idx]^2))
loo_prediction <- predict(loo_model, newdata = data.frame(x = x[loo_idx]))

# Calculate PSIS-LOO prediction for the point
psis_prediction <- 0
for (i in 1:n_samples) {
  beta <- beta_samples[i,]
  psis_prediction <- psis_prediction + smoothed_norm_weights[i] * 
    (beta[1] + beta[2] * x[loo_idx] + beta[3] * x[loo_idx]^2)
}

# Create visualization data
viz_data <- data.frame(
  x = x,
  y = y,
  full_fit = full_predictions,
  highlighted = ifelse(seq_along(x) == loo_idx, "Point Left Out", "Other Points")
)

# Sample subset for posterior draws
sample_indices <- sample(1:n_samples, 50)
posterior_lines <- data.frame(
  x = rep(x, length(sample_indices)),
  sample = rep(1:length(sample_indices), each = n),
  y_pred = NA
)

for (i in 1:length(sample_indices)) {
  s <- sample_indices[i]
  beta <- beta_samples[s,]
  posterior_lines$y_pred[posterior_lines$sample == i] <- 
    beta[1] + beta[2] * x + beta[3] * x^2
}

# Create data for weight visualization
weight_data <- data.frame(
  weight_idx = 1:100,  # Show only first 100 weights for clarity
  raw_weight = raw_norm_weights[1:100],
  smoothed_weight = smoothed_norm_weights[1:100]
)

# PLOT 1: Data and model fit
p1 <- ggplot(viz_data, aes(x = x, y = y)) +
  # Add posterior draws
  geom_line(data = posterior_lines, aes(y = y_pred, group = sample), 
            alpha = 0.1, color = "blue") +
  
  # Add full model fit
  geom_line(aes(y = full_fit), color = "blue", size = 1) +
  
  # Add data points
  geom_point(aes(color = highlighted, size = highlighted), alpha = 0.7) +
  
  # Add predictions for the left-out point
  geom_segment(
    aes(x = x[loo_idx], y = y[loo_idx], xend = x[loo_idx], yend = loo_prediction),
    arrow = arrow(length = unit(0.3, "cm")), color = "red", linetype = "dashed"
  ) +
  # Add the true LOO prediction point
  geom_point(
    aes(x = x[loo_idx], y = loo_prediction),
    color = "red", size = 4, shape = 18
  ) +
  geom_segment(
    aes(x = x[loo_idx] + 0.1, y = y[loo_idx], xend = x[loo_idx] + 0.1, yend = psis_prediction),
    arrow = arrow(length = unit(0.3, "cm")), color = "purple", linetype = "dashed"
  ) +
  # Add the PSIS-LOO prediction point
  geom_point(
    aes(x = x[loo_idx] + 0.1, y = psis_prediction),
    color = "purple", size = 4, shape = 18
  ) +
  
  # Add legend annotations
  annotate("text", x = x[loo_idx] - 0.3, y = (y[loo_idx] + loo_prediction)/2, 
           label = "True LOO", color = "red", hjust = 1) +
  annotate("text", x = x[loo_idx] + 0.4, y = (y[loo_idx] + psis_prediction)/2, 
           label = "PSIS-LOO", color = "purple", hjust = 0) +
  
  # Styling
  scale_color_manual(values = c("Other Points" = "black", "Point Left Out" = "red")) +
  scale_size_manual(values = c("Other Points" = 2, "Point Left Out" = 3)) +
  labs(
    title = "PSIS-LOO Approximation of Leave-One-Out Cross-Validation",
    subtitle = "Comparing true LOO (red) vs. PSIS-LOO approximation (purple)",
    x = "x",
    y = "y",
    color = NULL,
    size = NULL
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")

# PLOT 2: Importance weights before and after Pareto smoothing
p2 <- ggplot(weight_data, aes(x = weight_idx)) +
  # Raw weights
  geom_segment(aes(xend = weight_idx, y = 0, yend = raw_weight), 
               color = "gray50", alpha = 0.7) +
  geom_point(aes(y = raw_weight), color = "blue", size = 2, alpha = 0.7) +
  
  # Smoothed weights
  geom_segment(aes(xend = weight_idx, y = 0, yend = smoothed_weight), 
               color = "gray50", alpha = 0.7) +
  geom_point(aes(y = smoothed_weight), color = "purple", size = 2, alpha = 0.7) +
  
  # Connect raw and smoothed weights for visual clarity
  geom_segment(aes(xend = weight_idx, y = raw_weight, yend = smoothed_weight),
               arrow = arrow(length = unit(0.1, "cm")), 
               color = "purple", alpha = 0.3) +
  
  # Styling
  labs(
    title = "Importance Sampling Weights for PSIS-LOO",
    subtitle = "Pareto smoothing stabilizes extreme weights (arrows show the adjustment)",
    x = "Sample Index (ordered by weight)",
    y = "Normalized Weight"
  ) +
  scale_x_continuous(breaks = seq(0, 100, by = 20)) +
  theme_minimal() +
  annotate("text", x = 20, y = max(weight_data$raw_weight) * 0.9, 
           label = "Before smoothing", color = "blue", hjust = 0) +
  annotate("text", x = 60, y = max(weight_data$smoothed_weight) * 0.9, 
           label = "After Pareto smoothing", color = "purple", hjust = 0)

# PLOT 3: Conceptual diagram of PSIS-LOO process
p3 <- ggplot() +
  # Add process steps
  annotate("rect", xmin = 0.5, xmax = 3.5, ymin = 4, ymax = 5, 
           fill = "lightblue", alpha = 0.7, color = "black") +
  annotate("text", x = 2, y = 4.5, 
           label = "1. Fit Model to All Data\nObtain posterior samples p(θ|y₁,...,yₙ)") +
  
  annotate("rect", xmin = 0.5, xmax = 3.5, ymin = 2.5, ymax = 3.5, 
           fill = "lightgreen", alpha = 0.7, color = "black") +
  annotate("text", x = 2, y = 3, 
           label = "2. Calculate Log-Likelihood for Each Observation\nℓᵢ(θ) = log p(yᵢ|θ)") +
  
  annotate("rect", xmin = 0.5, xmax = 3.5, ymin = 1, ymax = 2, 
           fill = "lightyellow", alpha = 0.7, color = "black") +
  annotate("text", x = 2, y = 1.5, 
           label = "3. Compute Importance Weights\nwᵢ(θ) ∝ 1/p(yᵢ|θ)") +
  
  annotate("rect", xmin = 0.5, xmax = 3.5, ymin = -0.5, ymax = 0.5, 
           fill = "mistyrose", alpha = 0.7, color = "black") +
  annotate("text", x = 2, y = 0, 
           label = "4. Apply Pareto Smoothing\nStabilize extreme weights") +
  
  annotate("rect", xmin = 0.5, xmax = 3.5, ymin = -2, ymax = -1, 
           fill = "lavender", alpha = 0.7, color = "black") +
  annotate("text", x = 2, y = -1.5, 
           label = "5. Compute ELPD Using Smoothed Weights\nApproximate leave-one-out prediction") +
  
  # Add arrows
  annotate("segment", x = 2, y = 4, xend = 2, yend = 3.5, 
           arrow = arrow(length = unit(0.2, "cm"))) +
  annotate("segment", x = 2, y = 2.5, xend = 2, yend = 2, 
           arrow = arrow(length = unit(0.2, "cm"))) +
  annotate("segment", x = 2, y = 1, xend = 2, yend = 0.5, 
           arrow = arrow(length = unit(0.2, "cm"))) +
  annotate("segment", x = 2, y = -0.5, xend = 2, yend = -1, 
           arrow = arrow(length = unit(0.2, "cm"))) +
  
  # Explain the advantage
  annotate("text", x = 2, y = -2.7, 
           label = "PSIS-LOO provides accurate LOO approximation\nfrom a single model fit",
           fontface = "italic") +
  
  # Add Pareto k value diagnostic information
  annotate("rect", xmin = 3.7, xmax = 5.7, ymin = -0.2, ymax = 1.5, 
           fill = "white", alpha = 0.8, color = "black", linetype = "dashed") +
  annotate("text", x = 4.7, y = 1.2, label = "Pareto k Diagnostics", fontface = "bold") +
  annotate("text", x = 4.7, y = 0.8, label = "k < 0.5: Reliable", color = "darkgreen") +
  annotate("text", x = 4.7, y = 0.4, label = "0.5 < k < 0.7: Use caution", color = "darkorange") +
  annotate("text", x = 4.7, y = 0, label = "k > 0.7: Unreliable", color = "darkred") +
  
  # Styling
  theme_void() +
  labs(title = "PSIS-LOO Process Flow")

# Arrange the plots in a grid
p1 / p2 
p3
```

The red circle represents our "left-out" data point, while the blue line shows the model fit using all data points (including that red circle). The red diamond shows the prediction we get when we actually refit the model without the red circle.

When we fit a model (the blue line), each data point "pulls" the model fit toward itself to some degree. The red circle data point influenced the original model to bend slightly closer to it. This is why the red circle appears relatively close to the blue line—it helped shape that line!

When we perform true leave-one-out cross-validation, we remove that red circle point completely and refit the model using only the remaining data. Without the "pull" from that point, the model (which we don't directly show) follows a slightly different path determined solely by the other points. The prediction from this new model (the red diamond) naturally lands in a different position.

This difference between the original model prediction and the leave-one-out prediction is exactly what makes cross-validation valuable:

* It reveals how much individual data points influence your model

* It gives a more honest assessment of how your model will perform on truly unseen data

* Large differences can help identify influential or outlier points

The purple diamond (PSIS-LOO prediction) attempts to approximate where that red diamond would be without actually refitting the model, by mathematically down-weighting the influence of the left-out point—which is why it's positioned close to the red diamond if the approximation is working well.


## Simulation-Based Model Comparison

Now that we understand the principles, let's apply these techniques to compare cognitive models using a simulation-based approach. This approach has two key advantages:

1. We know the "ground truth" (which model generated each dataset)
2. We can systematically test if our comparison methods work

We'll simulate data from two different model types:

* Biased agents: Make choices based on a fixed bias parameter
* Memory agents: Make choices influenced by the history of their opponent's actions

By fitting both models to data generated from each type of agent, we can evaluate whether our model comparison techniques correctly identify the true generating model.

### Define Simulation Parameters

```{r}
# Shared parameters
agents <- 100  # Number of agents to simulate
trials <- 120  # Number of trials per agent
noise <- 0     # Base noise level (probability of random choice)

# Biased agents parameters (on log-odds scale)
rateM <- 1.386  # Population mean of bias (~0.8 in probability space)
rateSD <- 0.65  # Population SD of bias (~0.1 in probability space)

# Memory agents parameters
biasM <- 0      # Population mean of baseline bias
biasSD <- 0.1   # Population SD of baseline bias
betaM <- 1.5    # Population mean of memory sensitivity
betaSD <- 0.3   # Population SD of memory sensitivity

# Print the parameters in probability space for easier interpretation
cat("Biased agents have mean choice probability of:", round(plogis(rateM), 2), "\n")
cat("Memory agents have mean baseline bias of:", round(plogis(biasM), 2), "\n")
cat("Memory agents have mean sensitivity to opponent's history of:", betaM, "\n")
```

### Define Agent Functions

```{r}
# Random agent function: makes choices based on bias parameter
RandomAgentNoise_f <- function(rate, noise) {
  # Generate choice based on agent's bias parameter (on log-odds scale)
  choice <- rbinom(1, 1, plogis(rate))
  
  # With probability 'noise', override choice with random 50/50 selection
  if (rbinom(1, 1, noise) == 1) {
    choice = rbinom(1, 1, 0.5)
  }
  
  return(choice)
}

# Memory agent function: makes choices based on opponent's historical choices
MemoryAgentNoise_f <- function(bias, beta, otherRate, noise) {
  # Calculate log-odds of choosing option 1, influenced by opponent's historical choice rate
  log_odds <- bias + beta * qlogis(otherRate)
  
  # Convert to probability and generate choice
  choice <- rbinom(1, 1, plogis(log_odds))
  
  # With probability 'noise', override choice with random 50/50 selection
  if (rbinom(1, 1, noise) == 1) {
    choice = rbinom(1, 1, 0.5)
  }
  
  return(choice)
}
```

### Generate Simulation Data

```{r}
# Function to simulate one run of agents playing against each other
simulate_agents <- function() {
  # Create empty dataframe to store results
  d <- NULL
  
  # Loop through all agents
  for (agent in 1:agents) {
    # Sample individual agent parameters from population distributions
    rate <- rnorm(1, rateM, rateSD)         # Individual bias for random agent
    bias <- rnorm(1, biasM, biasSD)         # Individual baseline bias for memory agent
    beta <- rnorm(1, betaM, betaSD)         # Individual memory sensitivity
    
    # Initialize choice vectors
    randomChoice <- rep(NA, trials)
    memoryChoice <- rep(NA, trials)
    memoryRate <- rep(NA, trials)
    
    # Generate choices for each trial
    for (trial in 1:trials) {
      # Random (biased) agent makes choice
      randomChoice[trial] <- RandomAgentNoise_f(rate, noise)
      
      # Memory agent responds (with no history for first trial)
      if (trial == 1) {
        memoryChoice[trial] <- rbinom(1, 1, 0.5)  # First choice is random
      } else {
        # Use mean of random agent's previous choices as "memory"
        memoryChoice[trial] <- MemoryAgentNoise_f(
          bias, beta, mean(randomChoice[1:(trial - 1)], na.rm = TRUE), noise
        )
      }
    }
    
    # Create data frame for this agent
    temp <- tibble(
      agent = agent,
      trial = seq(trials),
      randomChoice = randomChoice,
      randomRate = rate,
      memoryChoice = memoryChoice,
      noise = noise,
      rateM = rateM,
      rateSD = rateSD,
      bias = bias,
      beta = beta,
      biasM = biasM,
      biasSD = biasSD,
      betaM = betaM,
      betaSD = betaSD
    )
    
    # Append to main dataframe
    if (agent > 1) {
      d <- rbind(d, temp)
    } else {
      d <- temp
    }
  }
  
  # Calculate cumulative choice rates
  d <- d %>% group_by(agent) %>% mutate(
    randomRate_cumulative = cumsum(randomChoice) / seq_along(randomChoice),
    memoryRate_cumulative = cumsum(memoryChoice) / seq_along(memoryChoice)
  )
  
  return(d)
}

# Generate the data
d <- simulate_agents()

# Show a quick summary of the generated data
summary_stats <- d %>%
  filter(trial == trials) %>%  # Get final trial stats
  summarize(
    mean_random_rate = mean(randomRate_cumulative),
    mean_memory_rate = mean(memoryRate_cumulative)
  )

print(summary_stats)
```

### Visualize the Simulation Data

```{r}
# Select a random sample of agents to visualize
sample_agents <- sample(unique(d$agent), 6)

# Filter data for these agents
sample_data <- d %>% filter(agent %in% sample_agents)

# Create plot showing choice patterns
p1 <- ggplot(sample_data, aes(x = trial)) +
  geom_line(aes(y = randomRate_cumulative, color = "Biased Agent"), size = 1) +
  geom_line(aes(y = memoryRate_cumulative, color = "Memory Agent"), size = 1) +
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "gray50") +
  facet_wrap(~agent, ncol = 3) +
  scale_color_brewer(palette = "Set1") +
  labs(
    title = "Choice Patterns of Biased vs. Memory Agents",
    subtitle = "Lines show cumulative proportion of 'right' choices",
    x = "Trial",
    y = "Cumulative Proportion Right Choices",
    color = "Agent Type"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")

# Create density plot of final choice rates
p2 <- d %>%
  filter(trial == trials) %>%
  dplyr::select(agent, randomRate_cumulative, memoryRate_cumulative) %>%
  pivot_longer(cols = c(randomRate_cumulative, memoryRate_cumulative),
               names_to = "agent_type", values_to = "rate") %>%
  mutate(agent_type = ifelse(agent_type == "randomRate_cumulative", 
                            "Biased Agent", "Memory Agent")) %>%
  ggplot(aes(x = rate, fill = agent_type)) +
  geom_density(alpha = 0.5) +
  scale_fill_brewer(palette = "Set1") +
  labs(
    title = "Distribution of Final Choice Rates",
    subtitle = "After 120 trials",
    x = "Proportion of Right Choices",
    y = "Density",
    fill = "Agent Type"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")

# Combine the plots
p1 / p2 + plot_layout(heights = c(2, 1))
```

### Prepare Data for Stan Models

```{r}
# Prepare the data for the biased agent model
d_random <- d %>% 
  dplyr::select(agent, randomChoice) %>% 
  mutate(row = row_number()) %>% 
  pivot_wider(names_from = agent, values_from = randomChoice)

d_memory <- d %>% 
  dplyr::select(agent, memoryChoice) %>% 
  mutate(row = row_number()) %>% 
  pivot_wider(names_from = agent, values_from = memoryChoice)

# Create data lists for Stan
data_biased <- list(
  trials = trials,
  agents = agents,
  h = as.matrix(d_random[, 2:(agents + 1)]),  # Matrix where columns are agents, rows are trials
  other = as.matrix(d_memory[, 2:(agents + 1)])  # Other agent's choices (for memory model)
)

data_memory <- list(
  trials = trials,
  agents = agents,
  h = as.matrix(d_memory[, 2:(agents + 1)]),  # For memory agent model, we'll predict memory choices
  other = as.matrix(d_random[, 2:(agents + 1)])  # And use random agent choices as input
)

# Show dimensions of the data matrices
cat("Dimensions of data matrices for biased agent model:\n")
cat("h matrix:", dim(data_biased$h), "\n")
cat("other matrix:", dim(data_biased$other), "\n\n")

cat("Dimensions of data matrices for memory agent model:\n")
cat("h matrix:", dim(data_memory$h), "\n")
cat("other matrix:", dim(data_memory$other), "\n")
```

## Implementing Models for Comparison

Now we need to implement our two competing models in Stan. Both will be multilevel (hierarchical) to account for individual differences among agents. The key feature for model comparison is that we'll include a `log_lik` calculation in the `generated quantities` block of each model.

### Understanding Log-Likelihood in Model Comparison

When comparing models, we need a way to quantify how well each model explains the observed data. The log-likelihood represents the logarithm of the probability that a model would generate the observed data given specific parameter values.

Given certain values for our parameters (let's say a bias of 0 and beta for memory of 1) and for our variables (let's say the vector of memory values estimated by the agent on a trial by trial basis), the model will predict a certain distribution of outcomes, that is, a certain distribution of choices (n times right, m times left hand). Comparing this to the actual data, we can identify how likely the model is to produce it. In other words, the probability that the model will actually generate the data we observed out of all its possible outcomes. Remember that we are doing Bayesian statistics, so this probability needs to be combined with the probability of the parameter values given the priors on those parameters. This would give us a *posterior likelihood* of the model's parameter values given the data. The last step is that we need to work on a log scale. Working on a log scale is very useful because it avoids low probabilities (close to 0) being rounded down to exactly 0. By log-transforming the posterior likelihood, we now have the *log-posterior likelihood*.

Now, remember that our agent's memory varies on a trial by trial level. In other words, for each data point, for each agent we can calculate separate values of log-posterior likelihood for each of the possible values of the parameters. That is, we can have a distribution of log-posterior likelihood for each data point.

Telling Stan to calculate these distributions is straightforward: we add to the `generated quantities` block the same log probability statements used in the model block, but save them to variables instead of adding them to the target.

N.B. Some of you might be wandering: if Stan is already using the log-posterior probability in the sampling process, why do we need to tell it to calculate and save it? Fair enough point. But Stan does not save by default (to avoid clogging your computer with endless data) and we need the log posterior likelihood saved as "log_lik" in order to be able to use more automated functions later on.

### Multilevel Biased Agent Model

Here's the Stan model for the biased agent (remember that we will add the log_lik part in the generated quantities block!).


```{r 06 Multilevel baised agents}

# Stan model for multilevel biased agent
stan_biased_model <- "
// Multilevel Biased Agent Model
//
// This model assumes each agent has a fixed bias (theta) that determines
// their probability of choosing option 1 ('right')

functions{
  // Helper function for generating truncated normal random numbers
  real normal_lb_rng(real mu, real sigma, real lb) { 
    real p = normal_cdf(lb | mu, sigma);  // CDF for bounds
    real u = uniform_rng(p, 1);
    return (sigma * inv_Phi(u)) + mu;  // Inverse CDF for value
  }
}

// Input data
data {
 int<lower = 1> trials;         // Number of trials per agent
 int<lower = 1> agents;         // Number of agents
 array[trials, agents] int h;   // Choice data (0/1 for each trial and agent)
}

// Parameters to be estimated
parameters {
  real thetaM;                  // Population mean of bias (log-odds scale)
  real<lower = 0> thetaSD;      // Population SD of bias
  array[agents] real theta;     // Individual agent biases (log-odds scale)
}

// Model definition
model {
  // Population-level priors
  target += normal_lpdf(thetaM | 0, 1);
  
  // Prior for SD with lower bound at zero (half-normal)
  target += normal_lpdf(thetaSD | 0, .3) - normal_lccdf(0 | 0, .3);

  // Individual-level parameters drawn from population distribution
  target += normal_lpdf(theta | thetaM, thetaSD); 
 
  // Likelihood: predict each agent's choices
  for (i in 1:agents)
    target += bernoulli_logit_lpmf(h[,i] | theta[i]);
}

// Additional quantities to calculate
generated quantities{
   // Prior predictive samples
   real thetaM_prior;
   real<lower=0> thetaSD_prior;
   real<lower=0, upper=1> theta_prior;
   real<lower=0, upper=1> theta_posterior;
   
   // Posterior predictive samples
   int<lower=0, upper = trials> prior_preds;
   int<lower=0, upper = trials> posterior_preds;
   
   // Log-likelihood for each observation (crucial for model comparison)
   array[trials, agents] real log_lik;
   
   // Generate prior samples
   thetaM_prior = normal_rng(0, 1);
   thetaSD_prior = normal_lb_rng(0, 0.3, 0);
   theta_prior = inv_logit(normal_rng(thetaM_prior, thetaSD_prior));
   theta_posterior = inv_logit(normal_rng(thetaM, thetaSD));
   
   // Generate predictions from prior and posterior
   prior_preds = binomial_rng(trials, inv_logit(thetaM_prior));
   posterior_preds = binomial_rng(trials, inv_logit(thetaM));
   
   // Calculate log-likelihood for each observation
   for (i in 1:agents){
    for (t in 1:trials){
      log_lik[t, i] = bernoulli_logit_lpmf(h[t, i] | theta[i]);
    }
   }
}
"

# Write the Stan model to a file
write_stan_file(
  stan_biased_model,
  dir = "stan/",
  basename = "W7_MultilevelBias.stan"
)

# Compile the model
file <- file.path("stan/W7_MultilevelBias.stan")
mod_biased <- cmdstan_model(
  file, 
  cpp_options = list(stan_threads = TRUE),
  stanc_options = list("O1")
)

```

Let's break down this Stan model:

Data Block: Defines the input data - number of trials, number of agents, and the choice data matrix.
Parameters Block: Specifies the parameters we want to estimate:

* thetaM: The population mean bias

* thetaSD: The population standard deviation of bias

* theta: Individual bias parameters for each agent


Model Block: Defines the prior distributions and likelihood function:

* Priors for population parameters

* Individual parameters drawn from the population distribution

* Likelihood of observing the choice data given the parameters


Generated Quantities Block: Calculates additional quantities of interest:

* Prior and posterior predictive samples

* Log-likelihood for each observation - this is crucial for model comparison

The most important part for model comparison is the log_lik calculation in the generated quantities block. This computes the log probability of each observation given the model and its parameters, which we'll use for comparing models.

### Multilevel Memory Agent Model ###

Now let's implement our second model - the memory agent model:


```{r 06 Multilevel memory agents}

# Stan model for multilevel memory agent
stan_memory_model <- "
// Multilevel Memory Agent Model
//
// This model assumes agents make choices based on their memory of
// the opponent's previous choices.

functions{
  // Helper function for generating truncated normal random numbers
  real normal_lb_rng(real mu, real sigma, real lb) {
    real p = normal_cdf(lb | mu, sigma);  // CDF for bounds
    real u = uniform_rng(p, 1);
    return (sigma * inv_Phi(u)) + mu;  // Inverse CDF for value
  }
}

// Input data
data {
 int<lower = 1> trials;         // Number of trials per agent
 int<lower = 1> agents;         // Number of agents
 array[trials, agents] int h;   // Choice data (0/1 for each trial and agent)
 array[trials, agents] int other;// Opponent's choices (input to memory)
}

// Parameters to be estimated
parameters {
  real biasM;                   // Population mean baseline bias
  real betaM;                   // Population mean memory sensitivity
  vector<lower = 0>[2] tau;     // Population SDs for bias and beta
  matrix[2, agents] z_IDs;      // Standardized individual parameters (non-centered)
  cholesky_factor_corr[2] L_u;  // Cholesky factor of correlation matrix
}

// Transformed parameters (derived quantities)
transformed parameters {
  // Memory state for each agent and trial
  array[trials, agents] real memory;
  
  // Individual parameters (bias and beta for each agent)
  matrix[agents, 2] IDs;
  
  // Transform standardized parameters to actual parameters (non-centered parameterization)
  IDs = (diag_pre_multiply(tau, L_u) * z_IDs)';
  
  // Calculate memory states based on opponent's choices
  for (agent in 1:agents){
    for (trial in 1:trials){
      // Initialize first trial with neutral memory
      if (trial == 1) {
        memory[trial, agent] = 0.5;
      } 
      // Update memory based on opponent's choices
      if (trial < trials){
        // Simple averaging memory update
        memory[trial + 1, agent] = memory[trial, agent] + 
                                ((other[trial, agent] - memory[trial, agent]) / trial);
        
        // Handle edge cases to avoid numerical issues
        if (memory[trial + 1, agent] == 0){memory[trial + 1, agent] = 0.01;}
        if (memory[trial + 1, agent] == 1){memory[trial + 1, agent] = 0.99;}
      }
    }
  }
}

// Model definition
model {
  // Population-level priors
  target += normal_lpdf(biasM | 0, 1);
  target += normal_lpdf(tau[1] | 0, .3) - normal_lccdf(0 | 0, .3);  // Half-normal for SD
  target += normal_lpdf(betaM | 0, .3);
  target += normal_lpdf(tau[2] | 0, .3) - normal_lccdf(0 | 0, .3);  // Half-normal for SD
  
  // Prior for correlation matrix
  target += lkj_corr_cholesky_lpdf(L_u | 2);

  // Standardized individual parameters have standard normal prior
  target += std_normal_lpdf(to_vector(z_IDs));
  
  // Likelihood: predict each agent's choices
  for (agent in 1:agents){
    for (trial in 1:trials){
      // choice ~ bias + memory_effect*beta
      target += bernoulli_logit_lpmf(h[trial, agent] | 
                biasM + IDs[agent, 1] + memory[trial, agent] * (betaM + IDs[agent, 2]));
    }
  }
}

// Additional quantities to calculate
generated quantities{
   // Prior predictive samples
   real biasM_prior;
   real<lower=0> biasSD_prior;
   real betaM_prior;
   real<lower=0> betaSD_prior;
   
   real bias_prior;
   real beta_prior;
   
   // Posterior predictive samples for different memory conditions
   array[agents] int<lower=0, upper = trials> prior_preds0;
   array[agents] int<lower=0, upper = trials> prior_preds1;
   array[agents] int<lower=0, upper = trials> prior_preds2;
   array[agents] int<lower=0, upper = trials> posterior_preds0;
   array[agents] int<lower=0, upper = trials> posterior_preds1;
   array[agents] int<lower=0, upper = trials> posterior_preds2;
   
   // Log-likelihood for each observation (crucial for model comparison)
   array[trials, agents] real log_lik;
   
   // Generate prior samples
   biasM_prior = normal_rng(0,1);
   biasSD_prior = normal_lb_rng(0,0.3,0);
   betaM_prior = normal_rng(0,1);
   betaSD_prior = normal_lb_rng(0,0.3,0);
   
   bias_prior = normal_rng(biasM_prior, biasSD_prior);
   beta_prior = normal_rng(betaM_prior, betaSD_prior);
   
   // Generate predictions for different memory conditions
   for (i in 1:agents){
      // Prior predictions for low, medium, high memory
      prior_preds0[i] = binomial_rng(trials, inv_logit(bias_prior + 0 * beta_prior));
      prior_preds1[i] = binomial_rng(trials, inv_logit(bias_prior + 1 * beta_prior));
      prior_preds2[i] = binomial_rng(trials, inv_logit(bias_prior + 2 * beta_prior));
      
      // Posterior predictions for low, medium, high memory
      posterior_preds0[i] = binomial_rng(trials, 
                           inv_logit(biasM + IDs[i,1] + 0 * (betaM + IDs[i,2])));
      posterior_preds1[i] = binomial_rng(trials, 
                           inv_logit(biasM + IDs[i,1] + 1 * (betaM + IDs[i,2])));
      posterior_preds2[i] = binomial_rng(trials, 
                           inv_logit(biasM + IDs[i,1] + 2 * (betaM + IDs[i,2])));
      
      // Calculate log-likelihood for each observation
      for (t in 1:trials){
        log_lik[t,i] = bernoulli_logit_lpmf(h[t, i] | 
                      biasM + IDs[i, 1] + memory[t, i] * (betaM + IDs[i, 2]));
      }
   }
}
"

# Write the Stan model to a file
write_stan_file(
  stan_memory_model,
  dir = "stan/",
  basename = "W7_MultilevelMemory.stan"
)

# Compile the model
file <- file.path("stan/W7_MultilevelMemory.stan")
mod_memory <- cmdstan_model(
  file, 
  cpp_options = list(stan_threads = TRUE),
  stanc_options = list("O1")
)
```


The memory agent model is more complex, but follows a similar structure:

Data Block: Includes the same data as the biased model, plus the opponent's choices.

Parameters Block: Includes parameters for the memory model:

* biasM: Population mean baseline bias

* betaM: Population mean memory sensitivity

* tau: Population standard deviations

* z_IDs: Standardized individual parameters

* L_u: Cholesky factor of correlation matrix

Transformed Parameters Block: Calculates derived quantities:

* memory: The memory state for each agent and trial

* IDs: Individual parameters for each agent

Model Block: Defines priors and likelihood:

* Priors for population parameters

* Memory-based choice likelihood

Generated Quantities Block: Calculates additional quantities:

* Prior and posterior predictive samples

* Log-likelihood for each observation

The use of a non-centered parameterization for the individual parameters (through z_IDs and transformation) is a technique to improve sampling efficiency in hierarchical models. This is important when estimating multilevel models with potentially correlated parameters.

## Fitting Models and Calculating Expected Log Predictive Density ##

Now that we've defined our models, we'll fit them to our simulated data and perform model comparison. We'll fit both models to both types of data:

* Biased agent model fitted to biased agent data

* Biased agent model fitted to memory agent data

* Memory agent model fitted to biased agent data

* Memory agent model fitted to memory agent data


## Fitting the models to the data

```{r, 07 fitting models}

# File path for saved model
model_file <- "simmodels/W7_fit_biased2biased.RDS"

# Check if we need to rerun the simulation or we can load a pre-run one (for computational efficiency)
if (regenerate_simulations || !file.exists(model_file)) {
  
  # Fitting biased agent model to biased agent data
  fit_biased2biased <- mod_biased$sample(
    data = data_biased,
    seed = 123,
    chains = 1,
    parallel_chains = 1,
    threads_per_chain = 1,
    iter_warmup = 2000,
    iter_sampling = 2000,
    refresh = 0,
    max_treedepth = 20,
    adapt_delta = 0.99
  )
  
  fit_biased2biased$save_object(file = model_file)
  cat("Generated new model fit and saved to", model_file, "\n")
} else {
  # Load existing results
  fit_biased2biased <- readRDS(model_file)
  cat("Loaded existing model fit from", model_file, "\n")
}

# File path for saved model
model_file <- "simmodels/W7_fit_biased2memory.RDS"

# Check if we need to rerun the simulation
if (regenerate_simulations || !file.exists(model_file)) {
  # Fitting biased agent model to memory agent data
  fit_biased2memory <- mod_biased$sample(
    data = data_memory,
    seed = 123,
    chains = 1,
    parallel_chains = 1,
    threads_per_chain = 1,
    iter_warmup = 2000,
    iter_sampling = 2000,
    refresh = 0,
    max_treedepth = 20,
    adapt_delta = 0.99
  )
  
  
  fit_biased2memory$save_object(file = model_file)
  cat("Generated new model fit and saved to", model_file, "\n")
} else {
  # Load existing results
  fit_biased2memory <- readRDS(model_file)
  cat("Loaded existing model fit from", model_file, "\n")
}

# File path for saved model
model_file <- "simmodels/W7_fit_memory2biased.RDS"

# Check if we need to rerun the simulation
if (regenerate_simulations || !file.exists(model_file)) {
  # Fitting memory agent model to biased agent data
  fit_memory2biased <- mod_memory$sample(
    data = data_biased,
    seed = 123,
    chains = 1,
    parallel_chains = 1,
    threads_per_chain = 1,
    iter_warmup = 2000,
    iter_sampling = 2000,
    refresh = 0,
    max_treedepth = 20,
    adapt_delta = 0.99
  )
  
  fit_memory2biased$save_object(file = model_file)
  cat("Generated new model fit and saved to", model_file, "\n")
} else {
  # Load existing results
  fit_memory2biased <- readRDS(model_file)
  cat("Loaded existing model fit from", model_file, "\n")
}

# File path for saved model
model_file <- "simmodels/W7_fit_memory2memory.RDS"

# Check if we need to rerun the simulation
if (regenerate_simulations || !file.exists(model_file)) {# Fitting memory agent model to memory agent data
  fit_memory2memory <- mod_memory$sample(
    data = data_memory,
    seed = 123,
    chains = 1,
    parallel_chains = 1,
    threads_per_chain = 1,
    iter_warmup = 2000,
    iter_sampling = 2000,
    refresh = 0,
    max_treedepth = 20,
    adapt_delta = 0.99
  )
  
  fit_memory2memory$save_object(file = model_file)
  cat("Generated new model fit and saved to", model_file, "\n")
} else {
  # Load existing results
  fit_memory2memory <- readRDS(model_file)
  cat("Loaded existing model fit from", model_file, "\n")
}

# Display basic summary of the models
cat("Biased model fitted to biased data:\n")
print(fit_biased2biased$summary(c("thetaM", "thetaSD")))

cat("\nBiased model fitted to memory data:\n")
print(fit_biased2memory$summary(c("thetaM", "thetaSD")))

cat("\nMemory model fitted to biased data:\n")
print(fit_memory2biased$summary(c("biasM", "betaM", "tau[1]", "tau[2]")))

cat("\nMemory model fitted to memory data:\n")
print(fit_memory2memory$summary(c("biasM", "betaM", "tau[1]", "tau[2]")))

print("Let's visualize some key parameters to better understand what the models have learned:")

# Extract posterior samples for key parameters
draws_biased2biased <- as_draws_df(fit_biased2biased$draws())
draws_biased2memory <- as_draws_df(fit_biased2memory$draws())
draws_memory2biased <- as_draws_df(fit_memory2biased$draws())
draws_memory2memory <- as_draws_df(fit_memory2memory$draws())

# Prepare data for plotting
param_data <- bind_rows(
  # Biased model parameters
  tibble(
    parameter = "Bias (θ)",
    value = draws_biased2biased$thetaM,
    model = "Biased Model",
    data = "Biased Data"
  ),
  tibble(
    parameter = "Bias (θ)",
    value = draws_biased2memory$thetaM,
    model = "Biased Model",
    data = "Memory Data"
  ),
  # Memory model parameters - converting to probability scale for bias
  tibble(
    parameter = "Bias (θ)",
    value = draws_memory2biased$biasM,
    model = "Memory Model",
    data = "Biased Data"
  ),
  tibble(
    parameter = "Bias (θ)",
    value = draws_memory2memory$biasM,
    model = "Memory Model",
    data = "Memory Data"
  ),
  # Memory sensitivity parameter
  tibble(
    parameter = "Memory Sensitivity (β)",
    value = draws_memory2biased$betaM,
    model = "Memory Model",
    data = "Biased Data"
  ),
  tibble(
    parameter = "Memory Sensitivity (β)",
    value = draws_memory2memory$betaM,
    model = "Memory Model",
    data = "Memory Data"
  )
)

# Create reference lines data frame for the true parameter values
true_param_lines <- bind_rows(
  tibble(parameter = "Bias (θ)", model = "Biased Model", true_value = rateM),
  tibble(parameter = "Bias (θ)", model = "Memory Model", true_value = biasM),
  tibble(parameter = "Memory Sensitivity (β)", model = "Memory Model", true_value = betaM)
)

# Create visualization of posterior distributions
ggplot(param_data, aes(x = value, fill = data)) +
  geom_density(alpha = 0.6) +
  # Add reference lines only to relevant panels
  geom_vline(data = true_param_lines, 
             aes(xintercept = true_value), 
             linetype = "dashed", 
             color = "black") +
  facet_grid(model ~ parameter, scales = "free") +
  scale_fill_brewer(palette = "Set1") +
  labs(
    title = "Posterior Distributions of Key Parameters",
    subtitle = "Comparing model fits to different data types",
    x = "Parameter Value",
    y = "Density",
    fill = "Data Type"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
```


## Cross-Validation for Model Comparison

Now that we've fit our models, we can use cross-validation techniques to compare them. We'll start with PSIS-LOO since it's computationally efficient, and then validate with true k-fold cross-validation.

### PSIS-LOO Comparison


```{r, 06 assess predictive performance}

# Calculate ELPD using loo
Loo_biased2biased <- fit_biased2biased$loo(save_psis = TRUE, cores = 4)
Loo_biased2memory <- fit_biased2memory$loo(save_psis = TRUE, cores = 4)
Loo_memory2biased <- fit_memory2biased$loo(save_psis = TRUE, cores = 4)
Loo_memory2memory <- fit_memory2memory$loo(save_psis = TRUE, cores = 4)

# Diagnostic check for LOO reliability
cat("Diagnostics for biased model on biased data:\n")
print(sum(Loo_biased2biased$diagnostics$pareto_k > 0.7))

cat("\nDiagnostics for biased model on memory data:\n")
print(sum(Loo_biased2memory$diagnostics$pareto_k > 0.7))

cat("\nDiagnostics for memory model on biased data:\n")
print(sum(Loo_memory2biased$diagnostics$pareto_k > 0.7))

cat("\nDiagnostics for memory model on memory data:\n")
print(sum(Loo_memory2memory$diagnostics$pareto_k > 0.7))

# Extract ELPD values
cat("\nELPD for each model-data combination:\n")
cat("Biased model on biased data:", round(Loo_biased2biased$estimates["elpd_loo", "Estimate"], 2), "\n")
cat("Biased model on memory data:", round(Loo_biased2memory$estimates["elpd_loo", "Estimate"], 2), "\n")
cat("Memory model on biased data:", round(Loo_memory2biased$estimates["elpd_loo", "Estimate"], 2), "\n")
cat("Memory model on memory data:", round(Loo_memory2memory$estimates["elpd_loo", "Estimate"], 2), "\n")

```

### Visualizing LOO-CV Results ###

To better understand how our models perform, let's visualize the pointwise differences in ELPD:

```{r}
# Create dataframe of pointwise ELPD differences
elpd_diff <- tibble(
  observation = seq(length(Loo_biased2biased$pointwise[, "elpd_loo"])),
  biased_data_diff = Loo_biased2biased$pointwise[, "elpd_loo"] - 
                     Loo_memory2biased$pointwise[, "elpd_loo"],
  memory_data_diff = Loo_memory2memory$pointwise[, "elpd_loo"] - 
                     Loo_biased2memory$pointwise[, "elpd_loo"]
)

# Create long format for easier plotting
elpd_long <- elpd_diff %>%
  pivot_longer(
    cols = c(biased_data_diff, memory_data_diff),
    names_to = "comparison",
    values_to = "elpd_diff"
  ) %>%
  mutate(
    comparison = case_when(
      comparison == "biased_data_diff" ~ "Biased Model vs Memory Model\non Biased Data",
      comparison == "memory_data_diff" ~ "Memory Model vs Biased Model\non Memory Data"
    ),
    favors_true_model = case_when(
      comparison == "Biased Model vs Memory Model\non Biased Data" & elpd_diff > 0 ~ TRUE,
      comparison == "Memory Model vs Biased Model\non Memory Data" & elpd_diff > 0 ~ TRUE,
      TRUE ~ FALSE
    )
  )

# Create visualization of ELPD differences
p1 <- ggplot(elpd_long, aes(x = observation, y = elpd_diff, color = favors_true_model)) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_point(alpha = 0.3, size = 1) +
  facet_wrap(~comparison) +
  scale_color_manual(values = c("TRUE" = "green4", "FALSE" = "red")) +
  labs(
    title = "Pointwise ELPD Differences Between Models",
    subtitle = "Positive values (represented in green) favor the true model (first model in comparison)",
    x = "Observation",
    y = "ELPD Difference"
  ) +
  theme_minimal() +
  theme(legend.position = "none")

# Create density plots of ELPD differences
p2 <- ggplot(elpd_long, aes(x = elpd_diff, fill = comparison)) +
  geom_density(alpha = 0.5) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  scale_fill_brewer(palette = "Set1") +
  labs(
    title = "Distribution of ELPD Differences",
    subtitle = "How consistently does the true model outperform the alternative?",
    x = "ELPD Difference",
    y = "Density",
    fill = "Comparison"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")

# Combine plots
p1 / p2 + plot_layout(heights = c(3, 2))

```

It's quite clear that the bulk of the data a equally well explained by the models. And the differences for the biased data are tricky to see in the plot. Yet, we can see that in memory data there are a non trivial amount of data points better explained by the memory model (the true underlying data-generating mechanism).

## Formal Model Comparison ## 

Now let's perform formal model comparison using the loo_compare function, which computes the difference in ELPD between models and the standard error of this difference:

```{r}
# Compare models fitted to biased data
comparison_biased <- loo_compare(Loo_biased2biased, Loo_memory2biased)
print("Model comparison for biased data:")
print(comparison_biased)

# Compare models fitted to memory data
comparison_memory <- loo_compare(Loo_memory2memory, Loo_biased2memory)
print("\nModel comparison for memory data:")
print(comparison_memory)

# Calculate model weights
weights_biased <- loo_model_weights(list(
  "Biased Model" = Loo_biased2biased, 
  "Memory Model" = Loo_memory2biased
))

weights_memory <- loo_model_weights(list(
  "Memory Model" = Loo_memory2memory, 
  "Biased Model" = Loo_biased2memory
))

# Create data for model weights visualization
weights_data <- tibble(
  model = c("Biased Model", "Memory Model", "Memory Model", "Biased Model"),
  data_type = c("Biased Data", "Biased Data", "Memory Data", "Memory Data"),
  weight = c(weights_biased[1], weights_biased[2], weights_memory[1], weights_memory[2])
)

# Visualize model weights
ggplot(weights_data, aes(x = model, y = weight, fill = model)) +
  geom_col() +
  facet_wrap(~ data_type) +
  scale_fill_brewer(palette = "Set1") +
  labs(
    title = "Model Comparison via Stacking Weights",
    subtitle = "Higher weights indicate better predictive performance",
    x = NULL,
    y = "Model Weight",
    fill = "Model Type"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    axis.text.x = element_text(angle = 45, hjust = 1)
  ) +
  geom_text(aes(label = scales::percent(weight, accuracy = 0.1)), vjust = -0.5)
```

Here it is clear that formal model comparison can clearly pick the right model. Hurrah!

### K-Fold Cross-Validation for Validation

While PSIS-LOO is efficient, we need to check how it relates with true cross-validation. 

First we create new stan models, which separates data into training and test data and include the ability to calculate log-likelihood for test data. This is crucial for cross-validation, as we need to evaluate the model's performance on unseen data.

```{r}
# Stan model for biased agent with cross-validation capabilities
stan_biased_cv_model <- "
// Multilevel Biased Agent Model with CV support
//
// This model has additional structures to handle test data for cross-validation

functions{
  real normal_lb_rng(real mu, real sigma, real lb) { 
    real p = normal_cdf(lb | mu, sigma);  // CDF for bounds
    real u = uniform_rng(p, 1);
    return (sigma * inv_Phi(u)) + mu;  // Inverse CDF for value
  }
}

// Input data - now includes both training and test data
data {
 int<lower = 1> trials;              // Number of trials per agent
 int<lower = 1> agents;              // Number of training agents
 array[trials, agents] int h;        // Training choice data
 
 int<lower = 1> agents_test;         // Number of test agents
 array[trials, agents_test] int h_test;  // Test choice data
}

// Parameters to be estimated
parameters {
  real thetaM;                       // Population mean of bias
  real<lower = 0> thetaSD;           // Population SD of bias
  array[agents] real theta;          // Individual agent biases
}

// Model definition - trained only on training data
model {
  // Population-level priors
  target += normal_lpdf(thetaM | 0, 1);
  target += normal_lpdf(thetaSD | 0, .3) - normal_lccdf(0 | 0, .3);

  // Individual-level parameters
  target += normal_lpdf(theta | thetaM, thetaSD); 
 
  // Likelihood for training data only
  for (i in 1:agents)
    target += bernoulli_logit_lpmf(h[,i] | theta[i]);
}

// Calculate log-likelihood for both training and test data
generated quantities{
   real thetaM_prior;
   real<lower=0> thetaSD_prior;
   real<lower=0, upper=1> theta_prior;
   real<lower=0, upper=1> theta_posterior;
   
   int<lower=0, upper = trials> prior_preds;
   int<lower=0, upper = trials> posterior_preds;
   
   // Log-likelihood for training data
   array[trials, agents] real log_lik;
   
   // Log-likelihood for test data - crucial for cross-validation
   array[trials, agents_test] real log_lik_test;
   
   // Generate prior and posterior samples
   thetaM_prior = normal_rng(0,1);
   thetaSD_prior = normal_lb_rng(0,0.3,0);
   theta_prior = inv_logit(normal_rng(thetaM_prior, thetaSD_prior));
   theta_posterior = inv_logit(normal_rng(thetaM, thetaSD));
   
   prior_preds = binomial_rng(trials, inv_logit(thetaM_prior));
   posterior_preds = binomial_rng(trials, inv_logit(thetaM));
   
   // Calculate log-likelihood for training data
   for (i in 1:agents){
    for (t in 1:trials){
      log_lik[t,i] = bernoulli_logit_lpmf(h[t,i] | theta[i]);
    }
   }
   
   // Calculate log-likelihood for test data
   // Note: We use population-level estimates for prediction
   for (i in 1:agents_test){
    for (t in 1:trials){
      log_lik_test[t,i] = bernoulli_logit_lpmf(h_test[t,i] | thetaM);
    }
  }
}
"

# Write the model to a file
write_stan_file(
  stan_biased_cv_model,
  dir = "stan/",
  basename = "W7_MultilevelBias_cv.stan"
)

# Compile the model
file <- file.path("stan/W7_MultilevelBias_cv.stan")
mod_biased_cv <- cmdstan_model(
  file, 
  cpp_options = list(stan_threads = TRUE),
  stanc_options = list("O1")
)

# Similarly, define the memory agent model with CV support
stan_memory_cv_model <- "
// Multilevel Memory Agent Model with CV support
//
// This model has additional structures to handle test data for cross-validation

functions{
  real normal_lb_rng(real mu, real sigma, real lb) {
    real p = normal_cdf(lb | mu, sigma);  // CDF for bounds
    real u = uniform_rng(p, 1);
    return (sigma * inv_Phi(u)) + mu;  // Inverse CDF for value
  }
}

// Input data - includes both training and test data
data {
 int<lower = 1> trials;              // Number of trials per agent
 int<lower = 1> agents;              // Number of training agents
 array[trials, agents] int h;        // Training choice data
 array[trials, agents] int other;    // Opponent's choices for training agents
 
 int<lower = 1> agents_test;         // Number of test agents
 array[trials, agents_test] int h_test;    // Test choice data
 array[trials, agents_test] int other_test; // Opponent's choices for test agents
}

// Parameters to be estimated
parameters {
  real biasM;                        // Population mean baseline bias
  real betaM;                        // Population mean memory sensitivity
  vector<lower = 0>[2] tau;          // Population SDs
  matrix[2, agents] z_IDs;           // Standardized individual parameters
  cholesky_factor_corr[2] L_u;       // Cholesky factor of correlation matrix
}

// Transformed parameters
transformed parameters {
  // Memory states for training data
  array[trials, agents] real memory;
  
  // Memory states for test data
  array[trials, agents_test] real memory_test;
  
  // Individual parameters
  matrix[agents,2] IDs;
  IDs = (diag_pre_multiply(tau, L_u) * z_IDs)';
  
  // Calculate memory states for training data
  for (agent in 1:agents){
    for (trial in 1:trials){
      if (trial == 1) {
        memory[trial, agent] = 0.5;
      } 
      if (trial < trials){
        memory[trial + 1, agent] = memory[trial, agent] + 
                                  ((other[trial, agent] - memory[trial, agent]) / trial);
        if (memory[trial + 1, agent] == 0){memory[trial + 1, agent] = 0.01;}
        if (memory[trial + 1, agent] == 1){memory[trial + 1, agent] = 0.99;}
      }
    }
  }
  
  // Calculate memory states for test data
  for (agent in 1:agents_test){
    for (trial in 1:trials){
      if (trial == 1) {
        memory_test[trial, agent] = 0.5;
      } 
      if (trial < trials){
        memory_test[trial + 1, agent] = memory_test[trial, agent] + 
                                      ((other_test[trial, agent] - memory_test[trial, agent]) / trial);
        if (memory_test[trial + 1, agent] == 0){memory_test[trial + 1, agent] = 0.01;}
        if (memory_test[trial + 1, agent] == 1){memory_test[trial + 1, agent] = 0.99;}
      }
    }
  }
}

// Model definition - trained only on training data
model {
  // Population-level priors
  target += normal_lpdf(biasM | 0, 1);
  target += normal_lpdf(tau[1] | 0, .3) - normal_lccdf(0 | 0, .3);
  target += normal_lpdf(betaM | 0, .3);
  target += normal_lpdf(tau[2] | 0, .3) - normal_lccdf(0 | 0, .3);
  target += lkj_corr_cholesky_lpdf(L_u | 2);

  // Standardized individual parameters
  target += std_normal_lpdf(to_vector(z_IDs));
  
  // Likelihood for training data only
  for (agent in 1:agents){
    for (trial in 1:trials){
      target += bernoulli_logit_lpmf(h[trial, agent] | 
                biasM + IDs[agent, 1] + memory[trial, agent] * (betaM + IDs[agent, 2]));
    }
  }
}

// Calculate log-likelihood for both training and test data
generated quantities{
   // Prior samples
   real biasM_prior;
   real<lower=0> biasSD_prior;
   real betaM_prior;
   real<lower=0> betaSD_prior;
   
   real bias_prior;
   real beta_prior;
   
   // Posterior predictive samples
   array[agents] int<lower=0, upper = trials> prior_preds0;
   array[agents] int<lower=0, upper = trials> prior_preds1;
   array[agents] int<lower=0, upper = trials> prior_preds2;
   array[agents] int<lower=0, upper = trials> posterior_preds0;
   array[agents] int<lower=0, upper = trials> posterior_preds1;
   array[agents] int<lower=0, upper = trials> posterior_preds2;
   
   // Log-likelihood for training data
   array[trials, agents] real log_lik;
   
   // Log-likelihood for test data - crucial for cross-validation
   array[trials, agents_test] real log_lik_test;
   
   // Generate prior samples
   biasM_prior = normal_rng(0,1);
   biasSD_prior = normal_lb_rng(0,0.3,0);
   betaM_prior = normal_rng(0,1);
   betaSD_prior = normal_lb_rng(0,0.3,0);
   
   bias_prior = normal_rng(biasM_prior, biasSD_prior);
   beta_prior = normal_rng(betaM_prior, betaSD_prior);
   
   // Generate predictions for different memory conditions
   for (i in 1:agents){
      prior_preds0[i] = binomial_rng(trials, inv_logit(bias_prior + 0 * beta_prior));
      prior_preds1[i] = binomial_rng(trials, inv_logit(bias_prior + 1 * beta_prior));
      prior_preds2[i] = binomial_rng(trials, inv_logit(bias_prior + 2 * beta_prior));
      posterior_preds0[i] = binomial_rng(trials, 
                           inv_logit(biasM + IDs[i,1] + 0 * (betaM + IDs[i,2])));
      posterior_preds1[i] = binomial_rng(trials, 
                           inv_logit(biasM + IDs[i,1] + 1 * (betaM + IDs[i,2])));
      posterior_preds2[i] = binomial_rng(trials, 
                           inv_logit(biasM + IDs[i,1] + 2 * (betaM + IDs[i,2])));
      
      // Calculate log-likelihood for training data
      for (t in 1:trials){
        log_lik[t,i] = bernoulli_logit_lpmf(h[t, i] | 
                      biasM + IDs[i, 1] + memory[t, i] * (betaM + IDs[i, 2]));
      }
   }
   
   // Calculate log-likelihood for test data
   // Note: We use population-level estimates for prediction
   for (i in 1:agents_test){
    for (t in 1:trials){
      log_lik_test[t,i] = bernoulli_logit_lpmf(h_test[t,i] | 
                         biasM + memory_test[t, i] * betaM);
    }
  }
}
"

# Write the model to a file
write_stan_file(
  stan_memory_cv_model,
  dir = "stan/",
  basename = "W7_MultilevelMemory_cv.stan"
)

# Compile the model
file <- file.path("stan/W7_MultilevelMemory_cv.stan")
mod_memory_cv <- cmdstan_model(
  file, 
  cpp_options = list(stan_threads = TRUE),
  stanc_options = list("O1")
)
```

These CV-ready models extend our original models with additional structures to handle test data and compute separate log-likelihoods for training and test observations.

Now, let's demonstrate how to implement k-fold cross-validation using these models (N.b. we only fit both models to the memory data, I still need to implement the full comparison against biased data as well)

```{r}

# Define file path for saved CV results
cv_results_file <- "simmodels/W7_CV_Biased&Memory.RData"

# Check if we need to run the CV analysis
if (regenerate_simulations || !file.exists(cv_results_file)) {
  # Load necessary packages for parallelization
  pacman::p_load(future, furrr)
  
  # Set up parallel processing (adjust workers based on your system)
  plan(multisession, workers = 8)
  
  # Split the data into folds (10 folds, grouped by agent)
  d$fold <- kfold_split_grouped(K = 10, x = d$agent)
  unique_folds <- unique(d$fold)
  
  # Initialize matrices to store log predictive densities
  # We'll fill these after parallel processing
  log_pd_biased_kfold <- matrix(nrow = 1000, ncol = nrow(d))
  log_pd_memory_kfold <- matrix(nrow = 1000, ncol = nrow(d))
  
  # Define function to process a single fold
  process_fold <- function(k) {
    cat("Starting processing of fold", k, "\n")
    
    # Create training set (all data except fold k)
    d_train <- d %>% filter(fold != k)  
    
    # Prepare training data for Stan
    d_memory1_train <- d_train %>% 
      dplyr::select(agent, memoryChoice) %>% 
      mutate(row = row_number()) %>% 
      pivot_wider(names_from = agent, values_from = memoryChoice)
    
    d_memory2_train <- d_train %>% 
      dplyr::select(agent, randomChoice) %>% 
      mutate(row = row_number()) %>% 
      pivot_wider(names_from = agent, values_from = randomChoice)
    
    agents_n <- length(unique(d_train$agent))
    
    # Create test set (only fold k)
    d_test <- d %>% filter(fold == k) 
    
    d_memory1_test <- d_test %>% 
      dplyr::select(agent, memoryChoice) %>% 
      mutate(row = row_number()) %>% 
      pivot_wider(names_from = agent, values_from = memoryChoice)
    
    d_memory2_test <- d_test %>% 
      dplyr::select(agent, randomChoice) %>% 
      mutate(row = row_number()) %>% 
      pivot_wider(names_from = agent, values_from = randomChoice)
    
    agents_test_n <- length(unique(d_test$agent))
    
    # Prepare data for Stan model
    data_memory <- list(
      trials = trials,
      agents = agents_n,
      agents_test = agents_test_n,
      h = as.matrix(d_memory1_train[, 2:(agents_n + 1)]),
      other = as.matrix(d_memory2_train[, 2:(agents_n + 1)]),
      h_test = as.matrix(d_memory1_test[, 2:(agents_test_n + 1)]),
      other_test = as.matrix(d_memory2_test[, 2:(agents_test_n + 1)])
    )
    
    # Fit models on training data and evaluate on test data
    # Note: removed threads_per_chain parameter
    fit_biased <- mod_biased_cv$sample(
      data = data_memory,
      seed = 123 + k,  # Unique seed per fold
      chains = 1,
      parallel_chains = 1,
      threads_per_chain = 1,
      iter_warmup = 1000,
      iter_sampling = 1000,
      refresh = 0,  # Suppress progress output in parallel runs
      max_treedepth = 20,
      adapt_delta = 0.99
    )
    
    fit_memory <- mod_memory_cv$sample(
      data = data_memory,
      seed = 456 + k,  # Unique seed per fold
      chains = 1,
      parallel_chains = 1,
      threads_per_chain = 1,
      iter_warmup = 1000,
      iter_sampling = 1000,
      refresh = 0,  # Suppress progress output in parallel runs
      max_treedepth = 20,
      adapt_delta = 0.99
    )
    
    # Extract log likelihood for test data points
    biased_log_lik <- fit_biased$draws("log_lik_test", format = "matrix")
    memory_log_lik <- fit_memory$draws("log_lik_test", format = "matrix")
    
    # Create indices to keep track of which positions in the full matrix these values belong to
    test_indices <- which(d$fold == k)
    
    # Return results for this fold
    result <- list(
      fold = k,
      test_indices = test_indices,
      biased_log_lik = biased_log_lik,
      memory_log_lik = memory_log_lik
    )
    
    cat("Completed processing of fold", k, "\n")
    return(result)
  }
  
  # Process all folds in parallel using furrr
  cat("Starting parallel processing of all folds\n")
  results <- future_map(unique_folds, process_fold, .options = furrr_options(seed = TRUE))
  
  # Now populate the full matrices with results from each fold
  for (i in seq_along(results)) {
    fold_result <- results[[i]]
    test_indices <- fold_result$test_indices
    
    # For each test index, populate the corresponding column in the full matrices
    for (j in seq_along(test_indices)) {
      idx <- test_indices[j]
      log_pd_biased_kfold[, idx] <- fold_result$biased_log_lik[, j]
      log_pd_memory_kfold[, idx] <- fold_result$memory_log_lik[, j]
    }
  }
  
  # Save results
  save(log_pd_biased_kfold, log_pd_memory_kfold, file = cv_results_file)
  cat("Generated new cross-validation results and saved to", cv_results_file, "\n")
} else {
  # Load existing results
  load(cv_results_file)
  cat("Loaded existing cross-validation results from", cv_results_file, "\n")
}

```

```{r}
# Load cross-validation results
cv_results_file <- "simmodels/W7_CV_Biased&Memory.RData"
load(cv_results_file)

# Calculate ELPD values from cross-validation results
elpd_biased_kfold <- elpd(log_pd_biased_kfold)
elpd_memory_kfold <- elpd(log_pd_memory_kfold)

# Extract ELPD values and standard errors
biased_elpd_value <- elpd_biased_kfold$estimates["elpd", "Estimate"]
memory_elpd_value <- elpd_memory_kfold$estimates["elpd", "Estimate"]
biased_se <- elpd_biased_kfold$estimates["elpd", "SE"]
memory_se <- elpd_memory_kfold$estimates["elpd", "SE"]

# Print values for comparison
cat("K-fold CV results:\n")
cat("Biased model ELPD:", biased_elpd_value, "±", biased_se, "\n")
cat("Memory model ELPD:", memory_elpd_value, "±", memory_se, "\n")

# Create a results table
kfold_results <- tibble(
  model = c("Biased Agent Model", "Memory Agent Model"),
  elpd = c(biased_elpd_value, memory_elpd_value),
  se = c(biased_se, memory_se)
)

# Calculate model weights from ELPD values
max_elpd <- max(c(biased_elpd_value, memory_elpd_value))
relative_elpd <- c(biased_elpd_value, memory_elpd_value) - max_elpd
model_weights <- exp(relative_elpd) / sum(exp(relative_elpd))

# Add weights to results
kfold_results$weight <- model_weights

# Display the results table
kfold_results
```

### Comparing LOO and K-Fold Results

```{r}
# Create comparison table of PSIS-LOO vs K-fold CV
loo_results <- tibble(
  model = c("Biased Agent Model", "Memory Agent Model"),
  loo_elpd = c(Loo_biased2biased$estimates["elpd_loo", "Estimate"],
              Loo_memory2biased$estimates["elpd_loo", "Estimate"]),
  loo_se = c(Loo_biased2biased$estimates["elpd_loo", "SE"],
            Loo_memory2biased$estimates["elpd_loo", "SE"]),
  loo_weight = c(weights_biased[1], weights_biased[2])
)

# Combine with k-fold results
comparison_results <- left_join(loo_results, kfold_results, by = "model") %>%
  rename(kfold_elpd = elpd, kfold_se = se, kfold_weight = weight)

# Display comparison
comparison_results %>%
  mutate(
    loo_elpd = round(loo_elpd, 1),
    loo_se = round(loo_se, 2),
    loo_weight = round(loo_weight, 3),
    kfold_elpd = round(kfold_elpd, 1),
    kfold_se = round(kfold_se, 2),
    kfold_weight = round(kfold_weight, 3)
  ) %>%
  knitr::kable(
    caption = "Comparison of PSIS-LOO and K-fold Cross-Validation Results",
    col.names = c("Model", "LOO ELPD", "LOO SE", "LOO Weight", 
                  "K-fold ELPD", "K-fold SE", "K-fold Weight")
  )
```

Funnily enough cross-validation indicates the wrong model.




## Limitations of Model Comparison Approaches ##

While cross-validation and ELPD provide powerful tools for model comparison, it's important to understand their limitations:

1. Training vs. Transfer Generalization
Cross-validation only assesses a model's ability to generalize to new data from the same distribution (training generalization). It doesn't evaluate how well models transfer to different contexts or populations (transfer generalization).

2. Model Misspecification
All models are wrong, but some are useful. Cross-validation helps identify which wrong model is most useful for prediction, but doesn't guarantee we've captured the true generating process.

3. Limited Data
With limited data, cross-validation estimates can have high variance, especially for complex models. K-fold CV with small k can help mitigate this issue.

4. Computational Cost
True cross-validation requires refitting models multiple times, which can be prohibitively expensive for complex Bayesian models. PSIS-LOO offers an efficient approximation but may not always be reliable.

5. Parameter vs. Predictive Focus
Model comparison based on predictive performance might select different models than if we were focused on accurate parameter estimation. The "best" model depends on your goals.

## Exercises

1. Compare the models on different subsets of the data (e.g., early vs. late trials). Does the preferred model change depending on which portion of the data you use?

2. Experiment with different priors for the models. How sensitive are the model comparison results to prior choices?

3. Implement a different model (e.g., win-stay-lose-shift) and compare it to the biased and memory models. Which performs best?

4. Explore how the amount of data affects model comparison. How many trials do you need to reliably identify the true model?

5. Investigate the relationship between model complexity and predictive performance in this context. Are there systematic patterns in when simpler models are preferred?


***
### Rant on internal vs external validity
However, we need to think carefully about what we mean by "out of sample." There are actually two distinct types of test sets we might consider: internal and external.
Internal test sets come from the same data collection effort as our training data - for example, we might randomly set aside 20% of our matching pennies games to test on. While this approach helps us detect overfitting to specific participants or trials, it cannot tell us how well our model generalizes to truly new contexts. Our test set participants were recruited from the same population, played the game under the same conditions, and were influenced by the same experimental setup as our training participants.
External test sets, in contrast, come from genuinely different contexts. For our matching pennies model, this might mean testing on games played:

* In different cultures or age groups
* Under time pressure versus relaxed conditions
* For real money versus just for fun
* Against human opponents versus computer agents
* In laboratory versus online settings

The distinction matters because cognitive models often capture not just universal mental processes, but also specific strategies that people adopt in particular contexts. A model that perfectly predicts behavior in laboratory matching pennies games might fail entirely when applied to high-stakes poker games, even though both involve similar strategic thinking.
This raises deeper questions about what kind of generalization we want our models to achieve. Are we trying to build models that capture universal cognitive processes, or are we content with models that work well within specific contexts? The answer affects not just how we evaluate our models, but how we design them in the first place.
In practice, truly external test sets are rare in cognitive science - they require additional data collection under different conditions, which is often impractical. This means we must be humble about our claims of generalization. When we talk about a model's predictive accuracy, we should be clear that we're usually measuring its ability to generalize within a specific experimental context, not its ability to capture human cognition in all its diversity.
This limitation of internal test sets is one reason why cognitive scientists often complement predictive accuracy metrics with other forms of model evaluation, such as testing theoretical predictions on new tasks or examining whether model parameters correlate sensibly with individual differences. These approaches help us build confidence that our models capture meaningful cognitive processes rather than just statistical patterns specific to our experimental setup.
***
