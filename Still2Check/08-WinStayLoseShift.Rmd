---
title: "08-Bonus_WSLS"
output: html_document
date: "2023-03-18"
---

```{r w8 setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Bonus chapter: Additional models developed by students

## Additional models

Additional models that have been developed by previous students:

### Probabilistic Win Stay Lose Shift

### Probabilistic asymmetric Win Stay Lose Shift

### Memory agent with exponential memory decay (rate)

Memory = alpha * memory + (1 - alpha) * otherChoice

### Memory agent with exponential memory decay (exponential)

The agent uses a weighted memory store, where previous choices are weighted by a factor based on how far back in the past they were made, represented by power. The agent first creates a vector of weights for each of the previous choices based on their position in the memory store and the power value. This model implements exponential decay of memory, according to a power law. Specifically, the opposing agent’s choices up to the present trial are encoded as 1’s and -1’s. The choices are weighted, and then summed. So, on trial i, the memory of the previous trial j is given a weight corresponding to j^(-1). Higher values of j correspond to earlier trials, with j = 1 being the most recent. Then a choice is made based on this weight according to the following formula:

Theta = inv_logit(alpha + beta * weightedMemory)

[N.B. really poor parameter recovery]

### Memory agent with exponential memory decay, but separate memory for wins and losses

[N.B. really poor parameter recovery]

### Memory agent with exponential memory decay, and with confidence

Confidence is initialized at 0. There is a fixed confidence rate, which is added to confidence if the agent wins, subtracted if it loses. Confidence is then added to beta.

### Reinforcement learning (Rescorla Wagner)

### Hierarchical gaussian filter

## Implementing the WSLS model

[Motivating the choice to showcase the WSLS amongst all models]

## Load packages

```{r 08 load packages}
pacman::p_load(tidyverse,
               here,
               posterior,
               cmdstanr,
               brms, 
               tidybayes, 
               loo)
```

## Generate data

[MISSING: EXPLAIN THE MODEL STEP BY STEP]

[MISSING: PARALLELIZE AND MAKE IT MORE SIMILAR TO PREVIOUS DATASETS]

```{r pressure, echo=FALSE}
agents <- 100
trials <- 120

noise <- 0

rateM <- 1.4  ## N.B. Log odds
rateSD <- 0.3  ## N.B. Log odds

alphaM <- 0  ## N.B. Log odds
alphaSD <- 0.1  ## N.B. Log odds
betaWinM <- 1.5  ## N.B. Log odds
betaWinSD <- 0.3  ## N.B. Log odds

betaLoseM <- 1.5  ## N.B. Log odds
betaLoseSD <- 0.3  ## N.B. Log odds

RandomAgentNoise_f <- function(rate, noise) {
  choice <- rbinom(1, 1, inv_logit_scaled(rate))
  if (rbinom(1, 1, noise) == 1) {
    choice = rbinom(1, 1, 0.5)
  }
  return(choice)
}

WSLSAgentNoise_f <- function(alpha, betaWin, betaLose, win, lose, noise) {
  rate <- alpha + betaWin * win + betaLose * lose
  choice <- rbinom(1, 1, inv_logit_scaled(rate))
  if (rbinom(1, 1, noise) == 1) {
    choice = rbinom(1, 1, 0.5)
  }
  return(choice)
}

d <- NULL

for (agent in 1:agents) {
  
  rate <- rnorm(1, rateM, rateSD)
  alpha <- rnorm(1, alphaM, alphaSD)
  betaWin <- rnorm(1, betaWinM, betaWinSD)
  betaLose <- rnorm(1, betaLoseM, betaLoseSD)
  
  randomChoice <- rep(NA, trials)
  wslsChoice <- rep(NA, trials)
  win <- rep(NA, trials)
  lose <- rep(NA, trials)
  feedback <- rep(NA, trials)
  
  for (trial in 1:trials) {
    
    randomChoice[trial] <- RandomAgentNoise_f(rate, noise)
    
    if (trial == 1) {
      wslsChoice[trial] <- rbinom(1,1,0.5)
    } else {
      wslsChoice[trial] <- WSLSAgentNoise_f(alpha, betaWin, betaLose, win[trial - 1], lose[trial - 1], noise)
    }
    
    feedback[trial] <- ifelse(wslsChoice[trial] == randomChoice[trial], 1, 0)
    win[trial] <- ifelse(feedback[trial] == 1, ifelse(wslsChoice[trial] == 1, 1, -1), 0)
    lose[trial] <- ifelse(feedback[trial] == 0, ifelse(wslsChoice[trial] == 1, -1, 1), 0)
  }
  
  tempRandom <- tibble(agent, trial = seq(trials), choice = randomChoice, rate, noise, rateM, rateSD, 
                       alpha, alphaM, alphaSD, betaWin, betaWinM, betaWinSD, betaLose, betaLoseM, betaLoseSD, 
                       win, lose, feedback, strategy = "Random")
  tempWSLS <- tibble(agent, trial = seq(trials), choice = wslsChoice, rate, noise, rateM, rateSD, 
                     alpha, alphaM, alphaSD, betaWin, betaWinM, betaWinSD, betaLose, betaLoseM, betaLoseSD, 
                     win, lose, feedback, strategy = "WSLS")
  
  temp <- rbind(tempRandom, tempWSLS)
  
  if (agent > 1) {
    d <- rbind(d, temp)
  } else{
    d <- temp
  }
  
}

```

## Sanity check for the data

```{r}
d <- d %>% group_by(agent, strategy) %>% mutate(
  nextChoice = lead(choice),
  prevWin = lag(win),
  prevLose = lag(lose),
  cumulativerate = cumsum(choice) / seq_along(choice),
  performance = cumsum(feedback) / seq_along(feedback)
) %>% subset(complete.cases(d)) %>% subset(trial > 1)

p1 <- ggplot(d, aes(trial, cumulativerate, group = agent, color = agent)) + 
  geom_line() + 
  geom_hline(yintercept = 0.5, linetype = "dashed") + ylim(0,1) + theme_classic() + facet_wrap(.~strategy)
p1
p2a <- ggplot(subset(d, strategy == "Random"), aes(trial, 1 - performance, group = agent, color = agent)) + 
  geom_line() + 
  geom_hline(yintercept = 0.5, linetype = "dashed") + ylim(0,1) + theme_classic()
p2b <- ggplot(subset(d, strategy == "WSLS"), aes(trial, performance, group = agent, color = agent)) + 
  geom_line() + 
  geom_hline(yintercept = 0.5, linetype = "dashed") + ylim(0,1) + theme_classic()
library(patchwork)
p2a + p2b
```

## More sanity check

```{r}
## Checking lose/win are orthogonal
ggplot(d, aes(win, lose)) + geom_point() + theme_bw()
## Checking lose/win do determine choice
d %>% subset(strategy == "WSLS") %>% 
  mutate(nextChoice = lead(choice)) %>%
  group_by(agent, win, lose) %>%
  summarize(heads = mean(nextChoice)) %>%
  ggplot(aes(win, heads)) +
       geom_point() +
       theme_bw() +
  facet_wrap(~lose)

```

## Create data for single agent model

```{r}
d_a <- d %>% subset(
  strategy == "WSLS" &  agent == 2
)

data_wsls_simple <- list(
  trials = trials - 1,
  h = d_a$choice,
  win = d_a$prevWin,
  lose = d_a$prevLose
)
```

## Create the model

[MISSING: MORE MEANINGFUL PREDICTIONS, BASED ON THE 4 SCENARIOS]

```{r}
stan_wsls_model <- "

functions{
  real normal_lb_rng(real mu, real sigma, real lb) {
    real p = normal_cdf(lb | mu, sigma);  // cdf for bounds
    real u = uniform_rng(p, 1);
    return (sigma * inv_Phi(u)) + mu;  // inverse cdf for value
  }
}

data {
 int<lower = 1> trials;
 array[trials] int h;
 vector[trials] win;
 vector[trials] lose;
}

parameters {
  real alpha;
  real winB;
  real loseB;
}

model {
  target += normal_lpdf(alpha | 0, .3);
  target += normal_lpdf(winB | 1, 1);
  target += normal_lpdf(loseB | 1, 1);
  target += bernoulli_logit_lpmf(h | alpha + winB * win + loseB * lose);
}

generated quantities{
   real alpha_prior;
   real winB_prior;
   real loseB_prior;
   array[trials] int prior_preds;
   array[trials] int posterior_preds;
   vector[trials] log_lik;

   alpha_prior = normal_rng(0, 1);
   winB_prior = normal_rng(0, 1);
   loseB_prior = normal_rng(0, 1);
   
   prior_preds = bernoulli_rng(inv_logit(winB_prior * win +  loseB_prior * lose));
   posterior_preds = bernoulli_rng(inv_logit(winB * win +  loseB * lose));
      
    for (t in 1:trials){
      log_lik[t] = bernoulli_logit_lpmf(h[t] | winB * win +  loseB * lose);
    }
  
}

"

write_stan_file(
  stan_wsls_model,
  dir = "stan/",
  basename = "W8_WSLS.stan")

file <- file.path("stan/W8_WSLS.stan")
mod_wsls_simple <- cmdstan_model(file, 
                                 cpp_options = list(stan_threads = TRUE),
                     stanc_options = list("O1"), 
                                 pedantic = TRUE)

samples_wsls_simple <- mod_wsls_simple$sample(
  data = data_wsls_simple,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  threads_per_chain = 2,
  iter_warmup = 2000,
  iter_sampling = 2000,
  refresh = 1000,
  max_treedepth = 20,
  adapt_delta = 0.99,
)
```

## Basic assessment

```{r}
samples_wsls_simple$summary() 

# Extract posterior samples and include sampling of the prior:
draws_df <- as_draws_df(samples_wsls_simple$draws())

# Now let's plot the density for theta (prior and posterior)
ggplot(draws_df) +
  geom_density(aes(alpha), fill = "blue", alpha = 0.3) +
  geom_density(aes(alpha_prior), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = d_a$alpha[1]) +
  xlab("Rate") +
  ylab("Posterior Density") +
  theme_classic()

ggplot(draws_df) +
  geom_density(aes(winB), fill = "blue", alpha = 0.3) +
  geom_density(aes(winB_prior), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = d_a$betaWin[1]) +
  xlab("Rate") +
  ylab("Posterior Density") +
  theme_classic()

ggplot(draws_df) +
  geom_density(aes(loseB), fill = "blue", alpha = 0.3) +
  geom_density(aes(loseB_prior), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = d_a$betaLose[1]) +
  xlab("Rate") +
  ylab("Posterior Density") +
  theme_classic()
```

[MISSING: FULL PARAMETER RECOVERY]

## Create multilevel data

```{r}
## Now multilevel model
d_wsls1 <- d %>% subset(strategy == "WSLS") %>% 
  subset(select = c(agent, choice)) %>% 
  mutate(row = row_number()) %>% 
  pivot_wider(names_from = agent, values_from = choice)
d_wsls2 <- d %>% subset(strategy == "WSLS") %>% 
  subset(select = c(agent, prevWin)) %>% 
  mutate(row = row_number()) %>% 
  pivot_wider(names_from = agent, values_from = prevWin)
d_wsls3 <- d %>% subset(strategy == "WSLS") %>% 
  subset(select = c(agent, prevLose)) %>% 
  mutate(row = row_number()) %>% 
  pivot_wider(names_from = agent, values_from = prevLose)

## Create the data
data_wsls <- list(
  trials = trials - 1,
  agents = agents,
  h = as.matrix(d_wsls1[,2:(agents + 1)]),
  win = as.matrix(d_wsls2[,2:(agents + 1)]),
  lose = as.matrix(d_wsls3[,2:(agents + 1)])
)
```

## Create the model

[MISSING: ADD BIAS]

[MISSING: BETTER PREDICTIONS BASED ON 4 SCENARIOS]

```{r}
stan_wsls_ml_model <- "
functions{
  real normal_lb_rng(real mu, real sigma, real lb) {
    real p = normal_cdf(lb | mu, sigma);  // cdf for bounds
    real u = uniform_rng(p, 1);
    return (sigma * inv_Phi(u)) + mu;  // inverse cdf for value
  }
}

// The input (data) for the model. 
data {
 int<lower = 1> trials;
 int<lower = 1> agents;
 array[trials, agents] int h;
 array[trials, agents] real win;
 array[trials, agents] real lose;
}

parameters {
  real winM;
  real loseM;
  vector<lower = 0>[2] tau;
  matrix[2, agents] z_IDs;
  cholesky_factor_corr[2] L_u;
}

transformed parameters {
  matrix[agents,2] IDs;
  IDs = (diag_pre_multiply(tau, L_u) * z_IDs)';
 }

model {
  target += normal_lpdf(winM | 0, 1);
  target += normal_lpdf(tau[1] | 0, .3)  -
    normal_lccdf(0 | 0, .3);
  target += normal_lpdf(loseM | 0, .3);
  target += normal_lpdf(tau[2] | 0, .3)  -
    normal_lccdf(0 | 0, .3);
  target += lkj_corr_cholesky_lpdf(L_u | 2);

  target += std_normal_lpdf(to_vector(z_IDs));
  
  for (i in 1:agents)
    target += bernoulli_logit_lpmf(h[,i] | to_vector(win[,i]) * (winM + IDs[i,1]) +  to_vector(lose[,i]) * (loseM + IDs[i,2]));
}

generated quantities{
   real winM_prior;
   real<lower=0> winSD_prior;
   real loseM_prior;
   real<lower=0> loseSD_prior;
   
   real win_prior;
   real lose_prior;
   
   array[trials,agents] int<lower=0, upper = trials> prior_preds;
   array[trials,agents] int<lower=0, upper = trials> posterior_preds;
   
   array[trials, agents] real log_lik;


   winM_prior = normal_rng(0,1);
   winSD_prior = normal_lb_rng(0,0.3,0);
   loseM_prior = normal_rng(0,1);
   loseSD_prior = normal_lb_rng(0,0.3,0);
   
   win_prior = normal_rng(winM_prior, winSD_prior);
   lose_prior = normal_rng(loseM_prior, loseSD_prior);
   
   for (i in 1:agents){
      prior_preds[,i] = binomial_rng(trials, inv_logit(to_vector(win[,i]) * (win_prior) +  to_vector(lose[,i]) * (lose_prior)));
      posterior_preds[,i] = binomial_rng(trials, inv_logit(to_vector(win[,i]) * (winM + IDs[i,1]) +  to_vector(lose[,i]) * (loseM + IDs[i,2])));
      
    for (t in 1:trials){
      log_lik[t,i] = bernoulli_logit_lpmf(h[t,i] | to_vector(win[,i]) * (winM + IDs[i,1]) +  to_vector(lose[,i]) * (loseM + IDs[i,2]));
    }
  }
  
}


"

write_stan_file(
  stan_wsls_ml_model,
  dir = "stan/",
  basename = "W8_wsls_ml.stan")

file <- file.path("stan/W8_wsls_ml.stan")
mod_wsls <- cmdstan_model(file, cpp_options = list(stan_threads = TRUE),
                     stanc_options = list("O1"), 
                          pedantic = TRUE)

samples_wsls_ml <- mod_wsls$sample(
  data = data_wsls,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  threads_per_chain = 2,
  iter_warmup = 2000,
  iter_sampling = 2000,
  refresh = 1000,
  max_treedepth = 20,
  adapt_delta = 0.99,
)
```

## Quality checks

```{r}
samples_wsls_ml$summary() 

# Extract posterior samples and include sampling of the prior:
draws_df <- as_draws_df(samples_wsls_ml$draws())

# Now let's plot the density for theta (prior and posterior)

ggplot(draws_df) +
  geom_density(aes(winM), fill = "blue", alpha = 0.3) +
  geom_density(aes(win_prior), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = d$betaWinM[1]) +
  xlab("Rate") +
  ylab("Posterior Density") +
  theme_classic()

ggplot(draws_df) +
  geom_density(aes(`tau[1]`), fill = "blue", alpha = 0.3) +
  geom_density(aes(`winSD_prior`), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = d$betaWinSD[1]) +
  xlab("Rate") +
  ylab("Posterior Density") +
  theme_classic()

ggplot(draws_df) +
  geom_density(aes(loseM), fill = "blue", alpha = 0.3) +
  geom_density(aes(loseM_prior), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = d$betaLoseM[1]) +
  xlab("Rate") +
  ylab("Posterior Density") +
  theme_classic()

ggplot(draws_df) +
  geom_density(aes(`tau[2]`), fill = "blue", alpha = 0.3) +
  geom_density(aes(`loseSD_prior`), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = d$betaLoseSD[1]) +
  xlab("Rate") +
  ylab("Posterior Density") +
  theme_classic()
```

[MISSING: Model comparison with biased]
[MISSING: Mixture model with biased]


