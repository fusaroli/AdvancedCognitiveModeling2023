---
title: "09-BayesianModels"
output: html_document
date: "2023-03-18"
---

```{r 09 setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Bayesian models of cognition

## Load the packages

```{r 09 packages}
pacman::p_load(
  tidyverse,
  brms,
  cmdstanr,
  patchwork
)
```

## Design the model

[MISSING: EXPLAIN THE MODEL]
[MISSING: Explain the different outcomes, including examples of experiments]
[MISSING: explain the difference between a simple Bayes with weights of 1 - accumulating evidence - and one with weights that sum up to 1 - integrating/averaging evidence]

```{r 09 simple bayes model}

SimpleBayes_f <- function(bias, Source1, Source2){
  
  outcome <- inv_logit_scaled(bias + logit_scaled(Source1) + logit_scaled(Source2))
  
  return(outcome)
  
}

SimpleBayes_Sources_f <- function(bias, Source1n, Source2n){
  # Reconstruct theta from raw info sources
  outcome <- inv_logit_scaled(bias + logit_scaled(Source1) + logit_scaled(Source2))
  
  return(outcome)
  
}

SimpleBayes_MultiSource_f <- function(bias, sources) {

  outcome <- inv_logit_scaled(bias + sum(logit_scaled(sources)))
  
  return(outcome)
}

```

## Create the data

```{r 09 create simple bayes simulation}
bias <- 0
trials <- seq(10)
Source1 <- seq(0.1,0.9, 0.1)
Source2 <- seq(0.1,0.9, 0.1)

db <- expand.grid(bias = bias, trials = trials, Source1 = Source1, Source2 = Source2)

for (n in seq(nrow(db))) {
  db$belief[n] <- SimpleBayes_f(db$bias[n], db$Source1[n], db$Source2[n])
  db$choice[n] <- rbinom(1,1, db$belief[n])
  db$continuous[n] <- db$belief[n]*9
  db$discrete[n] <- round(db$belief[n]*9,0)
}
```

## Visualize

[MISSING: Explain]


```{r 09 visualize simple bayes simulation}
ggplot(db, aes(belief)) +
  geom_histogram(bins = 10, alpha = 0.3, color = "black") +
  theme_bw()

ggplot(db, aes(Source1, belief, color = Source2, group = Source2)) +
  geom_line() +
  theme_bw()

ggplot(db, aes(choice)) +
  geom_histogram(bins = 10, alpha = 0.3, color = "black") +
  theme_bw()

ggplot(db, aes(Source1, choice, color = Source2, group = Source2)) +
  geom_smooth(se = F) +
  theme_bw()

ggplot(db, aes(continuous)) +
  geom_histogram(bins = 10, alpha = 0.3, color = "black") +
  theme_bw()

ggplot(db, aes(Source1, continuous, color = Source2, group = Source2)) +
  geom_smooth() +
  theme_bw()

ggplot(db, aes(discrete)) +
  geom_histogram(bins = 10, alpha = 0.3, color = "black") +
  theme_bw()

ggplot(db, aes(Source1, discrete, color = Source2, group = Source2)) +
  geom_smooth() +
  theme_bw()
```

## Data for Stan

```{r 09 simple bayes data for stan}
data_simpleBayes <- list(
  N = nrow(db),
  y = db$choice,
  Source1 = db$Source1,
  Source2 = db$Source2
)

```

## Create the Stan Model

```{r 09 create stan version simple bayes}

stan_simpleBayes_model <- "
data {
  int<lower=0> N;
  array[N] int y;
  array[N] real<lower=0, upper = 1> Source1;
  array[N] real<lower=0, upper = 1> Source2;
}

transformed data{
  array[N] real l_Source1;
  array[N] real l_Source2;
  l_Source1 = logit(Source1);
  l_Source2 = logit(Source2);
}

parameters {
  real bias;
}

model {
  target +=  normal_lpdf(bias | 0, 1);
  target +=  bernoulli_logit_lpmf(y | bias + to_vector(l_Source1) + to_vector(l_Source2));
}

generated quantities{
  real bias_prior;
  array[N] real log_lik;
  
  bias_prior = normal_rng(0, 1);
  
  for (n in 1:N){  
    log_lik[n] = bernoulli_logit_lpmf(y[n] | bias + l_Source1[n] +  l_Source2[n]);
  }
  
}

"

write_stan_file(
  stan_simpleBayes_model,
  dir = "stan/",
  basename = "W9_SimpleBayes.stan")

file <- file.path("stan/W9_SimpleBayes.stan")
mod_simpleBayes <- cmdstan_model(file, cpp_options = list(stan_threads = TRUE),
                     stanc_options = list("O1"))


```

## Fitting the model
```{r 09 fitting simple bayes stan model}
samples_simple <- mod_simpleBayes$sample(
  data = data_simpleBayes,
  #fixed_param = TRUE,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  threads_per_chain = 2,
  iter_warmup = 1500,
  iter_sampling = 3000,
  refresh = 500
)

```



## Basic evaluation

```{r 09 evaluate simple bayes stan model}
samples_simple$cmdstan_diagnose()
samples_simple$summary()
samples_simple$loo()

draws_df <- as_draws_df(samples_simple$draws())

ggplot(draws_df, aes(.iteration, bias, group = .chain, color = .chain)) +
  geom_line(alpha = 0.5) +
  theme_classic()

ggplot(draws_df) +
  geom_density(aes(bias), alpha = 0.6, fill = "lightblue") +
  geom_density(aes(bias_prior), alpha = 0.6, fill = "pink") +
  geom_vline(xintercept = db$bias[1]) +
  theme_bw()

```

[MISSING PARAMETER RECOVERY]

## Weighted Bayes

[MISSING: EXPLANATION]
[MISSING: FOCUS ON WEIGHTS AND THEIR SCALE: 0-1 on log-odds; 0.5-1 on probability]

```{r 09 design weighted bayes agent and generate data}


WeightedBayes_f <- function(bias, Source1, Source2, w1, w2){
  w1 <- (w1 - 0.5)*2
  w2 <- (w2 - 0.5)*2
  outcome <- inv_logit_scaled(bias + w1 * logit_scaled(Source1) + w2 * logit_scaled(Source2))
  return(outcome)
}

## This version of the model is from Taking others into account (in the syllabus)
## It takes two sources of information and weights them, then adds a bias
## to generate a posterior on a 0-1 scale
WeightedBayes_f1 <- function(bias, Source1, Source2, w1, w2){
  outcome <- inv_logit_scaled(bias + weight_f(logit_scaled(Source1), w1) +
                                weight_f(logit_scaled(Source2), w2))
  return(outcome)
}

## The weight_f formula comes from https://www.nature.com/articles/ncomms14218
## and ensures that even if we work on a log-odds scale, we get the right weights
## It takes all values of L (- inf to +inf). Technically the only valid values for
## w are 0.5 (no consideration of the evidence) to 1 (taking the evidence at face value).
## In practice the function would also accept 0-0.5 (invert the evidence, at face value
## if 0, at decreased value as it grows towards 0.5), and slightly higher than 1
## (overweighing the evidence, but it's very unstable and quickly gives NaN).
weight_f <- function(L, w){
  return(log((w * exp(L) + 1 - w) / 
        ((1 - w) * exp(L) + w)))
      }


bias <- 0
trials <- seq(10)
Source1 <- seq(0.1,0.9, 0.1)
Source2 <- seq(0.1,0.9, 0.1)
w1 <- seq(0.5, 1, 0.1)
w2 <- seq(0.5, 1, 0.1)

db <- expand.grid(bias = bias, trials, Source1 = Source1, Source2 = Source2, w1 = w1, w2 = w2)

for (n in seq(nrow(db))) {
  db$belief[n] <- WeightedBayes_f(db$bias[n], db$Source1[n], db$Source2[n],db$w1[n], db$w2[n])
  db$belief1[n] <- WeightedBayes_f1(db$bias[n], db$Source1[n], db$Source2[n],db$w1[n], db$w2[n])
  db$binary[n] <- rbinom(1,1, db$belief[n])
  db$binary1[n] <- rbinom(1,1, db$belief1[n])
  db$continuous[n] <- db$belief[n] * 9
  db$continuous1[n] <- db$belief1[n] * 9
  db$discrete[n] <- round(db$belief[n] * 9,0)
  db$discrete1[n] <- round(db$belief1[n] * 9,0)
}
```

## Visualize

```{r 09 visualize weighted bayes agents}

ggplot(db, aes(belief, belief1)) +
  geom_point() +
  theme_bw()

ggplot(db) +
  geom_histogram(aes(belief), bins = 10, alpha = 0.3, color = "black", fill = "red") +
  geom_histogram(aes(belief1), bins = 10, alpha = 0.3, color = "black", fill = "blue") +
  theme_bw()

p1 <- ggplot(db, aes(Source1, belief, color = Source2, group = Source2)) +
  geom_line() +
  theme_bw() +
  facet_wrap(w1~w2)

p2 <- ggplot(db, aes(Source1, belief1, color = Source2, group = Source2)) +
  geom_line() +
  theme_bw() +
  facet_wrap(w1~w2)

p1 + p2

```

## Build the Weighted Bayes Stan model (simple formula)

```{r 09 stan weighted bayes model}

stan_WB_model <- "
data {
  int<lower=0> N;
  array[N] int y;
  array[N] real <lower = 0, upper = 1> Source1; 
  array[N] real <lower = 0, upper = 1> Source2; 
}

transformed data {
  array[N] real l_Source1;
  array[N] real l_Source2;
  l_Source1 = logit(Source1);
  l_Source2 = logit(Source2);
}
parameters {
  real bias;
  // meaningful weights are btw 0.5 and 1 (theory reasons)
  real<lower = 0.5, upper = 1> w1; 
  real<lower = 0.5, upper = 1> w2;
}
transformed parameters {
  real<lower = 0, upper = 1> weight1;
  real<lower = 0, upper = 1> weight2;
  // weight parameters are rescaled to be on a 0-1 scale (0 -> no effects; 1 -> face value)
  weight1 = (w1 - 0.5) * 2;  
  weight2 = (w2 - 0.5) * 2;
}
model {
  target += normal_lpdf(bias | 0, 1);
  target += beta_lpdf(weight1 | 1, 1);
  target += beta_lpdf(weight2 | 1, 1);
  for (n in 1:N)
    target += bernoulli_logit_lpmf(y[n] | bias + weight1 *l_Source1[n] + weight2 * l_Source2[n]);
}
generated quantities{
  array[N] real log_lik;
  real bias_prior;
  real w1_prior;
  real w2_prior;
  bias_prior = normal_rng(0, 1) ;
  w1_prior = 0.5 + inv_logit(normal_rng(0, 1))/2 ;
  w2_prior = 0.5 + inv_logit(normal_rng(0, 1))/2 ;
  for (n in 1:N)
    log_lik[n]= bernoulli_logit_lpmf(y[n] | bias + weight1 * l_Source1[n] + weight2 * l_Source2[n]);
}

"

write_stan_file(
  stan_WB_model,
  dir = "stan/",
  basename = "W9_WB.stan")

file <- file.path("stan/W9_WB.stan")
mod_wb <- cmdstan_model(file, cpp_options = list(stan_threads = TRUE),
                     stanc_options = list("O1"))

db1 <-  db %>% subset(w1 == 0.7 & w2 == 0.9) 


p3 <- ggplot(db1, aes(Source1, belief, color = Source2, group = Source2)) +
  geom_line() +
  theme_bw()
p3 

ggplot(db1, aes(Source1, binary)) +
  geom_smooth() +
  theme_bw()


data_weightedBayes <- list(
  N = nrow(db1),
  y = db1$binary,
  Source1 = db1$Source1,
  Source2 = db1$Source2
)

samples_weighted <- mod_wb$sample(
  data = data_weightedBayes,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  threads_per_chain = 2,
  iter_warmup = 1500,
  iter_sampling = 3000,
  refresh = 500
)

```

## Model evaluation

```{r 09 evaluate stan weighted bayes model}
samples_weighted$cmdstan_diagnose()
samples_weighted$summary()
samples_weighted$loo()

draws_df <- as_draws_df(samples_weighted$draws())

ggplot(draws_df, aes(.iteration, bias, group = .chain, color = .chain)) +
  geom_line(alpha = 0.5) +
  theme_classic()

ggplot(draws_df, aes(.iteration, w1, group = .chain, color = .chain)) +
  geom_line(alpha = 0.5) +
  theme_classic()

ggplot(draws_df, aes(.iteration, w2, group = .chain, color = .chain)) +
  geom_line(alpha = 0.5) +
  theme_classic()

p1 <- ggplot(draws_df) +
  geom_density(aes(bias), alpha = 0.6, fill = "lightblue") +
  geom_density(aes(bias_prior), alpha = 0.6, fill = "pink") +
  geom_vline(xintercept = db1$bias[1]) +
  theme_bw()

p2 <- ggplot(draws_df) +
  geom_density(aes(w1), alpha = 0.6, fill = "lightblue") +
  geom_density(aes(w1_prior), alpha = 0.6, fill = "pink") +
  geom_vline(xintercept = db1$w1[1]) +
  theme_bw()

p3 <- ggplot(draws_df) +
  geom_density(aes(w2), alpha = 0.6, fill = "lightblue") +
  geom_density(aes(w2_prior), alpha = 0.6, fill = "pink") +
  geom_vline(xintercept = db1$w2[1]) +
  theme_bw()

p1 + p2 + p3

ggplot(draws_df) +
  geom_point(aes(w1, w2), alpha = 0.3) +
  theme_bw()
ggplot(draws_df) +
  geom_point(aes(bias, w1), alpha = 0.3) +
  theme_bw()
ggplot(draws_df) +
  geom_point(aes(bias, w2), alpha = 0.3) +
  theme_bw()

```

## Model recovery

```{r 09 model recovery simple and weighted bayes}
bias <- 0
trials <- seq(10)
Source1 <- seq(0.1,0.9, 0.1)
Source2 <- seq(0.1,0.9, 0.1)
w1 <- 0.7
w2 <- 0.9

db <- expand.grid(bias = bias, trials, Source1 = Source1, Source2 = Source2, w1 = w1, w2 = w2)

for (n in seq(nrow(db))) {
  db$simple_belief[n] <- SimpleBayes_f(db$bias[n], db$Source1[n], db$Source2[n])
  db$weighted_belief[n] <- WeightedBayes_f(db$bias[n], db$Source1[n], db$Source2[n],db$w1[n], db$w2[n])
  db$simple_binary[n] <- rbinom(1,1, db$simple_belief[n])
  db$weighted_binary[n] <- rbinom(1,1, db$weighted_belief[n])
}

data_SB <- list(
  N = nrow(db),
  y = db$simple_binary,
  Source1 = db$Source1,
  Source2 = db$Source2
)

data_WB <- list(
  N = nrow(db),
  y = db$weighted_binary,
  Source1 = db$Source1,
  Source2 = db$Source2
)

## On the simple data
simple2simple <- mod_simpleBayes$sample(
  data = data_SB,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  threads_per_chain = 2,
  iter_warmup = 1500,
  iter_sampling = 3000,
  refresh = 500
)

weighted2simple <- mod_wb$sample(
  data = data_SB,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  threads_per_chain = 2,
  iter_warmup = 1500,
  iter_sampling = 3000,
  refresh = 500
)

simple2weighted <- mod_simpleBayes$sample(
  data = data_WB,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  threads_per_chain = 2,
  iter_warmup = 1500,
  iter_sampling = 3000,
  refresh = 500
)

weighted2weighted <- mod_wb$sample(
  data = data_WB,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  threads_per_chain = 2,
  iter_warmup = 1500,
  iter_sampling = 3000,
  refresh = 500
)

Loo_simple2simple <- simple2simple$loo(save_psis = TRUE, cores = 4)
p1 <- plot(Loo_simple2simple)

Loo_weighted2simple <- weighted2simple$loo(save_psis = TRUE, cores = 4)
p2 <- plot(Loo_weighted2simple)

Loo_simple2weighted <- simple2weighted$loo(save_psis = TRUE, cores = 4)
p3 <- plot(Loo_simple2weighted)

Loo_weighted2weighted <- weighted2weighted$loo(save_psis = TRUE, cores = 4)
p4 <- plot(Loo_weighted2weighted)


elpd <- tibble(
  n = seq(810),
  simple_diff_elpd = 
  Loo_simple2simple$pointwise[, "elpd_loo"] - 
  Loo_weighted2simple$pointwise[, "elpd_loo"],
  weighted_diff_elpd = 
  Loo_weighted2weighted$pointwise[, "elpd_loo"] -
  Loo_simple2weighted$pointwise[, "elpd_loo"])

p1 <- ggplot(elpd, aes(x = n, y = simple_diff_elpd)) +
  geom_point(alpha = .1) +
  #xlim(.5,1.01) +
  #ylim(-1.5,1.5) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  theme_bw()

p2 <- ggplot(elpd, aes(x = n, y = weighted_diff_elpd)) +
  geom_point(alpha = .1) +
  #xlim(.5,1.01) +
  #ylim(-1.5,1.5) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  theme_bw()

library(patchwork)
p1 + p2

loo_compare(Loo_simple2simple, Loo_weighted2simple)
loo_compare(Loo_weighted2weighted, Loo_simple2weighted)
loo_model_weights(list(Loo_simple2simple, Loo_weighted2simple))
loo_model_weights(list(Loo_weighted2weighted, Loo_simple2weighted))

```

## Multilevel

## Bonus: Build the Weighted Bayes Stan model (fancier formula)

```{r 09 fancy weighted bayes in stan}

stan_WB1_model <- "

functions{
  real weight_f(real L_raw, real w_raw) {
    real L;
    real w;
    L = exp(L_raw);
    w = 0.5 + inv_logit(w_raw)/2;
    return log((w * L + 1 - w)./((1 - w) * L + w));
  }
}


data {
  int<lower=0> N;
  array[N] int y;
  vector[N] Source1;
  vector[N] Source2;
}

parameters {
  real bias;
  real weight1;
  real weight2;
}

model {
  target += normal_lpdf(bias | 0, 1);
  target += normal_lpdf(weight1 | 0, 1.5);
  target += normal_lpdf(weight2 | 0, 1.5);
  
  for (n in 1:N){  
  target += bernoulli_logit_lpmf(y[n] | bias + weight_f(Source1[n], weight1) + weight_f(Source2[n], weight2));
  }
}

generated quantities{
  array[N] real log_lik;
  real bias_prior;
  real w1_prior;
  real w2_prior;
  real w1;
  real w2;
  
  bias_prior = normal_rng(0,1);
  w1_prior = normal_rng(0,1.5);
  w2_prior = normal_rng(0,1.5);
  
  w1_prior = 0.5 + inv_logit(normal_rng(0,1))/2;
  w2_prior = 0.5 + inv_logit(normal_rng(0,1))/2;
  w1 = 0.5 + inv_logit(weight1)/2;
  w2 = 0.5 + inv_logit(weight2)/2;
  
  for (n in 1:N){  
    log_lik[n] = bernoulli_logit_lpmf(y[n] | bias + weight_f(Source1[n], weight1) +
      weight_f(Source2[n], weight2));
  }
  
}

"

write_stan_file(
  stan_WB1_model,
  dir = "stan/",
  basename = "W9_WB1.stan")

file <- file.path("stan/W9_WB1.stan")
mod_wb1 <- cmdstan_model(file, cpp_options = list(stan_threads = TRUE),
                     stanc_options = list("O1"))

db1 <-  db %>% subset(w1 == 0.7 & w2 == 1) %>% mutate(
  l1 = logit_scaled(Source1),
  l2 = logit_scaled(Source2)
)



data_weightedBayes1 <- list(
  N = nrow(db1),
  y = db1$binary1,
  Source1 = logit_scaled(db1$Source1),
  Source2 = logit_scaled(db1$Source2)
)

samples_weighted1 <- mod_wb1$sample(
  data = data_weightedBayes1,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  threads_per_chain = 2,
  iter_warmup = 1500,
  iter_sampling = 3000,
  refresh = 500
)

```

## Evaluation
```{r 09 evaluate fancy weighted bayes model}

draws_df <- as_draws_df(samples_weighted1$draws())

ggplot(draws_df, aes(.iteration, bias, group = .chain, color = .chain)) +
  geom_line(alpha = 0.5) +
  theme_classic()

ggplot(draws_df, aes(.iteration, w1, group = .chain, color = .chain)) +
  geom_line(alpha = 0.5) +
  theme_classic()

ggplot(draws_df, aes(.iteration, w2, group = .chain, color = .chain)) +
  geom_line(alpha = 0.5) +
  theme_classic()

ggplot(draws_df) +
  geom_histogram(aes(bias), alpha = 0.6, fill = "lightblue") +
  geom_histogram(aes(bias_prior), alpha = 0.6, fill = "pink") +
  geom_vline(xintercept = db1$bias[1]) +
  theme_bw()

ggplot(draws_df) +
  geom_histogram(aes(w1), alpha = 0.6, fill = "lightblue") +
  geom_histogram(aes(w1_prior), alpha = 0.6, fill = "pink") +
  geom_vline(xintercept = db1$w1[1]) +
  theme_bw()

ggplot(draws_df) +
  geom_density(aes(w2), alpha = 0.6, fill = "lightblue") +
  geom_density(aes(w2_prior), alpha = 0.6, fill = "pink") +
  geom_vline(xintercept = db1$w2[1]) +
  theme_bw()

ggplot(draws_df) +
  geom_point(aes(w1, w2), alpha = 0.3) +
  theme_bw()
ggplot(draws_df) +
  geom_point(aes(bias, w1), alpha = 0.3) +
  theme_bw()
ggplot(draws_df) +
  geom_point(aes(bias, w2), alpha = 0.3) +
  theme_bw()
```

## Build a temporal Bayes (storing current belief as prior for next turn)

```{r 09 build a temporal bayes}

WeightedTimeBayes_f <- function(bias, Source1, Source2, w1, w2){
  w1 <- (w1 - 0.5)*2
  w2 <- (w2 - 0.5)*2
  outcome <- inv_logit_scaled(bias + w1 * logit_scaled(Source1) + w2 * logit_scaled(Source2))
  return(outcome)
}

bias <- 0
trials <- seq(10)
Source1 <- seq(0.1,0.9, 0.1)
w1 <- seq(0.5, 1, 0.1)
w2 <- seq(0.5, 1, 0.1)

db <- expand.grid(bias = bias, trials, Source1 = Source1, w1 = w1, w2 = w2) %>%
  mutate(Source2 = NA, belief = NA, binary = NA)

for (n in seq(nrow(db))) {
  if (n == 1) {db$Source2[1] = 0.5}
  db$belief[n] <- WeightedTimeBayes_f(db$bias[n], db$Source1[n], db$Source2[n],db$w1[n], db$w2[n])
  db$binary[n] <- rbinom(1,1, db$belief[n])
  if (n < nrow(db)) {db$Source2[n + 1] <- db$belief[n]}
}

stan_TB_model <- "
data {
  int<lower=0> N;
  array[N] int y;
  array[N] real <lower = 0, upper = 1> Source1; 
}

transformed data {
  array[N] real l_Source1;
  l_Source1 = logit(Source1);
}

parameters {
  real bias;
  // meaningful weights are btw 0.5 and 1 (theory reasons)
  real<lower = 0.5, upper = 1> w1; 
  real<lower = 0.5, upper = 1> w2;
}

transformed parameters {
  real<lower = 0, upper = 1> weight1;
  real<lower = 0, upper = 1> weight2;
  array[N] real l_Source2;

  // weight parameters are rescaled to be on a 0-1 scale (0 -> no effects; 1 -> face value)
  weight1 = (w1 - 0.5) * 2;  
  weight2 = (w2 - 0.5) * 2;
  
  l_Source2[1] = 0;
  
  for (n in 2:N){
    l_Source2[n] = bias + weight1 * l_Source1[n] + weight2 * l_Source2[n-1];
    }
}

model {
  target += normal_lpdf(bias | 0, 1);
  target += beta_lpdf(weight1 | 1, 1);
  target += beta_lpdf(weight2 | 1, 1);
  
  target += bernoulli_logit_lpmf(y[1] | bias + weight1 * l_Source1[1]);
  
  for (n in 2:N){  
    target += bernoulli_logit_lpmf(y[n] | l_Source2[n]);
  }
}

generated quantities{
  array[N] real log_lik;
  real bias_prior;
  real w1_prior;
  real w2_prior;
  bias_prior = normal_rng(0, 1) ;
  w1_prior = 0.5 + inv_logit(normal_rng(0, 1))/2 ;
  w2_prior = 0.5 + inv_logit(normal_rng(0, 1))/2 ;
  for (n in 1:N)
    log_lik[n]= bernoulli_logit_lpmf(y[n] | l_Source2[n]);
}
"

write_stan_file(
  stan_TB_model,
  dir = "stan/",
  basename = "W9_TB.stan")

file <- file.path("stan/W9_TB.stan")
mod_tb <- cmdstan_model(file, cpp_options = list(stan_threads = TRUE),
                     stanc_options = list("O1"))


db1 <- db %>% subset(w1 == 0.7 & w2 == 0.9)

data_TB <- list(
  N = nrow(db1),
  y = db1$binary,
  Source1 = db1$Source1
)

samples_TB <- mod_tb$sample(
  data = data_TB,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  threads_per_chain = 2,
  iter_warmup = 2000,
  iter_sampling = 3000,
  refresh = 500,
  adapt_delta = 0.99,
  max_treedepth = 20
)


```

## Evaluate
```{r 09 evaluate the temporal bayes model}
samples_TB$cmdstan_diagnose()
#samples_TB$loo()
samples_TB$summary()

draws_df <- as_draws_df(samples_TB$draws())

p1 <- ggplot(draws_df, aes(.iteration, bias, group = .chain, color = .chain)) +
  geom_line(alpha = 0.5) +
  theme_classic()

p2 <- ggplot(draws_df, aes(.iteration, w1, group = .chain, color = .chain)) +
  geom_line(alpha = 0.5) +
  theme_classic()

p3 <- ggplot(draws_df, aes(.iteration, w2, group = .chain, color = .chain)) +
  geom_line(alpha = 0.5) +
  theme_classic()

p1 + p2 + p3

p1 <- ggplot(draws_df) +
  geom_density(aes(bias), alpha = 0.6, fill = "lightblue") +
  geom_density(aes(bias_prior), alpha = 0.6, fill = "pink") +
  geom_vline(xintercept = db1$bias[1]) +
  theme_bw()

p2 <- ggplot(draws_df) +
  geom_density(aes(w1), alpha = 0.6, fill = "lightblue") +
  geom_density(aes(w1_prior), alpha = 0.6, fill = "pink") +
  geom_vline(xintercept = db1$w1[1]) +
  theme_bw()

p3 <- ggplot(draws_df) +
  geom_density(aes(w2), alpha = 0.6, fill = "lightblue") +
  geom_density(aes(w2_prior), alpha = 0.6, fill = "pink") +
  geom_vline(xintercept = db1$w2[1]) +
  theme_bw()

p1 + p2 + p3

p1 <- ggplot(draws_df) +
  geom_point(aes(w1, w2), alpha = 0.3) +
  theme_bw()
p2 <- ggplot(draws_df) +
  geom_point(aes(bias, w1), alpha = 0.3) +
  theme_bw()
p3 <- ggplot(draws_df) +
  geom_point(aes(bias, w2), alpha = 0.3) +
  theme_bw()

p1 + p2 + p3
```

[MISSING: model seems good at recovering. BUT HIGH correlations between weights and funnels, so we would probably be safer reparameterizing]

## Multilevel version of the simple bayes model

```{r 09 multilevel simple bayes}
stan_simpleBayes_ml_model <- "
data {
  int<lower=0> N;  // n of trials
  int<lower=0> S;  // n of participants
  array[N, S] int y;
  array[N, S] real<lower=0, upper = 1> Source1;
  array[N, S] real<lower=0, upper = 1> Source2;
}

transformed data{
  array[N, S] real l_Source1;
  array[N, S] real l_Source2;
  l_Source1 = logit(Source1);
  l_Source2 = logit(Source2);
}

parameters {
  real biasM;
  real biasSD;
  array[S] real z_bias;
}

transformed parameters {
  vector[S] biasC;
  vector[S] bias;
  biasC = biasSD * to_vector(z_bias);
  bias = biasM + biasC;
 }

model {
  target +=  normal_lpdf(biasM | 0, 1);
  target +=  normal_lpdf(biasSD | 0, 1)  -
    normal_lccdf(0 | 0, 1);
  target += std_normal_lpdf(to_vector(z_bias));
  
  for (s in 1:S){
  target +=  bernoulli_logit_lpmf(y[,s] | bias[s] + 
                                          to_vector(l_Source1[,s]) + 
                                          to_vector(l_Source2[,s]));
  }
}



"

write_stan_file(
  stan_simpleBayes_ml_model,
  dir = "stan/",
  basename = "W9_SimpleBayes_ml.stan")

file <- file.path("stan/W9_SimpleBayes_ml.stan")
mod_simpleBayes <- cmdstan_model(file, cpp_options = list(stan_threads = TRUE),
                     stanc_options = list("O1"))
```

## Multilevel version of the weighted bayes model

```{r 09 multilevel weighted bayes}
stan_WB_ml_model <- "
data {
  int<lower=0> N;  // n of trials
  int<lower=0> S;  // n of participants
  array[N, S] int y;
  array[N, S] real<lower=0, upper = 1> Source1;
  array[N, S] real<lower=0, upper = 1> Source2; 
}

transformed data {
  array[N,S] real l_Source1;
  array[N,S] real l_Source2;
  l_Source1 = logit(Source1);
  l_Source2 = logit(Source2);
}

parameters {
  real biasM;
  
  real w1_M; 
  real w2_M;
  
  vector<lower = 0>[3] tau;
  matrix[3, S] z_IDs;
  cholesky_factor_corr[3] L_u;
  
}

transformed parameters{
  matrix[S,3] IDs;
  IDs = (diag_pre_multiply(tau, L_u) * z_IDs)';
}

model {

  target += normal_lpdf(biasM | 0, 1);
  target +=  normal_lpdf(tau[1] | 0, 1)  -
    normal_lccdf(0 | 0, 1);
  target += normal_lpdf(w1_M | 0, 1);
  target +=  normal_lpdf(tau[2] | 0, 1)  -
    normal_lccdf(0 | 0, 1);
  target += normal_lpdf(w2_M | 0, 1);
  target +=  normal_lpdf(tau[3] | 0, 1)  -
    normal_lccdf(0 | 0, 1);
  target += lkj_corr_cholesky_lpdf(L_u | 3);
  
  target += std_normal_lpdf(to_vector(z_IDs));
    
  for (s in 1:S){
  for (n in 1:N){
    target += bernoulli_logit_lpmf(y[n,s] | biasM + IDs[s, 1] + 
          (w1_M + IDs[s, 2]) * l_Source1[n,s] + 
          (w2_M + IDs[s, 3]) * l_Source2[n,s]);
  }}
}

"

write_stan_file(
  stan_WB_ml_model,
  dir = "stan/",
  basename = "W9_WB_ml.stan")

file <- file.path("stan/W9_WB_ml.stan")
mod_wb <- cmdstan_model(file, cpp_options = list(stan_threads = TRUE),
                     stanc_options = list("O1"))
```


[MISSING: Fitting on real data and model comparison]