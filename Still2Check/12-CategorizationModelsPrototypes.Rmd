---
title: "12-CategorizationModelsPrototypes"
output: html_document
date: "2025-03-21"
---
# Prototype-Based Models of Categorization

## Theoretical Foundations

Prototype theory emerged as an alternative to both the classical view of categories (based on necessary and sufficient conditions) and exemplar-based accounts. The core idea is elegantly simple: rather than storing all individual exemplars in memory, the cognitive system abstracts a summary representation—a prototype—for each category. New items are then categorized based on their similarity to these prototypes.

### Core Assumptions of Prototype Models

1. **Category Abstraction**: Categories are represented by central tendencies or prototypes rather than collections of exemplars
2. **Economical Representation**: Only prototype information is stored, not individual exemplars
3. **Similarity-Based Decisions**: Categorization is based on similarity to category prototypes
4. **Typicality Effects**: Items more similar to the prototype are processed more fluently and judged as more typical

Unlike exemplar models, which maintain that people store individual instances, prototype models propose a more economical representation: the cognitive system extracts and stores the central tendency of the category. This provides a cognitively efficient way to represent categories while capturing many of the phenomena observed in human categorization.

## Prototype Models vs. Exemplar Models

The debate between prototype and exemplar theories has been one of the most productive in cognitive psychology. The key differences include:

| Aspect | Prototype Models | Exemplar Models |
|--------|------------------|-----------------|
| **Representation** | Abstract summary (central tendency) | Collection of individual instances |
| **Memory Requirements** | Economical (one prototype per category) | Potentially high (all exemplars) |
| **Typicality Prediction** | Items similar to prototype are most typical | Items similar to many exemplars are most typical |
| **Category Boundaries** | Smoother, based on distance from prototypes | Potentially more complex, based on similarity to all exemplars |
| **Unusual Members** | May not influence the prototype much | Explicitly represented and influence decisions |

## The Kalman Filter Approach to Prototype Learning

Traditional prototype models often assumed a static prototype computed as the average of all category members. However, this doesn't capture the dynamic nature of human learning, where we continuously update our category representations as we encounter new examples.

The Kalman filter provides an elegant mathematical framework for implementing a dynamic prototype model. Originally developed for tracking physical systems, the Kalman filter is ideal for modeling prototype learning because it:

1. **Updates incrementally** with each new observation
2. **Balances prior knowledge** with new evidence
3. **Maintains uncertainty** about the prototype location
4. **Adjusts learning rate** based on certainty

### Mathematical Formulation of the Kalman Filter Prototype Model

The Kalman filter prototype model tracks each category's prototype as a probability distribution rather than a fixed point. Specifically, for each category, we maintain:

- A prototype location (mean vector μ)
- An uncertainty measure (covariance matrix Σ)

#### 1. Initialization

For each category C, we initialize:
- Prototype location: μ₀ (often set to the first exemplar or a prior expectation)
- Uncertainty: Σ₀ (high initial uncertainty)

#### 2. Update Equations

When a new exemplar x is observed for category C, we update the prototype and uncertainty as follows:

**Prototype Update**:
μₜ = μₜ₋₁ + K(x - μₜ₋₁)

**Uncertainty Update**:
Σₜ = (I - K)Σₜ₋₁

**Kalman Gain**:
K = Σₜ₋₁(Σₜ₋₁ + R)⁻¹

Where:
- μₜ is the updated prototype location
- Σₜ is the updated uncertainty
- K is the Kalman gain (learning rate)
- R is the observation noise (constant or learned)

The Kalman gain K is crucial—it determines how much weight to give to the new observation versus the existing prototype. When uncertainty is high, K is larger, giving more weight to new observations. As uncertainty decreases with more observations, K decreases, making the prototype more stable.

#### 3. Categorization Decision

Given multiple categories, the probability of assigning a new stimulus x to category C is given by:

P(C|x) ∝ exp(-0.5(x - μ)ᵀ(Σ + R)⁻¹(x - μ))

This is based on the multivariate normal density, essentially measuring how likely the observation x is under category C's prototype distribution.

## Implementing the Kalman Filter Prototype Model

Let's implement a Kalman filter prototype model in R. For simplicity, we'll start with a univariate case and then extend to the multivariate case needed for our categorization task.

### Univariate Kalman Filter (for clear explanation)

```r
# Simple univariate Kalman filter for tracking a prototype
kalman_update <- function(mu_prev, sigma_prev, observation, r) {
  # Calculate Kalman gain
  k <- sigma_prev / (sigma_prev + r)
  
  # Update mean (prototype location)
  mu_new <- mu_prev + k * (observation - mu_prev)
  
  # Update variance (uncertainty)
  sigma_new <- (1 - k) * sigma_prev
  
  # Return updated values
  return(list(mu = mu_new, sigma = sigma_new, k = k))
}
```

This function:
1. Takes the current prototype (mu_prev), uncertainty (sigma_prev), and a new observation
2. Calculates the Kalman gain (k) based on current uncertainty
3. Updates the prototype by moving it toward the observation, weighted by the gain
4. Updates the uncertainty, which always decreases with more observations
5. Returns the updated prototype, uncertainty, and gain

The beauty of the Kalman filter is that the gain automatically adapts: when uncertainty is high (early learning), the gain is high, leading to larger updates. As uncertainty decreases (more observations), the gain decreases, making the prototype more stable.

### Multivariate Kalman Filter for Categorization

For categorization tasks with multiple feature dimensions, we need a multivariate version of the Kalman filter:

```r
# Multivariate Kalman filter for tracking category prototypes
multivariate_kalman_update <- function(mu_prev, sigma_prev, observation, r_matrix) {
  # Calculate Kalman gain
  # For diagonal R (independent dimensions), we can simplify the calculation
  k_matrix <- sigma_prev %*% solve(sigma_prev + r_matrix)
  
  # Update mean (prototype location)
  innovation <- observation - mu_prev
  mu_new <- mu_prev + k_matrix %*% innovation
  
  # Update covariance (uncertainty)
  n_dim <- length(mu_prev)
  sigma_new <- (diag(n_dim) - k_matrix) %*% sigma_prev
  
  # Return updated values
  return(list(mu = mu_new, sigma = sigma_new, k = k_matrix))
}
```

For our categorization task, we'll use this to track prototypes for each category:

```r
# Prototype model using Kalman filter for categorization
prototype_kalman <- function(r_value, obs, cat_one, quiet = TRUE) {
  # Create empty vector for response probabilities
  r <- c()
  
  n_trials <- nrow(obs)
  n_features <- ncol(obs)
  
  # Initialize prototypes for each category
  prototype_cat_0 <- list(
    mu = rep(0, n_features),  # Initial prototype location
    sigma = diag(10, n_features)  # Initial uncertainty (high)
  )
  
  prototype_cat_1 <- list(
    mu = rep(0, n_features),  # Initial prototype location
    sigma = diag(10, n_features)  # Initial uncertainty (high)
  )
  
  # Observation noise (fixed for simplicity, could be learned)
  r_matrix <- diag(r_value, n_features)
  
  # Process each trial
  for (i in 1:n_trials) {
    # Debug info
    if (!quiet && i %% 10 == 0) {
      print(paste("i =", i))
    }
    
    # If first trial or no examples of one category, set probability to 0.5
    if (i == 1 || sum(cat_one[1:(i-1)]) == 0 || sum(cat_one[1:(i-1)]) == (i-1)) {
      r <- c(r, 0.5)
    } else {
      # Calculate probabilities based on distance to each prototype
      current_obs <- as.numeric(obs[i, ])
      
      # Calculate distance from observation to prototypes (Mahalanobis distance)
      sigma_cat_0 <- prototype_cat_0$sigma + r_matrix
      dist_cat_0 <- mahalanobis(current_obs, prototype_cat_0$mu, solve(sigma_cat_0))
      
      sigma_cat_1 <- prototype_cat_1$sigma + r_matrix
      dist_cat_1 <- mahalanobis(current_obs, prototype_cat_1$mu, solve(sigma_cat_1))
      
      # Convert distances to probabilities using softmax
      # Negative distance because smaller distance means higher probability
      prob_cat_1 <- exp(-0.5 * dist_cat_1) / (exp(-0.5 * dist_cat_0) + exp(-0.5 * dist_cat_1))
      r <- c(r, prob_cat_1)
    }
    
    # Update prototype for the correct category after decision
    if (i < n_trials) {  # No need to update after the last trial
      if (cat_one[i] == 1) {
        # Update category 1 prototype
        update <- multivariate_kalman_update(
          prototype_cat_1$mu, 
          prototype_cat_1$sigma, 
          as.numeric(obs[i, ]), 
          r_matrix
        )
        prototype_cat_1$mu <- update$mu
        prototype_cat_1$sigma <- update$sigma
      } else {
        # Update category 0 prototype
        update <- multivariate_kalman_update(
          prototype_cat_0$mu, 
          prototype_cat_0$sigma, 
          as.numeric(obs[i, ]), 
          r_matrix
        )
        prototype_cat_0$mu <- update$mu
        prototype_cat_0$sigma <- update$sigma
      }
    }
  }
  
  # Return binary responses based on probabilities
  return(rbinom(n_trials, 1, r))
}
```

Let's break down this implementation:

1. **Initialization**:
   - We start with uninformative prototypes (centered at 0 with high uncertainty)
   - The observation noise (r_matrix) determines how much variance we expect around the prototype

2. **Decision Process**:
   - For each new stimulus, we calculate its Mahalanobis distance to each category's prototype
   - The Mahalanobis distance accounts for both the prototype location and uncertainty
   - We convert these distances to probabilities using a softmax function

3. **Prototype Update**:
   - After each trial, we update the prototype of the correct category using the Kalman filter
   - The update moves the prototype toward the new observation and reduces uncertainty
   - The amount of movement depends on the current uncertainty level

4. **Learning Dynamics**:
   - Early in learning, large updates occur due to high uncertainty
   - As learning progresses, updates become smaller, stabilizing the prototypes
   - Eventually, the prototypes converge to the category centers

### The Observation Noise Parameter

The observation noise parameter `r_value` has an important cognitive interpretation:

- Small r_value: Assumes observations are very precise; prototypes move less and become more certain quickly
- Large r_value: Assumes observations have high variability; prototypes move more and remain uncertain longer

This parameter can be interpreted as representing the learner's assumptions about category variability or their perceptual noise.

## Simulating Categorization Behavior with the Prototype Model

Let's now simulate categorization behavior using our prototype model with the same experimental setup we used for the GCM:

```r
# Function to simulate responses using the prototype model
simulate_prototype_responses <- function(agent, r_value) {
    observations <- experiment %>%
        select(c("height", "position"))
    
    category <- experiment$category
    
    # Simulate responses
    responses <- prototype_kalman(
        r_value,
        observations,
        category
    )
    
    # Record results
    tmp_simulated_responses <- experiment %>%
        mutate(
            trial = seq(nrow(experiment)),
            sim_response = responses,
            correct = ifelse(category == sim_response, 1, 0),
            performance = cumsum(correct) / seq_along(correct),
            r_value = r_value,
            agent = agent
        )

    return(tmp_simulated_responses)
}

# Simulate responses across different r_values
plan(multisession, workers = availableCores())

param_df <- dplyr::tibble(
    expand_grid(
        agent = 1:10,
        r_value = c(0.1, 0.5, 1.0, 2.0, 5.0)
    )
)

prototype_responses <- future_pmap_dfr(param_df,
    simulate_prototype_responses,
    .options = furrr_options(seed = TRUE)
)
```

We can visualize how the observation noise parameter affects performance:

```r
prototype_responses %>%
  mutate(r_value = as.factor(r_value)) %>%
  ggplot(aes(trial, performance, group = r_value, color = r_value)) +
  geom_smooth() +
  theme_bw() +
  labs(
    title = "Categorization Performance with Prototype Model",
    subtitle = "Effect of observation noise parameter",
    x = "Trial",
    y = "Proportion Correct",
    color = "r-value"
  )
```

## Visualizing Prototype Learning

To better understand how prototypes evolve over time, let's visualize the prototype locations and uncertainty throughout learning:

```r
# Function to track prototype evolution
track_prototypes <- function(r_value, obs, cat_one) {
  n_trials <- nrow(obs)
  n_features <- ncol(obs)
  
  # Initialize prototypes
  prototype_cat_0 <- list(
    mu = rep(0, n_features),
    sigma = diag(10, n_features)
  )
  
  prototype_cat_1 <- list(
    mu = rep(0, n_features),
    sigma = diag(10, n_features)
  )
  
  # Observation noise
  r_matrix <- diag(r_value, n_features)
  
  # Storage for tracking prototype evolution
  prototype_history <- tibble(
    trial = integer(),
    category = integer(),
    feature1_mean = numeric(),
    feature2_mean = numeric(),
    feature1_sd = numeric(),
    feature2_sd = numeric()
  )
  
  # Process each trial
  for (i in 1:n_trials) {
    # Store current prototype state
    prototype_history <- prototype_history %>% add_row(
      trial = i,
      category = 0,
      feature1_mean = prototype_cat_0$mu[1],
      feature2_mean = prototype_cat_0$mu[2],
      feature1_sd = sqrt(prototype_cat_0$sigma[1,1]),
      feature2_sd = sqrt(prototype_cat_0$sigma[2,2])
    ) %>% add_row(
      trial = i,
      category = 1,
      feature1_mean = prototype_cat_1$mu[1],
      feature2_mean = prototype_cat_1$mu[2],
      feature1_sd = sqrt(prototype_cat_1$sigma[1,1]),
      feature2_sd = sqrt(prototype_cat_1$sigma[2,2])
    )
    
    # Update prototype for the correct category
    if (i < n_trials) {  # No need to update after the last trial
      if (cat_one[i] == 1) {
        # Update category 1 prototype
        update <- multivariate_kalman_update(
          prototype_cat_1$mu, 
          prototype_cat_1$sigma, 
          as.numeric(obs[i, ]), 
          r_matrix
        )
        prototype_cat_1$mu <- update$mu
        prototype_cat_1$sigma <- update$sigma
      } else {
        # Update category 0 prototype
        update <- multivariate_kalman_update(
          prototype_cat_0$mu, 
          prototype_cat_0$sigma, 
          as.numeric(obs[i, ]), 
          r_matrix
        )
        prototype_cat_0$mu <- update$mu
        prototype_cat_0$sigma <- update$sigma
      }
    }
  }
  
  return(prototype_history)
}

# Track prototypes for visualization
prototype_trajectory <- track_prototypes(
  r_value = 1.0,
  obs = as.matrix(experiment[, c("height", "position")]),
  cat_one = experiment$category
)

# Visualize prototype evolution
ggplot() +
  # Plot stimuli
  geom_point(data = stimuli, 
             aes(position, height, color = category), 
             size = 3, alpha = 0.5) +
  
  # Plot prototype trajectory
  geom_path(data = prototype_trajectory, 
            aes(feature2_mean, feature1_mean, group = category, color = as.factor(category)),
            linetype = "dashed", arrow = arrow(type = "closed", length = unit(0.1, "inches"))) +
  
  # Plot final prototypes with uncertainty
  geom_point(data = prototype_trajectory %>% group_by(category) %>% filter(trial == max(trial)),
             aes(feature2_mean, feature1_mean, color = as.factor(category)),
             size = 5) +
  
  # Add uncertainty ellipses
  stat_ellipse(data = prototype_trajectory %>% group_by(category) %>% filter(trial == max(trial)),
               aes(feature2_mean, feature1_mean, color = as.factor(category)),
               type = "norm", level = 0.68, geom = "polygon", alpha = 0.2) +
  
  # Labels and theme
  labs(
    title = "Prototype Learning with Kalman Filter",
    subtitle = "Dashed lines show prototype trajectory, ellipses show final uncertainty",
    x = "Position",
    y = "Height",
    color = "Category"
  ) +
  theme_minimal()
```

This visualization shows:
1. The stimulus space with actual category members
2. The trajectory of each prototype as it updates with new observations
3. The final prototype locations with uncertainty ellipses

Notice how the prototypes start at (0,0) and gradually move toward the center of each category. The uncertainty ellipses show the model's confidence about each prototype's location.

## Implementing the Prototype Model in Stan

To estimate the parameters of our prototype model from observed categorization data, we'll implement it in Stan:

```stan
// Prototype Model using Kalman Filter

data {
  int<lower=1> ntrials;               // Number of trials
  int<lower=1> nfeatures;             // Number of feature dimensions
  array[ntrials] int<lower=0, upper=1> cat_one;  // True category labels
  array[ntrials] int<lower=0, upper=1> y;        // Observed decisions
  array[ntrials, nfeatures] real obs;  // Stimulus features
  real<lower=0, upper=1> b;           // Response bias
}

parameters {
  real<lower=0> r_value;              // Observation noise
}

transformed parameters {
  // Response probabilities
  array[ntrials] real<lower=0.0001, upper=0.9999> p;
  
  // Initialize prototypes and uncertainties
  vector[nfeatures] mu_cat0 = rep_vector(0, nfeatures);
  vector[nfeatures] mu_cat1 = rep_vector(0, nfeatures);
  matrix[nfeatures, nfeatures] sigma_cat0 = diag_matrix(rep_vector(10.0, nfeatures));
  matrix[nfeatures, nfeatures] sigma_cat1 = diag_matrix(rep_vector(10.0, nfeatures));
  
  // Observation noise matrix
  matrix[nfeatures, nfeatures] r_matrix = diag_matrix(rep_vector(r_value, nfeatures));
  
  // Process trials sequentially
  for (i in 1:ntrials) {
    // Extract current observation
    vector[nfeatures] current_obs = to_vector(obs[i]);
    
    // Calculate response probability based on current prototypes
    if (i == 1 || sum(cat_one[1:(i-1)]) == 0 || sum(cat_one[1:(i-1)]) == (i-1)) {
      // No examples of one category, use 0.5
      p[i] = 0.5;
    } else {
      // Calculate distances to prototypes
      matrix[nfeatures, nfeatures] cov_cat0 = sigma_cat0 + r_matrix;
      matrix[nfeatures, nfeatures] cov_cat1 = sigma_cat1 + r_matrix;
      
      real dist_cat0 = (current_obs - mu_cat0)' * inverse(cov_cat0) * (current_obs - mu_cat0);
      real dist_cat1 = (current_obs - mu_cat1)' * inverse(cov_cat1) * (current_obs - mu_cat1);
      
      // Convert to probabilities
      p[i] = b * exp(-0.5 * dist_cat1) / (b * exp(-0.5 * dist_cat1) + (1-b) * exp(-0.5 * dist_cat0));
      
      // Bound probabilities for numerical stability
      if (p[i] < 0.0001) p[i] = 0.0001;
      if (p[i] > 0.9999) p[i] = 0.9999;
    }
    
    // After decision, update prototype for correct category (except on last trial)
    if (i < ntrials) {
      if (cat_one[i] == 1) {
        // Update category 1 prototype
        // Kalman gain calculation
        matrix[nfeatures, nfeatures] k_matrix = sigma_cat1 * inverse(sigma_cat1 + r_matrix);
        
        // Update mean
        vector[nfeatures] innovation = current_obs - mu_cat1;
        mu_cat1 = mu_cat1 + k_matrix * innovation;
        
        // Update covariance
        sigma_cat1 = (diag_matrix(rep_vector(1.0, nfeatures)) - k_matrix) * sigma_cat1;
      } else {
        // Update category 0 prototype
        // Kalman gain calculation
        matrix[nfeatures, nfeatures] k_matrix = sigma_cat0 * inverse(sigma_cat0 + r_matrix);
        
        // Update mean
        vector[nfeatures] innovation = current_obs - mu_cat0;
        mu_cat0 = mu_cat0 + k_matrix * innovation;
        
        // Update covariance
        sigma_cat0 = (diag_matrix(rep_vector(1.0, nfeatures)) - k_matrix) * sigma_cat0;
      }
    }
  }
}

model {
  // Prior for observation noise
  target += gamma_lpdf(r_value | 1, 1);
  
  // Likelihood
  target += bernoulli_lpmf(y | p);
}

generated quantities {
  // Log likelihood for model comparison
  array[ntrials] real log_lik;
  for (i in 1:ntrials) {
    log_lik[i] = bernoulli_lpmf(y[i] | p[i]);
  }
  
  // Final prototype locations and uncertainties
  array[nfeatures] real final_prototype_cat0 = to_array_1d(mu_cat0);
  array[nfeatures] real final_prototype_cat1 = to_array_1d(mu_cat1);
  
  // Generate predictions
  array[ntrials] int pred;
  for (i in 1:ntrials) {
    pred[i] = bernoulli_rng(p[i]);
  }
}
```

This Stan implementation:
1. Takes observed categorization decisions and stimuli as input
2. Estimates the observation noise parameter from the data
3. Implements the same Kalman filter prototype updating as our R model
4. Calculates response probabilities based on similarity to prototypes
5. Returns final prototype locations and predictions

We can fit this model to behavioral data and compare it to the GCM to see which better describes human categorization behavior.

## Comparing Prototype and Exemplar Models

To directly compare the prototype (Kalman filter) model with the exemplar (GCM) model, we can:

1. Fit both models to the same data
2. Compare their fit using methods like LOO-CV
3. Analyze which model better captures human categorization patterns

One way to visualize the difference between these models is to look at their decision boundaries:

```r
# Create a grid of points in the stimulus space
grid_points <- expand.grid(
  position = seq(min(stimuli$position) - 0.5, max(stimuli$position) + 0.5, length.out = 50),
  height = seq(min(stimuli$height) - 0.5, max(stimuli$height) + 0.5, length.out = 50)
)

# Function to get prototype model predictions for grid points
get_prototype_predictions <- function(r_value, training_obs, training_cat) {
  # Train the model on observed data
  n_features <- ncol(training_obs)
  n_trials <- nrow(training_obs)
  
  # Initialize prototypes
  prototype_cat_0 <- list(
    mu = rep(0, n_features),
    sigma = diag(10, n_features)
  )
  
  prototype_cat_1 <- list(
    mu = rep(0, n_features),
    sigma = diag(10, n_features)
  )
  
  # Observation noise
  r_matrix <- diag(r_value, n_features)
  
  # Train model on observed data
  for (i in 1:n_trials) {
    if (training_cat[i] == 1) {
      # Update category 1 prototype
      update <- multivariate_kalman_update(
        prototype_cat_1$mu, 
        prototype_cat_1$sigma, 
        as.numeric(training_obs[i, ]), 
        r_matrix
      )
      prototype_cat_1$mu <- update$mu
      prototype_cat_1$sigma <- update$sigma
    } else {
      # Update category 0 prototype
      update <- multivariate_kalman_update(
        prototype_cat_0$mu, 
        prototype_cat_0$sigma, 
        as.numeric(training_obs[i, ]), 
        r_matrix
      )
      prototype_cat_0$mu <- update$mu
      prototype_cat_0$sigma <- update$sigma
    }
  }
  
  # Get predictions for grid points
  predictions <- apply(as.matrix(grid_points), 1, function(point) {
    # Calculate distances to prototypes
    sigma_cat_0 <- prototype_cat_0$sigma + r_matrix
    dist_cat_0 <- mahalanobis(point, prototype_cat_0$mu, solve(sigma_cat_0))
    
    sigma_cat_1 <- prototype_cat_1$sigma + r_matrix
    dist_cat_1 <- mahalanobis(point, prototype_cat_1$mu, solve(sigma_cat_1))
    
    # Convert to probability
    prob_cat_1 <- exp(-0.5 * dist_cat_1) / (exp(-0.5 * dist_cat_0) + exp(-0.5 * dist_cat_1))
    return(prob_cat_1)
  })
  
  return(predictions)
}

# Function to get GCM predictions for grid points
get_gcm_predictions <- function(w, c, training_obs, training_cat) {
  # Get predictions for grid points
  predictions <- apply(as.matrix(grid_points), 1, function(point) {
    similarities <- numeric(nrow(training_obs))
    
    # Calculate similarity to all training exemplars
    for (i in 1:nrow(training_obs)) {
      sim <- similarity(distance(point, as.numeric(training_obs[i,]), w), c)
      similarities[i] <- sim
    }
    
    # Calculate probability of category 1
    numerator <- mean(similarities[training_cat == 1])
    denominator <- mean(similarities[training_cat == 1]) + mean(similarities[training_cat == 0])
    
    if (denominator == 0) return(0.5)  # Avoid division by zero
    return(numerator / denominator)
  })
  
  return(predictions)
}

# Get predictions for both models
prototype_preds <- get_prototype_predictions(
  r_value = 1.0,
  training_obs = as.matrix(stimuli[, c("height", "position")]),
  training_cat = as.numeric(as.character(stimuli$category))
)

gcm_preds <- get_gcm_predictions(
  w = c(0.5, 0.5),
  c = 1.0,
  training_obs = as.matrix(stimuli[, c("height", "position")]),
  training_cat = as.numeric(as.character(stimuli$category))
)

# Create visualization data
decision_data <- grid_points %>%
  mutate(
    prototype_prob = prototype_preds,
    gcm_prob = gcm_preds,
    prototype_decision = prototype_prob > 0.5,
    gcm_decision = gcm_prob > 0.5
  )

# Visualize decision boundaries
p1 <- ggplot() +
  # Background colors for decision regions
  geom_tile(data = decision_data, 
            aes(position, height, fill = prototype_decision), 
            alpha = 0.3) +
  
  # Decision boundary contour
  stat_contour(data = decision_data, 
               aes(position, height, z = prototype_prob),
               breaks = 0.5, color = "black", size = 1) +
  
  # Actual stimuli
  geom_point(data = stimuli, 
             aes(position, height, color = category),
             size = 3) +
  
  # Labels and theme
  scale_fill_manual(values = c("FALSE" = "tomato", "TRUE" = "skyblue")) +
  labs(
    title = "Prototype Model Decision Boundary",
    x = "Position",
    y = "Height",
    fill = "Category 1",
    color = "True Category"
  ) +
  theme_minimal() +
  theme(legend.position = "none")

p2 <- ggplot() +
  # Background colors for decision regions
  geom_tile(data = decision_data, 
            aes(position, height, fill = gcm_decision), 
            alpha = 0.3) +
  
  # Decision boundary contour
  stat_contour(data = decision_data, 
               aes(position, height, z = gcm_prob),
               breaks = 0.5, color = "black", size = 1) +
  
  # Actual stimuli
  geom_point(data = stimuli, 
             aes(position, height, color = category),
             size = 3) +
  
  # Labels and theme
  scale_fill_manual(values = c("FALSE" = "tomato", "TRUE" = "skyblue")) +
  labs(
    title = "Exemplar Model (GCM) Decision Boundary",
    x = "Position",
    y = "Height",
    fill = "Category 1",
    color = "True Category"
  ) +
  theme_minimal() +
  theme(legend.position = "none")

# Compare the two models side by side
p1 + p2
```

This visualization highlights a key difference between prototype and exemplar models: the shape of their decision boundaries. The prototype model tends to create smoother, more regular boundaries based on distance from centroids. The exemplar model can create more complex boundaries that conform to the specific distribution of training examples.

## Cognitive Insights from the Prototype Model

The Kalman filter approach to prototype learning offers several important cognitive insights:

### 1. Incremental Learning

Unlike traditional prototype models that compute a static average, the Kalman filter model captures the dynamic, incremental nature of human learning. People don't wait to see all examples before forming a category representation—they update their understanding with each new example.

### 2. Uncertainty-Driven Learning

The Kalman gain modulates learning based on uncertainty, mirroring how humans learn faster when their knowledge is uncertain and more slowly as they become confident. This creates a natural "fast-then-slow" learning curve similar to what we observe in human behavior.

### 3. Selective Attention Emerges Naturally

Though we didn't implement it explicitly, the Kalman filter can naturally develop different levels of certainty along different feature dimensions. This creates an emergent form of selective attention without requiring explicit attention parameters like in the GCM.

### 4. Memory Efficiency

Prototype models provide a computationally efficient account of categorization, storing only summary statistics rather than individual exemplars. This aligns with the fact that humans can effectively categorize even when their memory for specific examples is poor.

## Contrasting with Exemplar Models

The Kalman filter prototype model contrasts with exemplar models in several key ways:

1. **Memory Requirements**: The prototype model stores only means and covariance matrices—a fixed memory footprint regardless of category size. The exemplar model's memory requirements grow linearly with the number of examples.

2. **Abstraction**: The prototype model abstracts away individual examples, focusing on the central tendency. The exemplar model preserves the details of each individual example.

3. **Decision Boundaries**: Prototype models typically produce smoother, more regular decision boundaries based on distance from category centers. Exemplar models can produce more complex boundaries shaped by the specific distribution of examples.

4. **Behavioral Predictions**:
   - Prototype models predict that the most typical (central) members will be categorized most easily
   - Exemplar models predict advantages for distinctive or isolated exemplars
   - Prototype models predict poorer memory for specific examples
   - Exemplar models predict a close link between recognition and categorization

5. **Forgetting**: The prototype model naturally accommodates forgetting of specific examples, while the exemplar model would need an explicit forgetting mechanism.

## Strengths of the Prototype Approach

The prototype approach has several strengths as a model of human categorization:

1. **Cognitive Efficiency**: Prototypes provide an efficient summary of category information, requiring minimal memory resources.

2. **Handling Noise**: By averaging across examples, prototype models naturally handle noisy or variable data.

3. **Graceful Degradation**: Prototype representations remain robust even when specific exemplars are forgotten.

4. **Explanation of Typicality Effects**: Prototype models naturally explain why typical category members are processed more fluently.

5. **Good Fit for Natural Categories**: Many natural categories have a graded, prototype structure (e.g., birds, furniture) that prototype models capture well.

## Limitations of the Prototype Approach

Despite its strengths, the prototype approach also has important limitations:

1. **Difficulty with Complex Categories**: Prototype models struggle with categories that have complex internal structure, such as those defined by rules or relations.

2. **Limited Use of Distributional Information**: By focusing on central tendency, traditional prototype models ignore useful information about the distribution of features.

3. **Insensitivity to Specific Examples**: Prototype models can't easily account for cases where specific examples strongly influence categorization decisions.

4. **Challenge of Disjunctive Categories**: Categories with multiple distinct clusters (e.g., the category of "games") are difficult for single-prototype models to handle.

## Extensions to the Basic Model

Several extensions can address some of these limitations:

1. **Multiple Prototypes per Category**: Allow categories to be represented by multiple prototypes, better handling disjunctive categories.

2. **Feature Correlations**: Explicitly model correlations between features in the prototype representation.

3. **Hierarchical Structure**: Implement hierarchical prototype models to capture taxonomic category structures.

4. **Mixture of Prototypes and Exemplars**: Combine elements of both approaches, using prototypes for some categories and exemplars for others.

## Summary: The Kalman Filter Prototype Model

The Kalman filter prototype model offers a dynamic approach to category learning that maintains the cognitive efficiency of prototype models while addressing some of their limitations. Key features include:

1. **Incremental Learning**: Categories are learned one example at a time, with each new example updating the prototype.

2. **Uncertainty Representation**: The model maintains uncertainty about the prototype location, allowing it to adapt its learning rate appropriately.

3. **Computational Efficiency**: Only summary statistics (means and covariance matrices) are stored, not individual examples.

4. **Principled Probabilistic Foundation**: The model is grounded in Bayesian principles, providing a mathematically rigorous basis for category learning.

In the next section, we'll explore rule-based models, which take a fundamentally different approach to categorization by focusing on explicit decision rules rather than similarity.

## Parameter Recovery Analysis for the Prototype Model

To validate our prototype model implementation, we should perform parameter recovery analysis. This involves:

1. Generating synthetic data with known parameter values
2. Fitting the model to recover these parameters
3. Comparing recovered parameters to the true generating values

Here, we'll focus on recovering the observation noise parameter (r_value), which is the key parameter in our Kalman filter prototype model:

```r
# Function to simulate data with known r_value
generate_prototype_data <- function(true_r_value) {
  # Use the same experimental setup as before
  obs <- as.matrix(experiment[, c("height", "position")])
  cat_one <- experiment$category
  
  # Generate responses using the prototype model with known r_value
  responses <- prototype_kalman(true_r_value, obs, cat_one)
  
  # Return the dataset
  return(list(
    responses = responses,
    true_r_value = true_r_value,
    obs = obs,
    cat_one = cat_one
  ))
}

# Function to prepare data for Stan
prepare_prototype_stan_data <- function(data) {
  list(
    ntrials = length(data$responses),
    nfeatures = ncol(data$obs),
    cat_one = data$cat_one,
    y = data$responses,
    obs = data$obs,
    b = 0.5  # Assuming no response bias
  )
}

# Generate synthetic data across a range of r_values
r_values_to_test <- c(0.1, 0.5, 1.0, 2.0, 5.0)
recovery_results <- tibble(
  true_r_value = numeric(),
  estimated_r_value = numeric(),
  est_lower = numeric(),
  est_upper = numeric()
)

# For each true r_value, generate data and fit the model
for (r_val in r_values_to_test) {
  # Generate synthetic data
  synth_data <- generate_prototype_data(r_val)
  
  # Prepare for Stan
  stan_data <- prepare_prototype_stan_data(synth_data)
  
  # Fit model (assuming the Stan model is already compiled)
  fit <- prototype_stan_model$sample(
    data = stan_data,
    seed = 123,
    chains = 2,
    parallel_chains = 2,
    iter_warmup = 1000,
    iter_sampling = 1000,
    refresh = 0
  )
  
  # Extract posterior for r_value
  draws <- as_draws_df(fit$draws("r_value"))
  estimate <- mean(draws$r_value)
  ci <- quantile(draws$r_value, c(0.025, 0.975))
  
  # Store results
  recovery_results <- recovery_results %>% add_row(
    true_r_value = r_val,
    estimated_r_value = estimate,
    est_lower = ci[1],
    est_upper = ci[2]
  )
}

# Visualize parameter recovery
ggplot(recovery_results, aes(x = true_r_value, y = estimated_r_value)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = est_lower, ymax = est_upper), width = 0.1) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  labs(
    title = "Parameter Recovery for Prototype Model",
    subtitle = "Error bars show 95% credible intervals",
    x = "True r-value",
    y = "Estimated r-value"
  ) +
  theme_minimal()
```

Good parameter recovery would show estimated values close to the true values (points near the diagonal line) with reasonable uncertainty (error bars that include the true value). Poor recovery would indicate potential issues with the model's identifiability or implementation.

## Conclusion: The Prototype and Exemplar Debate

The debate between prototype and exemplar theories of categorization has been one of the most productive in cognitive psychology, leading to refined theories and empirical tests that have deepened our understanding of human categorization.

Current evidence suggests that neither approach alone fully accounts for human categorization behavior:

1. Humans show prototype effects, categorizing items more quickly and accurately when they're close to the category center.

2. Humans also show exemplar effects, being influenced by specific, distinctive examples and showing correlations between recognition and categorization performance.

3. Many researchers now favor hybrid or multiple-system accounts, where prototype-based and exemplar-based processes coexist and potentially interact.

The Kalman filter implementation of prototype learning provides a dynamic, uncertainty-sensitive approach that addresses some criticisms of traditional prototype models while maintaining their cognitive efficiency. By integrating ideas from Bayesian learning theory with classic prototype models, this approach offers a sophisticated account of category learning that can be directly compared with exemplar models like the GCM.

The Prototype-Exemplar Spectrum and Multiple-Prototype Models
It's important to recognize that prototype and exemplar models are not strictly distinct approaches, but rather represent two ends of a theoretical spectrum. The core difference lies in the granularity of representation: exemplar models store all individual instances, while traditional prototype models store a single central tendency. However, we can imagine a continuum of intermediate approaches:

Pure exemplar models: Store all instances with no abstraction
Clustered exemplar models: Store instances but group similar ones
Multiple-prototype models: Store several prototypes per category
Single-prototype models: Store one prototype per category

Multiple-prototype models offer an appealing middle ground that maintains much of the computational efficiency of prototype models while capturing more complex category structures. There are several ways to extend our Kalman filter approach to implement multiple centroids per category:

K-means clustering: First cluster the exemplars of each category into k subclusters, then apply the Kalman filter separately to each cluster
Mixture of Gaussians: Represent each category as a mixture of Gaussian distributions, with each component tracking a different subcategory prototype
Adaptive resonance theory: Dynamically create new prototypes when an observation is too dissimilar from existing prototypes
Splitting criteria: Monitor the variance of exemplars around each prototype and split the prototype when variance exceeds a threshold

In the next section, we'll explore the third major approach to categorization: rule-based models, which represent a fundamentally different perspective on how humans organize the world into categories.
