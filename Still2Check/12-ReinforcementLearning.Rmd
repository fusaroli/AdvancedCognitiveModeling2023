---
title: "11 - Reinforcement Learning"
output: html_document
date: "2024-02-14"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(
    tidyverse,
    future,
    purrr,
    furrr,
    patchwork,
    brms,
    cmdstanr
)
```

# Reinforcement Learning

Reinforcement learning (RL) represents one of the most fundamental ways that organisms and artificial agents learn from experience. RL provides a nuanced framework for updating behavior based on feedback, which subsumes both biased random agents and the Win-Stay-Lose-Shift strategies we explored previously. In other words, extreme values of the model parameters (learning rate) are mathematically equivalent to the previous models. Rather than simply switching after failures, RL agents gradually adjust their expectations and behavior based on the difference between what they expected and what actually happened.
This approach closely mirrors how humans and animals learn in many situations. When we try a new restaurant, we don't just categorize it as "good" or "bad" - we develop a sense of how good it is compared to our expectations, and this shapes how likely we are to return. Similarly, when playing games or sports, we develop nuanced preferences for different strategies based on their history of success.
In this chapter, we'll implement a version of the Rescorla-Wagner model, one of the most influential RL models in cognitive science. This model, originally developed to explain animal learning, has proven remarkably useful in understanding human learning and decision-making across many domains. 

[Missing additional RL models]

```{r Functions}

softmax <- function(x, tau) {
  outcome = 1 / (1 + exp(-tau * x))
  return(outcome)
}

ValueUpdate = function(value, alpha, choice, feedback) {
  
  PE <- feedback - value
  
  v1 <- value[1] + alpha * (1 - choice) * (feedback - value[1])

  v2 <- value[2] + alpha * (choice) * (feedback - value[2])
  
  updatedValue <- c(v1, v2)
  
  return(updatedValue)
}
```

This softmax function converts raw value differences into choice probabilities. The tau parameter controls how deterministically the agent chooses the higher-valued option - high values of tau lead to more deterministic choice, while low values lead to more random exploration.

The ValueUpdate function implements the core learning mechanism. Let's break down how it works:

* It calculates the prediction error (PE) - the difference between what happened (feedback) and what was expected (value)

* It updates the value estimates using this PE, weighted by the learning rate (alpha)

* Only the chosen option's value gets updated, since we don't learn about options we didn't choose

## Defining parameters

```{r }
agents <- 100
trials <- 120
```

## Simulating with alpha 0.9 and p 0.9

```{r }
value <- c(0,0)
alpha <- 0.9
temperature <- 1
choice <- 0
feedback <- -1
p <- 0.9 # probability that choice 0 gives a prize (1-p is probability that choice 1 gives a prize)

ValueUpdate(value, alpha, choice, feedback)

d <- tibble(trial = rep(NA, trials),
            choice = rep(NA, trials), 
            value1 = rep(NA, trials), 
            value2 = rep(NA, trials), 
            feedback = rep(NA, trials))

Bot <- rbinom(trials, 1, p)

for (i in 1:trials) {
    
    choice <- 1 #rbinom(1, 1, softmax(value[2] - value[1], temperature))
    feedback <- ifelse(Bot[i] == choice, 1, -1)
    value <- ValueUpdate(value, alpha, choice, feedback)
    d$choice[i] <- choice
    d$value1[i] <- value[1]
    d$value2[i] <- value[2]
    d$feedback[i] <- feedback
  
}

d <- d %>% mutate(
  trial = seq(trials),
  prevFeedback = lead(feedback))

ggplot(subset(d, trial < 21)) + 
  geom_line(aes(trial, value1), color = "green") + 
  geom_line(aes(trial, value2), color = "blue") +
  geom_line(aes(trial, prevFeedback), color = "red") +
  theme_bw()
```

## Simulating with p = 0.75

```{r}
# Let's imagine a situation where the underlying rate is 0.75

alpha <- 0.9
temperature <- 5
choice <- 0
feedback <- -1
p <- 0.75 # probability that choice 0 gives a prize (1-p is probability that choice 1 gives a prize)



df <- NULL
n <- 1
for (temperature in c(0.01, 0.5, 1, 5, 10, 15)) {
  
  for (alpha in seq(0.1, 1, 0.1)) {
    
    value <- c(0,0)
    d <- tibble(trial = rep(NA, trials),
                choice = rep(NA, trials), 
                value1 = rep(NA, trials), 
                value2 = rep(NA, trials), 
                feedback = rep(NA, trials),
                alpha = rep(NA, trials),
                temperature = rep(NA, trials),
                agent = n)
    
    for (i in 1:trials) {
      
      choice <- rbinom(1, 1, softmax(value[2] - value[1], temperature))
      feedback <- ifelse(Bot[i] == choice, 1, -1)
      value <- ValueUpdate(value, alpha, choice, feedback)
      d$trial[i] <- i
      d$choice[i] <- choice
      d$value1[i] <- value[1]
      d$value2[i] <- value[2]
      d$feedback[i] <- feedback
      d$alpha[i] <- alpha
      d$temperature[i] <- temperature
      
    }
    if (exists("df")) {df <- rbind(df, d)} else {df <- d}
    n <- n + 1
  }
}
df <- df %>% group_by(alpha, temperature) %>% mutate(
  prevFeedback = lead(feedback))

d1 <- df %>% subset(trial < 21 & temperature == 0.01)

ggplot() + 
  geom_line(data = subset(d1, alpha == 1), aes(trial, prevFeedback), color = "red") +
  geom_line(data = subset(d1, alpha == 0.9), aes(trial, value2), color = "purple") +
  geom_line(data = subset(d1, alpha == 0.5), aes(trial, value2), color = "blue") +
  geom_line(data = subset(d1, alpha == 0.2), aes(trial, value2), color = "green") +
  theme_bw()

df <- df %>% group_by(alpha, temperature) %>% mutate(
  rate = cumsum(choice) / seq_along(choice),
  performance = cumsum(feedback) / seq_along(feedback)
)

ggplot(subset(df, trial < 41), aes(trial, performance, group = alpha, color = alpha)) +
  geom_line(alpha = 0.5) +
  facet_wrap(.~temperature) +
  theme_bw()

ggplot(subset(df, trial < 41), aes(trial, performance, group = temperature, color = temperature)) +
  geom_line(alpha = 0.5) +
  facet_wrap(.~alpha) +
  theme_bw()
```

## What about asymmetric learning?

[Missing: simulations of RL with different alphas for positive and negative feedback]

## Model fitting: symmetric RL

```{r}
d <- df %>% subset(alpha == 0.6 & temperature == 5)

data <- list(
  trials = trials,
  choice = d$choice + 1,
  feedback = d$feedback
)

stan_model <- "
data {
    int<lower=1> trials;
    array[trials] int<lower=1,upper=2> choice;
    array[trials] int<lower=-1,upper=1> feedback;
} 

transformed data {
  vector[2] initValue;  // initial values for V
  initValue = rep_vector(0.0, 2);
}

parameters {
    real<lower=0, upper=1> alpha; // learning rate
    real<lower=0, upper=20> temperature; // softmax inv.temp.
}

model {
    real pe;
    vector[2] value;
    vector[2] theta;
    
    target += uniform_lpdf(alpha | 0, 1);
    target += uniform_lpdf(temperature | 0, 20);
    
    value = initValue;
    
    for (t in 1:trials) {
        theta = softmax( temperature * value); // action prob. computed via softmax
        target += categorical_lpmf(choice[t] | theta);
        
        pe = feedback[t] - value[choice[t]]; // compute pe for chosen value only
        value[choice[t]] = value[choice[t]] + alpha * pe; // update chosen V
    }
    
}

generated quantities{
  real<lower=0, upper=1> alpha_prior;
  real<lower=0, upper=20> temperature_prior;
  
  real pe;
  vector[2] value;
  vector[2] theta;
  
  real log_lik;
  
  alpha_prior = uniform_rng(0,1);
  temperature_prior = uniform_rng(0,20);
  
  value = initValue;
  log_lik = 0;
  
  for (t in 1:trials) {
        theta = softmax( temperature * value); // action prob. computed via softmax
        log_lik = log_lik + categorical_lpmf(choice[t] | theta);
        
        pe = feedback[t] - value[choice[t]]; // compute pe for chosen value only
        value[choice[t]] = value[choice[t]] + alpha * pe; // update chosen V
    }
  
}
"

write_stan_file(
  stan_model,
  dir = "stan/",
  basename = "W11_RL_symmetric.stan")

file <- file.path("stan/W11_RL_symmetric.stan")
mod <- cmdstan_model(file, cpp_options = list(stan_threads = TRUE),
                     stanc_options = list("O1"), pedantic = TRUE)

samples <- mod$sample(
  data = data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  threads_per_chain = 2,
  iter_warmup = 2000,
  iter_sampling = 2000,
  refresh = 1000,
  output_dir = "simmodels",
  max_treedepth = 20,
  adapt_delta = 0.99,
)

# Same the fitted model
samples$save_object("simmodels/W11_RL_symmetric.rds")

samples$cmdstan_diagnose()
samples$summary() 

draws_df <- as_draws_df(samples$draws()) 

ggplot(draws_df, aes(.iteration, alpha, group = .chain, color = .chain)) +
  geom_line() +
  theme_classic()

ggplot(draws_df, aes(.iteration, temperature, group = .chain, color = .chain)) +
  geom_line() +
  theme_classic()


ggplot(draws_df) +
  geom_density(aes(alpha), fill = "blue", alpha = 0.3) +
  geom_density(aes(alpha_prior), fill = "red", alpha = 0.3) +
  xlab("Learning Rate") +
  ylab("Posterior Density") +
  theme_classic()

ggplot(draws_df) +
  geom_density(aes(temperature), fill = "blue", alpha = 0.3) +
  geom_density(aes(temperature_prior), fill = "red", alpha = 0.3) +
  xlab("(inverse) temperature") +
  ylab("Posterior Density") +
  theme_classic()
```

## Model fitting: asymmetric RL

```{r}
stan_model <- "
data {
    int<lower=1> trials;
    array[trials] int<lower=1,upper=2> choice;
    array[trials] int<lower=-1,upper=1> feedback;
} 

transformed data {
  vector[2] initValue;  // initial values for V
  initValue = rep_vector(0.0, 2);
}

parameters {
    real<lower=0, upper=1> alpha_pos; // learning rate
    real<lower=0, upper=1> alpha_neg; // learning rate
    real<lower=0, upper=20> temperature; // softmax inv.temp.
}

model {
    real pe;
    vector[2] value;
    vector[2] theta;
    
    target += uniform_lpdf(alpha_pos | 0, 1);
    target += uniform_lpdf(alpha_neg | 0, 1);
    target += uniform_lpdf(temperature | 0, 20);
    
    value = initValue;
    
    for (t in 1:trials) {
        theta = softmax( temperature * value); // action prob. computed via softmax
        target += categorical_lpmf(choice[t] | theta);
        
        pe = feedback[t] - value[choice[t]]; // compute pe for chosen value only
        
        if (pe < 0)
          value[choice[t]] = value[choice[t]] + alpha_neg * pe; // update chosen V
        if (pe > 0)
          value[choice[t]] = value[choice[t]] + alpha_pos * pe; // update chosen V
    }
    
}
"

write_stan_file(
  stan_model,
  dir = "stan/",
  basename = "W11_RL_asymmetric.stan")

file <- file.path("stan/W11_RL_asymmetric.stan")
mod <- cmdstan_model(file, cpp_options = list(stan_threads = TRUE),
                     stanc_options = list("O1"), pedantic = TRUE)

samples <- mod$sample(
  data = data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  threads_per_chain = 2,
  iter_warmup = 2000,
  iter_sampling = 2000,
  refresh = 1000,
  output_dir = "simmodels",
  max_treedepth = 20,
  adapt_delta = 0.99,
)

# Same the fitted model
samples$save_object("simmodels/W11_RL_asymmetric.rds")

```

## Model fitting: multilevel

```{r}
## Multilevel
agents <- 100
trials <- 120 

df <- NULL
for (agent in 1:agents) {
  
  temperature <- boot::inv.logit(rnorm(1, -2, 0.3))*20
  alpha <- boot::inv.logit(rnorm(1, 1.1, 0.3))  
    
    value <- c(0,0)
    d <- tibble(trial = rep(NA, trials),
                choice = rep(NA, trials), 
                value1 = rep(NA, trials), 
                value2 = rep(NA, trials), 
                feedback = rep(NA, trials),
                alpha = alpha,
                temperature = temperature,
                agent = agent)
    
    for (i in 1:trials) {
      
      choice <- rbinom(1, 1, softmax(value[2] - value[1], temperature))
      feedback <- ifelse(Bot[i] == choice, 1, -1)
      value <- ValueUpdate(value, alpha, choice, feedback)
      d$trial[i] <- i
      d$choice[i] <- choice
      d$value1[i] <- value[1]
      d$value2[i] <- value[2]
      d$feedback[i] <- feedback
      d$alpha[i] <- alpha
      d$temperature[i] <- temperature
      
    }
    if (exists("df")) {df <- rbind(df, d)} else {df <- d}

}

df <- df %>% group_by(alpha, temperature) %>% mutate(
  prevFeedback = lead(feedback))

## Create the data

trials <- trials
agents <- agents

d_choice <- df %>% 
  subset(select = c(agent, choice)) %>% 
  mutate(row = rep(seq(trials),agents)) %>% 
  pivot_wider(names_from = agent, values_from = choice)

d_feedback <- df %>% 
  subset(select = c(agent, feedback)) %>% 
  mutate(row = rep(seq(trials),agents)) %>% 
  pivot_wider(names_from = agent, values_from = feedback)


data <- list(
  trials = trials,
  agents = agents,
  choice = as.matrix(d_choice[,2:(agents + 1)]),
  feedback = as.matrix(d_feedback[,2:(agents + 1)])
)

data$choice <- data$choice + 1

stan_model <- "
data {
  int<lower=1> trials;
  int<lower=1> agents;
  array[trials, agents] int<lower=1,upper=2> choice;
  array[trials, agents] int<lower=-1,upper=1> feedback;
} 

transformed data {
  vector[2] initValue;  // initial values for V
  initValue = rep_vector(0.0, 2);
}

parameters {
  real alphaM; // learning rate
  real temperatureM; // softmax inv.temp.
  vector<lower = 0>[2] tau;
  matrix[2, agents] z_IDs;
  cholesky_factor_corr[2] L_u;
}

transformed parameters {
  matrix[agents,2] IDs;
  IDs = (diag_pre_multiply(tau, L_u) * z_IDs)';
}

model {
  
  real pe;
  vector[2] value;
  vector[2] theta;
  
  target += normal_lpdf(alphaM | 0, 1);
  target += normal_lpdf(temperatureM | 0, 1);
  target += normal_lpdf(tau[1] | 0, .3)  -
    normal_lccdf(0 | 0, .3);
  target += normal_lpdf(tau[2] | 0, .3)  -
    normal_lccdf(0 | 0, .3);
  
  target += lkj_corr_cholesky_lpdf(L_u | 2);
  target += std_normal_lpdf(to_vector(z_IDs));
  
  for (agent in 1:agents){
    value = initValue;
    
    for (t in 1:trials) {
      theta = softmax( inv_logit(temperatureM + IDs[agent,2]) * 20 * value); // action prob. computed via softmax
      target += categorical_lpmf(choice[t, agent] | theta);
      
      pe = feedback[t, agent] - value[choice[t, agent]]; // compute pe for chosen value only
      value[choice[t, agent]] = value[choice[t, agent]] + inv_logit(alphaM + IDs[agent,1]) * pe; // update chosen V
    }
  }
  
}

"

write_stan_file(
  stan_model,
  dir = "stan/",
  basename = "W11_RL_multilevel.stan")

file <- file.path("stan/W11_RL_multilevel_asym_ref.stan")
mod <- cmdstan_model(file, cpp_options = list(stan_threads = TRUE),
                     stanc_options = list("O1"), pedantic = TRUE)

samples <- mod$sample(
  data = data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  threads_per_chain = 2,
  iter_warmup = 500,
  iter_sampling = 500,
  refresh = 10,
  output_dir = "simmodels",
  max_treedepth = 20,
  adapt_delta = 0.99,
)

# Same the fitted model
samples$save_object("simmodels/W11_RL_multilevel.rds")

```

## Alternative Models of Reinforcement Learning

The basic Rescorla-Wagner model we've implemented represents just one approach to reinforcement learning. Several important variations capture different aspects of how organisms learn:

### Counterfactual Learning

In many situations, we learn not just from what happened, but also from what could have happened. For example, after choosing a restaurant and having a mediocre meal, we might see a friend's photos of delicious dishes from another restaurant they ate at. Counterfactual learning models capture how we update our preferences based on both experienced and foregone outcomes.

In computational terms, counterfactual learning models modify the basic value update equation to include learning from unchosen options:

CopyV(chosen) = V(chosen) + α(R(chosen) - V(chosen))
V(unchosen) = V(unchosen) + α'(R(unchosen) - V(unchosen))

Here, α' represents a potentially different learning rate for counterfactual outcomes, often smaller than α since indirect feedback may be less reliable or salient than direct experience.

### Sequential Learning

While our basic model treats each trial as independent, real-world learning often involves sequences of related decisions. Sequential learning models account for how previous choices and outcomes influence not just value estimates but also the learning process itself. For instance, after several successful choices in a row, we might become more confident and therefore less influenced by a single failure.
These models often incorporate additional parameters to capture sequence effects:

* Momentum terms that make agents more likely to repeat recent successful strategies

* Pattern detection mechanisms that look for regularities in outcome sequences

* Meta-learning processes that adjust learning rates based on recent experience

### Model-Based Learning

Perhaps the most sophisticated extension involves agents that build internal models of how their environment works, rather than just tracking reward statistics. These agents can simulate potential outcomes and plan sequences of actions, similar to how humans might mentally rehearse different strategies in a game. [Missing: details and refs]


## Adapting Learning Rates to Environmental Dynamics

A crucial limitation of basic reinforcement learning models is their use of fixed learning rates. Different environments call for different learning strategies:

### Stable vs Volatile Environments

In highly stable environments where true reward probabilities change rarely or never, optimal learning involves:

* Initially high learning rates to quickly acquire accurate value estimates

* Gradually decreasing learning rates as estimates become more reliable

* Eventually very low learning rates to avoid being misled by random variation

Conversely, volatile environments where reward probabilities change frequently require:

* Consistently higher learning rates to track changing conditions

* Greater willingness to update beliefs based on recent outcomes

* Some mechanism to detect and respond to change points

### Seasonal or Cyclic Changes

Many real-world environments show systematic patterns of change. Consider foraging for food throughout the year - certain locations might be reliably better in summer than winter. Fixed learning rates struggle with such environments because they:

* Can't capitalize on knowledge of seasonal patterns
* Either learn too slowly to track changes or too quickly to maintain stable long-term knowledge
* Miss opportunities to prepare for predictable changes

More sophisticated models address this by incorporating:

* Multiple timescales of learning (fast learning for current conditions, slow learning for general patterns)

* Explicit representation of environmental structure (like seasonality)

* Context-dependent learning rates that adapt to recognized patterns

These considerations highlight how reinforcement learning models, while powerful, represent significant simplifications of real learning processes. Understanding their limitations helps us both interpret their results more carefully and develop more sophisticated models that better capture the complexity of human and animal learning.

Reinforcement learning provides a powerful framework for understanding how agents learn from experience. Our implementation highlights both the elegance of the basic mechanism and the rich dynamics it can produce. While more sophisticated models exist, the core principles we've explored here - prediction errors, value updating, and the exploration-exploitation tradeoff - remain fundamental to understanding learning and decision-making.
