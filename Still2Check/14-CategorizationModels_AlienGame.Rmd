---
title: "Simulating Categorization Models on the Alien Task"
output:
  html_document:
    mathjax: default
    toc: true
    toc_float: true
  pdf_document:
    toc: true
date: "`r Sys.Date()`"
---

```{r 14_setup, include=FALSE}
# This chunk runs automatically when the document is knitted
# It sets default options for subsequent code chunks
knitr::opts_chunk$set(
  echo = TRUE,       # Show the code in the output document
  message = FALSE,   # Hide messages generated by code (e.g., package loading)
  warning = FALSE,   # Hide warnings generated by code
  fig.width = 10,    # Default figure width
  fig.height = 6     # Default figure height
  ) 
# --- Simulation Control ---
# Set to TRUE only if you want to re-run time-consuming simulations.
# Otherwise, the script will attempt to load pre-computed results.
regenerate_simulations <- FALSE 

# --- Load Packages ---
pacman::p_load(
    tidyverse,   # For data manipulation and plotting
    patchwork,   # For combining plots
    future,      # For parallel processing
    furrr,       # For parallel functional programming
    MASS         # For pseudo-inverse (ginv) fallback
)

# --- Setup Parallel Processing ---
plan(multisession, workers = availableCores())

# --- Ensure Output Directory Exists ---
simdata_dir <- "simdata"
if (!dir.exists(simdata_dir)) dir.create(simdata_dir, recursive = TRUE)

```

# Categorization models in action : The Alien Game

This document simulates three different cognitive models of categorization (Exemplar/GCM, Prototype/Kalman Filter, Rule-Based/Particle Filter) on a task adapted from Tylén et al. (2023). In this task, participants (or models) learn to categorize visually complex "alien" stimuli based on feedback.

## Stimuli and Features

The aliens vary along five binary features:

* Legs: Slim (0) / Fat (1)

* Arms: Downwards (0) / Upwards (1)

* Body Dots: Absent (0) / Present (1)

* Eyes: Normal (0) / On Stalks (1)

* Color: Green (0) / Blue (1)

Each alien stimulus can be represented as a 5-dimensional binary vector (e.g., c(0, 1, 0, 1, 1) means slim legs, arms up, no dots, eyes on stalks, blue color). There are $2^5 = 32$ unique aliens.

## Categorization Dimensions and Task

Participants must react to each alien, and their response depends on two independent binary categorizations:

* Dangerous: Is the alien dangerous (1) or peaceful (0)?

* Nutritious: Is the alien nutritious (1) or non-nutritious (0)?

This results in four possible response combinations (peaceful/non-nutritious, peaceful/nutritious, dangerous/non-nutritious, dangerous/nutritious). The goal is to learn the underlying rules or patterns that determine whether an alien is dangerous and whether it is nutritious based on its features.

## Experimental Sessions and Complexity

The task involves three sessions, each with increasing rule complexity for determining the 'Dangerous' and 'Nutritious' categories. Each session consists of multiple blocks where the 32 unique aliens are presented multiple times in random order, with feedback provided after each choice.

N.B. the article also discusses a test session where the models are presented with new aliens that they have not seen before. This is not implemented in this simulation, since the novelty is in how the features are visually represented, and that change is transparent to our models.

## Category Rules per Session:

We need to define the ground truth rules for each session (based on Tylén et al 2023).

* Session 1 (Low Complexity):

** Dangerous = Feature 1 (Fat Legs) == 1 AND Feature 3 (Dots) == 1

** Nutritious = Feature 4 (Eyes on Stalks) == 1 

* Session 2 (Intermediate Complexity):

** Dangerous = Feature 4 (Eyes on Stalks) == 1 

** Nutritious = At least 2 out of Features 1 (Fat Legs), 2 (Arms Up), 3 (Dots) are 1. (i.e., f1 + f4 + f3 >= 2) 

* Session 3 (High Complexity):

** Dangerous = Feature 4 (Eyes on Stalks) == 1 AND Feature 5 (Blue Color) == 1

** Nutritious = A complex 1: (f2 + f3 == 2) | (f1 == 1 & ((f4 + f5) >= 1) & ((f2 + f3) >= 1))


## Modeling Goal

We will simulate how the three different categorization models learn the 'Dangerous' and 'Nutritious' categories across these three sessions. We are interested in comparing their learning trajectories (accuracy over trials) under different assumptions (e.g., attention weights for GCM, noise levels for Kalman, rule complexity for Particle Filter).

## Setup: Data Structures and Helper Functions

First, let's define the stimuli and the true category labels for each session.

```{r}
# --- Define Stimuli (all 32 combinations of 5 binary features) ---
n_features <- 5
stimuli_features <- expand.grid(replicate(n_features, c(0, 1), simplify = FALSE))
names(stimuli_features) <- paste0("f", 1:n_features)
stimuli_features <- as.matrix(stimuli_features) # Convert to matrix
rownames(stimuli_features) <- apply(stimuli_features, 1, paste, collapse="")

n_unique_stimuli <- nrow(stimuli_features) # Should be 32

# --- Define Category Rules Function ---
get_true_categories <- function(features_matrix, session) {
  f1 <- features_matrix[, 1]
  f2 <- features_matrix[, 2]
  f3 <- features_matrix[, 3]
  f4 <- features_matrix[, 4]
  f5 <- features_matrix[, 5]

  if (session == 1) {
    dangerous <- as.numeric(f2 == 1 & f3 == 1)
    nutritious <- as.numeric(f4 == 1)
  } else if (session == 2) {
    dangerous <- as.numeric(f4 == 1)
    nutritious <- as.numeric((f1 + f2 + f3) >= 2)
  } else if (session == 3) {
    dangerous <- as.numeric(f4 == 1 & f5 == 1)
    nutritious <- as.numeric((f2 + f3 == 2) | (f1 == 1 & ((f4 + f5) >= 1) & ((f2 + f3) >= 1)))
  } else {
    stop("Invalid session number")
  }
  
  return(tibble(dangerous = dangerous, nutritious = nutritious))
}

# --- Generate Full Stimulus List with Categories per Session ---
stimuli_list <- list()
for (s in 1:3) {
  categories <- get_true_categories(stimuli_features, session = s)
  stimuli_list[[s]] <- stimuli_features %>% 
    as_tibble() %>%
    mutate(stimulus_id = rownames(stimuli_features),
           session = s) %>%
    bind_cols(categories)
}
all_stimuli_definitions <- bind_rows(stimuli_list)

# Display structure for one session
cat("Example Stimuli Definitions (Session 1):\n")
print(head(all_stimuli_definitions %>% filter(session == 1)))

# --- Define Experiment Structure ---
n_blocks <- 3
n_trials_per_block <- n_unique_stimuli
n_total_trials <- n_blocks * n_trials_per_block # 96 trials per session

# Function to create trial sequence for one session
create_session_trials <- function(session_num, stimuli_definitions) {
  session_stimuli <- stimuli_definitions %>% filter(session == session_num)
  
  trial_list <- list()
  set.seed(123 + session_num) # Ensure reproducibility per session
  for (block in 1:n_blocks) {
    # Randomize order within block
    shuffled_indices <- sample(1:n_unique_stimuli)
    trial_list[[block]] <- session_stimuli[shuffled_indices, ] %>%
      mutate(block = block,
             trial_in_block = 1:n_unique_stimuli)
  }
  
  # Combine blocks and add overall trial number
  bind_rows(trial_list) %>%
    mutate(trial = 1:n_total_trials) %>%
    dplyr::select(session, block, trial, stimulus_id, f1:f5, dangerous, nutritious)
}

# Create experiment structure for all sessions
experiment_structure <- map_dfr(1:3, create_session_trials, 
                                stimuli_definitions = all_stimuli_definitions)

cat("\nExperiment Structure (First 6 trials of Session 1):\n")
print(head(experiment_structure %>% filter(session == 1)))
```

## Model Implementations

Now, let's define the R functions for our three models. These are adapted from the previous chapters and the provided scripts to handle 5 features and the two independent decisions (Dangerous, Nutritious).

### 1. Exemplar Model (GCM)
The GCM calculates similarity to stored exemplars. We need the distance and similarity functions, and the main GCM function which will be called twice per trial (once for dangerous, once for nutritious)
 
```{r}
# --- GCM Helper Functions ---
distance <- function(vect1, vect2, w) {
  # Ensure inputs are numeric vectors of the same length
  if (!is.numeric(vect1) || !is.numeric(vect2) || length(vect1) != length(vect2) || length(w) != length(vect1)) {
     # Handle cases like first trial where one vector might be empty or incorrect length
     return(Inf) # Return infinite distance for invalid comparison
  }
  return(sum(w * abs(vect1 - vect2)))
}

similarity <- function(distance, c) {
  # Handle infinite distance
  if (is.infinite(distance)) return(0)
  return(exp(-c * distance))
}

# --- GCM Agent Function ---
# This function simulates choices for ONE category dimension (e.g., dangerous OR nutritious)
gcm_single_decision <- function(w, # Attention weights (vector of length n_features)
                                c, # Sensitivity parameter
                                obs_matrix, # Matrix of observed stimuli (trials x features)
                                category_feedback, # Vector of true category labels (0/1) for this decision
                                quiet = TRUE) { 
  
  response_probs <- c() # Probability of responding 1
  n_trials <- nrow(obs_matrix)
  n_features <- ncol(obs_matrix)
  
  # Store exemplars encountered so far {features, category}
  exemplars_features <- matrix(NA_real_, nrow = 0, ncol = n_features)
  exemplars_category <- numeric(0)
  
  for (i in 1:n_trials) {
    if (!quiet && i %% 10 == 0) print(paste("GCM Trial", i))
    
    current_stimulus <- obs_matrix[i, ]
    n_seen <- nrow(exemplars_features)
    
    # --- Decision ---
    # If first trial or no examples of one category seen yet, guess randomly
    if (n_seen == 0 || length(unique(exemplars_category)) < 2) {
      prob_cat_1 <- 0.5
    } else {
      # Calculate similarity to all stored exemplars
      similarities <- numeric(n_seen)
      for (e in 1:n_seen) {
          dist_val <- distance(current_stimulus, exemplars_features[e, ], w)
          similarities[e] <- similarity(dist_val, c)
      }
      
      # Calculate summed similarity to each category
      sum_sim_cat_1 <- sum(similarities[exemplars_category == 1])
      sum_sim_cat_0 <- sum(similarities[exemplars_category == 0])
      
      # Calculate probability using Luce choice rule (with check for zero denominator)
      total_sim <- sum_sim_cat_1 + sum_sim_cat_0
      if (total_sim > 1e-9) {
        prob_cat_1 <- sum_sim_cat_1 / total_sim
      } else {
        prob_cat_1 <- 0.5 # Guess if no similarity to anything
      }
    }
    
    # Store probability and ensure bounds
    response_probs[i] <- max(min(prob_cat_1, 0.9999), 0.0001)
    
    # --- Learning ---
    # Add the current stimulus and its TRUE category feedback to memory
    exemplars_features <- rbind(exemplars_features, current_stimulus)
    exemplars_category <- c(exemplars_category, category_feedback[i])
    
  } # End trial loop
  
  # Return simulated binary responses
  return(rbinom(n_trials, 1, response_probs))
}

cat("GCM functions defined.\n")
```
 
 
### 2. Prototype Model (Kalman Filter)

This model tracks the mean and covariance (uncertainty) for each category prototype. We need the multivariate update function and the main agent function, again called twice per trial.

```{r}
# --- Multivariate Kalman Filter Update Function (from previous chapter) ---
# Performs one update step for a single category's prototype
multivariate_kalman_update <- function(mu_prev,    # Vector of previous means
                                       sigma_prev, # Previous covariance matrix
                                       observation,# Vector of observed features
                                       r_matrix    # Observation noise matrix
                                       ) {
  # Ensure inputs are numeric/matrix
  mu_prev <- as.numeric(mu_prev)
  observation <- as.numeric(observation)
  sigma_prev <- as.matrix(sigma_prev)
  r_matrix <- as.matrix(r_matrix)
  
  n_dim <- length(mu_prev)
  I <- diag(n_dim) # Identity matrix
  
  # Calculate Kalman gain (K)
  combined_cov <- sigma_prev + r_matrix
  S_inv <- tryCatch(solve(combined_cov), error = function(e) {
      MASS::ginv(combined_cov) # Fallback using Moore-Penrose pseudo-inverse
  })
  k_matrix <- sigma_prev %*% S_inv
  
  # Update mean (mu)
  innovation <- observation - mu_prev # Prediction error
  mu_new <- mu_prev + k_matrix %*% innovation
  
  # Update covariance (sigma) using Joseph form
  IK_term <- (I - k_matrix)
  sigma_new <- IK_term %*% sigma_prev %*% t(IK_term) + k_matrix %*% r_matrix %*% t(k_matrix)
  
  # Ensure symmetry
  sigma_new <- (sigma_new + t(sigma_new)) / 2
  
  return(list(mu = as.numeric(mu_new), sigma = sigma_new)) # Return only mu and sigma
}


# --- Prototype Kalman Agent Function ---
# Simulates choices for ONE category dimension (e.g., dangerous OR nutritious)
prototype_kalman_single_decision <- function(r_value, # Observation noise variance (scalar)
                                             obs_matrix, # Matrix of observations (trials x features)
                                             category_feedback, # Vector of true category labels (0/1)
                                             initial_mu = NULL, # Optional: list(mu0, mu1)
                                             initial_sigma_diag = 10.0, 
                                             b = 0.5, # Response bias
                                             quiet = TRUE) { 
  
  n_trials <- nrow(obs_matrix)
  n_features <- ncol(obs_matrix)
  
  # --- Initialization ---
  if (is.null(initial_mu)) {
    mu0_init <- rep(0, n_features)
    mu1_init <- rep(0, n_features)
  } else {
    mu0_init <- initial_mu[[1]] 
    mu1_init <- initial_mu[[2]]
  }
  prototype_cat_0 <- list(mu = mu0_init, sigma = diag(initial_sigma_diag, n_features))
  prototype_cat_1 <- list(mu = mu1_init, sigma = diag(initial_sigma_diag, n_features))
  r_matrix <- diag(r_value, n_features)
  
  response_probs <- numeric(n_trials)
  
  log_sum_exp <- function(v) {
    if (all(!is.finite(v))) return(-Inf) 
    max_v <- max(v[is.finite(v)])
    max_v + log(sum(exp(v - max_v)))
  }
  
  # --- Trial Loop ---
  for (i in 1:n_trials) {
    if (!quiet && i %% 10 == 0) print(paste("Kalman Trial", i))
    
    current_obs <- as.numeric(obs_matrix[i, ])
    
    # --- Decision ---
    cov_cat_0 <- prototype_cat_0$sigma + r_matrix
    cov_cat_1 <- prototype_cat_1$sigma + r_matrix
    
    log_pd_0 <- tryCatch(
      mvtnorm::dmvnorm(current_obs, mean = prototype_cat_0$mu, sigma = cov_cat_0, log = TRUE),
      error = function(e) -Inf 
    )
    log_pd_1 <- tryCatch(
      mvtnorm::dmvnorm(current_obs, mean = prototype_cat_1$mu, sigma = cov_cat_1, log = TRUE),
      error = function(e) -Inf
    )
      
    log_p0_biased <- log_pd_0 + log(1 - b) 
    log_p1_biased <- log_pd_1 + log(b)     
    log_total_p <- log_sum_exp(c(log_p0_biased, log_p1_biased))
    
    if (!is.finite(log_total_p)) { 
        prob_cat_1 <- b 
    } else {
        prob_cat_1 <- exp(log_p1_biased - log_total_p)
    }
        
    response_probs[i] <- max(min(prob_cat_1, 0.9999), 0.0001)
    
    # --- Learning Update ---
    true_cat <- category_feedback[i]
    
    if (true_cat == 1) {
      update <- multivariate_kalman_update(prototype_cat_1$mu, prototype_cat_1$sigma, current_obs, r_matrix)
      prototype_cat_1 <- update # Update list directly
    } else { 
      update <- multivariate_kalman_update(prototype_cat_0$mu, prototype_cat_0$sigma, current_obs, r_matrix)
      prototype_cat_0 <- update # Update list directly
    }
  } # End trial loop
  
  return(rbinom(n_trials, 1, response_probs))
}

cat("Kalman Prototype functions defined.\n")
```

### 3. Rule-Based Model (Particle Filter)

This model maintains a set of rule hypotheses. We need functions to represent/evaluate rules, generate random rules, initialize particles, update weights, and resample. The main function is called twice per trial because we have two potentially independent decisions (dangerous and nutritious).

```{r}
# --- Rule Helper Functions ---
# Evaluate a rule (copied from Chapter 13, ensure consistency)
evaluate_rule <- function(rule, stimulus) {
  tryCatch({
    if (length(rule$dimensions) == 1) {
      dim_idx <- rule$dimensions[1]
      threshold <- rule$thresholds[1]
      operation <- rule$operations[1]
      if (dim_idx > length(stimulus)) return(0.5) # Avoid error if stimulus is wrong length
      condition_met <- switch(operation, ">" = stimulus[dim_idx] > threshold, "<" = stimulus[dim_idx] < threshold, FALSE)
    } else if (length(rule$dimensions) == 2) {
       if (any(rule$dimensions > length(stimulus))) return(0.5)
       dim1_idx <- rule$dimensions[1]; thresh1 <- rule$thresholds[1]; op1 <- rule$operations[1]
       cond1_met <- switch(op1, ">" = stimulus[dim1_idx] > thresh1, "<" = stimulus[dim1_idx] < thresh1, FALSE)
       dim2_idx <- rule$dimensions[2]; thresh2 <- rule$thresholds[2]; op2 <- rule$operations[2]
       cond2_met <- switch(op2, ">" = stimulus[dim2_idx] > thresh2, "<" = stimulus[dim2_idx] < thresh2, FALSE)
       if (is.null(rule$logic) || is.na(rule$logic)) rule$logic <- "AND" # Default logic if missing
       condition_met <- switch(rule$logic, "AND" = cond1_met & cond2_met, "OR" = cond1_met | cond2_met, FALSE)
    } else { # Extend to handle more dimensions if needed, default to AND logic
        if (any(rule$dimensions > length(stimulus))) return(0.5)
        results <- logical(length(rule$dimensions))
        for(d in 1:length(rule$dimensions)){
             dim_idx <- rule$dimensions[d]; threshold <- rule$thresholds[d]; operation <- rule$operations[d]
             results[d] <- switch(operation, ">" = stimulus[dim_idx] > threshold, "<" = stimulus[dim_idx] < threshold, FALSE)
        }
        # Assuming AND logic for >2 dimensions if not specified
        condition_met <- all(results) 
    }
    prediction <- ifelse(condition_met, rule$prediction_if_true, 1 - rule$prediction_if_true)
    return(prediction)
  }, error = function(e) { return(0.5) }) # Return 0.5 on error
}


# Generate a random rule (allowing up to max_dims)
generate_random_rule <- function(n_features, max_dims = 3, feature_range = list(c(0, 1), c(0, 1), c(0, 1), c(0, 1), c(0, 1))) {
  num_dims_in_rule <- sample(1:min(max_dims, n_features), 1)
  dimensions_used <- sort(sample(1:n_features, size = num_dims_in_rule, replace = FALSE))
  thresholds <- numeric(num_dims_in_rule)
  operations <- character(num_dims_in_rule)
  for (i in 1:num_dims_in_rule) {
    dim_idx <- dimensions_used[i]
    # Threshold for binary features is typically 0.5
    thresholds[i] <- 0.5 
    operations[i] <- sample(c(">", "<"), 1) # Or could fix to e.g., ">" which means "== 1"
  }
  rule_logic <- if (num_dims_in_rule > 1) sample(c("AND", "OR"), 1) else NA
  prediction_if_true <- sample(0:1, 1)
  list(dimensions = dimensions_used, thresholds = thresholds, operations = operations,
       logic = rule_logic, prediction_if_true = prediction_if_true)
}

# Initialize particles
initialize_particles <- function(n_particles, n_features, max_dims, feature_range) {
  particles <- map(1:n_particles, ~generate_random_rule(n_features, max_dims, feature_range))
  weights <- rep(1/n_particles, n_particles)
  list(particles = particles, weights = weights)
}

# Update weights
update_weights <- function(particles, weights, stimulus, true_category, error_prob) {
  n_particles <- length(particles)
  likelihoods <- numeric(n_particles)
  for(i in 1:n_particles){
      pred <- evaluate_rule(particles[[i]], stimulus)
      likelihoods[i] <- ifelse(pred == true_category, 1 - error_prob, error_prob)
  }
  new_unnormalized_weights <- weights * likelihoods
  total_weight <- sum(new_unnormalized_weights)
  if (total_weight > 1e-9) {
      normalized_weights <- new_unnormalized_weights / total_weight
  } else {
      normalized_weights <- rep(1/n_particles, n_particles)
  }
  return(normalized_weights)
}

# Resample particles
resample_particles <- function(particles, weights) {
  n_particles <- length(particles)
  # Check for invalid weights before sampling
  if (any(is.na(weights)) || sum(weights) <= 0 || any(weights < 0)) {
    # warning("Invalid weights detected during resampling; reinitializing.", call. = FALSE)
    # Reinitialize if weights are problematic (assumes access to n_features, max_dims)
    # This part needs context or modification if used standalone
    n_features_context <- length(particles[[1]]$dimensions) # Infer crudely
    max_dims_context <- max(sapply(particles, function(p) length(p$dimensions)))
    return(initialize_particles(n_particles, n_features_context, max_dims_context, 
                                list(c(0,1),c(0,1),c(0,1),c(0,1),c(0,1)))) 
  }
  resampled_indices <- sample.int(n = n_particles, size = n_particles, replace = TRUE, prob = weights)
  new_particles <- particles[resampled_indices]
  new_weights <- rep(1/n_particles, n_particles)
  list(particles = new_particles, weights = new_weights)
}


# --- Rule Particle Filter Agent Function ---
# Simulates choices for ONE category dimension
rule_particle_filter_single_decision <- function(n_particles, 
                                                 max_dims, # Max dimensions rule can use
                                                 error_prob, 
                                                 obs_matrix, 
                                                 category_feedback, 
                                                 resample_threshold_factor = 0.5, 
                                                 quiet = TRUE) {
  
  n_trials <- nrow(obs_matrix)
  n_features <- ncol(obs_matrix)
  feature_range <- replicate(n_features, c(0, 1), simplify = FALSE) # Binary features
  
  # Initialize
  particle_system <- initialize_particles(n_particles, n_features, max_dims, feature_range)
  particles <- particle_system$particles
  weights <- particle_system$weights
  
  response_probs <- numeric(n_trials)
  
  # Trial Loop
  for (i in 1:n_trials) {
    if (!quiet && i %% 10 == 0) print(paste("Rule Trial", i))
    
    current_stimulus <- as.numeric(obs_matrix[i, ])
    
    # Decision
    rule_predictions <- sapply(particles, evaluate_rule, stimulus = current_stimulus)
    prob_cat1_given_rules <- ifelse(rule_predictions == 1, 1 - error_prob, error_prob)
    response_probs[i] <- sum(weights * prob_cat1_given_rules)
    response_probs[i] <- max(min(response_probs[i], 0.9999), 0.0001)

    # Learning Update
    true_category <- category_feedback[i]
    weights <- update_weights(particles, weights, current_stimulus, true_category, error_prob)
    
    # Resampling
    ess <- 1 / sum(weights^2)
    if (!is.nan(ess) && ess < n_particles * resample_threshold_factor) {
      if (!quiet) cat(" Resampling at trial", i, "ESS =", round(ess, 1), "\n")
      resampled_system <- resample_particles(particles, weights)
      particles <- resampled_system$particles
      weights <- resampled_system$weights
    }
    
  } # End trial loop
  
  return(rbinom(n_trials, 1, response_probs))
}

cat("Rule Particle Filter functions defined.\n")
```

## Simulating the Models

Now we set up the simulation loop. For each session and each simulated agent, we run the appropriate model twice: once to learn the 'Dangerous' category and once for the 'Nutritious' category.

```{r}
# --- Simulation Parameters ---
n_agents_per_setting <- 10 # Number of agents to simulate for each parameter setting (reduce for speed)

# GCM Parameters
gcm_c_value <- 1.0 # Fixed sensitivity

# Kalman Parameters
kalman_r_values <- c(0.5, 1.0, 2.0) # Different observation noise levels

# Rule Model Parameters (Simple, Medium, Complex)
rule_params <- list(
  simple = list(n_particles = 50, max_dims = 1, error_prob = 0.2),
  medium = list(n_particles = 150, max_dims = 2, error_prob = 0.1),
  complex = list(n_particles = 250, max_dims = 3, error_prob = 0.05) 
)

# --- Simulation Wrapper Function ---
# Runs all models for one agent in one session
run_agent_session <- function(agent_id, session_num, experiment_df) {
  
  session_data <- experiment_df %>% filter(session == session_num)
  obs_matrix <- as.matrix(session_data[, paste0("f", 1:n_features)])
  feedback_dangerous <- session_data$dangerous
  feedback_nutritious <- session_data$nutritious
  
  results_list <- list()
  
  # --- GCM Simulations ---
   # Optimal weights (session dependent)
  if (session_num == 1) {
      w_opt_dang = c(1/2, 0, 1/2, 0, 0); w_opt_nutr = c(0, 0, 0, 1, 0)
  } else if (session_num == 2) {
      w_opt_dang = c(0, 0, 0, 1, 0); w_opt_nutr = c(1/3, 1/3, 1/3, 0, 0)
  } else { # Session 3
      w_opt_dang = c(0, 0, 0, 1/2, 1/2); w_opt_nutr = c(1/5, 1/5, 1/5, 1/5, 1/5)
  }
  
  # GCM Optimal
  sim_gcm_opt_dang <- gcm_single_decision(w_opt_dang, gcm_c_value, obs_matrix, feedback_dangerous)
  sim_gcm_opt_nutr <- gcm_single_decision(w_opt_nutr, gcm_c_value, obs_matrix, feedback_nutritious)
  results_list[["gcm_optimal"]] <- session_data %>% 
      mutate(agent = agent_id, model = "GCM", param = "Optimal", 
             sim_dangerous = sim_gcm_opt_dang, sim_nutritious = sim_gcm_opt_nutr)
             
  # GCM Equal
  w_equal = rep(1/n_features, n_features)
  sim_gcm_eq_dang <- gcm_single_decision(w_equal, gcm_c_value, obs_matrix, feedback_dangerous)
  sim_gcm_eq_nutr <- gcm_single_decision(w_equal, gcm_c_value, obs_matrix, feedback_nutritious)
  results_list[["gcm_equal"]] <- session_data %>% 
      mutate(agent = agent_id, model = "GCM", param = "Equal", 
             sim_dangerous = sim_gcm_eq_dang, sim_nutritious = sim_gcm_eq_nutr)

  # --- Kalman Simulations ---
  for (r_val in kalman_r_values) {
    sim_kal_dang <- prototype_kalman_single_decision(r_val, obs_matrix, feedback_dangerous)
    sim_kal_nutr <- prototype_kalman_single_decision(r_val, obs_matrix, feedback_nutritious)
    param_label <- paste0("r=", r_val)
    results_list[[paste0("kalman_", r_val)]] <- session_data %>% 
        mutate(agent = agent_id, model = "Kalman", param = param_label, 
               sim_dangerous = sim_kal_dang, sim_nutritious = sim_kal_nutr)
  }
  
  # --- Rule Simulations ---
   for (param_name in names(rule_params)) {
      params <- rule_params[[param_name]]
      sim_rule_dang <- rule_particle_filter_single_decision(params$n_particles, params$max_dims, params$error_prob, obs_matrix, feedback_dangerous)
      sim_rule_nutr <- rule_particle_filter_single_decision(params$n_particles, params$max_dims, params$error_prob, obs_matrix, feedback_nutritious)
      results_list[[paste0("rule_", param_name)]] <- session_data %>% 
          mutate(agent = agent_id, model = "Rule", param = param_name, 
                 sim_dangerous = sim_rule_dang, sim_nutritious = sim_rule_nutr)
   }
  
  bind_rows(results_list)
}

# --- Create Simulation Grid ---
simulation_grid <- expand_grid(
  agent_id = 1:n_agents_per_setting,
  session_num = 1:3
)

# --- Run Simulations (or Load) ---
sim_results_file <- file.path(simdata_dir, "W1X_alien_sim_results.rds")

if (regenerate_simulations || !file.exists(sim_results_file)) {
  cat("Running all simulations...\n")
  plan(multisession, workers = availableCores()) # Ensure parallel plan
  
  all_simulation_results <- future_map2_dfr(
    simulation_grid$agent_id, 
    simulation_grid$session_num,
    run_agent_session,
    experiment_df = experiment_structure, # Pass experiment structure
    .options = furrr_options(seed = TRUE),
    .progress = TRUE
  )
  
  saveRDS(all_simulation_results, sim_results_file)
  cat("Simulations finished and saved to:", sim_results_file, "\n")
  
} else {
  cat("Loading existing simulation results from:", sim_results_file, "\n")
  all_simulation_results <- readRDS(sim_results_file)
  cat("Results loaded.\n")
}

# Display structure of results
cat("\nStructure of Simulation Results:\n")
glimpse(all_simulation_results)
```

## Visualizing Simulation Results

Let's visualize the learning curves (cumulative accuracy) for each model across the three sessions, separating the results for the 'Dangerous' and 'Nutritious' decisions.

```{r}
# --- Calculate Performance Metrics ---
performance_data <- all_simulation_results %>%
  mutate(
    # Correctness for each decision
    dangerous_correct = ifelse(sim_dangerous == dangerous, 1, 0),
    nutritious_correct = ifelse(sim_nutritious == nutritious, 1, 0),
    # Overall correctness (both decisions must be right)
    overall_correct = ifelse(dangerous_correct == 1 & nutritious_correct == 1, 1, 0)
  ) %>%
  group_by(session, agent, model, param) %>%
  arrange(trial, .by_group = TRUE) %>%
  mutate(
    # Cumulative accuracy for each decision type
    cum_acc_dangerous = cumsum(dangerous_correct) / trial,
    cum_acc_nutritious = cumsum(nutritious_correct) / trial,
    cum_acc_overall = cumsum(overall_correct) / trial
  ) %>%
  ungroup() %>%
  # Create combined model/parameter label for plotting legends
  mutate(model_param = paste0(model, " (", param, ")"))

# --- Summarize for Plotting (Separate Decisions) ---
performance_summary_separate <- performance_data %>%
  # Select only the separate accuracies
  dplyr::select(session, model, param, model_param, trial, agent, cum_acc_dangerous, cum_acc_nutritious) %>%
  # Pivot longer for faceting
  pivot_longer(cols = c(cum_acc_dangerous, cum_acc_nutritious), 
               names_to = "decision_type", 
               values_to = "accuracy",
               names_prefix = "cum_acc_") %>%
  group_by(session, model, param, model_param, decision_type, trial) %>%
  summarize(
    mean_accuracy = mean(accuracy, na.rm = TRUE),
    se = sd(accuracy, na.rm = TRUE) / sqrt(n()),
    lower_ci = mean_accuracy - 1.96 * se,
    upper_ci = mean_accuracy + 1.96 * se,
    .groups = "drop"
  )

# --- Summarize for Plotting (Overall Accuracy) ---
performance_summary_overall <- performance_data %>%
  # Select only the overall accuracy
  dplyr::select(session, model, param, model_param, trial, agent, cum_acc_overall) %>%
  group_by(session, model, param, model_param, trial) %>%
  summarize(
    mean_accuracy = mean(cum_acc_overall, na.rm = TRUE),
    se = sd(cum_acc_overall, na.rm = TRUE) / sqrt(n()),
    lower_ci = mean_accuracy - 1.96 * se,
    upper_ci = mean_accuracy + 1.96 * se,
    .groups = "drop"
  )
  
# --- Create Plot 1: Separate Decision Accuracy ---
plot_separate <- ggplot(performance_summary_separate, 
       aes(x = trial, y = mean_accuracy, color = model_param, fill = model_param)) +
  geom_line(linewidth = 0.8) +
  geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci), alpha = 0.1, linetype = 0) +
  facet_grid(decision_type ~ session, 
             labeller = labeller(session = function(s) paste("Session", s),
                                 decision_type = function(d) str_to_title(d))) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  labs(
    title = "Simulated Learning Curves: Separate Decisions",
    subtitle = "Accuracy for 'Dangerous' and 'Nutritious' decisions independently",
    x = "Trial Number",
    y = "Cumulative Accuracy (Mean +/- 95% CI)",
    color = "Model (Parameter)",
    fill = "Model (Parameter)"
  ) +
  theme_bw(base_size = 11) +
  theme(legend.position = "bottom", 
        axis.text.x = element_text(angle = 45, hjust = 1),
        strip.text = element_text(face = "bold")) +
  guides(color = guide_legend(nrow = 3), fill = guide_legend(nrow = 3)) # Adjust legend layout

# --- Create Plot 2: Overall Accuracy ---
plot_overall <- ggplot(performance_summary_overall, 
       aes(x = trial, y = mean_accuracy, color = model_param, fill = model_param)) +
  geom_line(linewidth = 0.8) +
  geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci), alpha = 0.1, linetype = 0) +
  facet_grid(. ~ session, 
             labeller = labeller(session = function(s) paste("Session", s))) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  labs(
    title = "Simulated Learning Curves: Overall Accuracy",
    subtitle = "Accuracy requires *both* 'Dangerous' and 'Nutritious' decisions to be correct",
    x = "Trial Number",
    y = "Cumulative Accuracy (Mean +/- 95% CI)",
    color = "Model (Parameter)",
    fill = "Model (Parameter)"
  ) +
  theme_bw(base_size = 11) +
  theme(legend.position = "bottom", 
        axis.text.x = element_text(angle = 45, hjust = 1),
        strip.text = element_text(face = "bold")) +
  guides(color = guide_legend(nrow = 3), fill = guide_legend(nrow = 3)) 

# --- Display Plots ---
print(plot_separate)
print(plot_overall)



```

## Interpretation of Simulations

Examine the learning curves plotted above. Consider the following for each model type:

* GCM (Exemplar): How does performance differ between 'Equal' and 'Optimal' attention weights? Does optimal attention provide a significant advantage, especially in later sessions where rules become more complex? GCM often learns steadily but might be slower than rule-based models if a simple rule exists.

* Kalman (Prototype): How does the r_value (observation noise) affect learning? Does a specific level perform best across sessions, or does the optimal noise level change? Prototype models might struggle more with the complex, disjunctive rules in later sessions compared to rule-based or optimal GCM models.

* Rule (Particle Filter): How do the 'simple', 'medium', and 'complex' parameter settings affect performance? Does allowing more complex rules (max_dims) help in later sessions? Does a lower error_prob lead to better final performance if the rules are deterministic? Rule models might show sharp increases in performance if/when they discover the correct underlying rule.

Comparing across models, which approach seems to learn most effectively or quickly in each session? Does any model consistently outperform others? These simulations provide a baseline for understanding how different cognitive strategies might fare on this specific task, setting the stage for comparing against human data or fitting the models using Bayesian inference (e.g., with Stan).

## Observations from the Learning Curves

Based on the visualizations generated in the previous step (plotting `performance_summary`), we can draw some initial conclusions about how these models handle the Alien Categorization task:

1.  **Effect of Complexity (Sessions):** As expected, performance generally tends to decrease or plateau at a lower level as the session number increases (complexity increases). This is visible for most model variants, reflecting the increased difficulty in identifying the underlying category structures.

2.  **GCM Performance:**
    * The GCM with *Optimal* weights typically outperforms the GCM with *Equal* weights, especially in sessions 2 and 3 where the rules involve specific feature combinations. This highlights the importance of attention allocation for exemplar models.
    * Even with optimal weights, the GCM might not reach perfect accuracy, potentially due to interference from similar but differently categorized exemplars, especially with complex rules. Its learning curve is often gradual.

3.  **Kalman Filter Performance:**
    * The performance of the Kalman filter prototype model is sensitive to the `r_value` (observation noise). An intermediate value (e.g., `r=1.0` or `r=2.0`) often appears to strike a good balance.
    * Very low `r_value` might cause the prototypes to be too sensitive to early, potentially unrepresentative examples.
    * Very high `r_value` slows down learning considerably as the model discounts noisy observations.
    * Prototype models might face challenges with the disjunctive rules (e.g., "at least 2 out of 3 features") in later sessions, as these structures don't easily map onto a single central tendency (prototype). Performance might plateau below that of models capable of capturing the specific rules.

4.  **Rule Particle Filter Performance:**
    * The performance is highly dependent on the parameter settings (`n_particles`, `max_dims`, `error_prob`). 
    * Allowing more complex rules (`max_dims = 2` or `3`) seems crucial for achieving good performance in sessions 2 and 3, where the underlying rules involve multiple features. Models restricted to `max_dims = 1` likely perform poorly in these later sessions.
    * A sufficient number of particles (`n_particles`) is needed to adequately explore the rule space. Too few particles might lead the model to get stuck on suboptimal rules.
    * The `error_prob` influences the model's tolerance for imperfect rules. A low `error_prob` might be beneficial if the true rule is deterministic and discoverable, while a higher `error_prob` might be better if the categories have inherent noise or overlap.
    * Rule models, particularly the 'complex' variant, show the potential for high accuracy, sometimes surpassing the GCM or Kalman models, especially if they successfully identify the correct underlying rule structure. Their learning curves can sometimes exhibit sharper increases compared to the more gradual learning of similarity-based models.

## Next Steps

These forward simulations provide valuable intuition about how each model class behaves under different conditions and assumptions within the context of the Alien Categorization task.

The next logical step would be to compare these simulated patterns to actual human performance data from the Tylén et al. (2023) study. This involves:

1.  **Loading and Processing Human Data:** Extracting trial-by-trial responses for the 'Dangerous' and 'Nutritious' decisions from the human participants.
2.  **Visual Comparison:** Plotting human learning curves alongside the simulated model curves to qualitatively assess which models best capture the human learning trajectory.
3.  **Quantitative Model Fitting:** Implementing these models in a Bayesian framework (like Stan, as previewed in the previous chapters) to fit them directly to individual participant data. This allows for:
    * Estimating model parameters (e.g., GCM sensitivity `c` and weights `w`, Kalman `r_value`, Rule `error_prob`) for each participant.
    * Performing quantitative model comparison (e.g., using LOO-CV or WAIC) to determine which model provides the best account of the human data, balancing model fit and complexity.
    * Analyzing individual differences in strategy use by examining which model best fits each participant and the distribution of estimated parameters.

By combining forward simulation with rigorous model fitting and comparison, we can gain deeper insights into the cognitive mechanisms underlying human categorization in complex, rule-based environments.

