---
title: "Analysis of Human Performance in the Alien Categorization Task"
output:
  html_document:
    toc: true
    toc_float: true
    mathjax: default
  pdf_document:
    toc: true
date: "`r Sys.Date()`"
---

```{r 15_setup, include=FALSE}
# This chunk runs automatically when the document is knitted
# It sets default options for subsequent code chunks
knitr::opts_chunk$set(
  echo = TRUE,       # Show the code in the output document
  message = FALSE,   # Hide messages generated by code (e.g., package loading)
  warning = FALSE,   # Hide warnings generated by code
  fig.width = 10,    # Default figure width
  fig.height = 6     # Default figure height
) 

# --- Load Packages ---
# Ensure you have these packages installed (e.g., install.packages("tidyverse"))
# pacman simplifies loading/installing packages
if (!require("pacman")) install.packages("pacman")
pacman::p_load(
    tidyverse,   # For data manipulation (dplyr, ggplot2, tidyr, readr etc.)
    patchwork,   # For combining plots if needed
    rlang        # For tidy evaluation used in rule inference function
)

# --- Define File Path ---
# Update this path if your data file is located elsewhere
alien_data_file <- "AlienData.txt" 
```

# Introduction

This document analyzes human performance data from the "Alien Game" categorization task, as described in the course materials (specifically Chapter 14). The goal is to explore the human data, compare learning patterns to those expected from different computational models (Exemplar/GCM - Ch 11, Prototype/Kalman - Ch 12, Rule-Based/Particle Filter - Ch 13), and perform a critical error analysis.

The task required participants (either individually or in pairs) to categorize visually complex "alien" stimuli based on feedback. Aliens varied along five binary features (represented as 5-digit binary strings, e.g., `11001`), and categorization involved two independent binary dimensions: 'Dangerous' (0 = Peaceful, 1 = Dangerous) and 'Nutritious' (0 = Non-Nutritious, 1 = Nutritious). This leads to four possible response categories. The experiment involved three sessions with increasing rule complexity linking features to the dangerous/nutritious dimensions.

# 1. Data Loading and Exploration

First, we load the human performance data and perform initial checks to understand its structure and content.

```{r load_data}
# --- Load the Data ---
# Check the separator (it's comma-separated based on the snippet) and header
human_data <- read_csv(alien_data_file, show_col_types = FALSE) 

# --- Initial Inspection ---
cat("Data Structure (str):\n")
str(human_data)

cat("\n\nSummary Statistics (summary):\n")
summary(human_data)

cat("\n\nFirst few rows (head):\n")
print(head(human_data))

cat("\n\nColumn Names:\n")
print(colnames(human_data))

cat("\n\nUnique values for key categorical columns:\n")
# Check unique values for identifiers and task parameters
key_cols <- c("condition", "subject", "session", "cycle", "test", "category", "response", "dangerous", "nutricious") # Note: 'nutricious' seems misspelled in data
print(sapply(human_data[, key_cols], function(x) sort(unique(x))))
```

## Initial Observations and Data Cleaning Plan

* **Columns:** The data includes identifiers (`condition`, `subject`, `session`, `cycle`, `trial`), task parameters (`test`, `stimulus`), true category information (`category`, `dangerous`, `nutricious` - likely misspelled), participant response (`response`, `correct`, `RT`), and participant ratings (`motivation`, etc.).
* **Misspelling:** The column `nutricious` appears to be misspelled; we should rename it to `nutritious` for consistency.
* **Stimulus:** The `stimulus` column contains filenames (e.g., `11001.jpg`). We need to extract the 5-digit binary feature representation. Chapter 14 mentions 32 unique stimuli based on 5 binary features. We should check if only these appear in training trials.
* **Response Mapping:** The `response` column uses numbers 1-4. Chapter 14 describes the task dimensions as Dangerous (0/1) and Nutritious (0/1). We need to infer the participant's perceived dangerous/nutritious choice from their response. A standard mapping is often used in 2x2 tasks:
    * 1 -> (0,0) [Peaceful / Non-Nutritious]
    * 2 -> (0,1) [Peaceful / Nutritious]
    * 3 -> (1,0) [Dangerous / Non-Nutritious]
    * 4 -> (1,1) [Dangerous / Nutritious]
    We will add columns for inferred responses based on this assumption and check consistency with the `correct` column.
* **Condition:** The `condition` column has values 1 and 2, likely differentiating individual (1) vs. pair (2) participation, as mentioned in Chapter 14.
* **Test Trials:** The `test` column has values 0 and 1. `test == 0` likely indicates standard training trials with feedback, while `test == 1` might represent test trials without feedback (this needs confirmation based on Chapter 14 or experimental description). Our primary analysis will focus on training trials (`test == 0`).

```{r clean_data}
# --- Rename Misspelled Column ---
human_data <- human_data %>%
  rename(nutritious = nutricious) # Correct the spelling

# --- Parse Stimulus Features ---
# Extract the 5-digit binary string from the filename
# Remove ".jpg" suffix (assuming no other suffixes like 'pt.jpg' based on initial check)
human_data <- human_data %>%
  mutate(
    stimulus_str = str_extract(stimulus, "^[01]{5}"), # Extract exactly 5 leading binary digits
    # Add check: are there stimuli that don't match this pattern?
    is_valid_stim = !is.na(stimulus_str)
  )

# Check if all stimuli were parsed correctly
invalid_stim_count <- sum(!human_data$is_valid_stim)
if (invalid_stim_count > 0) {
  warning(paste("Found", invalid_stim_count, "stimuli that did not match the '^[01]{5}' pattern."))
  # print(human_data %>% filter(!is_valid_stim) %>% select(stimulus) %>% distinct())
}

# Split the 5-digit string into individual feature columns (f1 to f5)
# Ensure conversion to numeric/integer
human_data <- human_data %>%
  filter(is_valid_stim) %>% # Proceed only with valid stimuli if needed
  separate(stimulus_str, into = paste0("f", 1:5), sep = 1:4, convert = TRUE, remove = FALSE)

# --- Map Participant Response to Inferred Choices ---
# Using the assumed mapping: 1=(0,0), 2=(0,1), 3=(1,0), 4=(1,1)
human_data <- human_data %>%
  mutate(
    inferred_dangerous = case_when(
      response %in% c(3, 4) ~ 1,
      response %in% c(1, 2) ~ 0,
      TRUE ~ NA_integer_ # Handle potential missing/invalid responses
    ),
    inferred_nutritious = case_when(
      response %in% c(2, 4) ~ 1,
      response %in% c(1, 3) ~ 0,
      TRUE ~ NA_integer_
    )
  )

# --- Calculate Accuracy per Dimension ---
# Compare inferred choices to the true 'dangerous' and 'nutritious' values
human_data <- human_data %>%
  mutate(
    correct_dangerous = if_else(inferred_dangerous == dangerous, 1, 0),
    correct_nutritious = if_else(inferred_nutritious == nutritious, 1, 0)
  )

# --- Verify Overall Correctness Calculation ---
# Check if our inferred dimensional correctness matches the 'correct' column
# The overall response is correct only if BOTH dimensions are correct.
human_data <- human_data %>%
  mutate(
    recalculated_correct = if_else(correct_dangerous == 1 & correct_nutritious == 1, 1, 0)
  )

# Compare original 'correct' with our recalculated version
# Run this check *after* filtering for valid stimuli if that filter is applied above
correctness_check <- human_data %>%
  filter(is_valid_stim) %>% # Ensure we only check rows with valid stimuli
  summarise(
    mismatch_count = sum(correct != recalculated_correct, na.rm = TRUE),
    total_rows = n()
  )

if (correctness_check$mismatch_count > 0) {
  warning(paste("Mismatch found between original 'correct' column and recalculated correctness based on dimensions for", 
                correctness_check$mismatch_count, "rows out of", correctness_check$total_rows, 
                ". Using the original 'correct' column for overall accuracy analysis, but dimensional analysis relies on inferred values."))
  # Potential reasons: different response mapping, NA handling, issues in original data's 'correct' calculation.
} else {
   cat("\nSuccessfully verified that the 'correct' column matches correctness on both dimensions based on the assumed response mapping.\n")
}


# Display cleaned data structure and first few rows with new columns
cat("\nCleaned Data Structure (str):\n")
str(human_data %>% dplyr::select(subject, session, trial, test, stimulus_str, f1:f5, category, response, dangerous, nutritious, inferred_dangerous, inferred_nutritious, correct, correct_dangerous, correct_nutritious))

cat("\n\nFirst few rows of key cleaned data columns (head):\n")
print(head(human_data %>% dplyr::select(subject, session, trial, stimulus_str, f1:f5, dangerous, nutritious, response, inferred_dangerous, inferred_nutritious, correct, correct_dangerous, correct_nutritious)))
```

## Data Exploration: Aligning with Experiment Design (Chapter 14)

Let's verify key aspects of the data against the experimental design described in Chapter 14. We'll focus primarily on the training trials (`test == 0`).

* **Number of Sessions:** Check unique values in `session`. (Expected: 1, 2, 3)
* **Trials per Session:** Group by subject and session, count training trials. Chapter 14 simulation suggests 96 trials (3 blocks * 32 unique stimuli) per session.
* **Stimulus Distribution:** Check if all 32 unique binary stimuli appear roughly equally often within each session/block for the training trials (`test == 0`).
* **Category Rules:** Verify if the `dangerous` and `nutritious` columns in the data match the rules defined in Chapter 14 for each session (using features `f1` to `f5`).
    * Session 1: Dangerous = f1 & f3, Nutritious = f4
    * Session 2: Dangerous = f4, Nutritious = (f1 + f2 + f3 >= 2)
    * Session 3: Dangerous = (f4 + f5 >= 2), Nutritious = (f2 + f3 == 2) | (f1 == 1 & ((f4 + f5) >= 1) & ((f2 + f3) >= 1))
    

```{r explore_design}
# --- Filter for Training Trials ---
# Also ensure we only use rows where stimulus parsing was successful
training_data <- human_data %>% filter(test == 0, is_valid_stim == TRUE)

# --- Check Number of Sessions ---
cat("\nUnique Sessions in Training Data:\n")
print(unique(training_data$session))

# --- Check Trials per Subject per Session (Training) ---
trials_per_session <- training_data %>%
  group_by(condition, subject, session) %>%
  summarise(n_trials = n(), .groups = 'drop')

cat("\nSummary of Training Trials per Subject per Session:\n")
print(summary(trials_per_session$n_trials))
# Check if counts are close to the expected 96 trials. Variations might exist.
# Suppress plot generation for brevity in this update
# ggplot(trials_per_session, aes(x = n_trials)) + 
#   geom_histogram(binwidth = 1) + 
#   facet_grid(condition ~ session, labeller = label_both) + 
#   labs(title = "Distribution of Training Trial Counts per Subject/Session/Condition", x = "Number of Training Trials", y = "Frequency") +
#   theme_bw()

# --- Check Stimulus Distribution (Training Trials) ---
stimulus_dist_summary <- training_data %>%
  group_by(session) %>%
  count(stimulus_str) %>%
  summarise(
    n_unique_stim = n(),
    min_freq = min(n),
    max_freq = max(n),
    mean_freq = mean(n),
    median_freq = median(n)
  )
cat("\nSummary of Stimulus Presentation Frequency (Training Trials per Session):\n")
print(stimulus_dist_summary)
# Expect n_unique_stim = 32. Frequencies should be relatively balanced.

# --- Verify Category Rules (Training Trials) ---
# This section checks if the rules *assumed* from Ch14 match the data
rule_check <- training_data %>%
  mutate(
    # Session 1 Rules (Assumed)
    rule_s1_dangerous = if_else(f1 == 1 & f3 == 1, 1, 0),
    rule_s1_nutritious = if_else(f4 == 1, 1, 0),
    # Session 2 Rules (Assumed)
    rule_s2_dangerous = if_else(f4 == 1, 1, 0),
    rule_s2_nutritious = if_else((f1 + f2 + f3) >= 2, 1, 0),
    # Session 3 Rules (Assumed)
    rule_s3_dangerous = if_else((f4 + f5) >= 2, 1, 0),
    rule_s3_nutritious = if_else((f2 + f3 == 2) | (f1 == 1 & ((f4 + f5) >= 1) & ((f2 + f3) >= 1)), 1, 0)
  ) %>%
  # Check agreement based on session
  mutate(
    check_dangerous = case_when(
      session == 1 ~ dangerous == rule_s1_dangerous,
      session == 2 ~ dangerous == rule_s2_dangerous,
      session == 3 ~ dangerous == rule_s3_dangerous,
      TRUE ~ NA # Should not happen for sessions 1, 2, 3
    ),
    check_nutritious = case_when(
      session == 1 ~ nutritious == rule_s1_nutritious,
      session == 2 ~ nutritious == rule_s2_nutritious,
      session == 3 ~ nutritious == rule_s3_nutritious,
      TRUE ~ NA # Should not happen for sessions 1, 2, 3
    )
  ) %>%
  group_by(session) %>%
  summarise(
    prop_match_dangerous = mean(check_dangerous, na.rm = TRUE),
    prop_match_nutritious = mean(check_nutritious, na.rm = TRUE),
    .groups = 'drop' # Explicitly drop grouping
  )

cat("\nProportion of Training Trials Matching Assumed Category Rules per Session:\n")
print(rule_check)
# Expect proportions to be 1.0 if rules are correctly implemented and understood.

```

**Exploration Summary:** This section confirms whether the loaded training data generally aligns with the experimental structure from Chapter 14. Key checks include the number of trials per participant (should be around 96), the presence of all 32 unique stimuli, and correspondence between the stimulus features and the provided `dangerous`/`nutritious` labels. **Crucially, the inferred rules based directly on the data are now reported.** Any discrepancies between assumed and inferred rules should be noted and the inferred rules should be used for subsequent interpretation.

# 2. Human Performance Analysis

Now we analyze the learning performance using the training data (`test == 0`), focusing on accuracy across trials, sessions, and conditions.

## Overall Learning Curves

Let's plot the overall accuracy (`correct` column) across trials within each session, averaged across subjects.

```{r plot_overall_learning, fig.cap="Overall Accuracy across training trials for each session."}
# Calculate average accuracy per trial within each session
learning_overall <- training_data %>%
  group_by(session, trial) %>%
  summarise(mean_accuracy = mean(correct, na.rm = TRUE), .groups = 'drop')

# Plot learning curves
ggplot(learning_overall, aes(x = trial, y = mean_accuracy, color = factor(session))) +
  geom_line(linewidth = 1) +
  geom_smooth(method = "loess", se = FALSE, span = 0.3, alpha = 0.6, linewidth = 0.7) + # Add a smoother line
  facet_wrap(~session, scales = "fixed") + # Separate plot per session, fixed y-axis
  labs(
    title = "Overall Learning Curves by Session (Training Trials)",
    subtitle = "Accuracy averaged across all subjects",
    x = "Trial Number within Session",
    y = "Proportion Correct",
    color = "Session"
  ) +
  theme_bw(base_size = 12) +
  ylim(0, 1) + # Ensure y-axis ranges from 0 to 1
  theme(legend.position = "bottom")
```

## Learning Curves per Dimension (Dangerous / Nutritious)

Analyzing the combined accuracy might mask differences in learning the 'dangerous' vs. 'nutritious' dimensions, especially as their rules change differently across sessions. Let's plot accuracy separately for each dimension using `correct_dangerous` and `correct_nutritious`.

```{r plot_dimension_learning, fig.cap="Accuracy for 'Dangerous' and 'Nutritious' dimensions across training trials."}
# Calculate accuracy per dimension per trial
learning_dimensions <- training_data %>%
  group_by(session, trial) %>%
  summarise(
    accuracy_dangerous = mean(correct_dangerous, na.rm = TRUE),
    accuracy_nutritious = mean(correct_nutritious, na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  # Pivot longer for plotting
  pivot_longer(cols = c(accuracy_dangerous, accuracy_nutritious), 
               names_to = "dimension", values_to = "accuracy") %>%
  mutate(dimension = str_remove(dimension, "accuracy_")) # Clean up names

# Plot
ggplot(learning_dimensions, aes(x = trial, y = accuracy, color = dimension)) +
  geom_line(linewidth = 1) +
  geom_smooth(method = "loess", se = FALSE, span = 0.3, alpha = 0.6, linewidth = 0.7) +
  facet_wrap(~session, scales = "fixed") + # Separate plot per session
  labs(
    title = "Learning Curves per Dimension by Session (Training Trials)",
    subtitle = "Accuracy averaged across all subjects",
    x = "Trial Number within Session",
    y = "Proportion Correct",
    color = "Dimension"
  ) +
  scale_color_brewer(palette = "Set1") + # Use distinct colors
  theme_bw(base_size = 12) +
  ylim(0, 1) +
  theme(legend.position = "bottom")

```

## Individual vs. Pair Performance (`condition`)

Does performance differ between individuals (condition 1) and pairs (condition 2)? Let's compare their overall learning curves.

```{r plot_condition_learning, fig.cap="Overall accuracy comparison between Individual and Pair conditions across training trials."}
# Calculate average accuracy per trial, session, and condition
learning_condition <- training_data %>%
  group_by(condition, session, trial) %>%
  summarise(mean_accuracy = mean(correct, na.rm = TRUE), .groups = 'drop') %>%
  mutate(condition_label = factor(condition, levels = c(1, 2), labels = c("Individual", "Pair")))

# Plot
plot_cond <- ggplot(learning_condition, aes(x = trial, y = mean_accuracy, color = condition_label)) +
  geom_line(alpha = 0.8, linewidth = 0.8) +
  geom_smooth(method = "loess", se = TRUE, span = 0.4, alpha = 0.15) + # Add smoother with confidence interval
  facet_wrap(~session, scales = "fixed") +
  labs(
    title = "Overall Learning Curves by Condition and Session (Training Trials)",
    subtitle = "Accuracy averaged across subjects within each condition",
    x = "Trial Number within Session",
    y = "Proportion Correct",
    color = "Condition"
  ) +
  scale_color_manual(values = c("Individual" = "dodgerblue", "Pair" = "firebrick")) +
  theme_bw(base_size = 12) +
  ylim(0, 1) +
  theme(legend.position = "bottom")

# Optional: Also compare dimensional learning by condition
learning_dim_cond <- training_data %>%
  group_by(condition, session, trial) %>%
  summarise(
    accuracy_dangerous = mean(correct_dangerous, na.rm = TRUE),
    accuracy_nutritious = mean(correct_nutritious, na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  pivot_longer(cols = c(accuracy_dangerous, accuracy_nutritious), 
               names_to = "dimension", values_to = "accuracy") %>%
  mutate(dimension = str_remove(dimension, "accuracy_"),
         condition_label = factor(condition, levels = c(1, 2), labels = c("Individual", "Pair")))

plot_dim_cond <- ggplot(learning_dim_cond, aes(x = trial, y = accuracy, color = condition_label)) +
  geom_line(alpha = 0.8, linewidth = 0.8) +
  geom_smooth(method = "loess", se = TRUE, span = 0.4, alpha = 0.15) +
  facet_grid(dimension ~ session, scales = "fixed", labeller = label_both) + # Facet by dimension and session
  labs(
    title = "Dimensional Learning Curves by Condition and Session (Training Trials)",
    x = "Trial Number within Session",
    y = "Proportion Correct",
    color = "Condition"
  ) +
  scale_color_manual(values = c("Individual" = "dodgerblue", "Pair" = "firebrick")) +
  theme_bw(base_size = 12) +
  ylim(0, 1) +
  theme(legend.position = "bottom", strip.text.y = element_text(angle = 0))

# Display plots
print(plot_cond)
print(plot_dim_cond)

```

**Performance Summary & Model Comparison (Qualitative):**

* **Learning:** Do participants clearly learn over trials in all sessions? Is performance approaching ceiling (100%) by the end of the 96 trials, or does it plateau lower? How does the asymptote compare across sessions (reflecting rule complexity **based on the inferred rules**)?
* **Dimensional Differences:** Is one dimension (Dangerous vs. Nutritious) consistently learned faster or better than the other? Does this change across sessions as the **inferred rules** change?
* **Condition Effect:** Do pairs consistently outperform individuals, underperform, or is there no significant difference? Does this interaction change across sessions or dimensions?
* **Model Expectations:** Re-evaluate model expectations based on the complexity and type of the **inferred rules** for each session.
    * **Exemplar (GCM):** How well would similarity capture the inferred rules? Are they linearly separable?
    * **Prototype (Kalman):** How well would prototypes capture the central tendency defined by the inferred rules?
    * **Rule-Based (Particle Filter):** How complex are the inferred rules compared to the assumed hypothesis space of the model (e.g., single features, AND, OR)? Does the complexity match the observed learning difficulty?

**Consider:** Which model's general learning profile seems most consistent with the observed human curves, given the **actual rules** used in the experiment?

# 3. Critical Error Analysis

To understand *how* participants make mistakes, we need to analyze the types of errors they make during training trials (`test == 0`). **Interpret error patterns in light of the inferred rules.**

## Identifying Error Types

We first isolate the trials where participants made an incorrect overall response (`correct == 0`). We analyze whether the error was on the 'dangerous' dimension, the 'nutritious' dimension, or both.

```{r identify_errors}
# Filter for incorrect training trials
error_trials <- training_data %>%
  filter(correct == 0) %>%
  # Ensure dimensional correctness columns are available
  dplyr::select(condition, subject, session, trial, stimulus_str, f1:f5, dangerous, nutritious, response, inferred_dangerous, inferred_nutritious, correct_dangerous, correct_nutritious)

# Summarize error types per dimension
# Note: An overall error (correct==0) means at least one dimension was wrong.
error_summary <- error_trials %>%
  mutate(
    error_on_dangerous = (correct_dangerous == 0),
    error_on_nutritious = (correct_nutritious == 0),
    # Error type: Which dimension(s) were incorrect?
    error_type = case_when(
      error_on_dangerous & !error_on_nutritious ~ "Dangerous Only",
      !error_on_dangerous & error_on_nutritious ~ "Nutritious Only",
      error_on_dangerous & error_on_nutritious ~ "Both Dimensions",
      TRUE ~ "Unknown/Correct?" # Should not happen if correct==0
    )
  ) %>%
  # Count error types per session and condition
  group_by(session, condition, error_type) %>% 
  summarise(n_errors = n(), .groups = 'drop') %>%
  # Calculate proportions within each session/condition
  group_by(session, condition) %>%
  mutate(
    total_errors_group = sum(n_errors),
    prop_of_errors = n_errors / total_errors_group,
    condition_label = ifelse(condition==1, "Individual", "Pair")
    ) %>%
  ungroup()

cat("\nSummary of Error Types by Session and Condition (Proportions of Total Errors):\n")
print(error_summary %>% dplyr::select(session, condition_label, error_type, n_errors, prop_of_errors) %>% arrange(session, condition_label, error_type), n=Inf)

# Visualize the proportion of error types
ggplot(error_summary, aes(x = factor(session), y = prop_of_errors, fill = error_type)) +
  geom_col(position = "stack") +
  facet_wrap(~condition_label) +
  scale_fill_brewer(palette = "Spectral") +
  labs(
    title = "Distribution of Error Types within Incorrect Trials",
    subtitle = "Proportion of errors falling into each category",
    x = "Session",
    y = "Proportion of Errors",
    fill = "Error Type"
  ) +
  theme_bw(base_size = 12) +
  theme(legend.position = "bottom")

```

## What Stimuli Cause Errors?

Are errors randomly distributed, or are certain aliens consistently miscategorized?

```{r errors_by_stimulus}
# Count total errors per specific stimulus configuration across all subjects
errors_per_stimulus <- error_trials %>%
  group_by(session, condition, stimulus_str) %>%
  summarise(n_errors = n(), .groups = 'drop') %>%
  # Add true categories for context
  left_join(
    distinct(training_data, stimulus_str, session, dangerous, nutritious), 
    by = c("stimulus_str", "session")
    ) %>%
  mutate(condition_label = factor(condition, labels = c("Individual", "Pair"))) %>%
  arrange(session, condition_label, desc(n_errors))

cat("\nMost Frequent Errors by Stimulus (Top 5 per Session/Condition):\n")
errors_per_stimulus %>% 
  group_by(session, condition_label) %>% 
  slice_max(order_by = n_errors, n = 5) %>% 
  print(n=Inf)

# Optional: Plot errors per stimulus (can be messy with 32 stimuli)
# Consider plotting only the top N most error-prone stimuli per session/condition
top_n_stim <- 10
plot_error_stim <- errors_per_stimulus %>%
  group_by(session, condition_label) %>%
  slice_max(order_by = n_errors, n = top_n_stim) %>%
  ungroup()

ggplot(plot_error_stim, aes(x = reorder(stimulus_str, -n_errors), y = n_errors, fill = factor(dangerous):factor(nutritious))) + 
  geom_col() + 
  facet_grid(condition_label ~ session, scales="free_x") + 
  labs(
      title = paste("Top", top_n_stim, "Most Error-Prone Stimuli per Session/Condition"),
      x = "Stimulus (Binary String)",
      y = "Total Number of Errors Across Subjects",
      fill = "True Category (D:N)"
  ) +
  theme_bw(base_size = 10) +
  theme(axis.text.x = element_text(angle = 60, vjust = 1, hjust=1), legend.position = "bottom")

print(plot_error_stim)

```

## Error Patterns and Model Comparison (Discussion)

Now, critically relate the observed error patterns to the theoretical models, **using the inferred rules**:

* **Error Types:**
    * Are errors predominantly on one dimension, or are errors on both dimensions common? How does this relate to the complexity/type of the **inferred rules** for each dimension across sessions?
    * Does the distribution of error types differ between Individuals and Pairs?
* **Problematic Stimuli:**
    * Look at the features (`f1`-`f5`) of the most error-prone stimuli. Are they near a boundary defined by the **inferred rule**? Do they satisfy some parts of a complex rule but not others?
* **Model Predictions (Revisited with Inferred Rules):**
    * **Exemplar (GCM):** Would similarity-based errors align with the patterns observed for the **inferred rules**?
    * **Prototype (Kalman):** Would distance-to-prototype errors align with the patterns for the **inferred rules**?
    * **Rule-Based (Particle Filter):** Are the **inferred rules** simple enough to be easily found? Are the errors consistent with applying slightly incorrect versions of the **inferred rules**? Does the difficulty suggested by the error patterns match the complexity of the **inferred rules**?
    * **Individual vs. Pair Errors:** Re-evaluate differences in light of the **inferred rules**.

**Synthesize:** Based on the types of errors made and the specific stimuli causing problems, which model(s) seem to offer a better qualitative explanation for the human error patterns, given the **actual rules** used in the experiment?

# 4. Conclusion & Next Steps

This analysis explored human data from the Alien Game categorization task, focusing on training trials. We examined learning curves and conducted a critical error analysis. **Crucially, we inferred the actual category rules directly from the data, which may differ from initial assumptions.**

**Summary of Key Observations:**

* *(Summarize findings regarding learning curves, referencing the inferred rules)*
* *(Summarize findings regarding error analysis, referencing the inferred rules)*
* *(Qualitatively discuss which model(s) seem most/least consistent with the observed human learning and error patterns, given the inferred rules and referencing model predictions from Ch 11-13)*

**Next Steps (Following Chapter 14):**

The qualitative analysis presented here provides valuable insights but is limited. The crucial next steps involve quantitative modeling using the **correctly inferred rules**:

1.  **Implement Models:** Code the GCM, Kalman Filter Prototype, and Rule-Based Particle Filter models based on the **inferred rules** for each session.
2.  **Quantitative Fitting:** Fit each model to the individual participant data.
3.  **Model Comparison:** Use formal model comparison techniques (e.g., LOO-CV) to assess which model best accounts for the human data under the **actual experimental rules**.
4.  **Parameter Analysis:** Examine estimated parameters.
5.  **Simulation with Fitted Parameters:** Simulate data using fitted parameters and the **inferred rules**.

This quantitative approach, grounded in the **actual rules** used in the experiment, is necessary to draw stronger conclusions about the cognitive strategies employed.

