<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Candidate Verbal Models | Advanced Cognitive Modeling Notes</title>
  <meta name="description" content="My notes for the advanced cognitive modeling course - 2026" />
  <meta name="generator" content="bookdown 0.46 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Candidate Verbal Models | Advanced Cognitive Modeling Notes" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="My notes for the advanced cognitive modeling course - 2026" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Candidate Verbal Models | Advanced Cognitive Modeling Notes" />
  
  <meta name="twitter:description" content="My notes for the advanced cognitive modeling course - 2026" />
  

<meta name="author" content="Riccardo Fusaroli" />


<meta name="date" content="2026-01-30" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="building-models-of-strategic-decision-making.html"/>
<link rel="next" href="from-verbal-descriptions-to-formal-models.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="lib/css/bootstrap.min.css" type="text/css" />
<link rel="stylesheet" href="lib/css/style.css" type="text/css" />
<link rel="stylesheet" href="lib/css/lesson.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Advanced Cognitive Modeling</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Advanced Cognitive Modeling</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#course-philosophy-and-approach"><i class="fa fa-check"></i><b>1.1</b> Course Philosophy and Approach</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#course-structure-and-learning-path"><i class="fa fa-check"></i><b>1.2</b> Course Structure and Learning Path</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#prerequisites-and-preparation"><i class="fa fa-check"></i><b>1.3</b> Prerequisites and Preparation</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#course-resources"><i class="fa fa-check"></i><b>1.4</b> Course Resources</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#about-these-notes"><i class="fa fa-check"></i><b>1.5</b> About These Notes</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-pizza-experiment.html"><a href="the-pizza-experiment.html"><i class="fa fa-check"></i><b>2</b> The Pizza Experiment</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-pizza-experiment.html"><a href="the-pizza-experiment.html#from-pizza-to-cognitive-models-an-introduction"><i class="fa fa-check"></i><b>2.1</b> From Pizza to Cognitive Models: An Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="the-pizza-experiment.html"><a href="the-pizza-experiment.html#why-start-with-pizza"><i class="fa fa-check"></i><b>2.2</b> Why Start with Pizza?</a></li>
<li class="chapter" data-level="2.3" data-path="the-pizza-experiment.html"><a href="the-pizza-experiment.html#learning-objectives"><i class="fa fa-check"></i><b>2.3</b> Learning Objectives</a></li>
<li class="chapter" data-level="2.4" data-path="the-pizza-experiment.html"><a href="the-pizza-experiment.html#part-1-exploring-the-pizza-stone-temperature-data"><i class="fa fa-check"></i><b>2.4</b> Part 1: Exploring the Pizza Stone Temperature Data</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="the-pizza-experiment.html"><a href="the-pizza-experiment.html#initial-data-visualization"><i class="fa fa-check"></i><b>2.4.1</b> Initial Data Visualization</a></li>
<li class="chapter" data-level="2.4.2" data-path="the-pizza-experiment.html"><a href="the-pizza-experiment.html#key-observations"><i class="fa fa-check"></i><b>2.4.2</b> Key Observations</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="the-pizza-experiment.html"><a href="the-pizza-experiment.html#part-2-initial-statistical-modeling"><i class="fa fa-check"></i><b>2.5</b> Part 2: Initial Statistical Modeling</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="the-pizza-experiment.html"><a href="the-pizza-experiment.html#model-setup-and-priors"><i class="fa fa-check"></i><b>2.5.1</b> Model Setup and Priors</a></li>
<li class="chapter" data-level="2.5.2" data-path="the-pizza-experiment.html"><a href="the-pizza-experiment.html#linear-mixed-effects-model"><i class="fa fa-check"></i><b>2.5.2</b> Linear Mixed-Effects Model</a></li>
<li class="chapter" data-level="2.5.3" data-path="the-pizza-experiment.html"><a href="the-pizza-experiment.html#lognormal-mixed-effects-model"><i class="fa fa-check"></i><b>2.5.3</b> Lognormal Mixed-Effects Model</a></li>
<li class="chapter" data-level="2.5.4" data-path="the-pizza-experiment.html"><a href="the-pizza-experiment.html#model-comparison-and-visualization"><i class="fa fa-check"></i><b>2.5.4</b> Model Comparison and Visualization</a></li>
<li class="chapter" data-level="2.5.5" data-path="the-pizza-experiment.html"><a href="the-pizza-experiment.html#model-assessment"><i class="fa fa-check"></i><b>2.5.5</b> Model Assessment</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="the-pizza-experiment.html"><a href="the-pizza-experiment.html#part-3-understanding-the-physics-model"><i class="fa fa-check"></i><b>2.6</b> Part 3: Understanding the Physics Model</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="the-pizza-experiment.html"><a href="the-pizza-experiment.html#the-basic-temperature-evolution-equation"><i class="fa fa-check"></i><b>2.6.1</b> The Basic Temperature Evolution Equation</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="the-pizza-experiment.html"><a href="the-pizza-experiment.html#part-4-implementing-the-physics-based-model"><i class="fa fa-check"></i><b>2.7</b> Part 4: Implementing the Physics-Based Model</a></li>
<li class="chapter" data-level="2.8" data-path="the-pizza-experiment.html"><a href="the-pizza-experiment.html#part-5-model-analysis-and-practical-applications"><i class="fa fa-check"></i><b>2.8</b> Part 5: Model Analysis and Practical Applications</a></li>
<li class="chapter" data-level="2.9" data-path="the-pizza-experiment.html"><a href="the-pizza-experiment.html#conclusion-from-pizza-to-cognitive-principles"><i class="fa fa-check"></i><b>2.9</b> Conclusion: From Pizza to Cognitive Principles</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="building-models-of-strategic-decision-making.html"><a href="building-models-of-strategic-decision-making.html"><i class="fa fa-check"></i><b>3</b> Building Models of Strategic Decision-Making</a>
<ul>
<li class="chapter" data-level="3.1" data-path="building-models-of-strategic-decision-making.html"><a href="building-models-of-strategic-decision-making.html#learning-goals"><i class="fa fa-check"></i><b>3.1</b> Learning Goals</a></li>
<li class="chapter" data-level="3.2" data-path="building-models-of-strategic-decision-making.html"><a href="building-models-of-strategic-decision-making.html#introduction-observing-behavior-to-theorize-mechanisms"><i class="fa fa-check"></i><b>3.2</b> Introduction: Observing Behavior to Theorize Mechanisms</a></li>
<li class="chapter" data-level="3.3" data-path="building-models-of-strategic-decision-making.html"><a href="building-models-of-strategic-decision-making.html#the-matching-pennies-game"><i class="fa fa-check"></i><b>3.3</b> The Matching Pennies Game</a></li>
<li class="chapter" data-level="3.4" data-path="building-models-of-strategic-decision-making.html"><a href="building-models-of-strategic-decision-making.html#game-structure"><i class="fa fa-check"></i><b>3.4</b> Game Structure</a></li>
<li class="chapter" data-level="3.5" data-path="building-models-of-strategic-decision-making.html"><a href="building-models-of-strategic-decision-making.html#empirical-investigation"><i class="fa fa-check"></i><b>3.5</b> Empirical Investigation</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="building-models-of-strategic-decision-making.html"><a href="building-models-of-strategic-decision-making.html#data-collection-protocol"><i class="fa fa-check"></i><b>3.5.1</b> Data Collection Protocol</a></li>
<li class="chapter" data-level="3.5.2" data-path="building-models-of-strategic-decision-making.html"><a href="building-models-of-strategic-decision-making.html#initial-observations"><i class="fa fa-check"></i><b>3.5.2</b> Initial Observations</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="building-models-of-strategic-decision-making.html"><a href="building-models-of-strategic-decision-making.html#empirical-explorations"><i class="fa fa-check"></i><b>3.6</b> Empirical explorations</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="building-models-of-strategic-decision-making.html"><a href="building-models-of-strategic-decision-making.html#from-observation-to-theory-identifying-potential-mechanisms"><i class="fa fa-check"></i><b>3.6.1</b> From Observation to Theory: Identifying Potential Mechanisms</a></li>
<li class="chapter" data-level="3.6.2" data-path="building-models-of-strategic-decision-making.html"><a href="building-models-of-strategic-decision-making.html#the-distinction-between-participant-and-researcher-perspectives"><i class="fa fa-check"></i><b>3.6.2</b> The distinction between participant and researcher perspectives</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="candidate-verbal-models.html"><a href="candidate-verbal-models.html"><i class="fa fa-check"></i><b>4</b> Candidate Verbal Models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="candidate-verbal-models.html"><a href="candidate-verbal-models.html#plausibility-check-cognitive-constraints"><i class="fa fa-check"></i><b>4.1</b> Plausibility Check: Cognitive Constraints</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="candidate-verbal-models.html"><a href="candidate-verbal-models.html#memory-limitations"><i class="fa fa-check"></i><b>4.1.1</b> Memory Limitations</a></li>
<li class="chapter" data-level="4.1.2" data-path="candidate-verbal-models.html"><a href="candidate-verbal-models.html#perseveration-tendencies"><i class="fa fa-check"></i><b>4.1.2</b> Perseveration Tendencies</a></li>
<li class="chapter" data-level="4.1.3" data-path="candidate-verbal-models.html"><a href="candidate-verbal-models.html#noise-and-errors"><i class="fa fa-check"></i><b>4.1.3</b> Noise and Errors</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="candidate-verbal-models.html"><a href="candidate-verbal-models.html#relationships-between-models"><i class="fa fa-check"></i><b>4.2</b> Relationships Between Models</a></li>
<li class="chapter" data-level="4.3" data-path="candidate-verbal-models.html"><a href="candidate-verbal-models.html#handling-heterogeneity-mixture-models"><i class="fa fa-check"></i><b>4.3</b> Handling Heterogeneity: Mixture Models</a></li>
<li class="chapter" data-level="4.4" data-path="candidate-verbal-models.html"><a href="candidate-verbal-models.html#cognitive-modeling-vs.-traditional-statistical-approaches-e.g.-glm"><i class="fa fa-check"></i><b>4.4</b> Cognitive Modeling vs. Traditional Statistical Approaches (e.g., GLM)</a></li>
<li class="chapter" data-level="4.5" data-path="candidate-verbal-models.html"><a href="candidate-verbal-models.html#conclusion-from-observations-to-verbal-theories"><i class="fa fa-check"></i><b>4.5</b> Conclusion: From Observations to Verbal Theories</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="from-verbal-descriptions-to-formal-models.html"><a href="from-verbal-descriptions-to-formal-models.html"><i class="fa fa-check"></i><b>5</b> From verbal descriptions to formal models</a>
<ul>
<li class="chapter" data-level="5.1" data-path="from-verbal-descriptions-to-formal-models.html"><a href="from-verbal-descriptions-to-formal-models.html#learning-goals-1"><i class="fa fa-check"></i><b>5.1</b> Learning Goals</a></li>
<li class="chapter" data-level="5.2" data-path="from-verbal-descriptions-to-formal-models.html"><a href="from-verbal-descriptions-to-formal-models.html#the-value-of-formalization-and-simulation"><i class="fa fa-check"></i><b>5.2</b> The Value of Formalization and Simulation</a></li>
<li class="chapter" data-level="5.3" data-path="from-verbal-descriptions-to-formal-models.html"><a href="from-verbal-descriptions-to-formal-models.html#defining-general-conditions"><i class="fa fa-check"></i><b>5.3</b> Defining general conditions</a></li>
<li class="chapter" data-level="5.4" data-path="from-verbal-descriptions-to-formal-models.html"><a href="from-verbal-descriptions-to-formal-models.html#implementing-a-random-agent"><i class="fa fa-check"></i><b>5.4</b> Implementing a Random Agent</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="from-verbal-descriptions-to-formal-models.html"><a href="from-verbal-descriptions-to-formal-models.html#encapsulating-the-agent-in-a-function"><i class="fa fa-check"></i><b>5.4.1</b> Encapsulating the Agent in a Function</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="from-verbal-descriptions-to-formal-models.html"><a href="from-verbal-descriptions-to-formal-models.html#implementing-a-win-stay-lose-shift-wsls-agent"><i class="fa fa-check"></i><b>5.5</b> Implementing a Win-Stay-Lose-Shift (WSLS) Agent</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="from-verbal-descriptions-to-formal-models.html"><a href="from-verbal-descriptions-to-formal-models.html#implementing-the-wsls-function"><i class="fa fa-check"></i><b>5.5.1</b> Implementing the WSLS Function</a></li>
<li class="chapter" data-level="5.5.2" data-path="from-verbal-descriptions-to-formal-models.html"><a href="from-verbal-descriptions-to-formal-models.html#simulating-wsls-vs.-opponents"><i class="fa fa-check"></i><b>5.5.2</b> Simulating WSLS vs. Opponents</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="from-verbal-descriptions-to-formal-models.html"><a href="from-verbal-descriptions-to-formal-models.html#scaling-up-the-virtual-experiment"><i class="fa fa-check"></i><b>5.6</b> Scaling Up: The Virtual Experiment</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="from-verbal-descriptions-to-formal-models.html"><a href="from-verbal-descriptions-to-formal-models.html#visualizing-results"><i class="fa fa-check"></i><b>5.6.1</b> Visualizing Results</a></li>
<li class="chapter" data-level="5.6.2" data-path="from-verbal-descriptions-to-formal-models.html"><a href="from-verbal-descriptions-to-formal-models.html#the-problem-of-inference"><i class="fa fa-check"></i><b>5.6.2</b> The Problem of Inference</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="from-simulation-to-model-fitting.html"><a href="from-simulation-to-model-fitting.html"><i class="fa fa-check"></i><b>6</b> From simulation to model fitting</a>
<ul>
<li class="chapter" data-level="6.1" data-path="from-simulation-to-model-fitting.html"><a href="from-simulation-to-model-fitting.html#learning-goals-2"><i class="fa fa-check"></i><b>6.1</b> Learning Goals</a></li>
<li class="chapter" data-level="6.2" data-path="from-simulation-to-model-fitting.html"><a href="from-simulation-to-model-fitting.html#the-challenge-inferring-latent-parameters"><i class="fa fa-check"></i><b>6.2</b> The Challenge: Inferring Latent Parameters</a></li>
<li class="chapter" data-level="6.3" data-path="from-simulation-to-model-fitting.html"><a href="from-simulation-to-model-fitting.html#simulating-data"><i class="fa fa-check"></i><b>6.3</b> Simulating data</a></li>
<li class="chapter" data-level="6.4" data-path="from-simulation-to-model-fitting.html"><a href="from-simulation-to-model-fitting.html#building-our-first-stan-model-inferring-bias-rate"><i class="fa fa-check"></i><b>6.4</b> Building our First Stan Model: Inferring Bias Rate</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="from-simulation-to-model-fitting.html"><a href="from-simulation-to-model-fitting.html#model"><i class="fa fa-check"></i><b>6.4.1</b> Model</a></li>
<li class="chapter" data-level="6.4.2" data-path="from-simulation-to-model-fitting.html"><a href="from-simulation-to-model-fitting.html#assessing-model-quality"><i class="fa fa-check"></i><b>6.4.2</b> Assessing model quality</a></li>
<li class="chapter" data-level="6.4.3" data-path="from-simulation-to-model-fitting.html"><a href="from-simulation-to-model-fitting.html#summarizing-the-results"><i class="fa fa-check"></i><b>6.4.3</b> Summarizing the results</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="from-simulation-to-model-fitting.html"><a href="from-simulation-to-model-fitting.html#validating-the-model-parameter-recovery"><i class="fa fa-check"></i><b>6.5</b> Validating the Model: Parameter Recovery</a></li>
<li class="chapter" data-level="6.6" data-path="from-simulation-to-model-fitting.html"><a href="from-simulation-to-model-fitting.html#moving-beyond-simple-bias-memory-models"><i class="fa fa-check"></i><b>6.6</b> Moving Beyond Simple Bias: Memory Models</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="from-simulation-to-model-fitting.html"><a href="from-simulation-to-model-fitting.html#memory-model-1-glm-like-approach-external-predictor"><i class="fa fa-check"></i><b>6.6.1</b> Memory Model 1: GLM-like Approach (External Predictor)</a></li>
<li class="chapter" data-level="6.6.2" data-path="from-simulation-to-model-fitting.html"><a href="from-simulation-to-model-fitting.html#summarizing-the-results-1"><i class="fa fa-check"></i><b>6.6.2</b> Summarizing the results</a></li>
<li class="chapter" data-level="6.6.3" data-path="from-simulation-to-model-fitting.html"><a href="from-simulation-to-model-fitting.html#memory-model-2-internal-state-variable"><i class="fa fa-check"></i><b>6.6.3</b> Memory Model 2: Internal State Variable</a></li>
<li class="chapter" data-level="6.6.4" data-path="from-simulation-to-model-fitting.html"><a href="from-simulation-to-model-fitting.html#memory-model-3-exponential-forgetting-relation-to-rl"><i class="fa fa-check"></i><b>6.6.4</b> Memory Model 3: Exponential Forgetting (Relation to RL)</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="from-simulation-to-model-fitting.html"><a href="from-simulation-to-model-fitting.html#memory-model-4-bayesian-agent-optimal-updating"><i class="fa fa-check"></i><b>6.7</b> Memory Model 4: Bayesian Agent (Optimal Updating)</a></li>
<li class="chapter" data-level="6.8" data-path="from-simulation-to-model-fitting.html"><a href="from-simulation-to-model-fitting.html#relationship-to-rescorla-wagner"><i class="fa fa-check"></i><b>6.8</b> Relationship to Rescorla-Wagner</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="from-simulation-to-model-fitting.html"><a href="from-simulation-to-model-fitting.html#connection-to-kalman-filters"><i class="fa fa-check"></i><b>6.8.1</b> Connection to Kalman Filters</a></li>
<li class="chapter" data-level="6.8.2" data-path="from-simulation-to-model-fitting.html"><a href="from-simulation-to-model-fitting.html#connection-to-hierarchical-gaussian-filter-hgf"><i class="fa fa-check"></i><b>6.8.2</b> Connection to Hierarchical Gaussian Filter (HGF)</a></li>
<li class="chapter" data-level="6.8.3" data-path="from-simulation-to-model-fitting.html"><a href="from-simulation-to-model-fitting.html#implications-for-model-development"><i class="fa fa-check"></i><b>6.8.3</b> Implications for Model Development</a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="from-simulation-to-model-fitting.html"><a href="from-simulation-to-model-fitting.html#conclusion-estimating-parameters-and-exploring-memory"><i class="fa fa-check"></i><b>6.9</b> Conclusion: Estimating Parameters and Exploring Memory</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Advanced Cognitive Modeling Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="candidate-verbal-models" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Chapter 4</span> Candidate Verbal Models<a href="candidate-verbal-models.html#candidate-verbal-models" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Based on the behavioral patterns observed, participant discussions, and core cognitive principles, we can now propose several candidate <em>verbal models</em> for behavior in the matching pennies game. Each represents a different hypothesis about the underlying cognitive strategy. These are starting points, deliberately simplified:</p>
<ul>
<li><strong>Random Choice Model:</strong> Reflects the idea that players might try to be unpredictable or simply lack a clear strategy.</li>
<li><strong>Win-Stay-Lose-Shift (WSLS):</strong> Captures the common heuristic of repeating successful actions and changing unsuccessful ones.</li>
<li><strong>Bias Tracking (Memory Models):</strong> Addresses the observation that players seem to react to opponent patterns, potentially by estimating choice frequencies. Variations account for memory limits.</li>
<li><strong>Reinforcement Learning:</strong> A more formal learning model capturing trial-by-trial value updates based on prediction errors, potentially explaining adaptation.</li>
<li><strong>k-ToM (Theory of Mind):</strong> Accounts for the strategic nature of the game, where players model their opponent’s mind (or model the opponent modeling <em>their</em> mind).</li>
</ul>
<p>These verbal models, derived from our initial analysis, are the hypotheses we will formalize and test in subsequent chapters. Let’s briefly describe the core idea of each:</p>
<ul>
<li><strong>Random Choice Model:</strong>
<ul>
<li><em>Mechanism:</em> Choices are made randomly, potentially with a fixed bias (e.g., 60% right, 40% left), independent of game history or opponent actions.</li>
<li><em>Rationale:</em> Serves as the simplest baseline. It reflects the possibility that players might try to be deliberately unpredictable, haven’t figured out a strategy, or that their behavior appears random from the observer’s perspective. It introduces the basic concept of choice probability (the <span class="math inline">\(\theta\)</span> parameter we’ll estimate later).</li>
</ul></li>
<li><strong>Win-Stay-Lose-Shift (WSLS):</strong>
<ul>
<li><em>Mechanism:</em> A simple heuristic: repeat the last choice if it led to a win, switch to the other choice if it led to a loss.</li>
<li><em>Rationale:</em> Captures the intuitive tendency to stick with success and abandon failure. This is a common heuristic observed in simple learning tasks.</li>
<li><em>Formalization Hint:</em> This can be formalized using probabilities of staying/shifting conditional on the previous outcome. A simple version might be deterministic (always stay/shift), while a probabilistic version allows for occasional deviations:
<span class="math display">\[P(\text{stay} | \text{outcome}_{t-1}) = \begin{cases} p_{stay\_win} &amp; \text{if win at } t-1 \\ p_{stay\_loss} &amp; \text{if loss at } t-1 \end{cases}\]</span>
(Note: <span class="math inline">\(p_{shift} = 1 - p_{stay}\)</span>). The original formula used <span class="math inline">\(p_w\)</span> for staying after a win and <span class="math inline">\(1-p_l\)</span> for staying after a loss (meaning <span class="math inline">\(p_l\)</span> is the probability of <em>shifting</em> after a loss).</li>
</ul></li>
<li><strong>Memory-Based Bias Tracking:</strong>
<ul>
<li><em>Mechanism:</em> Assumes players estimate the opponent’s choice bias (e.g., probability of choosing ‘right’) based on past observations and use this estimate to guide their own choices (e.g., predict and counter).</li>
<li><em>Rationale:</em> Reflects the observation that players seem to react to opponent tendencies. Addresses the “what information do players use?” question.</li>
<li><em>Variations:</em>
<ul>
<li><em>Perfect Memory:</em> Uses all past trials equally to estimate the bias. (Less plausible cognitively).</li>
<li><em>Imperfect/Limited Memory:</em> Uses only recent trials (e.g., last <em>n</em> trials) or gives more weight to recent trials (exponential decay), reflecting cognitive constraints.</li>
</ul></li>
</ul></li>
<li><strong>Reinforcement Learning (RL):</strong>
<ul>
<li><em>Mechanism:</em> Players learn the expected value of each choice (‘left’ vs. ‘right’) based on the rewards (wins/losses) received. Choices are made based on these learned values (e.g., choosing the option with the higher expected value more often). Learning occurs trial-by-trial based on prediction errors (difference between expected and actual reward) modulated by a <em>learning rate</em>.</li>
<li><em>Rationale:</em> Provides a formal framework for learning from feedback, naturally incorporating imperfect memory (controlled by the learning rate – high learning rate means relying more on recent trials). Connects to broader theories of learning in psychology and neuroscience. <em>(Preview: Formal RL models will be detailed in Chapter 12).</em></li>
</ul></li>
<li><strong>k-ToM (Theory of Mind):</strong>
<ul>
<li><em>Mechanism:</em> Players explicitly model their opponent’s strategy. Level 0 (0-ToM) assumes the opponent is random/biased. Level 1 (1-ToM) assumes the opponent is using a 0-ToM strategy and tries to best respond. Level 2 (2-ToM) assumes the opponent is using a 1-ToM strategy, and so on.</li>
<li><em>Rationale:</em> Directly addresses the strategic, interactive nature of the game, where predicting the opponent’s <em>intentions</em> or <em>model</em> of you might be crucial, going beyond simple pattern detection. <em>(Preview: These models will be discussed in later chapters).</em></li>
</ul></li>
<li><strong>Combined/Switching Strategies:</strong>
<ul>
<li><em>Mechanism:</em> Real behavior might involve combining elements of the above (e.g., WSLS with a baseline bias) or switching between strategies (e.g., tracking bias for a while, then switching to random). Generating complex, non-stationary sequences might also be a strategy to appear unpredictable.</li>
<li><em>Rationale:</em> Generating random output is hard, so if we want to confuse the opponent, we could act first choosing tail 8 times, and then switching to a WSLS strategy for 4 trials, and then choosing head 4 times. Or implementing any of the previous strategies and doing the opposite “to mess with the opponent”. The combined strategies model also acknowledges that single, simple models might be insufficient. Motivates concepts like <em>mixture models</em> (Chapter 8) where behavior is seen as a probabilistic blend of simpler strategies, or models where strategy parameters change over time. Yet these mixture models are hard to fit to data and can quickly spiral out of control (many possible combinations)</li>
</ul></li>
</ul>
<div id="plausibility-check-cognitive-constraints" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Plausibility Check: Cognitive Constraints<a href="candidate-verbal-models.html#plausibility-check-cognitive-constraints" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>You probably noticed you couldn’t remember every single move your opponent made. Maybe you lost track of older trials, and recent trials felt more important. These aren’t failures: they’re features of human cognition. Our models should reflect these constraints to be cognitively realistic. Let’s see what difference these constraints make for predictions.</p>
<div id="memory-limitations" class="section level3 hasAnchor" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Memory Limitations<a href="candidate-verbal-models.html#memory-limitations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><em>Constraint:</em> Humans have limited working memory and exhibit forgetting, often approximated by exponential decay. Perfect recall of long trial sequences is unrealistic.</li>
<li><em>Modeling Implication:</em> This favors models incorporating memory decay or finite history windows (like imperfect memory models or RL with a learning rate &lt; 1) over perfect memory models. It suggests that even bias-tracking models should discount older information.</li>
</ul>
</div>
<div id="perseveration-tendencies" class="section level3 hasAnchor" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> Perseveration Tendencies<a href="candidate-verbal-models.html#perseveration-tendencies" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><em>Constraint:</em> People sometimes exhibit perseveration – repeating a previous action, especially if it was recently successful or chosen, even if a different strategy might suggest otherwise. This can be distinct from rational “win-stay”.</li>
<li><em>Modeling Implication:</em> This might be incorporated as an additional bias parameter influencing the choice probability (e.g., a small added probability of repeating the last action <span class="math inline">\(a_{t-1}\)</span> regardless of outcome) or interact with feedback processing (e.g., strengthening the ‘stay’ tendency after wins).</li>
</ul>
</div>
<div id="noise-and-errors" class="section level3 hasAnchor" number="4.1.3">
<h3><span class="header-section-number">4.1.3</span> Noise and Errors<a href="candidate-verbal-models.html#noise-and-errors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><em>Constraint:</em> Human behavior is inherently noisy. People make mistakes, have attentional lapses, press the wrong button, or misunderstand feedback. Behavior rarely perfectly matches a deterministic strategy.</li>
<li><em>Modeling Implication:</em> Models should almost always include a “noise” component. This can be implemented in several ways:
<ul>
<li><strong>Lapse Rate:</strong> A probability (e.g., <span class="math inline">\(\epsilon\)</span>) that on any given trial, the agent makes a random choice instead of following their primary strategy (as used in the Mixture Model chapter).</li>
<li><strong>Decision Noise (Softmax):</strong> In models where choices are based on comparing values (like RL), a ‘temperature’ parameter can control the stochasticity. High temperature leads to more random choices, low temperature leads to more deterministic choices based on values.</li>
<li><strong>Imperfect Heuristics:</strong> Parameters within a strategy might reflect imperfect application (e.g., in WSLS, <span class="math inline">\(p_{stay\_win} &lt; 1\)</span> or <span class="math inline">\(p_{shift\_loss} &lt; 1\)</span>). This can also capture asymmetric responses to feedback (e.g., being more likely to shift after a loss than stay after a win).</li>
<li><strong>Exploration:</strong> Random deviations can also be framed as adaptive exploration, allowing the agent to test actions that their current strategy deems suboptimal.</li>
</ul></li>
</ul>
</div>
</div>
<div id="relationships-between-models" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Relationships Between Models<a href="candidate-verbal-models.html#relationships-between-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>It’s useful to note that these candidate models aren’t always entirely distinct. Often, simpler models emerge as special cases of more complex ones:
* A Random Choice model is like a Memory-Based model where the influence of memory is zero.
* WSLS can be seen as a specific type of RL model with a very high learning rate and sensitivity only to the immediately preceding trial’s outcome.
* A 0-ToM model might resemble a Bias Tracking model.</p>
<p>Recognizing these connections can guide a principled modeling approach, starting simple and adding complexity only as needed and justified by data or theory.</p>
</div>
<div id="handling-heterogeneity-mixture-models" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Handling Heterogeneity: Mixture Models<a href="candidate-verbal-models.html#handling-heterogeneity-mixture-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>What if different participants use different strategies, or a single participant switches strategies during the game? This is where <em>mixture models</em> become relevant (explored in detail in Chapter 8).
* <em>Concept:</em> Instead of assuming <em>one</em> model generated all the data, a mixture model assumes the data is a probabilistic blend from <em>multiple</em> candidate models (e.g., 70% of choices from WSLS, 30% from Random Bias).
* <em>Purpose:</em> Allows capturing heterogeneity within or across individuals without needing to know <em>a priori</em> which strategy was used on which trial or by which person. The model estimates the <em>probability</em> that each data point came from each component strategy.
* <em>Challenge:</em> Mixture models often require substantial data to reliably distinguish between components and estimate their mixing proportions.</p>
</div>
<div id="cognitive-modeling-vs.-traditional-statistical-approaches-e.g.-glm" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> Cognitive Modeling vs. Traditional Statistical Approaches (e.g., GLM)<a href="candidate-verbal-models.html#cognitive-modeling-vs.-traditional-statistical-approaches-e.g.-glm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>How does this modeling approach differ from standard statistical analyses you might have learned, like ANOVAs or the General Linear Model - GLM?</p>
<ul>
<li><strong>Focus:</strong> GLM approaches typically focus on identifying <em>statistical effects</em>: Does factor X significantly influence outcome Y? (e.g., Does the opponent’s strategy affect the player’s win rate?). Cognitive modeling focuses on identifying the underlying <em>process or mechanism</em>: <em>How</em> does the opponent’s strategy lead to changes in the player’s choices via specific computations (like learning, memory updating, or strategic reasoning)?</li>
<li><strong>Theory:</strong> Cognitive models are usually derived from theories about mental processes. GLMs are more general statistical tools, often used agnostically regarding the specific cognitive mechanism.</li>
<li><strong>Parameters:</strong> Cognitive models estimate parameters that often have direct psychological interpretations (e.g., learning rate, memory decay, decision threshold, bias weight). GLM parameters represent statistical associations (e.g., regression coefficients).</li>
<li><strong>Data Level:</strong> Cognitive models often predict behavior at the trial level (e.g., predicting the choice on trial <em>t</em> based on history up to <em>t-1</em>). GLM analyses often aggregate data (e.g., comparing average win rates across conditions).</li>
<li><strong>Prediction vs. Explanation:</strong> While both aim to explain data, cognitive modeling often places a stronger emphasis on generating the observed behavior pattern from the hypothesized mechanism, allowing for simulation and prediction of fine-grained details.</li>
</ul>
<p><em>Example Revisited:</em> In the Matching Pennies game:
* A GLM approach might test if <code>Payoff ~ BotStrategy * Role + (1|ID)</code> shows a significant effect of <code>BotStrategy</code>.
* A cognitive modeling approach would fit different strategy models (WSLS, RL, etc.) to the choice data and compare them (using methods from Ch 7) to see which <em>mechanism</em> best explains the choices made against different bots, potentially revealing <em>why</em> performance differs (e.g., due to changes in estimated learning rates or strategy weights).</p>
<p>Both approaches are valuable, but cognitive modeling aims for a deeper, mechanistic level of explanation about the underlying cognitive processes.</p>
</div>
<div id="conclusion-from-observations-to-verbal-theories" class="section level2 hasAnchor" number="4.5">
<h2><span class="header-section-number">4.5</span> Conclusion: From Observations to Verbal Theories<a href="candidate-verbal-models.html#conclusion-from-observations-to-verbal-theories" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This chapter took us from observing behavior in a specific task – the Matching Pennies game – to the crucial stage of formulating initial theories about the cognitive processes involved. We explored how analyzing gameplay data, considering participant reports, applying cognitive principles (like memory limits and error proneness), and contrasting different potential strategies (Random, WSLS, Memory-based, RL, k-ToM) helps us generate plausible <em>verbal models</em>.</p>
<p>We saw that the path from raw behavior to a testable model involves significant abstraction and simplification. We also highlighted the importance of distinguishing between the participant’s experience and the researcher’s theoretical stance, and how cognitive modeling differs from traditional statistical approaches by focusing on underlying mechanisms.</p>
<p>You now have a conceptual map of candidate models and understand why cognitive constraints matter. But verbal descriptions like ‘win-stay-lose-shift’ hide crucial ambiguities: Does ‘stay’ mean always stay or usually stay? How do we handle the first trial? In Chapter 3, you’ll implement these models in code, forcing you to make every assumption explicit. This is where modeling becomes rigorous and often reveals that our verbal intuitions were vaguer than we thought.</p>
<p>The next chapter, “From verbal descriptions to formal models,” tackles exactly this challenge. We will take some of the candidate models discussed here (like Random Choice and WSLS) and translate them into precise mathematical algorithms and R functions. This formalization will force us to be explicit about our assumptions and enable us to simulate agent behavior, setting the stage for fitting these models to data and evaluating their performance in later chapters.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="building-models-of-strategic-decision-making.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="from-verbal-descriptions-to-formal-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/fusaroli/AdvancedCognitiveModeling/edit/master/02-BuildingModels.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["series.pdf"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
