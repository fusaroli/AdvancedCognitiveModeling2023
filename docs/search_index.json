[["index.html", "Advanced Cognitive Modeling Notes Chapter 1 Advanced Cognitive Modeling 1.1 Course Philosophy and Approach 1.2 Course Structure and Learning Path 1.3 Prerequisites and Preparation 1.4 Course Resources 1.5 About These Notes", " Advanced Cognitive Modeling Notes Riccardo Fusaroli 2026-02-12 Chapter 1 Advanced Cognitive Modeling These course notes support the Advanced Cognitive Modeling course taught in the Master’s program in Cognitive Science at Aarhus University. The course represents a journey into how we can understand cognitive processes through the formalization and implementation of hypothesized mechanisms, their testing and validation. 1.1 Course Philosophy and Approach Advanced cognitive modeling focuses on three interrelated objectives that shape how we approach the modeling of cognitive processes: The first objective centers on understanding the thought process behind model development. Rather than simply providing a toolbox of existing scripts, we explore how cognitive models are conceptualized and constructed from the ground up. This approach ensures you develop the skills to create novel models for unique research questions. The second objective emphasizes mastering the Bayesian workflow essential for robust model development. This workflow encompasses simulation design, prior assessment, parameter recovery testing, and thorough model fit evaluation. These skills ensure your models are not just theoretically sound but also practically reliable and generalize way beyond cognitive modeling. The third objective focuses on developing advanced probabilistic modeling capabilities. Through hands-on experience with Stan, you will learn to implement increasingly sophisticated models while maintaining scientific rigor. 1.2 Course Structure and Learning Path The course follows a carefully structured progression that builds your modeling capabilities step by step: After a deepdive into the physics of pizza ovens, we begin with simple scenarios that introduce fundamental modeling concepts. Each subsequent chapter introduces new modeling techniques while building upon previous knowledge. This cumulative approach ensures you develop a deep understanding of both basic principles and advanced applications. The chapters include theoretical discussions paired with practical coding exercises. During practical sessions, we work with real datasets, design models collaboratively, and implement them using modern statistical tools. This hands-on approach provides ample opportunity for questions and exploration. The course schedule maintains flexibility to adapt to the collective learning pace of each cohort. While we have clear learning objectives, we ensure everyone develops a solid foundation before moving to more advanced topics. 1.3 Prerequisites and Preparation To make the most of this course, students should prepare their technical environment and review fundamental concepts: Software Requirements: - R (version 4.4 or above) - RStudio (version 2024.12.0 or above) - brms package with proper configuration - cmdstanr package with complete installation Technical Prerequisites: - Working knowledge of R programming - Basic understanding of Bayesian statistics - Familiarity with cognitive science fundamentals Additional Resources: - Introduction to R and tidyverse: https://r4ds.had.co.nz/ - A condensed Bayesian statistics primer (by Chris Cox and me): https://4ccoxau.github.io/PriorsWorkshop/ - A condensed primer on all my wisdom on priors: https://youtu.be/kMSrRd4f2As 1.4 Course Resources The course materials include: - Lecture notes and presentations - Practical exercise guides - Example code and solutions - Additional readings and references For comprehensive information: - Course syllabus: [TBA] - Lecture videos: https://www.youtube.com/playlist?list=PL_f3yDs5oZx5Ff-rRIZtghpE2tPfyAzMk 1.5 About These Notes These notes represent an evolving resource that builds upon previous iterations of the course while incorporating new developments in the field. They are designed to serve both as a learning guide during the course and as a reference for your future research endeavors. "],["the-pizza-experiment.html", "Chapter 2 The Pizza Experiment 2.1 From Pizza to Cognitive Models: An Introduction 2.2 Why Start with Pizza? 2.3 Learning Objectives 2.4 Part 1: Exploring the Pizza Stone Temperature Data 2.5 Part 2: Initial Statistical Modeling 2.6 Part 3: Understanding the Physics Model 2.7 Part 4: Implementing the Physics-Based Model 2.8 Part 5: Model Analysis and Practical Applications 2.9 Conclusion: From Pizza to Cognitive Principles", " Chapter 2 The Pizza Experiment 2.1 From Pizza to Cognitive Models: An Introduction This chapter introduces core modeling concepts through a delicious lens: the physics of pizza stone heating. While this might seem far removed from cognitive science, it provides an insightful introduction to the challenges and methodologies of modeling complex phenomena. 2.2 Why Start with Pizza? Do I even need to answer that question? Because pizza, obviously. Beyond the universal appeal of pizza, starting with a physical system like a heating pizza stone offers a tangible way to grasp core principles fundamental to cognitive modeling. Understanding human cognition involves tackling immense complexity – often dealing with hidden mental states, noisy behavioral data, and competing theories. Modeling a pizza stone allows us to isolate and understand key methodological challenges in a more constrained environment. First, it demonstrates how to bridge theory and math. We rely on Prior Knowledge — using Newton’s Law of Cooling to structure our equations — just as cognitive scientists use theories of memory or attention to guide their models. This forces us to critically determine our Levels of Analysis: just as we simplify complex thermodynamics into a basic cooling formula, cognitive scientists must decide whether to model neurons, beliefs, or behavioral choices. It also highlights the problem of Latent Variables. Just as we cannot “see” a heating coefficient—only the noisy temperatures it produces—we cannot see a “learning rate” in the brain, only the choices it drives. Finally, this example acts as a sandbox for Model Complexity and Validation. We will explicitly compare detailed physics-based models against simpler statistical approximations to see which performs better on empirical data. This mirrors the central trade-off in cognitive science: balancing theoretical completeness with the practical utility of generating testable predictions from available data. 2.3 Learning Objectives This chapter uses the seemingly simple example of a heating pizza stone to introduce fundamental principles of computational modeling that are directly applicable to cognitive science. While the specific physics and code are illustrative, the core goals are to understand the process of modeling. By completing this chapter, you will be able to: Appreciate Generative Modeling: Understand the value of building models based on underlying mechanisms (like physics or cognitive processes) rather than just describing data patterns. Connect Theory and Data: See how theoretical knowledge (physics laws) informs model structure and how data is used to estimate unknown model parameters (like the heating coefficient). Implement and Fit Models: Gain initial experience implementing mathematical models in code (R and Stan) and using Bayesian inference (brms, cmdstanr) to fit them to empirical data. Evaluate Model Fit: Learn basic techniques for visualizing model predictions against data and understand the limitations of purely statistical models compared to theory-driven ones. Recognize Modeling Trade-offs: Understand the balance between model complexity, theoretical grounding, and practical utility through comparing statistical vs. physics-based approaches. Apply Modeling Principles: Recognize how the steps taken here – data exploration, model building, parameter estimation, validation – form a general workflow applicable to cognitive modeling problems explored later in the course. Oh, and you’ll probably get hungry as well! Required Packages # 1. Ensure &#39;pacman&#39; is installed if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) # 2. Load packages (pacman will verify, install if missing, and load them) pacman::p_load( tidyverse, # Core package for data manipulation and visualization brms, # Interface for Bayesian regression models using Stan bayesplot, # Convenient functions for plotting Bayesian model results tidybayes, # Utilities for working with Bayesian model outputs in a tidy format cmdstanr, # Backend interface for running Stan models efficiently here # For constructing file paths relative to project root ) 2.4 Part 1: Exploring the Pizza Stone Temperature Data In this study, we collected temperature measurements from a pizza stone in a gas-fired oven using an infrared temperature gun. Three different raters (N, TR, and R) took measurements over time to track how the stone heated up. Understanding how pizza stones heat up is crucial for achieving the perfect pizza crust, as consistent and sufficient stone temperature is essential for proper baking. The measurements were taken as follows: # Load and examine the data data &lt;- tibble( Order = rep(0:18, 3), Seconds = rep(c(0, 175, 278, 333, 443, 568, 731, 773, 851, 912, 980, 1040, 1074, 1124, 1175, 1237, 1298, 1359, 1394), 3), Temperature = c(15.1, 233, 244, 280, 289, 304, 343, NA, 333, 341, 320, 370, 325, 362, 363, 357, 380, 376, 380, 14.5, 139.9, 153, 36.1, 254, 459, 263, 369, rep(NA, 11), 12.9, 149.5, 159, 179.4, 191.7, 201, 210, NA, 256, 257, 281, 293, 297, 309, 318, 321, rep(NA, 3)), Rater = rep(c(&quot;N&quot;, &quot;TR&quot;, &quot;R&quot;), each = 19) ) # Create summary statistics summary_stats &lt;- data %&gt;% group_by(Rater) %&gt;% summarize( n_measurements = sum(!is.na(Temperature)), mean_temp = mean(Temperature, na.rm = TRUE), sd_temp = sd(Temperature, na.rm = TRUE), min_temp = min(Temperature, na.rm = TRUE), max_temp = max(Temperature, na.rm = TRUE) ) # Display summary statistics knitr::kable(summary_stats, digits = 1) Rater n_measurements mean_temp sd_temp min_temp max_temp N 18 312.0 86.4 15.1 380 R 15 229.0 83.9 12.9 321 TR 8 211.1 155.2 14.5 459 2.4.1 Initial Data Visualization Let’s visualize how the temperature evolves over time for each rater: ggplot(data, aes(x = Seconds/60, y = Temperature, color = Rater)) + geom_point(size = 3, alpha = 0.7) + geom_line(alpha = 0.5) + labs( title = &quot;Pizza Stone Temperature Evolution&quot;, subtitle = &quot;Measurements by three different raters&quot;, x = &quot;Time (minutes)&quot;, y = &quot;Temperature (°C)&quot;, color = &quot;Rater&quot; ) + theme_bw() + scale_color_brewer(palette = &quot;Set1&quot;) 2.4.2 Key Observations Several interesting patterns emerge from our data: Heating Patterns: The temperature generally increases over time, but not uniformly. We observe some fluctuations that might be due to: Variation in gas flame intensity Different measurement locations on the stone Measurement technique differences between raters Measurement Patterns by Rater Rater N maintained consistent measurements throughout the experiment Rater TR shows more variability and fewer total measurements Rater R shows a more gradual temperature increase pattern Missing Data: Some measurements are missing (NA values), particularly in the later time points for Rater TR. This is common in real-world data collection and needs to be considered in our analysis. Let’s examine the rate of temperature change: # --- Calculate Rate of Temperature Change --- # To understand how quickly the stone heats up, calculate the change rate per minute. data_with_rate &lt;- data %&gt;% group_by(Rater) %&gt;% # Perform calculation separately for each rater arrange(Seconds) %&gt;% # Ensure data is sorted chronologically for lag function mutate( # Calculate difference in temperature from previous measurement delta_temp = Temperature - lag(Temperature), # Calculate time elapsed since previous measurement delta_secs = Seconds - lag(Seconds), # Calculate rate in degrees C per second, then convert to degrees C per minute temp_change_per_min = (delta_temp / delta_secs) * 60, # Convert time to minutes for plotting minutes = Seconds / 60 ) %&gt;% # Remove rows where rate couldn&#39;t be calculated (i.e., the first row for each rater) filter(!is.na(temp_change_per_min)) # Visualize temperature change rate ggplot(data_with_rate, aes(x = minutes, y = temp_change_per_min, color = Rater)) + geom_point() + geom_line() + labs( title = &quot;Rate of Temperature Change Over Time&quot;, subtitle = &quot;Degrees Celsius per minute&quot;, x = &quot;Time (minutes)&quot;, y = &quot;Temperature Change Rate (°C/min)&quot;, color = &quot;Rater&quot; ) + theme_bw() + scale_color_brewer(palette = &quot;Set1&quot;) This visualization reveals that the heating rate is (likely) highest in the first few minutes and gradually decreases as the stone temperature approaches the oven temperature. This aligns with Newton’s Law of Cooling/Heating, which we will explore in the next section. 2.5 Part 2: Initial Statistical Modeling Before developing our physics-based model, let’s explore how standard statistical approaches perform in modeling our temperature data. We’ll implement two types of models using the brms package: a linear mixed-effects model and a lognormal mixed-effects model. Both models will account for variations between raters. 2.5.1 Model Setup and Priors First, let’s ensure we have a directory for our models and set up our computational parameters: # --- Model Fitting Setup --- # Define relative paths for saving model objects and Stan code. linear_model_path &lt;- here(&quot;models&quot;, &quot;01_pizza_linear_model&quot;) lognormal_model_path &lt;- here(&quot;models&quot;, &quot;01_pizza_lognormal_model&quot;) stan_model_path &lt;- here(&quot;models&quot;, &quot;pizza_physics_model.stan&quot;) # Create the &#39;models&#39; directory if it doesn&#39;t already exist if (!dir.exists(&quot;models&quot;)) { dir.create(&quot;models&quot;) message(&quot;Created &#39;models&#39; directory.&quot;) } # Define computational parameters mc_settings &lt;- list( chains = 2, iter = 6000, seed = 123, backend = &quot;cmdstanr&quot; ) 2.5.2 Linear Mixed-Effects Model We begin with a linear mixed-effects model, which assumes that temperature increases linearly with time but allows for different patterns across raters. This model includes both fixed effects (overall time trend) and random effects (rater-specific variations). data_scaled &lt;- data %&gt;% mutate(Minutes = Seconds / 60) # Define priors for linear model linear_priors &lt;- c( prior(normal(15, 20), class = &quot;Intercept&quot;), # Centered around room temperature prior(normal(0, 20), class = &quot;b&quot;), # Expected temperature change per minute prior(normal(0, 50), class = &quot;sigma&quot;), # Residual variation prior(normal(0, 50), class = &quot;sd&quot;), # Random effects variation prior(lkj(3), class = &quot;cor&quot;) # Random effects correlation ) # --- Linear Model --- linear_model &lt;- brm( Temperature ~ Minutes + (1 + Minutes | Rater), data = data_scaled, family = gaussian, prior = linear_priors, chains = mc_settings$chains, iter = mc_settings$iter, seed = mc_settings$seed, # Ensures reproducibility of MCMC sampling backend = mc_settings$backend, file = linear_model_path, # Use relative path file_refit = &quot;on_change&quot;, # Refit if model code or data changes cores = 2, # Specify cores used for parallel chains # Increased adapt_delta and max_treedepth help prevent divergent transitions adapt_delta = 0.99, max_treedepth = 20 ) # Display model summary summary(linear_model) ## Family: gaussian ## Links: mu = identity ## Formula: Temperature ~ Minutes + (1 + Minutes | Rater) ## Data: data_scaled (Number of observations: 41) ## Draws: 2 chains, each with iter = 6000; warmup = 3000; thin = 1; ## total post-warmup draws = 6000 ## ## Multilevel Hyperparameters: ## ~Rater (Number of levels: 3) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 57.01 28.21 7.42 120.86 1.00 2014 1104 ## sd(Minutes) 31.39 15.76 11.83 71.91 1.00 1655 2789 ## cor(Intercept,Minutes) 0.00 0.35 -0.65 0.65 1.00 2075 3209 ## ## Regression Coefficients: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 84.92 45.64 -13.93 166.45 1.00 1809 2567 ## Minutes -5.54 4.21 -13.23 3.41 1.00 1816 2506 ## ## Further Distributional Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 57.45 6.87 45.63 72.47 1.00 3734 3283 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). # Generate predictions linear_preds &lt;- fitted( linear_model, newdata = data_scaled, probs = c(0.025, 0.975) ) %&gt;% as_tibble() %&gt;% bind_cols(data) 2.5.3 Lognormal Mixed-Effects Model The lognormal model accounts for the fact that temperature changes might be proportional rather than additive, and ensures predictions cannot go below zero (I don’t bring my oven out in the freezing cold!). # Define priors for lognormal model lognormal_priors &lt;- c( prior(normal(2.7, 0.5), class = &quot;Intercept&quot;), # Log scale for room temperature prior(normal(0, 0.3), class = &quot;b&quot;), # Expected log-scale change per minute prior(normal(0, 1), class = &quot;sigma&quot;), # Log-scale residual variation prior(normal(0, 0.5), class = &quot;sd&quot;), # Random effects variation prior(lkj(3), class = &quot;cor&quot;) # Random effects correlation ) # --- Lognormal Model --- lognormal_model &lt;- brm( Temperature ~ Minutes + (1 + Minutes | Rater), data = data_scaled, family = lognormal, # Use lognormal distribution prior = lognormal_priors, chains = mc_settings$chains, cores = 2, adapt_delta = 0.99, max_treedepth = 20, iter = mc_settings$iter, seed = mc_settings$seed, # Use the same seed for comparison if desired backend = mc_settings$backend, file = lognormal_model_path # Use relative path ) # Generate predictions lognormal_preds &lt;- fitted( lognormal_model, newdata = data_scaled, probs = c(0.025, 0.975) ) %&gt;% as_tibble() %&gt;% bind_cols(data) 2.5.4 Model Comparison and Visualization Let’s compare how these models fit our data: # Compare models using LOO model_comparison &lt;- loo_compare( loo(linear_model), loo(lognormal_model) ) # Create comparison plot ggplot() + # Raw data points geom_point(data = data, aes(x = Seconds/60, y = Temperature, color = Rater), alpha = 0.5) + # Linear model predictions geom_line(data = linear_preds, aes(x = Seconds/60, y = Estimate, linetype = &quot;Linear&quot;), color = &quot;blue&quot;) + geom_ribbon(data = linear_preds, aes(x = Seconds/60, ymin = Q2.5, ymax = Q97.5), fill = &quot;blue&quot;, alpha = 0.1) + # Lognormal model predictions geom_line(data = lognormal_preds, aes(x = Seconds/60, y = Estimate, linetype = &quot;Lognormal&quot;), color = &quot;red&quot;) + geom_ribbon(data = lognormal_preds, aes(x = Seconds/60, ymin = Q2.5, ymax = Q97.5), fill = &quot;red&quot;, alpha = 0.1) + # Formatting facet_wrap(~Rater) + labs( title = &quot;Comparison of Statistical Models&quot;, subtitle = &quot;Linear vs Lognormal Mixed-Effects Models&quot;, x = &quot;Time (minutes)&quot;, y = &quot;Temperature (°C)&quot;, linetype = &quot;Model Type&quot; ) + theme_bw() # Create comparison plot but capping the y axis ggplot() + # Raw data points geom_point(data = data, aes(x = Seconds/60, y = Temperature, color = Rater), alpha = 0.5) + # Linear model predictions geom_line(data = linear_preds, aes(x = Seconds/60, y = Estimate, linetype = &quot;Linear&quot;), color = &quot;blue&quot;) + geom_ribbon(data = linear_preds, aes(x = Seconds/60, ymin = Q2.5, ymax = Q97.5), fill = &quot;blue&quot;, alpha = 0.1) + # Lognormal model predictions geom_line(data = lognormal_preds, aes(x = Seconds/60, y = Estimate, linetype = &quot;Lognormal&quot;), color = &quot;red&quot;) + geom_ribbon(data = lognormal_preds, aes(x = Seconds/60, ymin = Q2.5, ymax = Q97.5), fill = &quot;red&quot;, alpha = 0.1) + ylim(0, 1000) + # Formatting facet_wrap(~Rater) + labs( title = &quot;Comparison of Statistical Models&quot;, subtitle = &quot;Linear vs Lognormal Mixed-Effects Models&quot;, x = &quot;Time (minutes)&quot;, y = &quot;Temperature (°C)&quot;, linetype = &quot;Model Type&quot; ) + theme_bw() 2.5.5 Model Assessment I have seen worse models in my time, but they do seem to have important issues: The linear mixed-effects model assumes a constant rate of temperature change, which we can see is not at all accurate. The actual temperature increase is fast at the beginning and appears to slow down over time, particularly at higher temperatures. While this model has the advantage of simplicity, it is not likely to produce accurate predictions as it seem to fail to capture the underlying physics of heat transfer. The lognormal mixed-effects model is completely off. Further, the lognormal model produce some divergences, which is often a sign that they are not well suited to the data. I would argue that the core issue is that neither model incorporates our knowledge of heat transfer physics, which suggests an exponential approach to equilibrium temperature. This limitation motivates our next section, where we’ll develop a physics-based model. 2.6 Part 3: Understanding the Physics Model Temperature evolution in a pizza stone follows Newton’s Law of Cooling/Heating. We’ll start by exploring this physical model before applying it to real data. 2.6.1 The Basic Temperature Evolution Equation The temperature evolution of a pizza stone in a gas-fired oven is governed by the heat diffusion equation, which describes how heat flows through solid materials: \\[\\rho c_p \\frac{\\partial T}{\\partial t} = k\\nabla^2T + Q\\] where: \\(\\rho\\) represents the stone’s density (kg/m³) \\(c_p\\) denotes specific heat capacity (J/kg·K) \\(T\\) is temperature (K) \\(t\\) represents time (s) \\(k\\) is thermal conductivity (W/m·K) \\(\\nabla^2\\) is the Laplacian operator \\(Q\\) represents heat input from the oven (W/m³) While this equation provides a complete description of heat flow, we can significantly simplify our analysis by applying the lumped capacitance model. his simplification assumes that the stone conducts heat internally much faster than it absorbs it from the surface. This allows us to ignore the complex internal gradients (\\(\\nabla^2T\\)) and focus purely on the surface heat transfer, reducing our model to: \\[\\frac{dT}{dt} = \\frac{hA}{mc_p}(T_{\\infty} - T)\\] where: \\(h\\) is the heat transfer coefficient (W/m²·K) \\(A\\) is the surface area exposed to heat (m²) \\(m\\) is the stone’s mass (kg) \\(T_{\\infty}\\) is the oven temperature (K) This simplified equation relates the rate of temperature change to the difference between the current stone temperature T and the ambient temperature T∞ (as determined by the flame). The coefficient h represents the heat transfer coefficient between the ambient and stone, A is the stone’s surface area exposed to heat, m is its mass, and cp remains the specific heat capacity. To solve this differential equation, we begin by separating variables: \\[\\frac{dT}{T_{\\infty} - T} = \\left(\\frac{hA}{mc_p}\\right)dt\\] Integration of both sides yields: \\[-\\ln|T_{\\infty} - T| = \\left(\\frac{hA}{mc_p}\\right)t + C\\] where C is an integration constant. Using the initial condition \\(T = T_i\\) at \\(t = 0\\), we can determine the integration constant: \\[C = -\\ln|T_{\\infty} - T_i|\\] Substituting this back and solving for temperature gives us: \\[T = T_{\\infty} + (T_i - T_{\\infty})\\exp\\left(-\\frac{hA}{mc_p}t\\right)\\] For practical reasons, we combine physical parameters into a single coefficient, which we will call HOT: \\[HOT = \\frac{hA}{mc_p}\\] Giving our working equation: \\[T = T_{\\infty} + (T_i - T_{\\infty})\\exp(-HOT * t)\\] This equation retains the essential physics while providing a practical model for analyzing our experimental data. The HOT coefficient encapsulates the combined effects of heat transfer efficiency, stone geometry, and material properties into a single parameter that determines how quickly the stone approaches the flame temperature. 2.7 Part 4: Implementing the Physics-Based Model Having established the theoretical foundation, we now face a practical challenge: we cannot measure \\(h\\), \\(A\\), or \\(c_p\\) directly during the bake. Instead, we must infer the aggregate parameter HOT from the noisy temperature data collected by our raters. This is where Stan comes in, enabling us to create a Bayesian implementation of our physics-based model, and to account for measurement uncertainty and variation between raters. First, we prepare our data for the Stan model. Our model requires initial temperatures, time measurements, and observed temperatures from each rater: # Create data structure for Stan stan_data &lt;- list( N = nrow(data %&gt;% filter(!is.na(Temperature))), time = data %&gt;% filter(!is.na(Temperature)) %&gt;% pull(Seconds), temp = data %&gt;% filter(!is.na(Temperature)) %&gt;% pull(Temperature), n_raters = 3, rater = as.numeric(factor(data %&gt;% filter(!is.na(Temperature)) %&gt;% pull(Rater))), Ti = c(100, 100, 100), # Initial temperature estimates Tinf = 450 # Flame temperature estimate ) Next, we implement our physics-based model in Stan. The model incorporates our derived equation while allowing for rater-specific heating coefficients: stan_code &lt;- &quot; data { int&lt;lower=0&gt; N; // Number of observations vector[N] time; // Time points vector[N] temp; // Observed temperatures int&lt;lower=0&gt; n_raters; // Number of raters array[N] int&lt;lower=1,upper=n_raters&gt; rater; // Rater indices vector[n_raters] Ti; // Initial temperatures real Tinf; // Flame temperature } parameters { vector&lt;lower=0&gt;[n_raters] HOT; // Heating coefficients vector&lt;lower=0&gt;[n_raters] sigma; // Measurement error } model { // Physics-based temperature prediction vector[N] mu = Tinf + (Ti[rater] - Tinf) .* exp(-HOT[rater] .* time); // Prior distributions target += normal_lpdf(HOT | 0.005, 0.005); // Prior for heating rate target += exponential_lpdf(sigma | 1); // Prior for measurement error // Likelihood target += normal_lpdf(temp | mu, sigma[rater]); } &quot; # Save the Stan model code using the relative path writeLines(stan_code, stan_model_path) # Compile and fit the Stan model using the relative path # Compilation happens once; subsequent runs load the compiled object if unchanged. mod &lt;- cmdstan_model(&quot;models/pizza_physics_model.stan&quot;) fit &lt;- mod$sample( data = stan_data, seed = 123, chains = 2, parallel_chains = 2 ) ## Running MCMC with 2 parallel chains... ## ## Chain 1 Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1 Iteration: 100 / 2000 [ 5%] (Warmup) ## Chain 1 Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1 Iteration: 300 / 2000 [ 15%] (Warmup) ## Chain 1 Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1 Iteration: 500 / 2000 [ 25%] (Warmup) ## Chain 1 Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1 Iteration: 700 / 2000 [ 35%] (Warmup) ## Chain 1 Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1 Iteration: 900 / 2000 [ 45%] (Warmup) ## Chain 1 Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1 Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1 Iteration: 1100 / 2000 [ 55%] (Sampling) ## Chain 1 Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1 Iteration: 1300 / 2000 [ 65%] (Sampling) ## Chain 1 Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1 Iteration: 1500 / 2000 [ 75%] (Sampling) ## Chain 1 Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1 Iteration: 1700 / 2000 [ 85%] (Sampling) ## Chain 1 Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1 Iteration: 1900 / 2000 [ 95%] (Sampling) ## Chain 1 Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2 Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2 Iteration: 100 / 2000 [ 5%] (Warmup) ## Chain 2 Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2 Iteration: 300 / 2000 [ 15%] (Warmup) ## Chain 2 Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2 Iteration: 500 / 2000 [ 25%] (Warmup) ## Chain 2 Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2 Iteration: 700 / 2000 [ 35%] (Warmup) ## Chain 2 Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2 Iteration: 900 / 2000 [ 45%] (Warmup) ## Chain 2 Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2 Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2 Iteration: 1100 / 2000 [ 55%] (Sampling) ## Chain 2 Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2 Iteration: 1300 / 2000 [ 65%] (Sampling) ## Chain 2 Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2 Iteration: 1500 / 2000 [ 75%] (Sampling) ## Chain 2 Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2 Iteration: 1700 / 2000 [ 85%] (Sampling) ## Chain 2 Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2 Iteration: 1900 / 2000 [ 95%] (Sampling) ## Chain 2 Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1 finished in 0.1 seconds. ## Chain 2 finished in 0.1 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.1 seconds. ## Total execution time: 0.2 seconds. The Stan implementation translates our mathematical model into a computational framework. We assign informative priors to our parameters based on physical understanding: the heating coefficient (HOT) is expected to be small but positive, while measurement error (sigma) follows an exponential distribution to ensure positivity while allowing for varying levels of uncertainty between raters. To visualize our model’s predictions and assess its performance, we extract posterior samples and generate predictions across our time range: # Extract draws post &lt;- as_draws_df(fit$draws()) %&gt;% dplyr::select(starts_with(&quot;HOT&quot;), starts_with(&quot;sigma&quot;)) %&gt;% slice_sample(n = 100) # Create prediction grid pred_data &lt;- tidyr::crossing( time = seq(0, max(stan_data$time), length.out = 100), rater = 1:stan_data$n_raters ) %&gt;% mutate( Ti = stan_data$Ti[rater], Tinf = stan_data$Tinf ) # Generate predictions pred_matrix &lt;- matrix(NA, nrow = nrow(pred_data), ncol = 100) for (i in 1:nrow(pred_data)) { pred_matrix[i,] &lt;- with(pred_data[i,], Tinf + (Ti - Tinf) * exp(-as.matrix(post)[,rater] * time)) } # Summarize predictions predictions &lt;- pred_data %&gt;% mutate( mean = rowMeans(pred_matrix), lower = apply(pred_matrix, 1, quantile, 0.025), upper = apply(pred_matrix, 1, quantile, 0.975) ) # Create visualization ggplot(predictions, aes(x = time/60)) + geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) + geom_line(aes(y = mean)) + geom_point( data = data %&gt;% filter(!is.na(Temperature)) %&gt;% mutate(rater = case_when( Rater == &quot;N&quot; ~ 1, Rater == &quot;TR&quot; ~ 2, Rater == &quot;R&quot; ~ 3 )), aes(x = Seconds/60, y = Temperature) ) + facet_wrap(~rater, labeller = labeller(rater = c( &quot;1&quot; = &quot;Rater N&quot;, &quot;2&quot; = &quot;Rater TR&quot;, &quot;3&quot; = &quot;Rater R&quot; ))) + labs( title = &quot;Physics-Based Model Predictions&quot;, x = &quot;Time (minutes)&quot;, y = &quot;Temperature (°C)&quot; ) + theme_bw() Our implementation combines the theoretical understanding developed in Part 3 with practical considerations for real-world data analysis. The model accounts for measurement uncertainty while maintaining the fundamental physics of heat transfer, providing a robust framework for understanding pizza stone temperature evolution. 2.8 Part 5: Model Analysis and Practical Applications Having fitted our physics-based model, we must first verify that it actually describes the data well (Validation) before we can use it to make predictions. Once we trust our estimates of the HOT parameter, we can develop practical insights for pizza stone management. A key question for any pizzaiolo is: How long must I wait before the stone is ready? To answer this, we don’t need a new model; we simply need to look at our physics equation from a different angle. By rearranging our original cooling law to solve for time (\\(t\\)) instead of temperature (\\(T\\)), we create a predictive tool: \\[t = -\\frac{1}{HOT} \\ln\\left( \\frac{T_{\\text{target}} - T_{\\infty}}{T_{\\text{initial}} - T_{\\infty}} \\right)\\] We will now implement this function in R to calculate precise waiting times under different conditions. time_to_temp &lt;- function(target_temp, HOT, Ti, Tinf) { # Solve: target = Tinf + (Ti - Tinf) * exp(-HOT * t) # for t t = -1/HOT * log((target_temp - Tinf)/(Ti - Tinf)) return(t/60) # Convert seconds to minutes } To understand heating times across different oven conditions, we examine how varying flame temperatures affect the time needed to reach pizza-making temperatures. We extract the heating coefficients from our fitted model and analyze temperature scenarios: # Extract HOT samples from our posterior hot_samples &lt;- as_draws_df(fit$draws()) %&gt;% dplyr::select(starts_with(&quot;HOT&quot;)) # Create prediction grid for different flame temperatures pred_data &lt;- tidyr::crossing( Tinf = seq(450, 1200, by = 50), # Range of flame temperatures rater = 1:3 ) %&gt;% mutate( Ti = stan_data$Ti[rater], target_temp = 400 # Target temperature for pizza cooking ) # Calculate heating times across conditions n_samples &lt;- 100 time_preds &lt;- map_dfr(1:nrow(pred_data), function(i) { times &lt;- sapply(1:n_samples, function(j) { hot &lt;- hot_samples[j, paste0(&quot;HOT[&quot;, pred_data$rater[i], &quot;]&quot;)][[1]] time_to_temp( pred_data$target_temp[i], hot, pred_data$Ti[i], pred_data$Tinf[i] ) }) data.frame( rater = pred_data$rater[i], Tinf = pred_data$Tinf[i], mean_time = mean(times), lower = quantile(times, 0.025), upper = quantile(times, 0.975) ) }) # Visualize heating time predictions ggplot(time_preds, aes(x = Tinf)) + geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) + geom_line(aes(y = mean_time)) + facet_wrap(~rater, labeller = labeller(rater = c( &quot;1&quot; = &quot;Rater N&quot;, &quot;2&quot; = &quot;Rater TR&quot;, &quot;3&quot; = &quot;Rater R&quot; ))) + labs( title = &quot;Time Required to Reach Pizza-Making Temperature&quot;, subtitle = &quot;Target temperature: 400°C&quot;, x = &quot;Flame Temperature (°C)&quot;, y = &quot;Minutes to reach target&quot; ) + theme_bw() Our analysis reveals several important insights for practical pizza making. First, the heating time decreases non-linearly with flame temperature, showing diminishing returns at very high temperatures. We also observed differences between raters in their measured heating times. These variations likely stem from differences in measurement technique and location on the stone, highlighting the importance of consistent temperature monitoring practices. E.g. N measure the temperature at the boundary between flame and stone, which matches the model’s assumptions better than R, who measures at the center of the stone. For practical application, we can provide specific heating guidelines based on our model. At a typical flame temperature of 800°C, the model predicts it will take approximately 20-30 minutes to reach optimal pizza-making temperature, assuming room temperature start. However, this time can vary significantly based on: Initial stone temperature Flame temperature and consistency Environmental conditions. Can we really wait that long? Well, the model is telling us that there are no shortcuts to the perfect crust! 2.9 Conclusion: From Pizza to Cognitive Principles The journey from modeling a heating pizza stone to understanding cognitive processes might seem unusual, but it illustrates fundamental principles that will guide us throughout this course. Through this seemingly simple physics problem, we have encountered core challenges faced daily by cognitive modelers: Levels of Analysis: We saw how simplifying complex equations (heat diffusion) mirrors the need to choose the right level of abstraction when modeling cognition (e.g., neural vs. behavioral). Prior Knowledge: We used physics laws to build a generative model, much like cognitive theories guide model development. Validation: We confronted the necessity of comparing model predictions to real data, a process central to testing theories. Parameter Estimation: We used the model to quantify an unobservable property (HOT) from noisy data, analogous to estimating latent cognitive parameters like learning rates or decision thresholds. Crucially, the comparison between purely statistical models (linear/lognormal) and the physics-based Stan model highlighted the value of mechanistic understanding. While the statistical models struggled to capture the saturation curve — often predicting impossible temperatures via extrapolation — the physics model offered deeper insight and robust predictions. This is a primary goal in cognitive modeling: we do not simply to describe which decisions were made, but we want to capture the underlying mechanism that can increase our understanding of how decisions were made, and generalize to novel unseen decisions. The pizza stone experiment also demonstrated the importance of rigorous methodology. We saw how multiple measurements from different raters revealed variability in our data, leading us to consider measurement error and individual differences—themes that will become crucial when studying human behavior. As we progress, these principles – careful data handling, theoretically motivated model specification (generative models), parameter estimation, rigorous validation, and balancing complexity – will be our guide. In the next chapter, we shift focus to cognition, applying these ideas to model strategic decision-making in the Matching Pennies game. We’ll encounter similar challenges: translating verbal theories of strategy into formal models, simulating agent behavior, and beginning the process of fitting models to observed choices, using the statistical and computational tools introduced here. Finally, I hope this has whetted your appetite – both for pizza and for computational modeling! "],["building-models-of-strategic-decision-making.html", "Chapter 3 Building Models of Strategic Decision-Making 3.1 Learning Goals 3.2 Introduction: Observing Behavior to Theorize Mechanisms 3.3 The Matching Pennies Game 3.4 Game Structure 3.5 Empirical Investigation 3.6 Empirical explorations 3.7 Candidate Verbal Models 3.8 Conclusion: From Observations to Verbal Theories", " Chapter 3 Building Models of Strategic Decision-Making You’ve just played Matching Pennies and discussed strategies with your classmates. You probably noticed patterns in your opponent’s play, tried to be unpredictable, and maybe even changed your strategy mid-game. This chapter helps you translate those observations and intuitions into the language of cognitive modeling—preparing you to implement formal models in Chapter 3. 3.1 Learning Goals This chapter bridges the gap between observing behavior and developing testable theories. By the end of this chapter, using the Matching Pennies game as a case study, you will be able to: Identify Key Modeling Steps: Understand the process of moving from behavioral observations and participant reflections to formulating initial verbal theories of underlying cognitive strategies. Appreciate Theory Building Challenges: Recognize common issues in theory development, such as the participant vs. researcher perspective, the need for simplification, and incorporating known cognitive constraints. Generate Candidate Models: Propose several distinct verbal models (e.g., random choice, simple heuristics, memory-based strategies) that could plausibly explain behavior in a strategic decision-making task. Connect to Formalization: Understand why translating these verbal models into precise, formal models (covered in the next chapter) is a necessary step for rigorous testing and simulation. 3.2 Introduction: Observing Behavior to Theorize Mechanisms Chapter 1 emphasized the importance of modeling underlying generative mechanisms. To do this for cognition, we first need a behavior to explain. This chapter uses the Matching Pennies game as our initial cognitive phenomenon. It’s a simple strategic interaction, yet rich enough to illustrate the process of developing and refining cognitive models. Our goal here is not yet to build the final computational models, but to practice the crucial preceding steps: 1. Observing behavior in a specific task (through experiments and data exploration). 2. Reflecting on potential cognitive strategies and constraints (drawing on observations, participant reports, and cognitive science principles). 3. Formulating initial verbal theories or candidate models that describe the potential underlying mechanisms. This process lays the groundwork for Chapter 3, where we will translate these verbal ideas into precise, formal models ready for simulation and testing. 3.3 The Matching Pennies Game In the matching pennies game, two players engage in a series of choices. One player attempts to match the other’s choice, while the other player aims to achieve a mismatch, and they repeatedly play with each other. This is a prototypical example of interacting behaviors that are usually tackled by game theory, and bring up issues of theory of mind and recursivity. For an introduction see the paper: Waade, Peter T., et al. “Introducing tomsup: Theory of mind simulations using Python.” Behavior Research Methods 55.5 (2023): 2197-2231. 3.4 Game Structure The game proceeds as follows: Two players sit facing each other Each round, both players choose either “left” or “right” to indicate where they believe a penny is hidden The matcher wins by choosing the same hand as their opponent The hider wins by choosing the opposite hand Points are awarded: +1 for winning, -1 for losing Repeat This simple structure creates a rich environment for studying decision-making strategies, learning, and adaptation. 3.5 Empirical Investigation 3.5.1 Data Collection Protocol If you are attending my class you have been (or will be) asked to participate in a matching pennies game. This game provides the foundation for our modeling efforts. By observing gameplay and collecting data, we can develop models that capture the cognitive processes underlying decision-making in strategic situations. Participants play 30 rounds as the matcher and 30 rounds as the hider, allowing us to observe behavior in both roles. While playing, participants track their scores, which can provide quantitative data for later analysis. Participants are also asked to reflect on their strategies and the strategies they believe their opponents are using, as that provides valuable materials to build models on. 3.5.2 Initial Observations Through the careful observation and discussion of gameplay we do in class, several patterns typically emerge. For instance, players often demonstrate strategic adaptation, adjusting their choices based on their opponent’s previous moves. They may attempt to identify patterns in their opponent’s behavior while trying to make their own choices less predictable. The tension between exploitation of perceived patterns and maintenance of unpredictability creates fascinating dynamics for modeling. 3.6 Empirical explorations Below you can observe how a previous year of CogSci did against bots (computational agents) playing according to different strategies. Look at the plots below, where the x axes indicate trial, the y axes how many points the CogSci’ers scored (0 being chance, negative means being completely owned by the bots, positive owning the bot) and the different colors indicate different strategies employed by the bots. Strategy “-2” was a Win-Stay-Lose-Shift bot: when it got a +1, it repeated its previous move (e.g. right if it had just played right), otherwise it would perform the opposite move (e.g. left if it had just played right). Strategy “-1” was a biased Nash both, playing “right” 80% of the time. Strategy “0” indicates a reinforcement learning bot; “1” a bot assuming you were playing according to a reinforcement learning strategy and trying to infer your learning and temperature parameters; “2” a bot assuming you were following strategy “1” and trying to accordingly infer your parameters. # Load necessary libraries library(tidyverse) library(tidyverse) # --- 1. Data Loading / Generation --- data_path &lt;- file.path(&quot;data&quot;, &quot;MP_MSc_CogSci22.csv&quot;) if (file.exists(data_path)) { d &lt;- read_csv(data_path) } else { # Generate synthetic data if file is missing (Reproducibility check) set.seed(42) n_students &lt;- 20 n_trials &lt;- 30 strategies &lt;- c(-2, -1, 0, 1, 2) d &lt;- expand_grid( ID = factor(1:n_students), BotStrategy = strategies, Role = c(0, 1), # 0=Matcher, 1=Hider Trial = 1:n_trials ) %&gt;% mutate( # Random payoffs for demonstration Payoff = sample(c(-1, 1), n(), replace = TRUE, prob = c(0.45, 0.55)) ) warning(&quot;Using synthetic data for demonstration.&quot;) } # --- 2. Data Cleaning (Crucial Step!) --- # Map cryptic codes to human-readable labels bot_labels &lt;- c( &quot;-2&quot; = &quot;WSLS Bot&quot;, &quot;-1&quot; = &quot;Bias Bot (80%)&quot;, &quot;0&quot; = &quot;RL Bot&quot;, &quot;1&quot; = &quot;ToM-1 Bot&quot;, &quot;2&quot; = &quot;ToM-2 Bot&quot; ) d_clean &lt;- d %&gt;% mutate( # Make BotStrategy a factor with meaningful names BotStrategy = factor(BotStrategy, levels = names(bot_labels), labels = bot_labels), # Make Role a factor Role = factor(Role, levels = c(0, 1), labels = c(&quot;Matcher&quot;, &quot;Hider&quot;)) ) # --- 3. Plot Collective Performance --- ggplot(d_clean, aes(x = Trial, y = Payoff, color = BotStrategy)) + geom_smooth(se = FALSE, method = &quot;loess&quot;, span = 0.5) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;, alpha = 0.5) + facet_wrap(~Role) + labs( title = &quot;Human vs. Machine: Average Performance&quot;, subtitle = &quot;Positive values indicate humans winning against bots&quot;, y = &quot;Average Payoff&quot;, color = &quot;Opponent Strategy&quot; ) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) That doesn’t look too good, ah? What about individual variability? In the plot below we indicate the score of each of the former students, against the different bots. # --- Plot 2: Individual Variability in Scores --- # Calculate the total score for each student (ID) against each bot strategy. d_summary &lt;- d %&gt;% group_by(ID, BotStrategy, Role) %&gt;% # Group by student, bot, and role summarize(TotalScore = sum(Payoff), .groups = &quot;drop&quot;) # Calculate total score # Visualize the distribution of total scores for each bot strategy. # geom_boxplot shows the distribution, geom_point shows individual student scores. print( ggplot(d_summary, aes(x = BotStrategy, y = TotalScore)) + geom_boxplot(aes(fill = Role), alpha = 0.3, outlier.shape = NA) + # Boxplot showing distribution geom_jitter(aes(color = ID), width = 0.2, alpha = 0.7) + # Individual student points facet_wrap(~Role) + # Separate plots for Matcher and Hider labs( title = &quot;Distribution of Total Scores Against Different Bots&quot;, subtitle = &quot;Shows individual student variability&quot;, x = &quot;Bot Strategy&quot;, y = &quot;Total Score (Sum of Payoffs)&quot;, fill = &quot;Player Role&quot; ) + theme_classic() + theme(legend.position = &quot;none&quot;) # Hide legend for individual IDs ) 3.6.1 From Observation to Theory: Identifying Potential Mechanisms The plots above reveal patterns: average performance changes over time, varies by opponent, and differs across individuals. Gameplay observations and participant reflections (from class discussion or collected data) add qualitative insights – perhaps players mention trying to be unpredictable, guessing opponent biases, or repeating winning moves. The crucial next step is to distill these rich, complex observations into simplified, plausible mechanisms or strategies. This involves abstraction: Identifying Core Patterns: What recurring behaviors seem most important? (e.g., reacting to wins/losses, tracking opponent frequencies). Simplifying: Can we capture the essence of a strategy without modeling every detail of a player’s thought process or interaction? (e.g., modeling WSLS instead of complex pattern detection). Drawing on Cognitive Principles: How do known cognitive constraints (like limited memory or processing errors, discussed below) shape plausible strategies? For instance, observing that players often change their choice after a loss might lead us to propose a “Lose-Shift” component as part of a candidate model. Observing that performance differs against biased vs. adaptive bots suggests players might be trying to learn or adapt, leading to memory-based or learning models. This process generates verbal models – initial hypotheses about the strategies at play. Key modeling considerations guide this translation: What information do players likely use? (Own past choices? Opponent’s choices? Payoffs?) How far back does memory plausibly extend? (Last trial? Last 5 trials? Exponential decay?) What is the role of randomness? (True randomness? Exploration? Implementation errors?) How might strategies adapt over time or differ between individuals? Answering these helps refine our verbal models, paving the way for formalization. The goal isn’t to capture everything, but to propose distinct, testable mechanisms. 3.6.2 The distinction between participant and researcher perspectives As participants we might not be aware of the strategy we use, or we might believe something erroneous. The exercise here is to act as researchers: what are the principles underlying the participants’ behaviors, no matter what the participants know or believe? Note that talking to participants and being participants helps developing ideas, but it’s not the end point of the process. Also note that as cognitive scientists we can rely on what we have learned about cognitive processes (e.g. memory). Another important component of the distinction is that participants leave in a rich world: they rely on facial expressions and bodily posture, the switch strategies, etc. On the other hand, the researcher is trying to identify one or few at most “simple” strategies. Rich bodily interactions and mixtures or sequences of multiple strategies are not a good place to start modeling. These aspects are a poor starting point for building your first model, and are often pretty difficult to fit to empirical data. Nevertheless, they are important intuitions that the researcher should (eventually?) accommodate. 3.7 Candidate Verbal Models Based on the behavioral patterns observed, participant discussions, and core cognitive principles, we can now propose several candidate verbal models for behavior in the matching pennies game. Each represents a different hypothesis about the underlying cognitive strategy. These are starting points, deliberately simplified: Random Choice Model: Reflects the idea that players might try to be unpredictable or simply lack a clear strategy. Win-Stay-Lose-Shift (WSLS): Captures the common heuristic of repeating successful actions and changing unsuccessful ones. Bias Tracking (Memory Models): Addresses the observation that players seem to react to opponent patterns, potentially by estimating choice frequencies. Variations account for memory limits. Reinforcement Learning: A more formal learning model capturing trial-by-trial value updates based on prediction errors, potentially explaining adaptation. k-ToM (Theory of Mind): Accounts for the strategic nature of the game, where players model their opponent’s mind (or model the opponent modeling their mind). These verbal models, derived from our initial analysis, are the hypotheses we will formalize and test in subsequent chapters. Let’s briefly describe the core idea of each: Random Choice Model: Mechanism: Choices are made randomly, potentially with a fixed bias (e.g., 60% right, 40% left), independent of game history or opponent actions. Rationale: Serves as the simplest baseline. It reflects the possibility that players might try to be deliberately unpredictable, haven’t figured out a strategy, or that their behavior appears random from the observer’s perspective. It introduces the basic concept of choice probability (the \\(\\theta\\) parameter we’ll estimate later). Win-Stay-Lose-Shift (WSLS): Mechanism: A simple heuristic: repeat the last choice if it led to a win, switch to the other choice if it led to a loss. Rationale: Captures the intuitive tendency to stick with success and abandon failure. This is a common heuristic observed in simple learning tasks. Formalization Hint: This can be formalized using probabilities of staying/shifting conditional on the previous outcome. A simple version might be deterministic (always stay/shift), while a probabilistic version allows for occasional deviations: \\[P(\\text{stay} | \\text{outcome}_{t-1}) = \\begin{cases} p_{stay\\_win} &amp; \\text{if win at } t-1 \\\\ p_{stay\\_loss} &amp; \\text{if loss at } t-1 \\end{cases}\\] (Note: \\(p_{shift} = 1 - p_{stay}\\)). The original formula used \\(p_w\\) for staying after a win and \\(1-p_l\\) for staying after a loss (meaning \\(p_l\\) is the probability of shifting after a loss). Win-Stay-Lose-Shift (WSLS) as Logistic Regression: Mechanism: While often described as a deterministic rule (“if win, stay”), in cognitive modeling we treat it as a probabilistic tendency. The probability of repeating a choice depends on the previous outcome’s success. Formalization: We can define the probability of staying (\\(P_{stay}\\)) using a logit link function, which maps a continuous “decision variable” onto a 0-1 probability scale: \\[P(choice_{t} = choice_{t-1} | outcome_{t-1}) = \\text{logit}^{-1}(\\beta_0 + \\beta_1 \\cdot outcome_{t-1})\\] Here, \\(\\beta_1\\) represents the sensitivity to the feedback. A high \\(\\beta_1\\) approximates the deterministic heuristic, while a lower \\(\\beta_1\\) captures “noisy” application of the rule. Rationale: This transforms a rigid verbal rule into a continuous parameter space we can estimate from data. Learning Mechanisms: A Hierarchy of Uncertainty: When we say an agent “learns,” we usually mean they are estimating some hidden value (like the probability of the opponent playing “Right”). But how do they update that estimate? We can organize candidate models by how sophisticated they are about history and uncertainty. 1. The Moving Average (Windowed Memory): * Concept: “I think the next value will be exactly like the average of the last N values.” The Math: \\(\\mu_{t} = \\frac{1}{N} \\sum_{i=t-N+1}^{t} x_{i}\\) Critique: This approach gives equal weight to every trial in the window and zero weight to anything before it. It has no “confidence” metric—it’s just a raw average. 2. Reinforcement Learning (Rescorla-Wagner): Concept: “I keep a running estimate, and I nudge it slightly whenever I’m wrong.” The Math: The classic update rule is \\(V_{t} = V_{t-1} + \\alpha \\cdot (Reward_{t} - V_{t-1})\\). Pedagogical Derivation: Why is this “learning”? Notice that \\((Reward_{t} - V_{t-1})\\) is the Prediction Error (PE). If the reward was higher than expected, the PE is positive, and the value \\(V\\) increases. Connection to Memory: If we expand this equation recursively, we can see that the current value is actually a weighted sum of all past rewards, where the weight decays exponentially: \\(Weight_{k} = \\alpha(1-\\alpha)^k\\). Implication: The learning rate \\(\\alpha\\) isn’t just a “speed” parameter; it defines the effective memory. A high \\(\\alpha\\) means the agent relies heavily on the most recent trial (fast forgetting). 3. The Bayesian Update (Static Uncertainty): Concept: “I don’t just have a number; I have a belief distribution. I only update if the data is trustworthy enough to move my solid prior.” The Math: In a Bayesian framework, the update weight (often called the Kalman Gain, \\(K\\)) emerges from the ratio of uncertainties: \\[K = \\frac{\\sigma^{2}_{prior}}{\\sigma^{2}_{prior} + \\sigma^{2}_{noise}}\\] Key Insight: If the world is truly static, your prior becomes more and more precise over time (\\(\\sigma^2_{prior} \\to 0\\)). This means \\(K \\to 0\\): eventually, you stop learning because you are “sure” you know the truth. This is great for fixed values but terrible for changing environments. 4. The Kalman Filter (Dynamic Uncertainty): Concept: “The world is not static; it drifts. Even if I was sure yesterday, my knowledge degrades today.” The Math: The Kalman filter adds a Process Noise (\\(Q\\)) to the uncertainty at every time step. This prevents the agent from ever becoming “too sure.” The gain becomes dynamic: \\[K_{t} = \\frac{P_{t|t-1}}{P_{t|t-1} + R}\\] (Where \\(P_{t|t-1}\\) is the predicted uncertainty, which includes the new process noise). Implication: Unlike the static Bayesian update, the learning rate here does not go to zero. It settles at a balance point. The agent stays permanently alert to changes. 5. The Hierarchical Gaussian Filter (HGF - Meta-Learning): Concept: “The world changes, but sometimes it changes fast (volatile) and sometimes slow (stable). I need to learn the speed of change.” The Math: The HGF extends the Kalman logic by assuming the volatility itself is a hidden state that must be learned. Level 1: What is the current value? Level 2: How volatile is the value? (i.e., “Is the opponent switching strategies right now?”) Implication: This creates a dynamic learning rate. When the agent detects high volatility (Level 2), it “opens up” its learning rate at Level 1 to adapt explosively. When it detects stability, it tightens the learning rate to ignore noise. k-ToM (Theory of Mind): Mechanism: Players explicitly model their opponent’s strategy. Level 0 (0-ToM) assumes the opponent is random/biased. Level 1 (1-ToM) assumes the opponent is using a 0-ToM strategy and tries to best respond. Level 2 (2-ToM) assumes the opponent is using a 1-ToM strategy, and so on. Rationale: Directly addresses the strategic, interactive nature of the game, where predicting the opponent’s intentions or model of you might be crucial, going beyond simple pattern detection. (Preview: These models will be discussed in later chapters). Combined/Switching Strategies: Mechanism: Real behavior might involve combining elements of the above (e.g., WSLS with a baseline bias) or switching between strategies (e.g., tracking bias for a while, then switching to random). Generating complex, non-stationary sequences might also be a strategy to appear unpredictable. Rationale: Generating random output is hard, so if we want to confuse the opponent, we could act first choosing tail 8 times, and then switching to a WSLS strategy for 4 trials, and then choosing head 4 times. Or implementing any of the previous strategies and doing the opposite “to mess with the opponent”. The combined strategies model also acknowledges that single, simple models might be insufficient. Motivates concepts like mixture models (Chapter 8) where behavior is seen as a probabilistic blend of simpler strategies, or models where strategy parameters change over time. Yet these mixture models are hard to fit to data and can quickly spiral out of control (many possible combinations) 3.7.1 Plausibility Check: Cognitive Constraints You probably noticed you couldn’t remember every single move your opponent made. Maybe you lost track of older trials, and recent trials felt more important. These aren’t failures: they’re features of human cognition. Our models should reflect these constraints to be cognitively realistic. Let’s see what difference these constraints make for predictions. 3.7.1.1 Memory Limitations Constraint: Humans have limited working memory and exhibit forgetting, often approximated by exponential decay. Perfect recall of long trial sequences is unrealistic. Modeling Implication: This favors models incorporating memory decay or finite history windows (like imperfect memory models or RL with a learning rate &lt; 1) over perfect memory models. It suggests that even bias-tracking models should discount older information. 3.7.1.2 Perseveration Tendencies Constraint: People sometimes exhibit perseveration – repeating a previous action, especially if it was recently successful or chosen, even if a different strategy might suggest otherwise. This can be distinct from rational “win-stay”. Modeling Implication: This might be incorporated as an additional bias parameter influencing the choice probability (e.g., a small added probability of repeating the last action \\(a_{t-1}\\) regardless of outcome) or interact with feedback processing (e.g., strengthening the ‘stay’ tendency after wins). 3.7.1.3 Noise and Errors Constraint: Human behavior is inherently noisy. People make mistakes, have attentional lapses, press the wrong button, or misunderstand feedback. Behavior rarely perfectly matches a deterministic strategy. Modeling Implication: Models should almost always include a “noise” component. This can be implemented in several ways: Lapse Rate: A probability (e.g., \\(\\epsilon\\)) that on any given trial, the agent makes a random choice instead of following their primary strategy (as used in the Mixture Model chapter). Decision Noise (Softmax): In models where choices are based on comparing values (like RL), a ‘temperature’ parameter can control the stochasticity. High temperature leads to more random choices, low temperature leads to more deterministic choices based on values. Imperfect Heuristics: Parameters within a strategy might reflect imperfect application (e.g., in WSLS, \\(p_{stay\\_win} &lt; 1\\) or \\(p_{shift\\_loss} &lt; 1\\)). This can also capture asymmetric responses to feedback (e.g., being more likely to shift after a loss than stay after a win). Exploration: Note that we talked about noise and errors, but random deviations can also be framed as adaptive exploration, allowing the agent to test actions that their current strategy deems suboptimal. 3.7.1.4 A Note on Ideal Observers It is important to distinguish between Heuristic Models (like WSLS) and Ideal Observer Models (like the Kalman Filter or HGF). Heuristics attempt to describe the process a human uses. Ideal observers describe the optimal computation given the uncertainty. When we fit models like the HGF to human data, we are effectively asking: “In what specific ways does the human deviate from optimality?” (e.g., do they overestimate how volatile the opponent is?). 3.7.2 Relationships Between Models It’s useful to note that these candidate models aren’t always entirely distinct. Often, simpler models emerge as special cases of more complex ones: A Random Choice model is like a Memory-Based model where the influence of memory is zero. WSLS can be seen as a specific type of RL model with a very high learning rate and sensitivity only to the immediately preceding trial’s outcome. A 0-ToM model might resemble a Bias Tracking or RL model. Recognizing these connections can guide a principled modeling approach, starting simple and adding complexity only as needed and justified by data or theory. This is what we called “model nesting” in a future chapter. It’s very elegant in that it allows to directly compare several models in a non-exclusive fashion, simply based on parameter values. A good example is also provided in Betancourt’s case study on reading times: https://betanalpha.github.io/assets/chapters_html/reading_times.html 3.7.3 Handling Heterogeneity: Mixture Models Sometimes the models cannot be nested, or we might want to capture the possibility that different participants (or even the same participant at different times) use different strategies. For instance, some players might predominantly use WSLS, while others rely more on bias tracking. This is where mixture models become relevant (explored in detail in a future chapter). Concept: Instead of assuming one model generated all the data, a mixture model assumes the data is a probabilistic blend from multiple candidate models (e.g., 70% of choices from WSLS, 30% from Random Bias). Purpose: Allows capturing heterogeneity within or across individuals without needing to know a priori which strategy was used on which trial or by which person. The model estimates the probability that each data point came from each component strategy. Challenge: Mixture models often require substantial data to reliably distinguish between components and estimate their mixing proportions. 3.7.4 Cognitive Modeling vs. Traditional Statistical Approaches (e.g., GLM) How does this modeling approach differ from standard statistical analyses you might have learned, like ANOVAs or the General Linear Model - GLM? Focus: GLM approaches typically focus on identifying statistical effects: Does factor X significantly influence outcome Y? (e.g., Does the opponent’s strategy affect the player’s win rate?). Cognitive modeling focuses on identifying the underlying process or mechanism: How does the opponent’s strategy lead to changes in the player’s choices via specific computations (like learning, memory updating, or strategic reasoning)? Theory: Cognitive models are usually derived from theories about mental processes. GLMs are more general statistical tools, often used agnostically regarding the specific cognitive mechanism. Parameters: Cognitive models estimate parameters that often have direct psychological interpretations (e.g., learning rate, memory decay, decision threshold, bias weight). GLM parameters represent statistical associations (e.g., regression coefficients). Data Level: Cognitive models often predict behavior at the trial level (e.g., predicting the choice on trial t based on history up to t-1). GLM analyses often aggregate data (e.g., comparing average win rates across conditions). Prediction vs. Explanation: While both aim to explain data, cognitive modeling often places a stronger emphasis on generating the observed behavior pattern from the hypothesized mechanism, allowing for simulation and prediction of fine-grained details. Example Revisited: In the Matching Pennies game: * A GLM approach might test if Payoff ~ BotStrategy * Role + (1|ID) shows a significant effect of BotStrategy. * A cognitive modeling approach would fit different strategy models (WSLS, RL, etc.) to the choice data and compare them (using methods from Ch 7) to see which mechanism best explains the choices made against different bots, potentially revealing why performance differs (e.g., due to changes in estimated learning rates or strategy weights). Both approaches are valuable, but cognitive modeling aims for a deeper, mechanistic level of explanation about the underlying cognitive processes. 3.8 Conclusion: From Observations to Verbal Theories This chapter took us from observing behavior in a specific task – the Matching Pennies game – to the crucial stage of formulating initial theories about the cognitive processes involved. We explored how analyzing gameplay data, considering participant reports, applying cognitive principles (like memory limits and error proneness), and contrasting different potential strategies (Random, WSLS, Memory-based, RL, k-ToM) helps us generate plausible verbal models. We saw that the path from raw behavior to a testable model involves significant abstraction and simplification. We also highlighted the importance of distinguishing between the participant’s experience and the researcher’s theoretical stance, and how cognitive modeling differs from traditional statistical approaches by focusing on underlying mechanisms. You now have a conceptual map of candidate models and understand why cognitive constraints matter. But verbal descriptions like ‘win-stay-lose-shift’ hide crucial ambiguities: Does ‘stay’ mean always stay or usually stay? How do we handle the first trial? In Chapter 3, you’ll implement these models in code, forcing you to make every assumption explicit. This is where modeling becomes rigorous and often reveals that our verbal intuitions were vaguer than we thought. The next chapter, “From verbal descriptions to formal models,” tackles exactly this challenge. We will take some of the candidate models discussed here (like Random Choice and WSLS) and translate them into precise mathematical algorithms and R functions. This formalization will force us to be explicit about our assumptions and enable us to simulate agent behavior, setting the stage for fitting these models to data and evaluating their performance in later chapters. "],["from-verbal-descriptions-to-formal-models.html", "Chapter 4 From verbal descriptions to formal models 4.1 Learning Goals 4.2 The Value of Formalization and Simulation 4.3 Defining general conditions 4.4 Implementing a Random Agent 4.5 Implementing a Win-Stay-Lose-Shift (WSLS) Agent 4.6 Scaling Up: The Virtual Experiment 4.7 Conclusion: The Forward Model", " Chapter 4 From verbal descriptions to formal models In Chapter 1, we explored how the “physics” of a pizza stone—Newton’s Law of Cooling—could generate temperature data. We called this a generative model. In Chapter 2, you observed your own behavior in the Matching Pennies game and proposed verbal theories like “Win-Stay-Lose-Shift” or “Random Choice.” Now, we bridge the gap. In this chapter, we will build “models of the mind.” We will translate the verbal strategies from Chapter 2 into precise algorithms—creating agents, or Synthetic Subjects. Just as we used a formula to simulate the pizza stone heating up, we will write code to simulate a cognitive agent playing a game. If our code can generate choice data that looks like human data, we have a candidate model for how the mind works. If it cannot, our theory is wrong. 4.1 Learning Goals This chapter focuses on translating the verbal theories from Chapter 2 into concrete, testable computational models. After completing this chapter, you will be able to: Formalize Verbal Models: Translate abstract strategy descriptions (like “Random Choice” or “Win-Stay-Lose-Shift”) into precise algorithms. Implement Agent-Based Simulations: Create simple software “agents” that execute these algorithms within a simulated environment (the Matching Pennies game). Run Virtual Experiments: Use simulation to see how different strategies perform against different opponents before we ever touch real data. Analyze Simulated Behavior: Generate and visualize simulation data to understand how different strategies perform and interact under controlled conditions. Prepare for Model Fitting: Appreciate why simulating models is a crucial step prior to estimating parameters from empirical data (covered in Chapter 4). 4.2 The Value of Formalization and Simulation Verbal models are comfortable but dangerous. In Chapter 2, we might have said: “The agent stays after a win.” But formalization forces us to answer uncomfortable questions, for instance: The Persistence Question: If they win 10 times, do they stay forever? Or is there a limit? The Error Question: Do they always stay, or do they sometimes press the wrong button by mistake? The First Trial Question: What do they do before they have any feedback? Writing code forces us to strip away ambiguity. It reveals hidden assumptions and forces us to be explicit about the mechanisms driving behavior. However, the value of this process extends beyond just clarity. Once formalized, we can create computational agents that act out the strategy. Simulation allows us to: Understand Emergent Behavior: See the consequences of our model’s rules over many trials or interactions. Does the strategy lead to expected patterns? Are there surprising outcomes that weren’t obvious from the verbal description? Explore Parameter Space: How does behavior change if we alter a model parameter (e.g., the bias in a random agent, or the stay probability in WSLS)? Generate Testable Predictions: What specific behavioral patterns (e.g., choice sequences) does the model predict under different conditions? These predictions can then be compared to real data. Debug Model Logic: Does the simulated behavior make sense? If not, it might indicate a flaw in our formalization of the verbal theory. In this chapter, we will: Formalize: Define precise algorithms for two key strategies (Random Choice and WSLS). Implement: Code these algorithms as reusable R functions. Simulate: Run these agents in a “Virtual Arena” against different opponents. Scale: Simulate many agents to understand average behavior and visualize the results to compare strategy effectiveness. 4.3 Defining general conditions # Load necessary packages # patchwork helps combine multiple plots pacman::p_load(tidyverse, patchwork) # Number of trials per simulation trials &lt;- 120 # Number of agents to simulate agents &lt;- 100 # Optional: Set random seed for reproducibility # set.seed(123) 4.4 Implementing a Random Agent Our simplest model from Chapter 2 was the Random Choice agent. In cognitive science, this serves as our Null Hypothesis. A Random Agent is not just “acting crazily”; it represents a specific, strong theoretical claim: The agent has no memory and no learning. It does not care about the past (history) or the future (planning). It only has a bias. Algorithm: On every trial \\(t\\), independently of what happened at \\(t-1\\), choose ‘right’ (1) with probability \\(\\theta\\) and ‘left’ (0) with probability \\(1-\\theta\\). Let’s implement this synthetic subject. # --- Simulation Parameters --- trials &lt;- 120 # Number of trials for a single simulation run # --- Basic Implementation --- # Simulate an unbiased random agent (rate = 0.5) rate_unbiased &lt;- 0.5 choices_unbiased &lt;- rbinom(trials, size = 1, prob = rate_unbiased) # rbinom generates random draws from a binomial distribution. # size = 1 makes it a Bernoulli trial (0 or 1 outcome). # prob = rate specifies the probability of getting a 1. # Simulate a biased random agent (rate = 0.8, chooses &#39;right&#39; 80% of the time) rate_biased &lt;- 0.8 choices_biased &lt;- rbinom(trials, size = 1, prob = rate_biased) # --- Visualization 1: Raw Choices --- # Create data frames for plotting d_unbiased &lt;- tibble(trial = 1:trials, choice = choices_unbiased, rate = rate_unbiased) d_biased &lt;- tibble(trial = 1:trials, choice = choices_biased, rate = rate_biased) # Plot raw choice sequence (0s and 1s) p1 &lt;- ggplot(d_unbiased, aes(x = trial, y = choice)) + geom_line(color = &quot;blue&quot;) + geom_point(color = &quot;blue&quot;, size = 1) + labs(title = &quot;Unbiased Random Agent (Rate = 0.5)&quot;, y = &quot;Choice (0/1)&quot;) + theme_classic() + ylim(-0.1, 1.1) p2 &lt;- ggplot(d_biased, aes(x = trial, y = choice)) + geom_line(color = &quot;red&quot;) + geom_point(color = &quot;red&quot;, size = 1) + labs(title = &quot;Biased Random Agent (Rate = 0.8)&quot;, y = &quot;Choice (0/1)&quot;) + theme_classic() + ylim(-0.1, 1.1) # Show plots side-by-side print(p1 + p2 + plot_layout(guides = &quot;collect&quot;)) This first visualization shows the behavior of a purely random agent - one that chooses between options with equal probability (rate = 0.5). Looking at the jagged line jumping between 0 and 1, we can see that the agent’s choices appear truly random, with no discernible pattern. This represents what we might expect from a player who is deliberately trying to be unpredictable in the matching pennies game. However, this raw choice plot can be hard to interpret. A more informative way to look at the agent’s behavior is to examine how its average rate of choosing option 1 evolves over time: # --- Visualization 2: Cumulative Rate --- # Calculate the cumulative proportion of &#39;right&#39; choices (1s) d_unbiased &lt;- d_unbiased %&gt;% mutate(cumulative_rate = cumsum(choice) / row_number()) d_biased &lt;- d_biased %&gt;% mutate(cumulative_rate = cumsum(choice) / row_number()) # Combine data for comparison plot d_combined &lt;- bind_rows(d_unbiased, d_biased) %&gt;% mutate(Agent = paste(&quot;Rate =&quot;, rate)) # Plot cumulative rates p3 &lt;- ggplot(d_combined, aes(x = trial, y = cumulative_rate, color = Agent)) + geom_line(size = 1) + geom_hline(yintercept = 0.5, linetype = &quot;dashed&quot;, color = &quot;black&quot;) + # Reference line for 50% ylim(0, 1) + labs( title = &quot;Cumulative Choice Rate Over Time&quot;, subtitle = &quot;Shows convergence towards the agent&#39;s underlying bias&quot;, x = &quot;Trial Number&quot;, y = &quot;Cumulative Proportion of Choosing &#39;Right&#39;&quot;, color = &quot;Agent Type&quot; ) + theme_classic() print(p3) We can clearly see how bias affects choice behavior. The unbiased agent (rate = 0.5) stabilizes around choosing each option equally often, while the biased agent (rate = 0.8) shows a strong preference for option 1, choosing it approximately 80% of the time. This comparison helps us understand how we might detect biases in real players’ behavior - consistent deviation from 50-50 choice proportions could indicate an underlying preference or strategy. 4.4.1 Encapsulating the Agent in a Function To make our agent reusable, we’ll wrap the logic in a function. This is good practice for building more complex simulations. Note that the function is preceded by a number of commented lines. They are called Roxygen comments, a special documentation format used in R packages. By using these comments, we keep the code and its manual in the same place (and if you build a package, the format allows for automatically generating the help pages). 4.4.1.1 How to read it: Description: The first lines describe what the function does. (param?): Describes the inputs (arguments) the function expects (e.g., n_trials, rate). (return?): Describes the output the function produces (e.g., a vector of 0s and 1s). #&#39; Random Agent Function #&#39; #&#39; Generates a sequence of choices based on a fixed probability (rate). #&#39; Includes optional noise causing random 50/50 choices. #&#39; #&#39; @param n_trials Integer, the number of choices to generate. #&#39; @param rate Numeric, the probability (0-1) of choosing option 1 (&#39;right&#39;). #&#39; @param noise Numeric, the probability (0-1) of making a random 50/50 choice #&#39; instead of following the rate. Default is 0 (no noise). #&#39; #&#39; @return A numeric vector of choices (0s and 1s). #&#39; RandomAgent_f &lt;- function(n_trials, rate = 0.5, noise = 0) { # Input validation if (!is.numeric(rate) || rate &lt; 0 || rate &gt; 1) { stop(&quot;Rate must be a probability between 0 and 1.&quot;) } if (!is.numeric(noise) || noise &lt; 0 || noise &gt; 1) { stop(&quot;Noise must be a probability between 0 and 1.&quot;) } # Generate base choices according to the rate choices &lt;- rbinom(n_trials, size = 1, prob = rate) # Apply noise: identify trials where noise occurs noise_trials &lt;- rbinom(n_trials, size = 1, prob = noise) == 1 # Replace choices with random 50/50 on noise trials if (any(noise_trials)) { choices[noise_trials] &lt;- rbinom(sum(noise_trials), size = 1, prob = 0.5) } return(choices) } # Example usage: example_choices &lt;- RandomAgent_f(n_trials = 10, rate = 0.7, noise = 0.1) print(paste(&quot;Example choices (rate=0.7, noise=0.1):&quot;, paste(example_choices, collapse = &quot; &quot;))) ## [1] &quot;Example choices (rate=0.7, noise=0.1): 1 1 1 0 0 1 1 1 0 1&quot; 4.5 Implementing a Win-Stay-Lose-Shift (WSLS) Agent The Random Agent was memoryless. The Win-Stay-Lose-Shift (WSLS) agent is our first step into strategic behavior: it reacts to the immediate past. Algorithm: Initialize: Make a random guess on the first trial. Loop (Trial \\(t &gt; 1\\)): Check feedback from trial \\(t-1\\). If Win: Repeat the previous choice. If Loss: Switch to the opposite choice. Noise (The “Trembling Hand”): With a small probability noise, ignore the strategy and pick randomly. This accounts for attention lapses or motor errors (often called “trembling hand” in game theory). 4.5.1 Implementing the WSLS Function Note on Programming: In R, we usually prefer vectorization (doing everything at once) over loops. However, many cognitive models are inherently sequential: what I do now depends on what I just saw. Therefore, we use an explicit if/else logic here to mirror the cognitive process. #&#39; Win-Stay-Lose-Shift Agent Function #&#39; #&#39; Determines the next choice based on the previous choice and its outcome (feedback). #&#39; Includes optional noise. #&#39; #&#39; @param prevChoice Numeric, the agent&#39;s choice on the previous trial (0 or 1). #&#39; @param feedback Numeric, the outcome of the previous trial (1 for win, 0 for loss). #&#39; @param noise Numeric, the probability (0-1) of making a random 50/50 choice. Default is 0. #&#39; #&#39; @return Numeric, the agent&#39;s next choice (0 or 1). #&#39; WSLSAgent_f &lt;- function(prevChoice, feedback, noise = 0) { # Input validation if (!prevChoice %in% c(0, 1)) stop(&quot;Previous choice must be 0 or 1.&quot;) if (!feedback %in% c(0, 1)) stop(&quot;Feedback must be 0 or 1.&quot;) if (!is.numeric(noise) || noise &lt; 0 || noise &gt; 1) stop(&quot;Noise must be a probability between 0 and 1.&quot;) # Core WSLS logic: # If feedback is 1 (win), stay: choice = prevChoice # If feedback is 0 (loss), shift: choice = 1 - prevChoice choice &lt;- ifelse(feedback == 1, prevChoice, 1 - prevChoice) # Apply noise if specified if (noise &gt; 0 &amp;&amp; runif(1) &lt; noise) { # Override with a random 50/50 choice choice &lt;- sample(c(0, 1), 1) } return(choice) } # Example usage: # Won previous trial (feedback=1) after choosing 1: print(paste(&quot;Next choice after win (chose 1):&quot;, WSLSAgent_f(prevChoice = 1, feedback = 1))) ## [1] &quot;Next choice after win (chose 1): 1&quot; # Lost previous trial (feedback=0) after choosing 1: print(paste(&quot;Next choice after loss (chose 1):&quot;, WSLSAgent_f(prevChoice = 1, feedback = 0))) ## [1] &quot;Next choice after loss (chose 1): 0&quot; 4.5.2 Simulating WSLS vs. Opponents Now, let’s simulate our WSLS agent playing against different opponents to see how it performs. # --- Simulation Setup --- trials &lt;- 120 # --- Simulation 1: WSLS vs. Biased Random Agent --- cat(&quot;Simulating WSLS vs. Biased Random (Rate = 0.8)...\\n&quot;) ## Simulating WSLS vs. Biased Random (Rate = 0.8)... # Opponent always chooses &#39;right&#39; with 80% probability opponent_choices_biased &lt;- RandomAgent_f(n_trials = trials, rate = 0.8, noise = 0) # Initialize vectors for WSLS agent wsls_choices_vs_biased &lt;- rep(NA, trials) feedback_vs_biased &lt;- rep(NA, trials) # First choice is random wsls_choices_vs_biased[1] &lt;- sample(c(0, 1), 1) # Simulation loop for (t in 2:trials) { # Determine feedback from previous trial (WSLS wins if choices match opponent) # Note: In Matching Pennies, win condition depends on role (Matcher/Hider). # Here we assume the WSLS agent is the &#39;Matcher&#39; for simplicity. prev_feedback &lt;- ifelse(wsls_choices_vs_biased[t - 1] == opponent_choices_biased[t - 1], 1, 0) feedback_vs_biased[t - 1] &lt;- prev_feedback # Store feedback for analysis # WSLS agent makes choice based on previous feedback wsls_choices_vs_biased[t] &lt;- WSLSAgent_f( prevChoice = wsls_choices_vs_biased[t - 1], feedback = prev_feedback, noise = 0 # Assuming no noise for this simulation ) } # Record feedback for the last trial feedback_vs_biased[trials] &lt;- ifelse(wsls_choices_vs_biased[trials] == opponent_choices_biased[trials], 1, 0) # Create dataframe for analysis df_vs_biased &lt;- tibble( trial = 1:trials, Self_WSLS = wsls_choices_vs_biased, Opponent_Random = opponent_choices_biased, Feedback = feedback_vs_biased # 1 if WSLS won ) %&gt;% mutate( Cumulative_Performance = cumsum(Feedback) / row_number() ) # --- Simulation 2: WSLS vs. WSLS --- # (Opponent plays WSLS but tries to mismatch - feedback is inverted) cat(&quot;Simulating WSLS vs. WSLS...\\n&quot;) ## Simulating WSLS vs. WSLS... wsls1_choices &lt;- rep(NA, trials) wsls2_choices &lt;- rep(NA, trials) feedback_wsls1 &lt;- rep(NA, trials) # Feedback for agent 1 wsls1_choices[1] &lt;- sample(c(0, 1), 1) wsls2_choices[1] &lt;- sample(c(0, 1), 1) for (t in 2:trials) { # Feedback for agent 1 from previous trial (wins if match) prev_feedback1 &lt;- ifelse(wsls1_choices[t - 1] == wsls2_choices[t - 1], 1, 0) feedback_wsls1[t - 1] &lt;- prev_feedback1 # Feedback for agent 2 (opponent) is the opposite (wins if mismatch) prev_feedback2 &lt;- 1 - prev_feedback1 # Both agents choose based on their own previous feedback wsls1_choices[t] &lt;- WSLSAgent_f(wsls1_choices[t - 1], prev_feedback1, noise = 0) wsls2_choices[t] &lt;- WSLSAgent_f(wsls2_choices[t - 1], prev_feedback2, noise = 0) } feedback_wsls1[trials] &lt;- ifelse(wsls1_choices[trials] == wsls2_choices[trials], 1, 0) # Create dataframe df_vs_wsls &lt;- tibble( trial = 1:trials, Self_WSLS = wsls1_choices, Opponent_WSLS = wsls2_choices, Feedback = feedback_wsls1 # 1 if Self_WSLS won ) %&gt;% mutate( Cumulative_Performance = cumsum(Feedback) / row_number() ) # --- Visualize Simulation Results --- # Plot choices over time p_choices_biased &lt;- ggplot(df_vs_biased, aes(x = trial)) + geom_line(aes(y = Self_WSLS, color = &quot;WSLS Agent&quot;)) + geom_line(aes(y = Opponent_Random + 0.05, color = &quot;Biased Opponent&quot;), linetype = &quot;dashed&quot;) + # Offset slightly labs(title = &quot;WSLS vs. Biased Random&quot;, y = &quot;Choice (0/1)&quot;) + theme_classic() + ylim(-0.1, 1.1) p_choices_wsls &lt;- ggplot(df_vs_wsls, aes(x = trial)) + geom_line(aes(y = Self_WSLS, color = &quot;WSLS Agent 1&quot;)) + geom_line(aes(y = Opponent_WSLS + 0.05, color = &quot;WSLS Agent 2&quot;), linetype = &quot;dashed&quot;) + # Offset slightly labs(title = &quot;WSLS vs. WSLS&quot;, y = &quot;Choice (0/1)&quot;) + theme_classic() + ylim(-0.1, 1.1) # Plot cumulative performance p_perf_biased &lt;- ggplot(df_vs_biased, aes(x = trial, y = Cumulative_Performance)) + geom_line(color = &quot;blue&quot;, size = 1) + geom_hline(yintercept = 0.5, linetype = &quot;dashed&quot;) + labs(title = &quot;WSLS Performance vs. Biased Random&quot;, y = &quot;Proportion Wins&quot;) + theme_classic() + ylim(0, 1) p_perf_wsls &lt;- ggplot(df_vs_wsls, aes(x = trial, y = Cumulative_Performance)) + geom_line(color = &quot;purple&quot;, size = 1) + geom_hline(yintercept = 0.5, linetype = &quot;dashed&quot;) + labs(title = &quot;WSLS Performance vs. WSLS&quot;, y = &quot;Proportion Wins&quot;) + theme_classic() + ylim(0, 1) # Arrange plots print((p_choices_biased | p_choices_wsls) / (p_perf_biased | p_perf_wsls) + plot_layout(guides = &quot;collect&quot;) &amp; theme(legend.position = &quot;bottom&quot;)) This cumulative performance plot reveals the overall effectiveness of the WSLS strategy. By tracking the running average of successes, we can see whether the strategy leads to above-chance performance in the long run. When playing against a biased random opponent, the WSLS agent can potentially exploit the opponent’s predictable tendencies, though success depends on how strong and consistent the opponent’s bias is. When we pit the WSLS agent against another WSLS agent, the dynamics become more complex. Both agents are now trying to adapt to each other’s adaptations, creating a more sophisticated strategic interaction. The resulting behavior often shows interesting patterns of mutual adaptation, where each agent’s attempts to exploit the other’s strategy leads to evolving patterns of play. 4.6 Scaling Up: The Virtual Experiment Now that we have our agents defined as functions, we can run virtual Experiments, to better understand our models. We will build a “Simulation Engine” function that: Takes two agents (e.g., WSLS vs. Random). Makes them play for N trials. Records every move. To understand the average behavior and robustness of the WSLS strategy, we need to simulate many agents playing against opponents with varying biases. First, let’s create a function that runs one simulation (one WSLS agent vs. one opponent) and returns the results. This avoids repeating code. #&#39; Run Matching Pennies Simulation #&#39; #&#39; Simulates a full game between an Agent (Subject) and an Opponent. #&#39; #&#39; @param n_trials Integer, number of trials. #&#39; @param agent_strategy String, &quot;WSLS&quot; or &quot;Random&quot;. #&#39; @param agent_params List, parameters for the agent (e.g., list(noise=0.1)). #&#39; @param opponent_strategy String, &quot;WSLS&quot; or &quot;Random&quot;. #&#39; @param opponent_params List, parameters for the opponent (e.g., list(rate=0.8)). #&#39; #&#39; @return A tibble with trial-by-trial choices and outcomes. #&#39; run_simulation &lt;- function(n_trials, agent_strategy, agent_params, opponent_strategy, opponent_params) { # Pre-allocate vectors for speed agent_choices &lt;- rep(NA, n_trials) opponent_choices &lt;- rep(NA, n_trials) agent_feedback &lt;- rep(NA, n_trials) # --- Trial 1 (Initialization) --- # Agents behave randomly or use a specific start bias on trial 1 agent_choices[1] &lt;- rbinom(1, 1, 0.5) opponent_choices[1] &lt;- rbinom(1, 1, 0.5) # Check winner (Assuming Agent is Matcher: Win if choices match) agent_feedback[1] &lt;- ifelse(agent_choices[1] == opponent_choices[1], 1, 0) # --- Loop through Game --- for (t in 2:n_trials) { # 1. Agent makes a choice based on history (t-1) if (agent_strategy == &quot;WSLS&quot;) { agent_choices[t] &lt;- WSLSAgent_f(agent_choices[t-1], agent_feedback[t-1], agent_params$noise) } else if (agent_strategy == &quot;Random&quot;) { agent_choices[t] &lt;- RandomAgent_f(1, agent_params$rate, agent_params$noise) } # 2. Opponent makes a choice if (opponent_strategy == &quot;Random&quot;) { opponent_choices[t] &lt;- RandomAgent_f(1, opponent_params$rate, opponent_params$noise) } else if (opponent_strategy == &quot;WSLS&quot;) { # Opponent is Hider: They win if choices MISMATCH. # So their feedback is the opposite of the Agent&#39;s feedback. op_feedback &lt;- 1 - agent_feedback[t-1] opponent_choices[t] &lt;- WSLSAgent_f(opponent_choices[t-1], op_feedback, opponent_params$noise) } # 3. Determine Outcome agent_feedback[t] &lt;- ifelse(agent_choices[t] == opponent_choices[t], 1, 0) } # Return data return(tibble( trial = 1:n_trials, choice = agent_choices, opponent = opponent_choices, win = agent_feedback )) } # --- Running Scaled Simulations --- cat(&quot;Running scaled simulations...\\n&quot;) ## Running scaled simulations... # Default WSLS parameters (Only &#39;noise&#39; is used by our simple WSLS agent) wsls_params_default &lt;- list(noise = 0) # Simulation 1: WSLS vs. Random agents with varying biases opponent_rates &lt;- seq(from = 0.5, to = 1, by = 0.05) results_vs_random &lt;- list() for (rate in opponent_rates) { # Create a list to store runs for this specific rate rate_runs &lt;- list() for (agent_i in 1:agents) { opponent_params_rand &lt;- list(rate = rate, noise = 0) # CORRECTED CALL: Added &quot;WSLS&quot; as the second argument sim_run &lt;- run_simulation(trials, &quot;WSLS&quot;, wsls_params_default, &quot;Random&quot;, opponent_params_rand) sim_run$agent &lt;- agent_i sim_run$opponent_rate &lt;- rate rate_runs[[agent_i]] &lt;- sim_run } # Combine all agents for this rate and store in main list results_vs_random[[as.character(rate)]] &lt;- bind_rows(rate_runs) } # Combine everything into one big dataframe df_scaled_vs_random &lt;- bind_rows(results_vs_random) 4.6.1 Visualizing Results Now let’s visualize the average performance of the WSLS agent against random opponents with different biases. # Calculate cumulative performance for each simulation run df_scaled_vs_random &lt;- df_scaled_vs_random %&gt;% group_by(agent, opponent_rate) %&gt;% # CHANGED &#39;Feedback&#39; to &#39;win&#39; here: mutate(Cumulative_Performance = cumsum(win) / row_number()) %&gt;% ungroup() # Plot average performance over time, grouped by opponent bias rate p_scaled_perf &lt;- ggplot(df_scaled_vs_random, aes(x = trial, y = Cumulative_Performance, group = opponent_rate, color = as.factor(opponent_rate))) + # made color a factor for cleaner legend stat_summary(fun = mean, geom = &quot;line&quot;, size = 1) + geom_hline(yintercept = 0.5, linetype = &quot;dashed&quot;) + scale_color_viridis_d() + # Discrete scale since we made rate a factor labs( title = &quot;Average WSLS Performance vs. Random Opponents&quot;, subtitle = &quot;Performance improves as opponent&#39;s bias increases&quot;, x = &quot;Trial Number&quot;, y = &quot;Average Proportion Wins&quot;, color = &quot;Opponent Bias&quot; ) + ylim(0, 1) + theme_classic() print(p_scaled_perf) This plot clearly shows that the WSLS strategy becomes more effective (higher average win rate) as the opponent becomes more predictable (i.e., their bias rate moves further away from 0.5 towards 1.0). Against a purely random opponent (rate=0.5), WSLS performs at chance level on average. This highlights WSLS’s ability to exploit predictability. 4.7 Conclusion: The Forward Model In this chapter, we successfully built a Forward Model. * We started with a verbal theory (WSLS). * We translated it into code (parameters + algorithm). * We ran the code to generate artificial data. This is the “models of the Mind” in action. We demonstrated that a simple mechanism (Win-Stay-Lose-Shift) can generate complex success rates depending on the environment (the opponent). 4.7.1 The Problem of Inference However, in real science, we don’t start with the parameters. We start with the data (the choices participants made) and we want to find the parameters (did they use WSLS? How much noise?). We need to run our machine in reverse. Forward (Simulation): Parameters \\(\\rightarrow\\) Data Inverse (Fitting): Data \\(\\rightarrow\\) Parameters In the next chapter, “From Simulation to Model Fitting,” we will learn how to take the simulated data we just created and use statistical methods (Maximum Likelihood and Bayesian Inference with Stan) to recover the original parameters. "],["from-simulation-to-model-fitting.html", "Chapter 5 From simulation to model fitting 5.1 Learning Goals 5.2 The Challenge: Inferring Latent Parameters 5.3 Simulating data 5.4 Building our First Stan Model: Inferring Bias Rate 5.5 Validating the Model: Parameter Recovery 5.6 Moving Beyond Simple Bias: Memory Models 5.7 Memory Model 4: Bayesian Agent (Optimal Updating) 5.8 Relationship to Rescorla-Wagner 5.9 Conclusion: Estimating Parameters and Exploring Memory", " Chapter 5 From simulation to model fitting In Chapter 4, we formalized verbal models into computational agents and explored their behavior through simulation by setting their parameters (like bias rate or WSLS rules). However, when analyzing real experimental data, we don’t know the true parameter values underlying a participant’s behavior. The core task shifts from simulation to parameter estimation: inferring the plausible values of model parameters given the observed data. 5.1 Learning Goals This chapter transitions from simulating predefined models (Chapter 4) to inferring model parameters from data using Bayesian methods. By the end of this chapter, you will be able to: Understand Parameter Estimation: Grasp the core challenge of estimating unobservable model parameters (like cognitive biases or memory effects) from observable behavioral data (like choices). Implement Bayesian Models in Stan: Write basic Stan programs to specify cognitive models, including defining data, parameters, priors, and the likelihood function . Fit Models with cmdstanr: Use the cmdstanr interface to compile Stan models and sample from the posterior distribution of parameters. Interpret Model Output: Understand and visualize basic MCMC diagnostics (trace plots) and parameter estimates (posterior distributions, prior-posterior updates). Perform Parameter Recovery: Design and execute simulations to verify that your Stan model and fitting procedure can accurately recover known parameter values, a crucial step in model validation. Model Cognitive Processes: Apply these techniques to estimate parameters for simple cognitive strategies, including biased choice and different types of memory models (GLM-like, internal state, exponential forgetting, Bayesian updating). Baby steps in reparametrization: A key tool for cognitive modelers is the ability to rethink the mathematical form of their models to better implement conceptual constraints, or avoid fitting issues. Here you will appreciate the utility of the log-odds transformation for modeling probability parameters. 5.2 The Challenge: Inferring Latent Parameters Why is this a challenge? Parameters are Unseen: Cognitive parameters (like bias strength, memory decay, learning rate) are not directly observable; we must infer them from behavior (choices, RTs, etc.). Behavior is Noisy: Human behavior is variable. Our models need to account for this noise while extracting the underlying parameter signal. Model Plausibility: We need to determine not just if a model fits, but how well it fits, and whether the estimated parameter values are theoretically meaningful. Consider the biased agent from Chapter 4. We simulated agents with a known rate. Now, imagine you only have the sequence of choices (the h data) from an unknown agent. How can you estimate the underlying rate that most likely generated those choices? Our Approach: Bayesian Inference with Stan This chapter introduces Bayesian inference as our primary tool for parameter estimation. The Bayesian approach allows us to combine prior knowledge about parameters with information from the observed data (likelihood) to arrive at an updated understanding (posterior distribution) of the parameters. We will use Stan, a powerful probabilistic programming language, implemented via the cmdstanr R package, to: Specify Models: Define our cognitive models formally, including parameters and their prior distributions. Fit Models: Use Stan’s algorithms (primarily Markov Chain Monte Carlo - MCMC) to sample from the posterior distribution of parameters given the data. Evaluate Fits: Examine the results to understand parameter estimates and their uncertainty. As a crucial validation step, we will first apply these methods to simulated data, where we know the true parameters. This allows us to check if our models and fitting procedures work correctly – a process called parameter recovery. 5.3 Simulating data As usual we start with simulated data, where we know the underlying mechanisms and parameter values. Simulated data are rarely enough (empirical data often offer unexpected challenges), but they are a great starting point to stress test your model: does the model reconstruct the right parameter values? Does it reproduce the overall patterns in the data? Here we build a new simulation of random agents with bias and noise. The code and visualization is really nothing different from last chapter. # Flag to control whether to regenerate simulation/fitting results # Set to TRUE to rerun everything (takes time!), FALSE to load saved results. regenerate_simulations &lt;- FALSE # Or TRUE # Load required packages using pacman::p_load # This function installs packages if missing, then loads them. pacman::p_load( tidyverse, # Data manipulation and plotting here, # Robust file paths cmdstanr, # Stan interface posterior, # Posterior analysis bayesplot, # Plotting MCMC furrr, # Parallel processing future # Future backend ) # Parallel Setup: Leave 2 cores free for system stability n_cores &lt;- parallel::detectCores() - 2 plan(multisession, workers = n_cores) # Optional: Set a default ggplot theme theme_set(theme_classic()) # --- Simulate Data for Fitting --- # Goal: Generate data from known models/parameters to test our fitting procedures. trials &lt;- 120 # Number of trials per simulated agent # Define the agent function (copied/adapted from Chapter 4 for self-containment) # This agent chooses &#39;1&#39; with probability &#39;rate&#39;, potentially adding noise. RandomAgentNoise_f &lt;- function(rate, noise) { # Ensure rate is probability (in case log-odds are passed, though not here) rate_prob &lt;- plogis(rate) # Assumes rate might be log-odds; use identity if rate is already probability choice &lt;- rbinom(1, 1, rate_prob) # Base choice if (noise &gt; 0 &amp;&amp; runif(1) &lt; noise) { # Check if noise occurs choice &lt;- rbinom(1, 1, 0.5) # Override with random 50/50 choice } return(choice) } # --- Generate Data Across Different Conditions --- # Define relative paths for saving/loading simulation data sim_data_dir &lt;- &quot;simdata&quot; # Assumes a &#39;simdata&#39; subdirectory sim_data_file &lt;- here(sim_data_dir, &quot;04_randomnoise.csv&quot;) # Create directory if it doesn&#39;t exist if (!dir.exists(sim_data_dir)) dir.create(sim_data_dir) if (regenerate_simulations || !file.exists(sim_data_file)) { cat(&quot;Generating new simulation data...\\n&quot;) # Use expand_grid for cleaner parameter combinations param_grid &lt;- expand_grid( noise = seq(0, 0.5, 0.1), # Noise levels rate = seq(0, 1, 0.1) # Bias rate levels (probability scale) ) # Use map_dfr for efficient simulation across parameters d &lt;- pmap_dfr(param_grid, function(noise, rate) { # Simulate choices for one agent/condition choices &lt;- map_int(1:trials, ~RandomAgentNoise_f(qlogis(rate), noise)) # Convert rate to log-odds for function if needed # Create tibble for this condition tibble( trial = 1:trials, choice = choices, true_rate = rate, # Store the true rate used for generation true_noise = noise # Store the true noise used ) }) %&gt;% # Calculate cumulative rate for visualization group_by(true_rate, true_noise) %&gt;% mutate(cumulative_rate = cumsum(choice) / row_number()) %&gt;% ungroup() # Save the generated data write_csv(d, sim_data_file) cat(&quot;Generated new simulation data and saved to&quot;, sim_data_file, &quot;\\n&quot;) } else { # Load existing simulation data d &lt;- read_csv(sim_data_file) cat(&quot;Loaded existing simulation data from&quot;, sim_data_file, &quot;\\n&quot;) } ## Loaded existing simulation data from /Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/simdata/04_randomnoise.csv # --- Visualize Simulated Data --- # Plot cumulative rate, faceted by noise level p1 &lt;- ggplot(d, aes(x = trial, y = cumulative_rate, group = true_rate, color = true_rate)) + geom_line(alpha = 0.8) + geom_hline(yintercept = 0.5, linetype = &quot;dashed&quot;) + scale_color_viridis_c() + # Use perceptually uniform color scale facet_wrap(~paste(&quot;Noise =&quot;, true_noise)) + # Facet by noise level labs( title = &quot;Simulated Choice Behavior (Cumulative Rate)&quot;, subtitle = &quot;Agents with different bias rates and noise levels&quot;, x = &quot;Trial Number&quot;, y = &quot;Cumulative Proportion Choosing &#39;Right&#39;&quot;, color = &quot;True Bias Rate&quot; ) + theme_minimal() + ylim(0, 1) print(p1) 5.4 Building our First Stan Model: Inferring Bias Rate N.B. Refer to the video and slides for the step by step build-up of the Stan code. Let’s use Stan to estimate the underlying bias rate (which we’ll call theta in the model) from just the sequence of choices (h) generated by one of our simulated agents. We’ll start with a simple case: an agent with rate = 0.8 and noise = 0. Preparing Data for Stan Why a list? Well, dataframes (now tibbles) are amazing. But they have a big drawback: they require each variable to have the same length. Lists do not have that limitation, they are more flexible. So, lists. We’ll have to learn how to live with them. # Subset data for one specific agent simulation (rate=0.8, noise=0) d1 &lt;- d %&gt;% filter(true_rate == 0.8, true_noise == 0.0) # Make sure noise level matches # Stan requires data in a list format data_for_stan &lt;- list( n = trials, # Number of trials (integer) h = d1$choice # Vector of choices (0s and 1s) for this agent ) str(data_for_stan) # Show the structure of the list ## List of 2 ## $ n: num 120 ## $ h: num [1:120] 1 1 1 1 1 1 0 1 1 1 ... 5.4.1 Model We write the stan code within the R code (so I can show it to you more easily), then we save it as a stan file, which can be loaded at a later stage in order to compile it. [Missing: more info on compiling etc.] Remember that the minimal Stan model requires 3 chunks, one specifying the data it will need as input; one specifying the parameters to be estimated; one specifying the model within which the parameters appear, and the priors for those parameters. # This R chunk defines the Stan code as a string and writes it to a file. # It&#39;s marked eval=FALSE because we don&#39;t run the R code defining the string here, # but the Stan code itself IS the important content. stan_model_code &lt;- &quot; // Stan Model: Simple Bernoulli for Bias Estimation // Goal: Estimate the underlying probability (theta) of choosing 1 (&#39;right&#39;) // given a sequence of binary choices. // 1. Data Block: Declares the data Stan expects from R data { int&lt;lower=1&gt; n; // Number of trials (must be at least 1) array[n] int&lt;lower=0, upper=1&gt; h; // Array &#39;h&#39; of length &#39;n&#39; containing choices (0 or 1) } // 2. Parameters Block: Declares the parameters the model will estimate parameters { real&lt;lower=0, upper=1&gt; theta; // The bias parameter (probability), constrained between 0 and 1 } // 3. Model Block: Defines the priors and the likelihood model { // Prior: Our belief about theta *before* seeing the data. // We use a Beta(1, 1) prior, which is equivalent to a Uniform(0, 1) distribution. // This represents maximal prior ignorance about the bias. // &#39;target +=&#39; adds the log-probability density to the overall model log-probability. target += beta_lpdf(theta | 1, 1); // lpdf = log probability density function // Likelihood: How the data &#39;h&#39; depend on the parameter &#39;theta&#39;. // We model each choice &#39;h&#39; as a Bernoulli trial with success probability &#39;theta&#39;. // The model assesses how likely the observed sequence &#39;h&#39; is given a value of &#39;theta&#39;. target += bernoulli_lpmf(h | theta); // lpmf = log probability mass function (for discrete data) } // 4. Generated Quantities Block (Optional but useful) // Code here is executed *after* sampling, using the estimated parameter values. // Useful for calculating derived quantities or predictions. generated quantities { // Example: Simulate a new dataset based on the estimated theta array[n] int h_pred = bernoulli_rng(rep_vector(theta, n)); // _rng = random number generation } &quot; # Define relative path for Stan model file stan_model_dir &lt;- &quot;stan&quot; stan_file_bernoulli &lt;- here(stan_model_dir, &quot;04_SimpleBernoulli.stan&quot;) # Create directory if needed if (!dir.exists(stan_model_dir)) dir.create(stan_model_dir) # Write the Stan code to the file writeLines(stan_model_code, stan_file_bernoulli) cat(&quot;Stan model written to:&quot;, stan_file_bernoulli, &quot;\\n&quot;) ## Stan model written to: /Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/04_SimpleBernoulli.stan Compiling and Fitting the Stan Model Now we use cmdstanr to compile this model (translating it into efficient C++ code) and then run the MCMC sampler to get posterior estimates for theta. ## --- Compile and Fit --- # Specify model file path stan_file_bernoulli &lt;- here(&quot;stan&quot;, &quot;04_SimpleBernoulli.stan&quot;) # Define path for saving fitted model object stan_results_dir &lt;- &quot;simmodels&quot; model_file_bernoulli &lt;- here(stan_results_dir, &quot;04_SimpleBernoulli.rds&quot;) if (!dir.exists(stan_results_dir)) dir.create(stan_results_dir) if (regenerate_simulations || !file.exists(model_file_bernoulli)) { cat(&quot;Compiling and fitting Stan model (Bernoulli)...\\n&quot;) # Compile the Stan model (only needs to be done once unless code changes) mod_bernoulli &lt;- cmdstan_model(stan_file_bernoulli, cpp_options = list(stan_threads = FALSE), # Enable threading stanc_options = list(&quot;O1&quot;)) # Basic optimization # Sample from the posterior distribution using MCMC fit_bernoulli &lt;- mod_bernoulli$sample( data = data_for_stan, # The data list we prepared seed = 123, # For reproducible MCMC sampling chains = 4, # Number of parallel Markov chains (recommend 4) parallel_chains = min(4, future::availableCores()), # Run chains in parallel #threads_per_chain = 1, # For within-chain parallelization (usually 1 is fine) iter_warmup = 1000, # Number of warmup iterations per chain (discarded) iter_sampling = 2000, # Number of sampling iterations per chain (kept) refresh = 500, # How often to print progress updates max_treedepth = 10, # Controls complexity of MCMC steps (adjust if needed) adapt_delta = 0.8 # Target acceptance rate (adjust if divergences occur) ) # Save the fitted model object fit_bernoulli$save_object(file = model_file_bernoulli) cat(&quot;Model fit completed and saved to:&quot;, model_file_bernoulli, &quot;\\n&quot;) } else { # Load existing fitted model object fit_bernoulli &lt;- readRDS(model_file_bernoulli) cat(&quot;Loaded existing model fit from:&quot;, model_file_bernoulli, &quot;\\n&quot;) } ## Loaded existing model fit from: /Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/simmodels/04_SimpleBernoulli.rds # --- Examine Results --- # Print summary statistics (mean, median, SD, quantiles, diagnostics like Rhat, ESS) fit_bernoulli$summary() ## # A tibble: 122 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lp__ -70.7 -70.4 0.728 0.312 -72.2 -70.2 1.00 3744. 3744. ## 2 theta 0.736 0.738 0.0399 0.0400 0.670 0.799 1.00 3082. 3634. ## 3 h_pred[1] 0.735 1 0.441 0 0 1 1.00 7946. NA ## 4 h_pred[2] 0.732 1 0.443 0 0 1 1.000 7758. NA ## 5 h_pred[3] 0.741 1 0.438 0 0 1 1.000 8030. NA ## 6 h_pred[4] 0.736 1 0.441 0 0 1 1.00 7392. NA ## 7 h_pred[5] 0.733 1 0.442 0 0 1 1.000 8172. NA ## 8 h_pred[6] 0.737 1 0.441 0 0 1 1.000 7830. NA ## 9 h_pred[7] 0.734 1 0.442 0 0 1 1.000 7956. NA ## 10 h_pred[8] 0.737 1 0.440 0 0 1 1.000 7914. NA ## # ℹ 112 more rows # Check MCMC diagnostics visually mcmc_trace(fit_bernoulli$draws(&quot;theta&quot;)) # Trace plot for theta # Plot the posterior distribution vs. prior draws_df &lt;- as_draws_df(fit_bernoulli$draws(&quot;theta&quot;)) %&gt;% mutate(theta_prior = rbeta(n(), 1, 1)) # Generate prior samples ggplot(draws_df) + geom_density(aes(theta, fill = &quot;Posterior&quot;), alpha = 0.6) + geom_density(aes(theta_prior, fill = &quot;Prior&quot;), alpha = 0.6) + geom_vline(xintercept = 0.8, linetype = &quot;dashed&quot;, color = &quot;black&quot;, size = 1.2) + # True value scale_fill_manual(values = c(&quot;Posterior&quot; = &quot;blue&quot;, &quot;Prior&quot; = &quot;red&quot;)) + labs( title = &quot;Prior-Posterior Update Check (Theta)&quot;, subtitle = &quot;Data shifts belief from uniform prior towards true value (0.8)&quot;, x = &quot;Theta (Probability of Choosing Right)&quot;, y = &quot;Density&quot;, fill = &quot;Distribution&quot; ) + theme_minimal() 5.4.2 Assessing model quality Then we need to look more in the details at the quality of the estimation: * the markov chains * how the prior and the posterior estimates relate to each other (whether the prior is constraining the posterior estimate) # Check if samples_biased exists if (!exists(&quot;samples_biased&quot;)) { model_file &lt;- here(&quot;simmodels&quot;,&quot;04_SimpleBernoulli.rds&quot;) if (file.exists(model_file)) { cat(&quot;Loading biased model samples...\\n&quot;) samples_biased &lt;- readRDS(model_file) cat(&quot;Available parameters:&quot;, paste(colnames(as_draws_df(samples_biased$draws())), collapse = &quot;, &quot;), &quot;\\n&quot;) } else { cat(&quot;Model file not found. Set regenerate_simulations=TRUE to create it.\\n&quot;) # Provide dummy data or skip the remaining code knitr::knit_exit() } } # Extract posterior samples and include sampling of the prior: draws_df_biased &lt;- as_draws_df(samples_biased$draws()) # Explicitly extract parameters theta_param &lt;- draws_df_biased$theta cat(&quot;Successfully extracted theta parameter with&quot;, length(theta_param), &quot;values\\n&quot;) ## Successfully extracted theta parameter with 8000 values # Checking the model&#39;s chains ggplot(draws_df_biased, aes(.iteration, theta, group = .chain, color = .chain)) + geom_line() + theme_classic() # add a prior for theta (ugly, but we&#39;ll do better soon) draws_df_biased &lt;- draws_df_biased %&gt;% mutate( theta_prior = rbeta(nrow(draws_df_biased), 1, 1) ) # Now let&#39;s plot the density for theta (prior and posterior) ggplot(draws_df_biased) + geom_density(aes(theta), fill = &quot;blue&quot;, alpha = 0.3) + geom_density(aes(theta_prior), fill = &quot;red&quot;, alpha = 0.3) + geom_vline(xintercept = 0.8, linetype = &quot;dashed&quot;, color = &quot;black&quot;, linewidth = 1.5) + xlab(&quot;Rate&quot;) + ylab(&quot;Posterior Density&quot;) + theme_classic() As we can see from the posterior estimates and the prior posterior update check, our model is doing a decent job. It doesn’t exactly reconstruct the rate of 0.8, but 0.755 is pretty close and 0.8 is included within the credible interval. Now we build the same model, but using the log odds scale for the theta parameter, which will become useful later when we condition theta on variables and build multilevel models (as we can do what we want in a log odds space and it will always be bound between 0 and 1). Alternative Parameterization: Log-Odds Scale While estimating theta directly as a probability (0-1) is intuitive, it can sometimes be computationally advantageous or necessary for more complex models (like multilevel models in a later chapter) to estimate parameters on an unbounded scale. The logit (log-odds) transformation converts a probability \\(p\\) (from 0 to 1) to log-odds (from \\(-\\infty\\) to \\(+\\infty\\)): \\[\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right)\\] The inverse transformation (logistic function or inv_logit) converts log-odds back to probability: \\[\\text{inv\\_logit}(x) = \\frac{1}{1 + \\exp(-x)}\\] We can rewrite our Stan model to estimate theta on the log-odds scale. However, we must be careful with our priors, as the non-linear transformation distorts the probability mass. Too Narrow: A standard normal prior \\(\\lambda \\sim \\text{Normal}(0, 1)\\) concentrates mass around \\(p=0.5\\), biasing the model toward chance. Too Wide: A wide prior (e.g., \\(\\lambda \\sim \\text{Normal}(0, 5)\\)) pushes mass to the extremes (0 and 1), implying we expect deterministic behavior. Just Right: To approximate a uniform distribution on the probability scale (like our Beta(1,1)), we typically use \\(\\lambda \\sim \\text{Normal}(0, 1.5)\\). This is the specific “sweet spot” that is essentially flat, avoiding both the center-bias of narrow priors and the extreme-bias of wide priors. # Stan code using logit parameterization stan_model_logit_code &lt;- &quot; // Stan Model: Simple Bernoulli (Logit Parameterization) // Estimate theta on the log-odds scale data { int&lt;lower=1&gt; n; array[n] int&lt;lower=0, upper=1&gt; h; } parameters { real theta_logit; // Parameter is now on the unbounded log-odds scale } model { // Prior on log-odds scale (e.g., Normal(0, 1.5)) // Normal(0, 1.5) on log-odds corresponds roughly to a diffuse prior on probability scale target += normal_lpdf(theta_logit | 0, 1.5); // Likelihood using the logit version of the Bernoulli PMF // This tells Stan that theta_logit is on the log-odds scale target += bernoulli_logit_lpmf(h | theta_logit); } generated quantities { // Convert estimate back to probability scale for easier interpretation real&lt;lower=0, upper=1&gt; theta = inv_logit(theta_logit); // Also generate prior sample on probability scale for comparison real&lt;lower=0, upper=1&gt; theta_prior = inv_logit(normal_rng(0, 1)); // Predictions can still be generated using the probability scale theta array[n] int h_pred = bernoulli_rng(rep_vector(theta, n)); } &quot; # Define file path stan_file_logit &lt;- here(stan_model_dir, &quot;04_SimpleBernoulli_logodds.stan&quot;) # Write the Stan code to the file writeLines(stan_model_logit_code, stan_file_logit) cat(&quot;Stan model (logit) written to:&quot;, stan_file_logit, &quot;\\n&quot;) ## With the logit format ## Specify where the model is file &lt;- here(&quot;stan&quot;, &quot;04_SimpleBernoulli_logodds.stan&quot;) # File path for saved model model_file &lt;- here(&quot;simmodels&quot;,&quot;04_SimpleBernoulli_logodds.rds&quot;) # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(model_file)) { # Compile the model mod &lt;- cmdstan_model(file, cpp_options = list(stan_threads = FALSE), stanc_options = list(&quot;O1&quot;)) # The following command calls Stan with specific options. samples_biased_logodds &lt;- mod$sample( data = data_for_stan, seed = 123, chains = 2, parallel_chains = 2, #threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 2000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99, ) # Save the fitted model samples_biased_logodds$save_object(file = model_file) cat(&quot;Generated new model fit and saved to&quot;, model_file, &quot;\\n&quot;) } else { # Load existing results cat(&quot;Loading biased model (log-odds) samples...\\n&quot;) samples_biased_logodds &lt;- readRDS(model_file) cat(&quot;Available parameters:&quot;, paste(colnames(as_draws_df(samples_biased_logodds$draws())), collapse = &quot;, &quot;), &quot;\\n&quot;) cat(&quot;Loaded existing model fit from&quot;, model_file, &quot;\\n&quot;) } 5.4.3 Summarizing the results if (!exists(&quot;samples_biased_logodds&quot;)) { cat(&quot;Loading biased model (log-odds) samples...\\n&quot;) samples_biased_logodds &lt;- readRDS(here(&quot;simmodels&quot;,&quot;04_SimpleBernoulli_logodds.rds&quot;)) cat(&quot;Available parameters:&quot;, paste(colnames(as_draws_df(samples_biased_logodds$draws())), collapse = &quot;, &quot;), &quot;\\n&quot;) } # Extract posterior samples and include sampling of the prior: draws_df_biased_logodds &lt;- as_draws_df(samples_biased_logodds$draws()) # Explicitly extract theta parameter theta_param_logodds &lt;- draws_df_biased_logodds$theta cat(&quot;Successfully extracted theta parameter with&quot;, length(theta_param_logodds), &quot;values\\n&quot;) ## Successfully extracted theta parameter with 4000 values ggplot(draws_df_biased_logodds, aes(.iteration, theta, group = .chain, color = .chain)) + geom_line() + theme_classic() # add a prior for theta (ugly, but we&#39;ll do better soon) draws_df_biased_logodds &lt;- draws_df_biased_logodds %&gt;% mutate( theta_prior = rnorm(nrow(draws_df_biased_logodds), 0, 1) ) # Now let&#39;s plot the density for theta (prior and posterior) ggplot(draws_df_biased_logodds) + geom_density(aes(theta), fill = &quot;blue&quot;, alpha = 0.3) + geom_density(aes(theta_prior), fill = &quot;red&quot;, alpha = 0.3) + geom_vline(xintercept = 1.38, linetype = &quot;dashed&quot;, color = &quot;black&quot;, size = 1.5) + xlab(&quot;Rate&quot;) + ylab(&quot;Posterior Density&quot;) + theme_classic() # Summary samples_biased_logodds$summary() ## # A tibble: 124 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lp__ -70.5 -70.2 0.765 0.301 -72.0 -70.0 1.00 1346. 1038. ## 2 theta_logit 1.01 1.01 0.205 0.201 0.677 1.35 1.00 848. 987. ## 3 theta 0.731 0.733 0.0398 0.0392 0.663 0.794 1.00 848. 987. ## 4 theta_prior 0.500 0.500 0.209 0.239 0.161 0.831 1.00 4067. 3949. ## 5 h_pred[1] 0.740 1 0.439 0 0 1 1.000 3838. NA ## 6 h_pred[2] 0.725 1 0.446 0 0 1 1.00 3857. NA ## 7 h_pred[3] 0.73 1 0.444 0 0 1 1.000 3713. NA ## 8 h_pred[4] 0.743 1 0.437 0 0 1 1.00 3543. NA ## 9 h_pred[5] 0.733 1 0.443 0 0 1 1.000 3592. NA ## 10 h_pred[6] 0.746 1 0.435 0 0 1 1.000 3579. NA ## # ℹ 114 more rows We can see that the results are very similar. 5.5 Validating the Model: Parameter Recovery Before using a model on real data, it’s crucial to verify that it can accurately recover known parameters from simulated data. This process, called parameter recovery, involves: 1. Simulating datasets with known ground truth parameter values (as we did earlier). 2. Fitting the model to these simulated datasets. 3. Comparing the model’s estimated parameters to the true values used in simulation. If the model consistently recovers the true parameters across different simulation conditions (e.g., different true rates, different noise levels), we gain confidence in its validity. Setting up Parallel Recovery Parameter recovery involves fitting the model many times. To speed this up, we use parallel processing with the future and furrr packages. First we need to define the function that will define the operations to be run on each core separately, here we simulate the data according to a seed, a n of trials, a rate and a noise, and then we fit the model to them. Second, we need to create a tibble of the seeds, n of trials, rate and noise values that should be simulated. Third, we use future_pmap_dfr to run the function on each row of the tibble above separately on a different core. Note that I set the system to split across 4 parallel cores (to work on my computer without clogging it). Do change it according to the system you are using. Note that if you have 40 “jobs” (rows of the tibble, sets of parameter values to run), using e.g. 32 cores will not substantially speed things more than using 20. # --- Parameter Recovery Setup --- # Define paths using here() stan_file_logit &lt;- here(&quot;stan&quot;, &quot;04_SimpleBernoulli_logodds.stan&quot;) exe_dir_logit &lt;- here(&quot;simmodels&quot;) # Persistent folder for the executable # Ensure directories exist if (!dir.exists(here(&quot;stan&quot;))) dir.create(here(&quot;stan&quot;)) if (!dir.exists(exe_dir_logit)) dir.create(exe_dir_logit) # compile to persisting directory all parallel workers can acces mod_logit &lt;- cmdstan_model( stan_file_logit, dir = exe_dir_logit, cpp_options = list(stan_threads = FALSE), stanc_options = list(&quot;O1&quot;) ) # Define relative path for recovery results recovery_file &lt;- here(sim_data_dir, &quot;04_recovery_logit.csv&quot;) # Function to simulate data AND fit the model for one condition # This function will be run in parallel for different parameter combinations. sim_and_fit &lt;- function(seed, n_trials, true_rate, true_noise, stan_model) { # 1. Simulate data for this specific condition set.seed(seed) # Ensure reproducibility for this specific run # We need to generate choices based on rate (probability) and noise rate_logit = qlogis(true_rate) # Convert true rate to logit for the agent function if needed sim_choices &lt;- map_int(1:n_trials, ~RandomAgentNoise_f(rate_logit, true_noise)) # 2. Prepare data for Stan stan_data &lt;- list(n = n_trials, h = sim_choices) # 3. Fit the Stan model # Use the pre-compiled model object passed as an argument fit &lt;- stan_model$sample( data = stan_data, seed = seed + 1000, # Separate seed for fitting chains = 1, # Use 1 chain for speed in recovery study (can use more) threads_per_chain = 1, iter_warmup = 500, # Fewer iterations for speed iter_sampling = 1000, refresh = 0, # Suppress verbose output adapt_delta = 0.9 # Maintain reasonable adaptation ) # 4. Extract results (Mean and 95% Credible Interval) # We ask for the mean and the 2.5% and 97.5% quantiles summ &lt;- fit$summary(&quot;theta&quot;, &quot;mean&quot;, ~quantile(.x, probs = c(0.025, 0.975))) est_mean &lt;- summ$mean est_lower &lt;- summ$`2.5%` est_upper &lt;- summ$`97.5%` # 5. Return tibble with true vs. estimated values AND coverage check tibble( true_rate = true_rate, true_noise = true_noise, estimated_rate = est_mean, lower_ci = est_lower, upper_ci = est_upper, # Check if the true value falls within the 95% CI captured = (true_rate &gt;= est_lower &amp; true_rate &lt;= est_upper), seed = seed, n_trials = n_trials ) } # --- Define Recovery Grid --- # Set up combinations of parameters to test recovery_grid &lt;- expand_grid( true_rate = seq(0.1, 0.9, 0.2), # Test various true bias rates true_noise = seq(0, 0.4, 0.2), # Test various noise levels n_trials = c(30, 120), # Test effect of sample size seed = 1:10 # Run multiple simulations per condition ) # --- Run Recovery in Parallel --- # Set up parallel plan (use slightly fewer cores than available) plan(multisession, workers = availableCores() - 2) if (regenerate_simulations || !file.exists(recovery_file)) { cat(&quot;Running parameter recovery simulation...\\n&quot;) # Use future_pmap_dfr to run sim_and_fit for each row in recovery_grid recovery_results &lt;- future_pmap_dfr( list(seed = recovery_grid$seed, n_trials = recovery_grid$n_trials, true_rate = recovery_grid$true_rate, true_noise = recovery_grid$true_noise), sim_and_fit, # The function to run stan_model = mod_logit, # Pass the compiled model object .options = furrr_options(seed = TRUE), # Ensure reproducibility across cores .progress = TRUE # Show progress bar ) # Save results write_csv(recovery_results, recovery_file) cat(&quot;Parameter recovery finished and results saved to:&quot;, recovery_file, &quot;\\n&quot;) } else { # Load existing results recovery_results &lt;- read_csv(recovery_file) cat(&quot;Loaded existing parameter recovery results from:&quot;, recovery_file, &quot;\\n&quot;) } ## Loaded existing parameter recovery results from: /Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/simdata/04_recovery_logit.csv # --- Check Coverage Probability --- coverage_summary &lt;- recovery_results %&gt;% group_by(n_trials, true_noise) %&gt;% summarize(coverage = mean(captured)) print(&quot;Parameter Recovery Coverage (Target = 0.95):&quot;) ## [1] &quot;Parameter Recovery Coverage (Target = 0.95):&quot; print(coverage_summary) ## # A tibble: 6 × 3 ## # Groups: n_trials [2] ## n_trials true_noise coverage ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 30 0 0.96 ## 2 30 0.2 0.8 ## 3 30 0.4 0.58 ## 4 120 0 1 ## 5 120 0.2 0.72 ## 6 120 0.4 0.38 # --- Visualize Recovery Results --- ggplot(recovery_results, aes(x = true_rate, y = estimated_rate)) + # geom_jitter shows individual simulation results, alpha for density geom_jitter(aes(color = factor(n_trials)), alpha = 0.3, width = 0.02) + # geom_smooth shows the average trend geom_smooth(method = &quot;loess&quot;, aes(linetype = factor(n_trials)), color = &quot;black&quot;, se = FALSE) + # Add y=x line for perfect recovery reference geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;, color = &quot;red&quot;) + # Facet by noise level and number of trials facet_grid(paste(&quot;Noise =&quot;, true_noise) ~ paste(&quot;N Trials =&quot;, n_trials)) + scale_color_viridis_d(end = 0.8) + labs( title = &quot;Parameter Recovery: Estimated vs. True Bias Rate&quot;, subtitle = &quot;Red dashed line = perfect recovery. Points = individual simulations.&quot;, x = &quot;True Bias Rate (theta)&quot;, y = &quot;Estimated Bias Rate (Posterior Mean)&quot;, color = &quot;N Trials&quot;, linetype = &quot;N Trials&quot; ) + coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) + # Ensure axes are comparable theme_bw() + theme(legend.position = &quot;bottom&quot;) There’s much to be said about the final plot, but for now let’s just say that it looks good. We can reconstruct in a nice ordered way true rate values. However, our ability to do so decreases with the increase in noise. So far no surprises. Wait, you say, shouldn’t we actually model the generative process, that is, include noise in the Stan model? Gold star, there! But let’s wait a bit before we get there, we’ll need mixture models. 5.6 Moving Beyond Simple Bias: Memory Models The simple biased agent model assumes choices are independent over time, influenced only by a fixed theta. However, behavior is often history-dependent. Let’s explore models where the choice probability theta on trial t depends on previous events. 5.6.1 Memory Model 1: GLM-like Approach (External Predictor) One way to incorporate memory is to treat a summary of past events as an external predictor influencing the current choice probability, similar to a predictor in a Generalized Linear Model (GLM). Let’s assume the choice probability theta depends on the cumulative rate of the opponent’s ‘right’ choices observed up to the previous trial.To make the variable more intuitive we code previous rate - which is bound to a probability 0-1 space - into log-odds via a logit link/transformation. In this way a previous rate with more left than right choices will result in a negative value, thereby decreasing our propensity to choose right; and one with more right than left choices will result in a positive value, thereby increasing our propensity to choose right. # We subset to only include no noise and a specific rate d1 &lt;- d %&gt;% subset(true_noise == 0 &amp; true_rate == 0.8) %&gt;% rename(Other = choice) %&gt;% mutate(cumulativerate = lag(cumulative_rate, 1)) d1$cumulativerate[1] &lt;- 0.5 # no prior info at first trial d1$cumulativerate[d1$cumulativerate == 0] &lt;- 0.01 d1$cumulativerate[d1$cumulativerate == 1] &lt;- 0.99 # Now we create the memory agent with a coefficient of 1 (in log odds) MemoryAgent_f &lt;- function(bias, beta, cumulativerate){ choice = rbinom(1, 1, inv_logit_scaled(bias + beta * logit_scaled(cumulativerate))) return(choice) } d1$Self[1] &lt;- RandomAgentNoise_f(0.5, 0) for (i in 2:trials) { d1$Self[i] &lt;- MemoryAgent_f(bias = 0, beta = 1, d1$cumulativerate[i]) } ## Create the data data_memory &lt;- list( n = 120, h = d1$Self, memory = d1$cumulativerate # this creates the new parameter: the rate of right hands so far in log-odds ) stan_model &lt;- &quot; // The input (data) for the model. n of trials and h for (right and left) hand data { int&lt;lower=1&gt; n; array[n] int h; vector[n] memory; // here we add the new variable between 0.01 and .99 } // The parameters accepted by the model. parameters { real bias; // how likely is the agent to pick right when the previous rate has no information (50-50)? real beta; // how strongly is previous rate impacting the decision? } // The model to be estimated. model { // priors target += normal_lpdf(bias | 0, .3); target += normal_lpdf(beta | 0, .5); // model target += bernoulli_logit_lpmf(h | bias + beta * logit(memory)); } &quot; write_stan_file( stan_model, dir = &quot;stan/&quot;, basename = &quot;04_MemoryBernoulli.stan&quot;) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/04_MemoryBernoulli.stan&quot; ## Specify where the model is file &lt;- here(&quot;stan&quot;,&quot;04_MemoryBernoulli.stan&quot;) # File path for saved model model_file_memory &lt;- here(&quot;simmodels&quot;, &quot;04_MemoryBernoulli.rds&quot;) # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(model_file_memory)) { # Compile the model mod_memory &lt;- cmdstan_model(file, cpp_options = list(stan_threads = FALSE), stanc_options = list(&quot;O1&quot;)) # The following command calls Stan with specific options. samples_memory &lt;- mod_memory$sample( data = data_memory, seed = 123, chains = 2, parallel_chains = 2, threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 1000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99, ) # Save the fitted model samples_memory$save_object(file = model_file_memory) cat(&quot;Generated new model fit and saved to&quot;, model_file_memory, &quot;\\n&quot;) } else { # Load existing results cat(&quot;Loading memory model samples...\\n&quot;) samples_memory &lt;- readRDS(model_file_memory) cat(&quot;Available parameters:&quot;, paste(colnames(as_draws_df(samples_memory$draws())), collapse=&quot;, &quot;), &quot;\\n&quot;) cat(&quot;Loaded existing model fit from&quot;, model_file_memory, &quot;\\n&quot;) } ## Loading memory model samples... ## Available parameters: lp__, bias, beta, .chain, .iteration, .draw ## Loaded existing model fit from /Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/simmodels/04_MemoryBernoulli.rds 5.6.2 Summarizing the results # Check if samples_memory exists if (!exists(&quot;samples_memory&quot;)) { cat(&quot;Loading memory model samples...\\n&quot;) samples_memory &lt;- readRDS(here(&quot;simmodels&quot;, &quot;04_MemoryBernoulli.rds&quot;)) cat(&quot;Available parameters:&quot;, paste(colnames(as_draws_df(samples_memory$draws())), collapse = &quot;, &quot;), &quot;\\n&quot;) } # Extract posterior samples and include sampling of the prior: draws_df_memory &lt;- as_draws_df(samples_memory$draws()) # Explicitly extract parameters bias_param &lt;- draws_df_memory$bias beta_param &lt;- draws_df_memory$beta cat(&quot;Successfully extracted&quot;, length(bias_param), &quot;values for bias parameter\\n&quot;) ## Successfully extracted 2000 values for bias parameter cat(&quot;Successfully extracted&quot;, length(beta_param), &quot;values for beta parameter\\n&quot;) ## Successfully extracted 2000 values for beta parameter # Trace plot for bias ggplot(draws_df_memory, aes(.iteration, bias, group = .chain, color = .chain)) + geom_line() + labs(title = &quot;Trace plot for bias parameter&quot;) + theme_classic() # Trace plot for beta ggplot(draws_df_memory, aes(.iteration, beta, group = .chain, color = .chain)) + geom_line() + labs(title = &quot;Trace plot for beta parameter&quot;) + theme_classic() # add prior distributions draws_df_memory &lt;- draws_df_memory %&gt;% mutate( bias_prior = rnorm(nrow(draws_df_memory), 0, .3), beta_prior = rnorm(nrow(draws_df_memory), 0, .5) ) # Now let&#39;s plot the density for bias (prior and posterior) ggplot(draws_df_memory) + geom_density(aes(bias), fill = &quot;blue&quot;, alpha = 0.3) + geom_density(aes(bias_prior), fill = &quot;red&quot;, alpha = 0.3) + geom_vline(xintercept = 0, linetype = &quot;dashed&quot;, color = &quot;black&quot;, size = 1.5) + labs(title = &quot;Prior-Posterior Update for Bias Parameter&quot;, subtitle = &quot;Blue: posterior, Red: prior, Dashed: true value&quot;) + xlab(&quot;Bias&quot;) + ylab(&quot;Posterior Density&quot;) + theme_classic() # Now let&#39;s plot the density for beta (prior and posterior) ggplot(draws_df_memory) + geom_density(aes(beta), fill = &quot;blue&quot;, alpha = 0.3) + geom_density(aes(beta_prior), fill = &quot;red&quot;, alpha = 0.3) + geom_vline(xintercept = 1, linetype = &quot;dashed&quot;, color = &quot;black&quot;, size = 1.5) + labs(title = &quot;Prior-Posterior Update for Beta Parameter&quot;, subtitle = &quot;Blue: posterior, Red: prior, Dashed: true value&quot;) + xlab(&quot;Beta&quot;) + ylab(&quot;Posterior Density&quot;) + theme_classic() # Print summary samples_memory$summary() ## # A tibble: 3 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lp__ -62.8 -62.5 0.900 0.639 -64.5 -61.9 1.01 646. 1034. ## 2 bias 0.0587 0.0597 0.234 0.231 -0.325 0.456 1.00 658. 728. ## 3 beta 0.869 0.869 0.215 0.217 0.506 1.22 1.00 678. 875. We can see that the model has now estimated both the bias and the role of previous memory. Bias should reflect the bias in the setup (0.5 which in log odds is 0), and the beta coefficient for memory (roughly 1). More on the quality checks of the models in the next chapter. 5.6.3 Memory Model 2: Internal State Variable Instead of feeding memory as external data, we can model it as an internal state that updates within the model. For this specific model—where memory is defined simply as the running average of the opponent’s choices—the update rule depends only on the observed data (other), and not on any parameters we are trying to estimate (like bias or beta). Therefore, we should calculate this in Stan’s transformed data block. In my first version of these notes, I had rather used the transformed parameters to calculate memory. This led me to better understand how these blocks work: code in transformed data runs only once before sampling begins. code in transformed parameters is calculated at every single step of the MCMC chain. In other words, if the variable we need to calculate is dependent on data only, calculating it in transformed data is more efficient. If it depends on parameters that are being estimated, then we need to calculate it in transformed parameters. ## Create the data data &lt;- list( n = 120, h = d1$Self, other = d1$Other ) stan_model &lt;- &quot; // Memory-based choice model with prior and posterior predictions data { int&lt;lower=1&gt; n; array[n] int h; array[n] int other; } transformed data { // We calculated memory here because it depends ONLY on observed data (&#39;other&#39;). // In transformed data it&#39;s calculated once, in transformed parameters it&#39;d be calculated at every iteration, which is unnecessary and computationally costly. vector[n] memory; memory[1] = 0.5; for (trial in 1:(n-1)) { memory[trial + 1] = memory[trial] + ((other[trial] - memory[trial]) / (trial + 1)); // Numerical stability clips if (memory[trial + 1] &lt; 0.01) { memory[trial + 1] = 0.01; } if (memory[trial + 1] &gt; 0.99) { memory[trial + 1] = 0.99; } } } parameters { real bias; real beta; } model { // Priors target += normal_lpdf(bias | 0, .3); target += normal_lpdf(beta | 0, .5); // Likelihood // A trial by trial version would be // for (trial in 1:n) { // target += bernoulli_logit_lpmf(h[trial] | bias + beta * logit(memory[trial])); // } // However, we vectorize the likelihood for speed. target += bernoulli_logit_lpmf(h | bias + beta * logit(memory)); } generated quantities { // Generate prior samples real bias_prior = normal_rng(0, .3); real beta_prior = normal_rng(0, .5); // Variables for predictions array[n] int prior_preds; array[n] int posterior_preds; vector[n] memory_prior; vector[n] log_lik; // Generate predictions at different memory levels array[3] real memory_levels = {0.2, 0.5, 0.8}; // Low, neutral, and high memory array[3] int prior_preds_memory; array[3] int posterior_preds_memory; // Generate predictions from prior for each memory level for (i in 1:3) { real logit_memory = logit(memory_levels[i]); prior_preds_memory[i] = bernoulli_logit_rng(bias_prior + beta_prior * logit_memory); posterior_preds_memory[i] = bernoulli_logit_rng(bias + beta * logit_memory); } // Generate predictions from prior memory_prior[1] = 0.5; for (trial in 1:n) { if (trial == 1) { prior_preds[trial] = bernoulli_logit_rng(bias_prior + beta_prior * logit(memory_prior[trial])); } else { memory_prior[trial] = memory_prior[trial-1] + ((other[trial-1] - memory_prior[trial-1]) / trial); if (memory_prior[trial] == 0) { memory_prior[trial] = 0.01; } if (memory_prior[trial] == 1) { memory_prior[trial] = 0.99; } prior_preds[trial] = bernoulli_logit_rng(bias_prior + beta_prior * logit(memory_prior[trial])); } } // Generate predictions from posterior for (trial in 1:n) { posterior_preds[trial] = bernoulli_logit_rng(bias + beta * logit(memory[trial])); log_lik[trial] = bernoulli_logit_lpmf(h[trial] | bias + beta * logit(memory[trial])); } } &quot; write_stan_file( stan_model, dir = &quot;stan/&quot;, basename = &quot;04_InternalMemory.stan&quot;) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/04_InternalMemory.stan&quot; ## Specify where the model is file &lt;- here(&quot;stan&quot;, &quot;04_InternalMemory.stan&quot;) # File path for saved model model_file &lt;- here(&quot;simmodels&quot;, &quot;04_InternalMemory.rds&quot;) # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(model_file)) { # Compile the model mod &lt;- cmdstan_model(file, cpp_options = list(stan_threads = FALSE), stanc_options = list(&quot;O1&quot;)) # The following command calls Stan with specific options. samples_memory_internal &lt;- mod$sample( data = data, seed = 123, chains = 1, parallel_chains = 2, threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 1000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99, ) # Save the fitted model samples_memory_internal$save_object(file = model_file) cat(&quot;Generated new model fit and saved to&quot;, model_file, &quot;\\n&quot;) } else { # Load existing results samples_memory_internal &lt;- readRDS(model_file) cat(&quot;Loaded existing model fit from&quot;, model_file, &quot;\\n&quot;) } ## Loaded existing model fit from /Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/simmodels/04_InternalMemory.rds draws_df &lt;- as_draws_df(samples_memory_internal$draws()) # 1. Check chain convergence # Plot traces for main parameters mcmc_trace(draws_df, pars = c(&quot;bias&quot;, &quot;beta&quot;)) + theme_minimal() + ggtitle(&quot;Parameter Traces Across Chains&quot;) # Plot rank histograms to check mixing mcmc_rank_hist(draws_df, pars = c(&quot;bias&quot;, &quot;beta&quot;)) # 2. Prior-Posterior Update Check p1 &lt;- ggplot() + geom_density(data = draws_df, aes(bias, fill = &quot;Posterior&quot;), alpha = 0.5) + geom_density(data = draws_df, aes(bias_prior, fill = &quot;Prior&quot;), alpha = 0.5) + geom_vline(xintercept = 0, linetype = &quot;dashed&quot;) + scale_fill_manual(values = c(&quot;Prior&quot; = &quot;red&quot;, &quot;Posterior&quot; = &quot;blue&quot;)) + theme_minimal() + ggtitle(&quot;Prior-Posterior Update: Bias Parameter&quot;) p2 &lt;- ggplot() + geom_density(data = draws_df, aes(beta, fill = &quot;Posterior&quot;), alpha = 0.5) + geom_density(data = draws_df, aes(beta_prior, fill = &quot;Prior&quot;), alpha = 0.5) + geom_vline(xintercept = 1, linetype = &quot;dashed&quot;) + scale_fill_manual(values = c(&quot;Prior&quot; = &quot;red&quot;, &quot;Posterior&quot; = &quot;blue&quot;)) + theme_minimal() + ggtitle(&quot;Prior-Posterior Update: Beta Parameter&quot;) p3 &lt;- ggplot() + geom_point(data = draws_df, aes(bias, beta), alpha = 0.5) + theme_minimal() + ggtitle(&quot;Correlation&quot;) p1 + p2 + p3 # First let&#39;s properly extract and organize our posterior predictions posterior_predictions &lt;- draws_df %&gt;% dplyr::select(starts_with(&quot;posterior_preds[&quot;)) %&gt;% # Select all posterior prediction columns pivot_longer(everything(), names_to = &quot;trial&quot;, values_to = &quot;prediction&quot;) %&gt;% # Clean up the trial number from the Stan array notation mutate(trial = as.numeric(str_extract(trial, &quot;\\\\d+&quot;))) # Calculate summary statistics for posterior predictions posterior_summary &lt;- posterior_predictions %&gt;% group_by(trial) %&gt;% summarise( mean = mean(prediction), lower = quantile(prediction, 0.025), upper = quantile(prediction, 0.975) ) # Do the same for prior predictions prior_predictions &lt;- draws_df %&gt;% dplyr::select(starts_with(&quot;prior_preds[&quot;)) %&gt;% pivot_longer(everything(), names_to = &quot;trial&quot;, values_to = &quot;prediction&quot;) %&gt;% mutate(trial = as.numeric(str_extract(trial, &quot;\\\\d+&quot;))) prior_summary &lt;- prior_predictions %&gt;% group_by(trial) %&gt;% summarise( mean = mean(prediction), lower = quantile(prediction, 0.025), upper = quantile(prediction, 0.975) ) # Now let&#39;s create our visualization # First the prior predictive check p4 &lt;- ggplot() + # Add prior prediction interval geom_ribbon(data = prior_summary, aes(x = trial, ymin = lower, ymax = upper), alpha = 0.2, fill = &quot;red&quot;) + # Add mean prior prediction geom_line(data = prior_summary, aes(x = trial, y = mean), color = &quot;red&quot;) + # Add actual data points geom_point(data = tibble(trial = 1:length(data$h), choice = data$h), aes(x = trial, y = choice), alpha = 0.5) + labs(title = &quot;Prior Predictive Check&quot;, x = &quot;Trial&quot;, y = &quot;Choice (0/1)&quot;) + theme_minimal() # Then the posterior predictive check p5 &lt;- ggplot() + # Add posterior prediction interval geom_ribbon(data = posterior_summary, aes(x = trial, ymin = lower, ymax = upper), alpha = 0.2, fill = &quot;blue&quot;) + # Add mean posterior prediction geom_line(data = posterior_summary, aes(x = trial, y = mean), color = &quot;blue&quot;) + # Add actual data points geom_point(data = tibble(trial = 1:length(data$h), choice = data$h), aes(x = trial, y = choice), alpha = 0.5) + labs(title = &quot;Posterior Predictive Check&quot;, x = &quot;Trial&quot;, y = &quot;Choice (0/1)&quot;) + theme_minimal() # Display plots side by side p4 + p5 # First, let&#39;s calculate the total number of 1s predicted in each posterior sample posterior_totals &lt;- draws_df %&gt;% dplyr::select(starts_with(&quot;posterior_preds[&quot;)) %&gt;% # Sum across rows to get total 1s per sample mutate(total_ones = rowSums(.)) # Do the same for prior predictions prior_totals &lt;- draws_df %&gt;% dplyr::select(starts_with(&quot;prior_preds[&quot;)) %&gt;% mutate(total_ones = rowSums(.)) # Calculate actual number of 1s in the data actual_ones &lt;- sum(data$h) # Create visualization comparing distributions ggplot() + # Prior predictive distribution geom_histogram(data = prior_totals, aes(x = total_ones, fill = &quot;Prior&quot;), alpha = 0.3) + # Posterior predictive distribution geom_histogram(data = posterior_totals, aes(x = total_ones, fill = &quot;Posterior&quot;), alpha = 0.3) + # Vertical line for actual data geom_vline(xintercept = actual_ones, linetype = &quot;dashed&quot;, color = &quot;black&quot;, size = 1) + # Aesthetics scale_fill_manual(values = c(&quot;Prior&quot; = &quot;red&quot;, &quot;Posterior&quot; = &quot;blue&quot;), name = &quot;Distribution&quot;) + labs(title = &quot;Distribution of Predicted Successes (1s) out of 120 Trials&quot;, subtitle = &quot;Comparing Prior, Posterior and Actual Data&quot;, x = &quot;Number of 1s&quot;, y = &quot;Density&quot;) + theme_minimal() + # Add annotation for actual value annotate(&quot;text&quot;, x = actual_ones, y = 0, label = paste(&quot;Actual:&quot;, actual_ones), vjust = -0.5) # Let&#39;s also print summary statistics prior_summary &lt;- prior_totals %&gt;% summarise( mean = mean(total_ones), sd = sd(total_ones), q025 = quantile(total_ones, 0.025), q975 = quantile(total_ones, 0.975) ) posterior_summary &lt;- posterior_totals %&gt;% summarise( mean = mean(total_ones), sd = sd(total_ones), q025 = quantile(total_ones, 0.025), q975 = quantile(total_ones, 0.975) ) print(&quot;Prior predictive summary:&quot;) ## [1] &quot;Prior predictive summary:&quot; print(prior_summary) ## # A tibble: 1 × 4 ## mean sd q025 q975 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 59.1 19.2 22 96 print(&quot;Posterior predictive summary:&quot;) ## [1] &quot;Posterior predictive summary:&quot; print(posterior_summary) ## # A tibble: 1 × 4 ## mean sd q025 q975 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 91.3 6.19 79 103 # First let&#39;s calculate predicted probabilities for each draw and memory level predicted_probs &lt;- draws_df %&gt;% mutate( # Calculate probability of choosing right for each memory level # using the logistic function on our parameter estimates prob_low = inv_logit_scaled(bias + beta * logit_scaled(0.2)), prob_mid = inv_logit_scaled(bias + beta * logit_scaled(0.5)), prob_high = inv_logit_scaled(bias + beta * logit_scaled(0.8)) ) %&gt;% # Reshape to long format for easier plotting pivot_longer( cols = starts_with(&quot;prob_&quot;), names_to = &quot;memory_level&quot;, values_to = &quot;probability&quot; ) %&gt;% mutate( memory_value = case_when( memory_level == &quot;prob_low&quot; ~ 0.2, memory_level == &quot;prob_mid&quot; ~ 0.5, memory_level == &quot;prob_high&quot; ~ 0.8 ) ) # Do the same for prior predictions prior_probs &lt;- draws_df %&gt;% mutate( prob_low = inv_logit_scaled(bias_prior + beta_prior * logit_scaled(0.2)), prob_mid = inv_logit_scaled(bias_prior + beta_prior * logit_scaled(0.5)), prob_high = inv_logit_scaled(bias_prior + beta_prior * logit_scaled(0.8)) ) %&gt;% pivot_longer( cols = starts_with(&quot;prob_&quot;), names_to = &quot;memory_level&quot;, values_to = &quot;probability&quot; ) %&gt;% mutate( memory_value = case_when( memory_level == &quot;prob_low&quot; ~ 0.2, memory_level == &quot;prob_mid&quot; ~ 0.5, memory_level == &quot;prob_high&quot; ~ 0.8 ) ) # Create visualization with density plots p1 &lt;- ggplot() + # Add prior distributions geom_density(data = prior_probs, aes(x = probability, fill = &quot;Prior&quot;), alpha = 0.3) + # Add posterior distributions geom_density(data = predicted_probs, aes(x = probability, fill = &quot;Posterior&quot;), alpha = 0.3) + # Separate by memory level facet_wrap(~memory_value, labeller = labeller(memory_value = c( &quot;0.2&quot; = &quot;Low Memory (20% Right)&quot;, &quot;0.5&quot; = &quot;Neutral Memory (50% Right)&quot;, &quot;0.8&quot; = &quot;High Memory (80% Right)&quot; ))) + # Aesthetics scale_fill_manual(values = c(&quot;Prior&quot; = &quot;red&quot;, &quot;Posterior&quot; = &quot;blue&quot;), name = &quot;Distribution&quot;) + labs(title = &quot;Distribution of Predicted Probabilities at Different Memory Levels&quot;, x = &quot;Probability of Choosing Right&quot;, y = &quot;Density&quot;) + theme_minimal() # Alternative visualization using violin plots p2 &lt;- ggplot() + # Add prior distributions geom_violin(data = prior_probs, aes(x = factor(memory_value), y = probability, fill = &quot;Prior&quot;), alpha = 0.3, position = position_dodge(width = 0.5)) + # Add posterior distributions geom_violin(data = predicted_probs, aes(x = factor(memory_value), y = probability, fill = &quot;Posterior&quot;), alpha = 0.3, position = position_dodge(width = 0.5)) + # Aesthetics scale_fill_manual(values = c(&quot;Prior&quot; = &quot;red&quot;, &quot;Posterior&quot; = &quot;blue&quot;), name = &quot;Distribution&quot;) + scale_x_discrete(labels = c(&quot;Low\\n(20% Right)&quot;, &quot;Neutral\\n(50% Right)&quot;, &quot;High\\n(80% Right)&quot;)) + labs(title = &quot;Distribution of Predicted Probabilities by Memory Level&quot;, x = &quot;Memory Level&quot;, y = &quot;Probability of Choosing Right&quot;) + theme_minimal() # Display both visualizations p1 / p2 # 4. Check for divergences # Extract divergent transitions n_div &lt;- sum(draws_df$.divergent) print(paste(&quot;Number of divergent transitions:&quot;, n_div)) ## [1] &quot;Number of divergent transitions: 0&quot; Now that we know how to model memory as an internal state, we can play with making the update discount the past, setting a parameter that indicates after how many trials memory is lost, etc. 5.6.4 Memory Model 3: Exponential Forgetting (Relation to RL) A more cognitively plausible memory model incorporates forgetting, where recent events have more influence than distant ones. This can be implemented by adding a forgetting parameter (often called a learning rate, α, in RL) that controls the weight of the most recent outcome versus the previous memory state. stan_model &lt;- &quot; // The input (data) for the model. n of trials and h for (right and left) hand data { int&lt;lower=1&gt; n; array[n] int h; array[n] int other; } // The parameters accepted by the model. parameters { real bias; // how likely is the agent to pick right when the previous rate has no information (50-50)? real beta; // how strongly is previous rate impacting the decision? real&lt;lower=0, upper=1&gt; forgetting; } // The model to be estimated. model { vector[n] memory; // Priors target += beta_lpdf(forgetting | 1, 1); target += normal_lpdf(bias | 0, .3); target += normal_lpdf(beta | 0, .5); // Model, looping to keep track of memory for (trial in 1:n) { if (trial == 1) { memory[trial] = 0.5; } target += bernoulli_logit_lpmf(h[trial] | bias + beta * logit(memory[trial])); if (trial &lt; n){ memory[trial + 1] = (1 - forgetting) * memory[trial] + forgetting * other[trial]; if (memory[trial + 1] == 0){memory[trial + 1] = 0.01;} if (memory[trial + 1] == 1){memory[trial + 1] = 0.99;} } } } &quot; write_stan_file( stan_model, dir = &quot;stan/&quot;, basename = &quot;04_InternalMemory2.stan&quot;) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/04_InternalMemory2.stan&quot; ## Specify where the model is file &lt;- here(&quot;stan&quot;,&quot;04_InternalMemory2.stan&quot;) # File path for saved model model_file &lt;- here(&quot;simmodels&quot;,&quot;04_InternalMemory2.rds&quot;) # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(model_file)) { # Compile the model mod &lt;- cmdstan_model(file, cpp_options = list(stan_threads = FALSE), stanc_options = list(&quot;O1&quot;)) # The following command calls Stan with specific options. samples_memory_forgetting &lt;- mod$sample( data = data, seed = 123, chains = 1, parallel_chains = 2, threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 1000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99, ) # Save the fitted model samples_memory_forgetting$save_object(file = model_file) cat(&quot;Generated new model fit and saved to&quot;, model_file, &quot;\\n&quot;) } else { # Load existing results samples_memory_forgetting &lt;- readRDS(model_file) cat(&quot;Loaded existing model fit from&quot;, model_file, &quot;\\n&quot;) } ## Loaded existing model fit from /Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/simmodels/04_InternalMemory2.rds samples_memory_forgetting$summary() ## # A tibble: 4 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lp__ -68.8 -68.4 1.28 1.00 -71.3 -67.4 0.999 345. 388. ## 2 bias 0.373 0.380 0.225 0.235 0.00137 0.756 1.00 298. 347. ## 3 beta 0.679 0.676 0.286 0.301 0.197 1.15 0.999 253. 340. ## 4 forgetting 0.175 0.151 0.103 0.0673 0.0677 0.356 1.01 325. 282. The memory model we’ve implemented can be seen as part of a broader family of models that track and update beliefs based on incoming evidence. Let’s explore how it relates to some key frameworks. 5.7 Memory Model 4: Bayesian Agent (Optimal Updating) A fully Bayesian agent wouldn’t just track the rate but maintain a full probability distribution (a Beta distribution) over the likely bias of the opponent, updating it optimally according to Bayes’ rule after each observation. stan_model &lt;- &quot; data { int&lt;lower=1&gt; n; // number of trials array[n] int h; // agent&#39;s choices (0 or 1) array[n] int other; // other player&#39;s choices (0 or 1) } parameters { real&lt;lower=0&gt; alpha_prior; // Prior alpha parameter real&lt;lower=0&gt; beta_prior; // Prior beta parameter } transformed parameters { vector[n] alpha; // Alpha parameter at each trial vector[n] beta; // Beta parameter at each trial vector[n] rate; // Expected rate at each trial // Initialize with prior alpha[1] = alpha_prior; beta[1] = beta_prior; rate[1] = alpha[1] / (alpha[1] + beta[1]); // Sequential updating of Beta distribution for(t in 2:n) { // Update Beta parameters based on previous observation alpha[t] = alpha[t-1] + other[t-1]; beta[t] = beta[t-1] + (1 - other[t-1]); // Calculate expected rate rate[t] = alpha[t] / (alpha[t] + beta[t]); } } model { // Priors on hyperparameters target += gamma_lpdf(alpha_prior | 2, 1); target += gamma_lpdf(beta_prior | 2, 1); // Agent&#39;s choices follow current rate estimates for(t in 1:n) { target += bernoulli_lpmf(h[t] | rate[t]); } } generated quantities { array[n] int prior_preds; array[n] int posterior_preds; real initial_rate = alpha_prior / (alpha_prior + beta_prior); // Prior predictions use initial rate for(t in 1:n) { prior_preds[t] = bernoulli_rng(initial_rate); } // Posterior predictions use sequentially updated rates for(t in 1:n) { posterior_preds[t] = bernoulli_rng(rate[t]); } } &quot; write_stan_file( stan_model, dir = &quot;stan/&quot;, basename = &quot;04_BayesianMemory.stan&quot;) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/04_BayesianMemory.stan&quot; ## Specify where the model is file &lt;- here(&quot;stan&quot;,&quot;04_BayesianMemory.stan&quot;) # File path for saved model model_file &lt;- here(&quot;simmodels&quot;,&quot;04_BayesianMemory.rds&quot;) # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(model_file)) { # Compile the model mod &lt;- cmdstan_model(file, cpp_options = list(stan_threads = FALSE), stanc_options = list(&quot;O1&quot;)) # The following command calls Stan with specific options. samples_memory_bayes &lt;- mod$sample( data = data, seed = 123, chains = 1, parallel_chains = 2, threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 1000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99, ) # Save the fitted model samples_memory_bayes$save_object(file = model_file) cat(&quot;Generated new model fit and saved to&quot;, model_file, &quot;\\n&quot;) } else { # Load existing results samples_memory_bayes &lt;- readRDS(model_file) cat(&quot;Loaded existing model fit from&quot;, model_file, &quot;\\n&quot;) } ## Loaded existing model fit from /Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/simmodels/04_BayesianMemory.rds samples_memory_bayes$summary() ## # A tibble: 604 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lp__ -63.8 -63.4 1.05 0.675 -66.1 -62.9 1.00 238. 336. ## 2 alpha_prior 2.71 2.47 1.65 1.40 0.602 5.75 0.999 448. 485. ## 3 beta_prior 0.679 0.572 0.485 0.403 0.134 1.60 1.000 352. 204. ## 4 alpha[1] 2.71 2.47 1.65 1.40 0.602 5.75 0.999 448. 485. ## 5 alpha[2] 3.71 3.47 1.65 1.40 1.60 6.75 0.999 448. 485. ## 6 alpha[3] 4.71 4.47 1.65 1.40 2.60 7.75 0.999 448. 485. ## 7 alpha[4] 5.71 5.47 1.65 1.40 3.60 8.75 0.999 448. 485. ## 8 alpha[5] 6.71 6.47 1.65 1.40 4.60 9.75 0.999 448. 485. ## 9 alpha[6] 7.71 7.47 1.65 1.40 5.60 10.8 0.999 448. 485. ## 10 alpha[7] 8.71 8.47 1.65 1.40 6.60 11.8 0.999 448. 485. ## # ℹ 594 more rows # Extract draws draws_df &lt;- as_draws_df(samples_memory_bayes$draws()) # First let&#39;s look at the priors ggplot(draws_df) + geom_density(aes(alpha_prior), fill = &quot;blue&quot;, alpha = 0.3) + geom_density(aes(beta_prior), fill = &quot;red&quot;, alpha = 0.3) + theme_classic() + labs(title = &quot;Prior Distributions&quot;, x = &quot;Parameter Value&quot;, y = &quot;Density&quot;) # Now let&#39;s look at how the rate evolves over trials # First melt the rate values across trials into long format rate_df &lt;- draws_df %&gt;% dplyr::select(starts_with(&quot;rate[&quot;)) %&gt;% pivot_longer(everything(), names_to = &quot;trial&quot;, values_to = &quot;rate&quot;, names_pattern = &quot;rate\\\\[(\\\\d+)\\\\]&quot;) %&gt;% mutate(trial = as.numeric(trial)) # Calculate summary statistics for each trial rate_summary &lt;- rate_df %&gt;% group_by(trial) %&gt;% summarise( mean_rate = mean(rate), lower = quantile(rate, 0.025), upper = quantile(rate, 0.975) ) plot_data &lt;- tibble(trial = seq(120), choices = data$other) # Plot the evolution of rate estimates ggplot(rate_summary, aes(x = trial)) + geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) + geom_line(aes(y = mean_rate), color = &quot;blue&quot;) + # Add true data points geom_line(data = plot_data, aes(x = trial, y = choices), color = &quot;orange&quot;, alpha = 0.5) + theme_classic() + labs(title = &quot;Evolution of Rate Estimates&quot;, x = &quot;Trial&quot;, y = &quot;Rate&quot;, subtitle = &quot;Blue line: posterior mean, Gray band: 95% CI&quot;) + ylim(0, 1) # Let&#39;s also look at the correlation between alpha and beta parameters ggplot(draws_df) + geom_point(aes(alpha_prior, beta_prior), alpha = 0.1) + theme_classic() + labs(title = &quot;Correlation between Alpha and Beta Parameters&quot;, x = &quot;Alpha&quot;, y = &quot;Beta&quot;) 5.8 Relationship to Rescorla-Wagner The Rescorla-Wagner model of learning—which drives the Reinforcement Learning agents we will build in in a later chapter follows the form: \\[V_{t+1} = V_{t} + \\alpha (R_{t} - V_{t})\\] where: \\(V_{t}\\) is the current Value estimate (in our code: memory[t]). \\(\\alpha\\) is the Learning Rate (in our code: forgetting). \\(R_{t}\\) is the Reward or Outcome (in our code: other[t]). \\((R_{t} - V_{t})\\) is the Prediction Error. Our memory model with a forgetting parameter follows a very similar structure:\\[\\text{memory}_{t+1} = (1 - \\text{forgetting}) \\times \\text{memory}_{t} + \\text{forgetting} \\times \\text{outcome}_{t}\\]If we expand this, we can see it is mathematically identical to the Rescorla-Wagner update rule:\\[\\begin{aligned} \\text{memory}_{t+1} &amp;= \\text{memory}_{t} - \\text{forgetting} \\times \\text{memory}_{t} + \\text{forgetting} \\times \\text{outcome}_{t} \\\\ \\text{memory}_{t+1} &amp;= \\text{memory}_{t} + \\text{forgetting} \\times (\\text{outcome}_{t} - \\text{memory}_{t}) \\end{aligned}\\] This reveals that our “exponential forgetting” memory model is actually a standard Reinforcement Learning agent with a big fake moustache. The forgetting parameter dictates how quickly the agent updates their beliefs (value estimate) based on new prediction errors. 5.8.1 Connection to Kalman Filters Our memory model updates beliefs about the probability of right-hand choices using a weighted average of past observations. This is conceptually similar to how a Kalman filter works, though simpler: Kalman filters maintain both an estimate and uncertainty about that estimate They optimally weight new evidence based on relative uncertainty Our forgetting model uses a fixed weighting scheme (1/trial or the forgetting parameter) The Bayesian Agent (04_BayesianMemory.stan) captures uncertainty via the Beta distribution, similar in spirit but simpler than a Kalman filter. 5.8.2 Connection to Hierarchical Gaussian Filter (HGF) The HGF extends these ideas by: Tracking beliefs at multiple levels Allowing learning rates to vary over time Explicitly modeling environmental volatility Our model could be seen as the simplest case of an HGF where: We only track one level (probability of right-hand choice) Have a fixed learning rate (forgetting parameter) Don’t explicitly model environmental volatility 5.8.3 Implications for Model Development Understanding these relationships helps us think about how models relate to each other and to extend our model: We could add uncertainty estimates to get Kalman-like behavior We could make the forgetting parameter dynamic to capture changing learning rates We could add multiple levels to track both immediate probabilities and longer-term trends Each extension would make the model more flexible but also more complex to fit to data. The choice depends on our specific research questions and available data. 5.9 Conclusion: Estimating Parameters and Exploring Memory This chapter marked a crucial transition from simulating models with known parameters to the core task of parameter estimation: inferring plausible parameter values from observed data using Bayesian inference and Stan. We learned how to: * Specify Bayesian models in Stan, defining data, parameters, priors, and likelihoods. * Fit models using cmdstanr to obtain posterior distributions for parameters like choice bias (theta). * Utilize transformations like log-odds for computational benefits and flexible prior specification. * Validate our fitting procedure through parameter recovery, ensuring our models can retrieve known values from simulated data. * Implement different cognitive models for history-dependent choice, exploring various ways to represent memory: as an external predictor, an internal updating state, incorporating exponential forgetting (linking to Reinforcement Learning principles), and even as a fully Bayesian belief update. * Recognize connections between these models and broader frameworks like Kalman filters and HGF. By fitting these models, we moved beyond simply describing behavior (like the cumulative rates in Chapter 4) to quantifying underlying latent parameters (like bias, beta, forgetting). However, fitting a model and recovering parameters is only part of the story. How do we know if the model is actually good? How well does it capture the patterns in the data beyond just the average parameter values? How sensitive are our conclusions to the specific priors we chose? These questions lead directly to the topic of model quality assessment, which we will tackle in the next chapter using techniques like prior/posterior predictive checks and sensitivity analyses. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
