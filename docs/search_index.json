[["index.html", "Advanced Cognitive Modeling Notes Chapter 1 Advanced Cognitive Modeling 1.1 The goal of the course 1.2 List of lectures and practical exercises 1.3 Preparation before the course", " Advanced Cognitive Modeling Notes Riccardo Fusaroli 2023-02-16 Chapter 1 Advanced Cognitive Modeling These are the teaching notes for Advanced Cognitive Modeling - taught in 2023 at the M.Sc. in Cognitive Science at Aarhus University. The syllabus is available at https://docs.google.com/document/d/1D8NTG0o4nD86AUdyyTp1hZIybg4ZEt9kdJ6mvfa_IQo/edit?usp=sharing The videos are available at https://youtube.com/playlist?list=PL_f3yDs5oZx6bfywYiGitMJPdJv6gZfXu 1.1 The goal of the course Advanced cognitive modeling is a course on how to think through, formalize and validate models of cognitive processes. In other words, we will be thinking about how people learn, and make decisions both in the lab and in the real world, and to robustly assess our hypothesized mechanisms. The course has 3 interrelated aims: to guide you through how models of cognitive processes are thought through and built (more than a toolbox of existing scripts); to provide (or reinforce) a good Bayesian workflow (simulation, prior assessment, parameter/model recovery, model fit assessment) to build robust and reliable models; to develop your probabilistic modeling skills (we will be dealing with brms, and also directly with stan). At the end of the course, you should be able to start thinking about how to use your own theoretical knowledge in cognitive science to build your own models, as well as to robustly evaluate existing models and their applicability. The course will be very hands-on. The main goal of the course is not just for you to understand how cognitive modeling works, but to build and use your own models. The lectures will include conceptual discussions of cognitive modeling and the specific models we will be dealing with, but also introduction to the coding exercises in the practical exercises (e.g. how to code in Stan). During the practical exercises, we will collect some data or explore existing datasets, design models together, and code them up: simulating how a person using those processes would perform, inferring parameters from simulated and real data, assessing model quality. We will take the time to do this together, and there will be time for lots of questions. The schedule for the course will therefore be somewhat flexible, and adaptive to your collective learning speed. See the planned schedule below. 1.2 List of lectures and practical exercises 1.3 Preparation before the course Before starting the course, you need to get your computers and brains in ship-shape so we can focus on modeling! In terms of computers, you need to make sure you have the following software installed and working: * up-to-date R (version 4 or above) and Rstudio (version 1.3 or above) installed and working. See here for a more detailed instruction on how to install R and Rstudio: https://happygitwithr.com/install-r-rstudio.html * the “brms” package installed: https://github.com/paul-buerkner/brms N.B. it’s not always as simple as doing an install.packages(“brms”), so do follow the linked guide! * the “cmdstanr” package: https://mc-stan.org/cmdstanr/articles/cmdstanr.html N.B. it’s not always as simple as doing an install.packages(“cmdstanr”), so do follow the linked guide! N.B. technically you can run all our exercises without cmdstanr if it turns to be too demanding, but your computer will be much slower. Without these packages working, you will not be able to tackle the practical exercises, so install them before you move to the next section and make sure there are no errors or worrying warnings. Once your computer is ready, you should also get your brain ready. This workshop focuses on how to do Bayesian data analysis and does not go into the details of Bayes’ theorem. If you are not familiar with the theorem or need a quick refresh, we strongly recommend you give this 15 min video a watch before the workshop. This should make talk of priors and posteriors much easier to parse. https://www.youtube.com/watch?v=HZGCoVF3YvM This workshop does not cover basic R coding and basic statistical modeling, they are taken for granted. I know not everybody comes from the Bsc in Cognitive Science, so if you feel you need some practice: * An amazing intro to R and the tidyverse (free online): https://r4ds.had.co.nz/ (I know some of you have also been referred to swirl and datacamp, I don’t know those resources, so have a look at the one above to check you know enough) * A intro to Bayesian statistics in brms (summarizing key points from methods 4 in the bachelor): https://4ccoxau.github.io/PriorsWorkshop/ videos + exercises. "],["practical-exercise-1---building-verbal-models-of-the-matching-pennies-game.html", "Chapter 2 Practical exercise 1 - Building verbal models of the matching pennies game 2.1 Trying out the game and collecting your own data 2.2 Start Theorizing 2.3 The distinction between participant and researcher perspectives 2.4 Strategies 2.5 Cognitive constraints 2.6 Continuity between models 2.7 Mixture of strategies", " Chapter 2 Practical exercise 1 - Building verbal models of the matching pennies game 2.1 Trying out the game and collecting your own data Today’s practical exercise is structured as follows: In order to do computational models we need a phenomenon to study (and ideally some data), you will therefore undergo an experiment, which will provide you with two specific cognitive domains to describe (one for now, one for later), and data from yourselves. You will now have to play the Matching Pennies Game against each other. In the Matching Pennies Game you and your opponent have to choose either “left” or “right” to indicate the hand in which the penny is hidded. If you are the matcher, you win by choosing the same as your opponent. If you are the capitalist with the penny, you win by choosing the opposite as your opponent. You should run 30 rounds with one of you being the capitalist and the other the matcher and then exchange roles for 30 more rounds. When you are the matcher, keep track of your score: every time you guess right you get +1, every time you don’t you get -1. The capitalist gets exactly the opposite, so if the matcher ends with a negative score, the capitalist has won and vice versa. Given you play many trials the game can take a while. If you want to take a break or do it in two sessions, feel free! Try to pay attention and aim at winning. As you play also try to figure out what kind of strategies might be at play for you and for the opponents. How are you deciding whether to choose left or right? Feel free to take notes. 2.2 Start Theorizing The goal of today’s assignment is to build models of the strategies and cognitive processes underlying behavior in the matching pennies game. In other words, to build hypotheses as to how the data is generated. The goal is to: 1) get you more aware of the issue of theory building (and assessment); 2) identify a small set of verbal models that we can then formalize in mathematical cognitive models and algorithms for simulations and model fitting. First, let’s take a little free discussion: Did you enjoy the game? What was the game about? What do you think your opponent was doing? Below you can observe how a previous year of CogSci did against bots (computational agents) playing according to different strategies. Look at the plots below, where the x axes indicate trial, the y axes how many points the CogSci’ers scored (0 being chance, negative means being completely owned by the bots, positive owning the bot) and the different colors indicate different strategies employed by the bots. Strategy “-2” was a Win-Stay-Lose-Shift bot: when it got a +1, it repeated its previous move (e.g. right if it had just played right), otherwise it would perform the opposite move (e.g. left if it had just played right). Strategy “-1” was a biased Nash both, playing “right” 80% of the time. Strategy “0” indicates a reinforcement learning bot; “1” a bot assuming you were playing according to a reinforcement learning strategy and trying to infer your learning and temperature parameters; “2” a bot assuming you were following strategy “1” and trying to accordingly infer your parameters. library(tidyverse) d &lt;- read_csv(&quot;data/MP_MSc_CogSci22.csv&quot;) %&gt;% mutate(BotStrategy = as.factor(BotStrategy)) ## Rows: 4400 Columns: 11 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): ID, BotParameters ## dbl (9): BotStrategyN, Role, player.tom_role, Choice, BotChoice, Payoff, BotPayoff, Trial, BotStrategy ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. d$Role &lt;- ifelse(d$Role == 0, &quot;Matcher&quot;, &quot;Mismatcher&quot;) ggplot(d, aes(Trial, Payoff, group = BotStrategy, color = BotStrategy)) + geom_smooth(se = F) + theme_classic() + facet_wrap(.~Role) ## `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39; That doesn’t look too good, ah? What about individual variability? In the plot below we indicate the score of each of the former students, against the different bots. d1 &lt;- d %&gt;% group_by(ID, BotStrategy) %&gt;% dplyr::summarize(Score = sum(Payoff)) ## `summarise()` has grouped output by &#39;ID&#39;. You can override using the `.groups` argument. ggplot(d1, aes(BotStrategy, Score, label = ID)) + geom_point(aes(color = ID)) + geom_boxplot(alpha = 0.3) + theme_classic() ## Warning: The following aesthetics were dropped during statistical transformation: label ## ℹ This can happen when ggplot fails to infer the correct grouping structure in the data. ## ℹ Did you forget to specify a `group` aesthetic or to convert a numerical variable into a factor? Now, let’s take a bit of group discussion. Get together in groups, and discuss which strategies and cognitive processes might underlie your and the agents’ behaviors in the game. One thing to keep in mind is what a model is: a simplification that can help us make sense of the world. In other words, any behavior is incredibly complex and involves many complex cognitive mechanisms. So start simple, and if you think it’s too simple, progressively add simple components. Once your study group has discussed a few (during the PE), add them here: https://docs.google.com/document/d/13OZL3CF9qM0744Y81BBKtvlu9k5E0F_tuuuU9DILRMU/edit?usp=sharing (shorturl.at/nrAKV) 2.3 The distinction between participant and researcher perspectives As participants we might not be aware of the strategy we use, or we might believe something erroneous. The exercise here is to act as researchers: what are the principles underlying the participants’ behaviors, no matter what the participants know or believe? Note that talking to participants and being participants help developing ideas, but it’s not the end point of the process. Also note that as cognitive scientists we can rely on what we have learned about cognitive processes (e.g. memory). 2.4 Strategies 2.4.1 Random strategies Players might simply be randomly choosing “head” or “tail” independently on the opponent’s choices and of how well they are doing. Choices could be fully at random (50% “head”, 50% “tail”) or biased (e.g. 60% “head”, 40% tail). 2.4.2 Immediate reaction Another simple strategy is simply to follow the previous choice: if it was successful keep it, if not change it. This strategy is also called Win-Stay-Lose-Shift (WSLS). Alternatively, one could do the opposite: Win-Shift-Lose-Stay. 2.4.3 Keep track of the bias (perfect memory) A player could keep track of biases in the opponent: count the proportion of “head” on the total trials so far and choose whichever choice has been made most often by the opponent. 2.4.4 Keep track of the bias (imperfect memory) A player could not be able to keep in mind all previous trials, or decide to forget old trials, in case the biase shifts over time. So we could use only the last n trials, or do a weighted mean with weigths proportional to temporal closeness (the more recent, the higher the weight). 2.4.5 Reinforcement learning Since there is a lot of leeway in how much memory we should keep of previous trials, we could also use a model that explicitly estimates how much players are learning on a trial by trial basis (high learning, low memory; low learning, high memory). This is the model of reinforcement learning, which we will deal with in future chapters. Shortly described, reinforcement learning assumes that each choice has a possible reward (probability of winning) and at every trial given the feedback received updates the expected value of the choice taken. The update depends on the prediction error (difference between expected and actual reward) and the learning rate. 2.4.6 k-ToM Reinforcement learning is a neat model, but can be problematic when playing against other agents: what the game is really about is not assessing the probability of the opponent choosing “head” generalizing from their past choices, but predicting what they will do. This requires making an explicit model of how the opponent chooses. k-ToM models will be dealt with in future chapters, but can be here anticipated as models assuming that the opponent follows a random bias (0-ToM), or models us as following a random bias (1-ToM), or models us modeling them as following a random bias (2-ToM), etc. 2.4.7 Other possible strategies Many additional strategies can be generated by combining former strategies. Generating random output is hard, so if we want to confuse the opponent, we could act first choosing tail 8 times, and then switching to a WSLS strategy for 4 trials, and then choosing head 4 times. Or implementing any of the previous strategies and doing the opposite “to mess with the opponent”. 2.5 Cognitive constraints As we discuss strategies, we can also identify several cognitive constraints that we know from former studies: in particular, memory and errors. 2.5.1 Memory Humans have limited memory and a tendency to forget that is roughly exponential. Models assuming perfect memory for longer stretches of trials are unrealistic. We could for instance use the exponential decay of memory to create weights following the same curve in the “keeping track of bias” models. Roughly, this is what reinforcement learning is doing via the learning rate parameter. 2.5.2 Errors Humans make mistakes, get distracted, push the wrong button, forget to check whether they won or lost before. So a realistic model of what happens in these games should contain a certain chance of making a mistake. E.g. a 10% chance that any choice will be perfectly random instead of following the strategy. Such random deviations from the strategy might also be conceptualized as explorations: keeping the door open to the strategy not being optimal and therefore testing other choices. For instance, one could have an imperfect WSLS where the probability of staying if winning (or shifting if losing) is only 80% and not 100%. Further, these deviations could be asymmetric, with the probability of staying if winning is 80% and of shifting if losing is 100%; for instance if negative and positive feedback are perceived asymmetrically. 2.6 Continuity between models Many of these models are simply extreme cases of others. For instance, WSLS is a reinforcement learning model with an extreme learning rate (reward replaces the formerly expected value without any moderation), which is also a memory model with a memory of 1 previous trial. 2.7 Mixture of strategies We discussed that there are techniques to consider the data generated by a mixture of models: estimating the probability that they are generated by model 1 or 2 or n. This probability can then be conditioned, according to our research question, to group (are people w schizophrenia more likely to employ model 1) or ID (are different participants using different models), or condition, or… We discussed that we often need lots of data to disambiguate between models, so conditioning e.g. on trial would in practice almost (?) never work. "],["practical-exercise-2---from-verbal-to-formal-models.html", "Chapter 3 Practical exercise 2 - From verbal to formal models 3.1 Defining general conditions 3.2 Implementing a random agent 3.3 Implementing a Win-Stay-Lose-Shift agent 3.4 Now we scale it up", " Chapter 3 Practical exercise 2 - From verbal to formal models The aim of this practical exercise is to go from verbal to formal models. We will not just write a formula, we will implement these models as algorithms in R. By implementing the models of algorithms, we are forced to make them very explicit in their assumptions; we become able to simulate the models in a variety of different situations and therefore better understand their implications So, the steps for today’s exercise are: choose two of the models and formalize them, that is, produce an algorithm that enacts the strategy, so we can simulate them. implement the algorithms as functions: getting an input and producing an output, so we can more easily implement them across various contexts (e.g. varying amount of trials, input, etc). See R4DataScience, if you need a refresher: https://r4ds.had.co.nz/functions.html implement a Random Bias agent (choosing “head” 70% of the times) and get your agents to play against it for 120 trials (and save the data) implement a Win-Stay-Lose-Shift agent (keeping the same choice if it won, changing it if it lost) and do the same. Now scale up the simulation: have 100 agents for each of your strategy playing against both Random Bias and Win-Stay-Lose-Shift and save their data. Figure out a good way to visualize the data to assess which strategy performs better, whether that changes over time and generally explore what the agents are doing. 3.1 Defining general conditions pacman::p_load(tidyverse, patchwork) trials &lt;- 120 agents &lt;- 100 3.2 Implementing a random agent Remember a random agent is an agent that picks at random between “right” and “left” independently on what the opponent is doing. A random agent might be perfectly random (50% chance of choosing “right”, same for “left”) or biased. The variable “rate” determines the rate of choosing “right”. rate &lt;- 0.5 RandomAgent &lt;- rbinom(trials, 1, rate) # we simply sample randomly from a binomial # Now let&#39;s plot how it&#39;s choosing d1 &lt;- tibble(trial = seq(trials), choice = RandomAgent) p1 &lt;- ggplot(d1, aes(trial, choice)) + geom_line() + theme_classic() p1 # What if we were to compare it to an agent being biased? rate &lt;- 0.8 RandomAgent &lt;- rbinom(trials, 1, rate) # we simply sample randomly from a binomial # Now let&#39;s plot how it&#39;s choosing d2 &lt;- tibble(trial = seq(trials), choice = RandomAgent) p2 &lt;- ggplot(d2, aes(trial, choice)) + geom_line() + theme_classic() p1 + p2 # Tricky to see, let&#39;s try writing the cumulative rate: d1$cumulativerate &lt;- cumsum(d1$choice) / seq_along(d1$choice) d2$cumulativerate &lt;- cumsum(d2$choice) / seq_along(d2$choice) p3 &lt;- ggplot(d1, aes(trial, cumulativerate)) + geom_line() + ylim(0,1) + theme_classic() p4 &lt;- ggplot(d2, aes(trial, cumulativerate)) + geom_line() + ylim(0,1) + theme_classic() p3 + p4 ## Now in the same plot d1$rate &lt;- 0.5 d2$rate &lt;- 0.8 d &lt;- rbind(d1,d2) p5 &lt;- ggplot(d, aes(trial, cumulativerate, color = rate, group = rate)) + geom_line() + ylim(0,1) + theme_classic() p5 # now as a function RandomAgent_f &lt;- function(input, rate){ n &lt;- length(input) choice &lt;- rbinom(n, 1, rate) return(choice) } input &lt;- rep(1,trials) # it doesn&#39;t matter, it&#39;s not taken into account choice &lt;- RandomAgent_f(input, rate) d3 &lt;- tibble(trial = seq(trials), choice) ggplot(d3, aes(trial, choice)) + geom_line() + theme_classic() ## What if there&#39;s noise? RandomAgentNoise_f &lt;- function(input, rate, noise){ n &lt;- length(input) choice &lt;- rbinom(n, 1, rate) if (rbinom(1, 1, noise) == 1) {choice = rbinom(1,1,0.5)} return(choice) } 3.3 Implementing a Win-Stay-Lose-Shift agent # as a function WSLSAgent_f &lt;- function(prevChoice, Feedback){ if (Feedback == 1) { choice = prevChoice } else if (Feedback == 0) { choice = 1 - prevChoice } return(choice) } WSLSAgentNoise_f &lt;- function(prevChoice, Feedback, noise){ if (Feedback == 1) { choice = prevChoice } else if (Feedback == 0) { choice = 1 - prevChoice } if (rbinom(1, 1, noise) == 1) {choice &lt;- rbinom(1, 1, .5)} return(choice) } WSLSAgent &lt;- WSLSAgent_f(1, 0) # Against a random agent Self &lt;- rep(NA, trials) Other &lt;- rep(NA, trials) Self[1] &lt;- RandomAgent_f(1, 0.5) Other &lt;- RandomAgent_f(seq(trials), rate) for (i in 2:trials) { if (Self[i - 1] == Other[i - 1]) { Feedback = 1 } else {Feedback = 0} Self[i] &lt;- WSLSAgent_f(Self[i - 1], Feedback) } sum(Self == Other) ## [1] 93 df &lt;- tibble(Self, Other, trial = seq(trials), Feedback = as.numeric(Self == Other)) ggplot(df) + theme_classic() + geom_line(color = &quot;red&quot;, aes(trial, Self)) + geom_line(color = &quot;blue&quot;, aes(trial, Other)) ggplot(df) + theme_classic() + geom_line(color = &quot;red&quot;, aes(trial, Feedback)) + geom_line(color = &quot;blue&quot;, aes(trial, 1 - Feedback)) df$cumulativerateSelf &lt;- cumsum(df$Feedback) / seq_along(df$Feedback) df$cumulativerateOther &lt;- cumsum(1 - df$Feedback) / seq_along(df$Feedback) ggplot(df) + theme_classic() + geom_line(color = &quot;red&quot;, aes(trial, cumulativerateSelf)) + geom_line(color = &quot;blue&quot;, aes(trial, cumulativerateOther)) # Against a Win-Stay-Lose Shift Self &lt;- rep(NA, trials) Other &lt;- rep(NA, trials) Self[1] &lt;- RandomAgent_f(1, 0.5) Other[1] &lt;- RandomAgent_f(1, 0.5) for (i in 2:trials) { if (Self[i - 1] == Other[i - 1]) { Feedback = 1 } else {Feedback = 0} Self[i] &lt;- WSLSAgent_f(Self[i - 1], Feedback) Other[i] &lt;- WSLSAgent_f(Other[i - 1], 1 - Feedback) } sum(Self == Other) ## [1] 60 df &lt;- tibble(Self, Other, trial = seq(trials), Feedback = as.numeric(Self == Other)) ggplot(df) + theme_classic() + geom_line(color = &quot;red&quot;, aes(trial, Self)) + geom_line(color = &quot;blue&quot;, aes(trial, Other)) ggplot(df) + theme_classic() + geom_line(color = &quot;red&quot;, aes(trial, Feedback)) + geom_line(color = &quot;blue&quot;, aes(trial, 1 - Feedback)) df$cumulativerateSelf &lt;- cumsum(df$Feedback) / seq_along(df$Feedback) df$cumulativerateOther &lt;- cumsum(1 - df$Feedback) / seq_along(df$Feedback) ggplot(df) + theme_classic() + geom_line(color = &quot;red&quot;, aes(trial, cumulativerateSelf)) + geom_line(color = &quot;blue&quot;, aes(trial, cumulativerateOther)) 3.4 Now we scale it up trials = 120 agents = 100 # WSLS vs agents with varying rates for (rate in seq(from = 0.5, to = 1, by = 0.05)) { for (agent in seq(agents)) { Self &lt;- rep(NA, trials) Other &lt;- rep(NA, trials) Self[1] &lt;- RandomAgent_f(1, 0.5) Other &lt;- RandomAgent_f(seq(trials), rate) for (i in 2:trials) { if (Self[i - 1] == Other[i - 1]) { Feedback = 1 } else {Feedback = 0} Self[i] &lt;- WSLSAgent_f(Self[i - 1], Feedback) } temp &lt;- tibble(Self, Other, trial = seq(trials), Feedback = as.numeric(Self == Other), agent, rate) if (agent == 1 &amp; rate == 0.5) {df &lt;- temp} else {df &lt;- bind_rows(df, temp)} } } ## WSLS with another WSLS for (agent in seq(agents)) { Self &lt;- rep(NA, trials) Other &lt;- rep(NA, trials) Self[1] &lt;- RandomAgent_f(1, 0.5) Other[1] &lt;- RandomAgent_f(1, 0.5) for (i in 2:trials) { if (Self[i - 1] == Other[i - 1]) { Feedback = 1 } else {Feedback = 0} Self[i] &lt;- WSLSAgent_f(Self[i - 1], Feedback) Other[i] &lt;- WSLSAgent_f(Other[i - 1], 1 - Feedback) } temp &lt;- tibble(Self, Other, trial = seq(trials), Feedback = as.numeric(Self == Other), agent, rate) if (agent == 1 ) {df1 &lt;- temp} else {df1 &lt;- bind_rows(df1, temp)} } 3.4.1 And we visualize it ggplot(df, aes(trial, Feedback, group = rate, color = rate)) + geom_smooth(se = F) + theme_classic() ## `geom_smooth()` using method = &#39;gam&#39; and formula = &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; We can see that the bigger the bias in the random agent, the bigger the performance in the WSLS (the higher the chances the random agent picks the same hand more than once in a row). Now it’s your turn to follow a similar process for your 2 chosen strategies. "],["practical-exercise-3---getting-into-stan.html", "Chapter 4 Practical exercise 3 - Getting into Stan", " Chapter 4 Practical exercise 3 - Getting into Stan "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
