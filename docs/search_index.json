[["index.html", "04-ModelQualityChecks Chapter 1 Advanced Cognitive Modeling 1.1 The goal of the course 1.2 List of lectures and practical exercises 1.3 Preparation before the course", " 04-ModelQualityChecks Riccardo Fusaroli 2023-02-22 Chapter 1 Advanced Cognitive Modeling These are the teaching notes for Advanced Cognitive Modeling - taught in 2023 at the M.Sc. in Cognitive Science at Aarhus University. The syllabus is available at https://docs.google.com/document/d/1D8NTG0o4nD86AUdyyTp1hZIybg4ZEt9kdJ6mvfa_IQo/edit?usp=sharing The videos are available at https://youtube.com/playlist?list=PL_f3yDs5oZx6bfywYiGitMJPdJv6gZfXu 1.1 The goal of the course Advanced cognitive modeling is a course on how to think through, formalize and validate models of cognitive processes. In other words, we will be thinking about how people learn, and make decisions both in the lab and in the real world, and to robustly assess our hypothesized mechanisms. The course has 3 interrelated aims: to guide you through how models of cognitive processes are thought through and built (more than a toolbox of existing scripts); to provide (or reinforce) a good Bayesian workflow (simulation, prior assessment, parameter/model recovery, model fit assessment) to build robust and reliable models; to develop your probabilistic modeling skills (we will be dealing with brms, and also directly with stan). At the end of the course, you should be able to start thinking about how to use your own theoretical knowledge in cognitive science to build your own models, as well as to robustly evaluate existing models and their applicability. The course will be very hands-on. The main goal of the course is not just for you to understand how cognitive modeling works, but to build and use your own models. The lectures will include conceptual discussions of cognitive modeling and the specific models we will be dealing with, but also introduction to the coding exercises in the practical exercises (e.g. how to code in Stan). During the practical exercises, we will collect some data or explore existing datasets, design models together, and code them up: simulating how a person using those processes would perform, inferring parameters from simulated and real data, assessing model quality. We will take the time to do this together, and there will be time for lots of questions. The schedule for the course will therefore be somewhat flexible, and adaptive to your collective learning speed. See the planned schedule below. 1.2 List of lectures and practical exercises 1.3 Preparation before the course Before starting the course, you need to get your computers and brains in ship-shape so we can focus on modeling! In terms of computers, you need to make sure you have the following software installed and working: * up-to-date R (version 4 or above) and Rstudio (version 1.3 or above) installed and working. See here for a more detailed instruction on how to install R and Rstudio: https://happygitwithr.com/install-r-rstudio.html * the “brms” package installed: https://github.com/paul-buerkner/brms N.B. it’s not always as simple as doing an install.packages(“brms”), so do follow the linked guide! * the “cmdstanr” package: https://mc-stan.org/cmdstanr/articles/cmdstanr.html N.B. it’s not always as simple as doing an install.packages(“cmdstanr”), so do follow the linked guide! N.B. technically you can run all our exercises without cmdstanr if it turns to be too demanding, but your computer will be much slower. Without these packages working, you will not be able to tackle the practical exercises, so install them before you move to the next section and make sure there are no errors or worrying warnings. Once your computer is ready, you should also get your brain ready. This workshop focuses on how to do Bayesian data analysis and does not go into the details of Bayes’ theorem. If you are not familiar with the theorem or need a quick refresh, we strongly recommend you give this 15 min video a watch before the workshop. This should make talk of priors and posteriors much easier to parse. https://www.youtube.com/watch?v=HZGCoVF3YvM This workshop does not cover basic R coding and basic statistical modeling, they are taken for granted. I know not everybody comes from the Bsc in Cognitive Science, so if you feel you need some practice: * An amazing intro to R and the tidyverse (free online): https://r4ds.had.co.nz/ (I know some of you have also been referred to swirl and datacamp, I don’t know those resources, so have a look at the one above to check you know enough) * A intro to Bayesian statistics in brms (summarizing key points from methods 4 in the bachelor): https://4ccoxau.github.io/PriorsWorkshop/ videos + exercises. "],["practical-exercise-1---building-verbal-models-of-the-matching-pennies-game.html", "Chapter 2 Practical exercise 1 - Building verbal models of the matching pennies game 2.1 Trying out the game and collecting your own data 2.2 Start Theorizing 2.3 The distinction between participant and researcher perspectives 2.4 Strategies 2.5 Cognitive constraints 2.6 Continuity between models 2.7 Mixture of strategies", " Chapter 2 Practical exercise 1 - Building verbal models of the matching pennies game 2.1 Trying out the game and collecting your own data Today’s practical exercise is structured as follows: In order to do computational models we need a phenomenon to study (and ideally some data), you will therefore undergo an experiment, which will provide you with two specific cognitive domains to describe (one for now, one for later), and data from yourselves. You will now have to play the Matching Pennies Game against each other. In the Matching Pennies Game you and your opponent have to choose either “left” or “right” to indicate the hand in which the penny is hidded. If you are the matcher, you win by choosing the same as your opponent. If you are the capitalist with the penny, you win by choosing the opposite as your opponent. You should run 30 rounds with one of you being the capitalist and the other the matcher and then exchange roles for 30 more rounds. When you are the matcher, keep track of your score: every time you guess right you get +1, every time you don’t you get -1. The capitalist gets exactly the opposite, so if the matcher ends with a negative score, the capitalist has won and vice versa. Given you play many trials the game can take a while. If you want to take a break or do it in two sessions, feel free! Try to pay attention and aim at winning. As you play also try to figure out what kind of strategies might be at play for you and for the opponents. How are you deciding whether to choose left or right? Feel free to take notes. 2.2 Start Theorizing The goal of today’s assignment is to build models of the strategies and cognitive processes underlying behavior in the matching pennies game. In other words, to build hypotheses as to how the data is generated. The goal is to: 1) get you more aware of the issue of theory building (and assessment); 2) identify a small set of verbal models that we can then formalize in mathematical cognitive models and algorithms for simulations and model fitting. First, let’s take a little free discussion: Did you enjoy the game? What was the game about? What do you think your opponent was doing? Below you can observe how a previous year of CogSci did against bots (computational agents) playing according to different strategies. Look at the plots below, where the x axes indicate trial, the y axes how many points the CogSci’ers scored (0 being chance, negative means being completely owned by the bots, positive owning the bot) and the different colors indicate different strategies employed by the bots. Strategy “-2” was a Win-Stay-Lose-Shift bot: when it got a +1, it repeated its previous move (e.g. right if it had just played right), otherwise it would perform the opposite move (e.g. left if it had just played right). Strategy “-1” was a biased Nash both, playing “right” 80% of the time. Strategy “0” indicates a reinforcement learning bot; “1” a bot assuming you were playing according to a reinforcement learning strategy and trying to infer your learning and temperature parameters; “2” a bot assuming you were following strategy “1” and trying to accordingly infer your parameters. library(tidyverse) d &lt;- read_csv(&quot;data/MP_MSc_CogSci22.csv&quot;) %&gt;% mutate(BotStrategy = as.factor(BotStrategy)) ## Rows: 4400 Columns: 11 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): ID, BotParameters ## dbl (9): BotStrategyN, Role, player.tom_role, Choice, BotChoice, Payoff, BotPayoff, Trial, BotStrategy ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. d$Role &lt;- ifelse(d$Role == 0, &quot;Matcher&quot;, &quot;Mismatcher&quot;) ggplot(d, aes(Trial, Payoff, group = BotStrategy, color = BotStrategy)) + geom_smooth(se = F) + theme_classic() + facet_wrap(.~Role) ## `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39; That doesn’t look too good, ah? What about individual variability? In the plot below we indicate the score of each of the former students, against the different bots. d1 &lt;- d %&gt;% group_by(ID, BotStrategy) %&gt;% dplyr::summarize(Score = sum(Payoff)) ## `summarise()` has grouped output by &#39;ID&#39;. You can override using the `.groups` argument. ggplot(d1, aes(BotStrategy, Score, label = ID)) + geom_point(aes(color = ID)) + geom_boxplot(alpha = 0.3) + theme_classic() ## Warning: The following aesthetics were dropped during statistical transformation: label ## ℹ This can happen when ggplot fails to infer the correct grouping structure in the data. ## ℹ Did you forget to specify a `group` aesthetic or to convert a numerical variable into a factor? Now, let’s take a bit of group discussion. Get together in groups, and discuss which strategies and cognitive processes might underlie your and the agents’ behaviors in the game. One thing to keep in mind is what a model is: a simplification that can help us make sense of the world. In other words, any behavior is incredibly complex and involves many complex cognitive mechanisms. So start simple, and if you think it’s too simple, progressively add simple components. Once your study group has discussed a few (during the PE), add them here: https://docs.google.com/document/d/13OZL3CF9qM0744Y81BBKtvlu9k5E0F_tuuuU9DILRMU/edit?usp=sharing (shorturl.at/nrAKV) 2.3 The distinction between participant and researcher perspectives As participants we might not be aware of the strategy we use, or we might believe something erroneous. The exercise here is to act as researchers: what are the principles underlying the participants’ behaviors, no matter what the participants know or believe? Note that talking to participants and being participants help developing ideas, but it’s not the end point of the process. Also note that as cognitive scientists we can rely on what we have learned about cognitive processes (e.g. memory). 2.4 Strategies 2.4.1 Random strategies Players might simply be randomly choosing “head” or “tail” independently on the opponent’s choices and of how well they are doing. Choices could be fully at random (50% “head”, 50% “tail”) or biased (e.g. 60% “head”, 40% tail). 2.4.2 Immediate reaction Another simple strategy is simply to follow the previous choice: if it was successful keep it, if not change it. This strategy is also called Win-Stay-Lose-Shift (WSLS). Alternatively, one could do the opposite: Win-Shift-Lose-Stay. 2.4.3 Keep track of the bias (perfect memory) A player could keep track of biases in the opponent: count the proportion of “head” on the total trials so far and choose whichever choice has been made most often by the opponent. 2.4.4 Keep track of the bias (imperfect memory) A player could not be able to keep in mind all previous trials, or decide to forget old trials, in case the biase shifts over time. So we could use only the last n trials, or do a weighted mean with weigths proportional to temporal closeness (the more recent, the higher the weight). 2.4.5 Reinforcement learning Since there is a lot of leeway in how much memory we should keep of previous trials, we could also use a model that explicitly estimates how much players are learning on a trial by trial basis (high learning, low memory; low learning, high memory). This is the model of reinforcement learning, which we will deal with in future chapters. Shortly described, reinforcement learning assumes that each choice has a possible reward (probability of winning) and at every trial given the feedback received updates the expected value of the choice taken. The update depends on the prediction error (difference between expected and actual reward) and the learning rate. 2.4.6 k-ToM Reinforcement learning is a neat model, but can be problematic when playing against other agents: what the game is really about is not assessing the probability of the opponent choosing “head” generalizing from their past choices, but predicting what they will do. This requires making an explicit model of how the opponent chooses. k-ToM models will be dealt with in future chapters, but can be here anticipated as models assuming that the opponent follows a random bias (0-ToM), or models us as following a random bias (1-ToM), or models us modeling them as following a random bias (2-ToM), etc. 2.4.7 Other possible strategies Many additional strategies can be generated by combining former strategies. Generating random output is hard, so if we want to confuse the opponent, we could act first choosing tail 8 times, and then switching to a WSLS strategy for 4 trials, and then choosing head 4 times. Or implementing any of the previous strategies and doing the opposite “to mess with the opponent”. 2.5 Cognitive constraints As we discuss strategies, we can also identify several cognitive constraints that we know from former studies: in particular, memory and errors. 2.5.1 Memory Humans have limited memory and a tendency to forget that is roughly exponential. Models assuming perfect memory for longer stretches of trials are unrealistic. We could for instance use the exponential decay of memory to create weights following the same curve in the “keeping track of bias” models. Roughly, this is what reinforcement learning is doing via the learning rate parameter. 2.5.2 Errors Humans make mistakes, get distracted, push the wrong button, forget to check whether they won or lost before. So a realistic model of what happens in these games should contain a certain chance of making a mistake. E.g. a 10% chance that any choice will be perfectly random instead of following the strategy. Such random deviations from the strategy might also be conceptualized as explorations: keeping the door open to the strategy not being optimal and therefore testing other choices. For instance, one could have an imperfect WSLS where the probability of staying if winning (or shifting if losing) is only 80% and not 100%. Further, these deviations could be asymmetric, with the probability of staying if winning is 80% and of shifting if losing is 100%; for instance if negative and positive feedback are perceived asymmetrically. 2.6 Continuity between models Many of these models are simply extreme cases of others. For instance, WSLS is a reinforcement learning model with an extreme learning rate (reward replaces the formerly expected value without any moderation), which is also a memory model with a memory of 1 previous trial. 2.7 Mixture of strategies We discussed that there are techniques to consider the data generated by a mixture of models: estimating the probability that they are generated by model 1 or 2 or n. This probability can then be conditioned, according to our research question, to group (are people w schizophrenia more likely to employ model 1) or ID (are different participants using different models), or condition, or… We discussed that we often need lots of data to disambiguate between models, so conditioning e.g. on trial would in practice almost (?) never work. "],["practical-exercise-2---from-verbal-to-formal-models.html", "Chapter 3 Practical exercise 2 - From verbal to formal models 3.1 Defining general conditions 3.2 Implementing a random agent 3.3 Implementing a Win-Stay-Lose-Shift agent 3.4 Now we scale it up", " Chapter 3 Practical exercise 2 - From verbal to formal models The aim of this practical exercise is to go from verbal to formal models. We will not just write a formula, we will implement these models as algorithms in R. By implementing the models of algorithms, we are forced to make them very explicit in their assumptions; we become able to simulate the models in a variety of different situations and therefore better understand their implications So, the steps for today’s exercise are: choose two of the models and formalize them, that is, produce an algorithm that enacts the strategy, so we can simulate them. implement the algorithms as functions: getting an input and producing an output, so we can more easily implement them across various contexts (e.g. varying amount of trials, input, etc). See R4DataScience, if you need a refresher: https://r4ds.had.co.nz/functions.html implement a Random Bias agent (choosing “head” 70% of the times) and get your agents to play against it for 120 trials (and save the data) implement a Win-Stay-Lose-Shift agent (keeping the same choice if it won, changing it if it lost) and do the same. Now scale up the simulation: have 100 agents for each of your strategy playing against both Random Bias and Win-Stay-Lose-Shift and save their data. Figure out a good way to visualize the data to assess which strategy performs better, whether that changes over time and generally explore what the agents are doing. 3.1 Defining general conditions pacman::p_load(tidyverse, patchwork) trials &lt;- 120 agents &lt;- 100 3.2 Implementing a random agent Remember a random agent is an agent that picks at random between “right” and “left” independently on what the opponent is doing. A random agent might be perfectly random (50% chance of choosing “right”, same for “left”) or biased. The variable “rate” determines the rate of choosing “right”. rate &lt;- 0.5 RandomAgent &lt;- rbinom(trials, 1, rate) # we simply sample randomly from a binomial # Now let&#39;s plot how it&#39;s choosing d1 &lt;- tibble(trial = seq(trials), choice = RandomAgent) p1 &lt;- ggplot(d1, aes(trial, choice)) + geom_line() + theme_classic() p1 # What if we were to compare it to an agent being biased? rate &lt;- 0.8 RandomAgent &lt;- rbinom(trials, 1, rate) # we simply sample randomly from a binomial # Now let&#39;s plot how it&#39;s choosing d2 &lt;- tibble(trial = seq(trials), choice = RandomAgent) p2 &lt;- ggplot(d2, aes(trial, choice)) + geom_line() + theme_classic() p1 + p2 # Tricky to see, let&#39;s try writing the cumulative rate: d1$cumulativerate &lt;- cumsum(d1$choice) / seq_along(d1$choice) d2$cumulativerate &lt;- cumsum(d2$choice) / seq_along(d2$choice) p3 &lt;- ggplot(d1, aes(trial, cumulativerate)) + geom_line() + ylim(0,1) + theme_classic() p4 &lt;- ggplot(d2, aes(trial, cumulativerate)) + geom_line() + ylim(0,1) + theme_classic() p3 + p4 ## Now in the same plot d1$rate &lt;- 0.5 d2$rate &lt;- 0.8 d &lt;- rbind(d1,d2) p5 &lt;- ggplot(d, aes(trial, cumulativerate, color = rate, group = rate)) + geom_line() + ylim(0,1) + theme_classic() p5 # now as a function RandomAgent_f &lt;- function(input, rate){ n &lt;- length(input) choice &lt;- rbinom(n, 1, rate) return(choice) } input &lt;- rep(1,trials) # it doesn&#39;t matter, it&#39;s not taken into account choice &lt;- RandomAgent_f(input, rate) d3 &lt;- tibble(trial = seq(trials), choice) ggplot(d3, aes(trial, choice)) + geom_line() + theme_classic() ## What if there&#39;s noise? RandomAgentNoise_f &lt;- function(input, rate, noise){ n &lt;- length(input) choice &lt;- rbinom(n, 1, rate) if (rbinom(1, 1, noise) == 1) {choice = rbinom(1,1,0.5)} return(choice) } 3.3 Implementing a Win-Stay-Lose-Shift agent # as a function WSLSAgent_f &lt;- function(prevChoice, Feedback){ if (Feedback == 1) { choice = prevChoice } else if (Feedback == 0) { choice = 1 - prevChoice } return(choice) } WSLSAgentNoise_f &lt;- function(prevChoice, Feedback, noise){ if (Feedback == 1) { choice = prevChoice } else if (Feedback == 0) { choice = 1 - prevChoice } if (rbinom(1, 1, noise) == 1) {choice &lt;- rbinom(1, 1, .5)} return(choice) } WSLSAgent &lt;- WSLSAgent_f(1, 0) # Against a random agent Self &lt;- rep(NA, trials) Other &lt;- rep(NA, trials) Self[1] &lt;- RandomAgent_f(1, 0.5) Other &lt;- RandomAgent_f(seq(trials), rate) for (i in 2:trials) { if (Self[i - 1] == Other[i - 1]) { Feedback = 1 } else {Feedback = 0} Self[i] &lt;- WSLSAgent_f(Self[i - 1], Feedback) } sum(Self == Other) ## [1] 77 df &lt;- tibble(Self, Other, trial = seq(trials), Feedback = as.numeric(Self == Other)) ggplot(df) + theme_classic() + geom_line(color = &quot;red&quot;, aes(trial, Self)) + geom_line(color = &quot;blue&quot;, aes(trial, Other)) ggplot(df) + theme_classic() + geom_line(color = &quot;red&quot;, aes(trial, Feedback)) + geom_line(color = &quot;blue&quot;, aes(trial, 1 - Feedback)) df$cumulativerateSelf &lt;- cumsum(df$Feedback) / seq_along(df$Feedback) df$cumulativerateOther &lt;- cumsum(1 - df$Feedback) / seq_along(df$Feedback) ggplot(df) + theme_classic() + geom_line(color = &quot;red&quot;, aes(trial, cumulativerateSelf)) + geom_line(color = &quot;blue&quot;, aes(trial, cumulativerateOther)) # Against a Win-Stay-Lose Shift Self &lt;- rep(NA, trials) Other &lt;- rep(NA, trials) Self[1] &lt;- RandomAgent_f(1, 0.5) Other[1] &lt;- RandomAgent_f(1, 0.5) for (i in 2:trials) { if (Self[i - 1] == Other[i - 1]) { Feedback = 1 } else {Feedback = 0} Self[i] &lt;- WSLSAgent_f(Self[i - 1], Feedback) Other[i] &lt;- WSLSAgent_f(Other[i - 1], 1 - Feedback) } sum(Self == Other) ## [1] 60 df &lt;- tibble(Self, Other, trial = seq(trials), Feedback = as.numeric(Self == Other)) ggplot(df) + theme_classic() + geom_line(color = &quot;red&quot;, aes(trial, Self)) + geom_line(color = &quot;blue&quot;, aes(trial, Other)) ggplot(df) + theme_classic() + geom_line(color = &quot;red&quot;, aes(trial, Feedback)) + geom_line(color = &quot;blue&quot;, aes(trial, 1 - Feedback)) df$cumulativerateSelf &lt;- cumsum(df$Feedback) / seq_along(df$Feedback) df$cumulativerateOther &lt;- cumsum(1 - df$Feedback) / seq_along(df$Feedback) ggplot(df) + theme_classic() + geom_line(color = &quot;red&quot;, aes(trial, cumulativerateSelf)) + geom_line(color = &quot;blue&quot;, aes(trial, cumulativerateOther)) 3.4 Now we scale it up trials = 120 agents = 100 # WSLS vs agents with varying rates for (rate in seq(from = 0.5, to = 1, by = 0.05)) { for (agent in seq(agents)) { Self &lt;- rep(NA, trials) Other &lt;- rep(NA, trials) Self[1] &lt;- RandomAgent_f(1, 0.5) Other &lt;- RandomAgent_f(seq(trials), rate) for (i in 2:trials) { if (Self[i - 1] == Other[i - 1]) { Feedback = 1 } else {Feedback = 0} Self[i] &lt;- WSLSAgent_f(Self[i - 1], Feedback) } temp &lt;- tibble(Self, Other, trial = seq(trials), Feedback = as.numeric(Self == Other), agent, rate) if (agent == 1 &amp; rate == 0.5) {df &lt;- temp} else {df &lt;- bind_rows(df, temp)} } } ## WSLS with another WSLS for (agent in seq(agents)) { Self &lt;- rep(NA, trials) Other &lt;- rep(NA, trials) Self[1] &lt;- RandomAgent_f(1, 0.5) Other[1] &lt;- RandomAgent_f(1, 0.5) for (i in 2:trials) { if (Self[i - 1] == Other[i - 1]) { Feedback = 1 } else {Feedback = 0} Self[i] &lt;- WSLSAgent_f(Self[i - 1], Feedback) Other[i] &lt;- WSLSAgent_f(Other[i - 1], 1 - Feedback) } temp &lt;- tibble(Self, Other, trial = seq(trials), Feedback = as.numeric(Self == Other), agent, rate) if (agent == 1 ) {df1 &lt;- temp} else {df1 &lt;- bind_rows(df1, temp)} } 3.4.1 And we visualize it ggplot(df, aes(trial, Feedback, group = rate, color = rate)) + geom_smooth(se = F) + theme_classic() ## `geom_smooth()` using method = &#39;gam&#39; and formula = &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; We can see that the bigger the bias in the random agent, the bigger the performance in the WSLS (the higher the chances the random agent picks the same hand more than once in a row). Now it’s your turn to follow a similar process for your 2 chosen strategies. "],["practical-exercise-3---getting-into-stan.html", "Chapter 4 Practical exercise 3 - Getting into Stan 4.1 Overview 4.2 Simulating data 4.3 Building our basic model in Stan 4.4 Parameter recovery 4.5 The memory model: conditioning theta", " Chapter 4 Practical exercise 3 - Getting into Stan 4.1 Overview The goal of the practical exercise is to build on the simulated data from Practical Exercise 2 to construct our Stan models of the generative processes of the data. Here we know the truth: we simulated the data ourselves, so we can assess how accurate the model is in reconstructing, e.g. the bias of the agents. 4.2 Simulating data Here we build a new simulation of random agents with bias and noise. The code and visualization is really nothing different from last week’s exercise. pacman::p_load(tidyverse, here, posterior, cmdstanr, brms, tidybayes) trials &lt;- 120 RandomAgentNoise_f &lt;- function(rate, noise) { choice &lt;- rbinom(1, 1, rate) # generating noiseless choices if (rbinom(1, 1, noise) == 1) { choice = rbinom(1, 1, 0.5) # introducing noise } return(choice) } d &lt;- NULL for (noise in seq(0, 0.5, 0.1)) { # looping through noise levels for (rate in seq(0, 1, 0.1)) { # looping through rate levels randomChoice &lt;- rep(NA, trials) for (t in seq(trials)) { # looping through trials (to make it homologous to more reactive models) randomChoice[t] &lt;- RandomAgentNoise_f(rate, noise) } temp &lt;- tibble(trial = seq(trials), choice = randomChoice, rate, noise) temp$cumulativerate &lt;- cumsum(temp$choice) / seq_along(temp$choice) if (exists(&quot;d&quot;)) { d &lt;- rbind(d, temp) } else{ d &lt;- temp } } } write_csv(d, &quot;simdata/W3_randomnoise.csv&quot;) # Now we visualize it p1 &lt;- ggplot(d, aes(trial, cumulativerate, group = rate, color = rate)) + geom_line() + geom_hline(yintercept = 0.5, linetype = &quot;dashed&quot;) + ylim(0,1) + facet_wrap(.~noise) + theme_classic() p1 4.3 Building our basic model in Stan N.B. Refer to the video and slides for the step by step build-up of the Stan code. Now we subset to a simple case, no noise and rate of 0.8, to focus on the Stan model. We make it into the right format for Stan, build the Stan model, and fit it. 4.3.1 Data Here we define the data and format it for Stan. Stan likes data as a list. Why a list? Well, dataframes (now tibbles) are amazing. But they have a big drawback: they require each variable to have the same length. Lists do not have that limitation, they are more flexible. So, lists. We’ll have to learn how to live with them. d1 &lt;- d %&gt;% subset(noise == 0 &amp; rate == 0.8) ## Create the data. N.B. note the two variables have different lengths: 1 for n, n for h. data &lt;- list( n = 120, # n of trials h = d1$choice # sequence of choices (h stands for hand) ) 4.3.2 Model We write the stan code within the R code (so I can show it to you more easily), then we save it as a stan file, which can be loaded at a later stage in order to compile it. [Missing: more info on compiling etc.] Remember that the minimal Stan model requires 3 chunks, one specifying the data it will need as input; one specifying the parameters to be estimated; one specifying the model within which the parameters appear, and the priors for those parameters. stan_model &lt;- &quot; // This model infers a random bias from a sequences of 1s and 0s (right and left hand choices) // The input (data) for the model. n of trials and the sequence of choices (right as 1, left as 0) data { int&lt;lower=1&gt; n; // n of trials array[n] int h; // sequence of choices (right as 1, left as 0) as long as n } // The parameters that the model needs to estimate (theta) parameters { real&lt;lower=0, upper=1&gt; theta; // rate or theta is a probability and therefore bound between 0 and 1 } // The model to be estimated (a bernoulli, parameter theta, prior on the theta) model { // The prior for theta is a beta distribution alpha of 1, beta of 1, equivalent to a uniform between 0 and 1 target += beta_lpdf(theta | 1, 1); // The model consists of a bernoulli distribution (binomial w 1 trial only) with a rate theta target += bernoulli_lpmf(h | theta); } &quot; write_stan_file( stan_model, dir = &quot;stan/&quot;, basename = &quot;W3_SimpleBernoulli.stan&quot;) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W3_SimpleBernoulli.stan&quot; 4.3.3 Compiling and fitting the model ## Specify where the model is file &lt;- file.path(&quot;stan/W3_SimpleBernoulli.stan&quot;) mod &lt;- cmdstan_model(file, # this specifies we can parallelize the gradient estimations on multiple cores cpp_options = list(stan_threads = TRUE), # this is a trick to make it faster stanc_options = list(&quot;O1&quot;)) # The following command calls Stan with specific options. samples &lt;- mod$sample( data = data, # the data :-) seed = 123, # a seed, so I always get the same results chains = 2, # how many chains should I fit (to check whether they give the same results) parallel_chains = 2, # how many of the chains can be run in parallel? threads_per_chain = 2, # distribute gradient estimations within chain across multiple cores iter_warmup = 1000, # warmup iterations through which hyperparameters (steps and step length) are adjusted iter_sampling = 2000, # total number of iterations refresh = 0, # how often to show that iterations have been run max_treedepth = 20, # how many steps in the future to check to avoid u-turns adapt_delta = 0.99, # how high a learning rate to adjust hyperparameters during warmup ) ## Running MCMC with 2 parallel chains, with 2 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Chain 2 finished in 0.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.0 seconds. ## Total execution time: 0.1 seconds. Now the model is ready to be assessed. First we simply generate a summary of the estimates to have a first idea. samples$summary() # summarize the model ## # A tibble: 2 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lp__ -65.0 -64.7 0.646 0.327 -66.3 -64.5 1.00 1105. 1155. ## 2 theta 0.780 0.781 0.0368 0.0383 0.717 0.837 1.00 1050. 995. Then we need to look more in the details at the quality of the estimation: * the markov chains * how the prior and the posterior estimates relate to each other (whether the prior is constraining the posterior estimate) # Extract posterior samples and include sampling of the prior: draws_df &lt;- as_draws_df(samples$draws()) # Checking the model&#39;s chains ggplot(draws_df, aes(.iteration, theta, group = .chain, color = .chain)) + geom_line() + theme_classic() # add a prior for theta (ugly, but we&#39;ll do better soon) draws_df &lt;- draws_df %&gt;% mutate( theta_prior = rbeta(nrow(draws_df), 1, 1) ) # Now let&#39;s plot the density for theta (prior and posterior) ggplot(draws_df) + geom_density(aes(theta), fill = &quot;blue&quot;, alpha = 0.3) + geom_density(aes(theta_prior), fill = &quot;red&quot;, alpha = 0.3) + geom_vline(xintercept = 0.8, linetype = &quot;dashed&quot;, color = &quot;black&quot;, size = 1.5) + xlab(&quot;Rate&quot;) + ylab(&quot;Posterior Density&quot;) + theme_classic() As we can see from the posterior estimates and the prior posterior update check, our model is doing a decent job. It doesn’t exactly reconstruct the rate of 0.8, but 0.755 is pretty close and 0.8 is included within the credible interval. Now we build the same model, but using the log odds scale for the theta parameter, which will become useful later when we condition theta on variables and build multilevel models (as we can do what we want in a log odds space and it will always be bound between 0 and 1). stan_model &lt;- &quot; // This model infers a random bias from a sequences of 1s and 0s (right and left hand choices) // The input (data) for the model. n of trials and the sequence of choices (right as 1, left as 0) data { int&lt;lower=1&gt; n; // n of trials array[n] int h; // sequence of choices (right as 1, left as 0) as long as n } // The parameters that the model needs to estimate (theta) parameters { real theta; // note it is unbounded as we now work on log odds } // The model to be estimated (a bernoulli, parameter theta, prior on the theta) model { // The prior for theta on a log odds scale is a normal distribution with a mean of 0 and a sd of 1. // This covers most of the probability space between 0 and 1, after being converted to probability. target += normal_lpdf(theta | 0, 1); // The model consists of a bernoulli distribution (binomial w 1 trial only) with a rate theta, // note we specify it uses a logit link (theta is in logodds) target += bernoulli_logit_lpmf(h | theta); } &quot; write_stan_file( stan_model, dir = &quot;stan/&quot;, basename = &quot;W3_SimpleBernoulli_logodds.stan&quot;) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W3_SimpleBernoulli_logodds.stan&quot; ## With the logit format ## Specify where the model is file &lt;- file.path(&quot;stan/W3_SimpleBernoulli_logodds.stan&quot;) mod &lt;- cmdstan_model(file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) # The following command calls Stan with specific options. samples &lt;- mod$sample( data = data, seed = 123, chains = 2, parallel_chains = 2, threads_per_chain = 2, iter_warmup = 1000, iter_sampling = 2000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99, ) ## Running MCMC with 2 parallel chains, with 2 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Chain 2 finished in 0.1 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.1 seconds. ## Total execution time: 0.1 seconds. samples ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## lp__ -64.95 -64.64 0.80 0.30 -66.60 -64.43 1.00 926 718 ## theta 1.25 1.25 0.22 0.21 0.90 1.62 1.00 1112 785 # Diagnostics samples$cmdstan_diagnose() ## Processing csv files: /var/folders/lt/zspkqnxd5yg92kybm5f433_cfjr0d6/T/RtmpCa0Wqb/W3_SimpleBernoulli_logodds-202302241444-1-648c0d.csv, /var/folders/lt/zspkqnxd5yg92kybm5f433_cfjr0d6/T/RtmpCa0Wqb/W3_SimpleBernoulli_logodds-202302241444-2-648c0d.csv ## ## Checking sampler transitions treedepth. ## Treedepth satisfactory for all transitions. ## ## Checking sampler transitions for divergences. ## No divergent transitions found. ## ## Checking E-BFMI - sampler transitions HMC potential energy. ## E-BFMI satisfactory. ## ## Effective sample size satisfactory. ## ## Split R-hat values satisfactory all parameters. ## ## Processing complete, no problems detected. # Extract posterior samples and include sampling of the prior: draws_df &lt;- as_draws_df(samples$draws()) ggplot(draws_df, aes(.iteration, theta, group = .chain, color = .chain)) + geom_line() + theme_classic() samples$summary() ## # A tibble: 2 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lp__ -65.0 -64.6 0.797 0.298 -66.6 -64.4 1.00 927. 718. ## 2 theta 1.25 1.25 0.220 0.209 0.903 1.62 1.00 1112. 785. We can see that the results are virtually identical, except for a slight shrinkage of the estimate from 0.755 to 0.75 (due to the prior being more conservative closer to 0 and 1). 4.4 Parameter recovery Now that we see that the model works in one case, we can run it throughout all possible rate and noise levels in the simulation. N.B. here is using loops, parallelized version in the next code chunk. # Now we need to scale it up to all possible rates and noises recovery_df &lt;- NULL for (noiseLvl in unique(d$noise)) { for (rateLvl in unique(d$rate)) { dd &lt;- d %&gt;% subset( noise == noiseLvl &amp; rate == rateLvl ) data &lt;- list( n = 120, h = dd$choice ) samples &lt;- mod$sample( data = data, seed = 123, chains = 1, parallel_chains = 1, threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 2000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99, ) draws_df &lt;- as_draws_df(samples$draws()) temp &lt;- tibble(biasEst = inv_logit_scaled(draws_df$theta), biasTrue = rateLvl, noise = noiseLvl) if (exists(&quot;recovery_df&quot;)) {recovery_df &lt;- rbind(recovery_df, temp)} else {recovery_df &lt;- temp} } } ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. write_csv(recovery_df, &quot;simdata/W3_recoverydf_simple.csv&quot;) ggplot(recovery_df, aes(biasTrue, biasEst)) + geom_point(alpha = 0.1) + geom_smooth() + facet_wrap(.~noise) + theme_classic() There’s much to be said about the final plot, but for now let’s just say that it looks good. We can reconstruct in a nice ordered way true rate values. However, our ability to do so decreases with the increase in noise. So far no surprises. Wait, you say, shouldn’t we actually model the generative process, that is, include noise in the Stan model? Gold star, there! But let’s wait a bit before we get there, we’ll need mixture models. One final note before moving to the memory model: what if we parallelized the parameter recovery, so that different models / datasets run on different cores? This was not necessary above (it ran in a few minutes anyway), but will become crucial with more complex models. To parallelize, we rely on furrr, a neat R package that distributes parallel operations across cores. First we need to define the function that will define the operations to be run on each core separately, here we simulate the data according to a seed, a n of trials, a rate and a noise, and then we fit the model to them. Second, we need to create a tibble of the seeds, n of trials, rate and noise values that should be simulated. Third, we use future_pmap_dfr to run the function on each row of the tibble above separately on a different core. Note that I set the system to split across 4 parallel cores (to work on my computer without clogging it). Do change it according to the system you are using. Note that if you have 40 “jobs” (rows of the tibble, sets of parameter values to run), using e.g. 32 cores will not substantially speed things more than using 20. pacman::p_load(future, purrr, furrr) plan(multisession, workers = 4) sim_d_and_fit &lt;- function(seed, trials, rateLvl, noiseLvl) { for (t in seq(trials)) { # looping through trials (to make it homologous to more reactive models) randomChoice[t] &lt;- RandomAgentNoise_f(rateLvl, noiseLvl) } temp &lt;- tibble(trial = seq(trials), choice = randomChoice, rate, noise) data &lt;- list( n = 120, h = temp$choice ) samples &lt;- mod$sample( data = data, seed = 1000, chains = 1, parallel_chains = 1, threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 2000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99, ) draws_df &lt;- as_draws_df(samples$draws()) temp &lt;- tibble(biasEst = inv_logit_scaled(draws_df$theta), biasTrue = rateLvl, noise = noiseLvl) return(temp) } temp &lt;- tibble(unique(d[,c(&quot;rate&quot;, &quot;noise&quot;)])) %&gt;% mutate(seed = 1000, trials = 120) %&gt;% rename(rateLvl = rate, noiseLvl = noise) recovery_df &lt;- future_pmap_dfr(temp, sim_d_and_fit, .options = furrr_options(seed = TRUE)) ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ggplot(recovery_df, aes(biasTrue, biasEst)) + geom_point(alpha = 0.1) + geom_smooth() + facet_wrap(.~noise) + theme_classic() 4.5 The memory model: conditioning theta Now that we fitted the base model, we can move onto more complex models. For instance a memory model (including all previous trials). Here we rely on a generalized linear model kind of thinking: the theta is the expression of a linear model (bias + b1 * PreviousRate), which is bound to a probability 0-1 space via a logit link/transformation. # We subset to only include no noise and a specific rate d1 &lt;- d %&gt;% subset(noise == 0 &amp; rate == 0.8) ## Create the data data &lt;- list( n = 120, h = d1$choice, memory = d1$cumulativerate # this creates the new parameter: the rate of right hands so far ) stan_model &lt;- &quot; // The input (data) for the model. n of trials and h for (right and left) hand data { int&lt;lower=1&gt; n; array[n] int h; vector&lt;lower=0, upper=1&gt;[n] memory; // here we add the new parameter } // The parameters accepted by the model. parameters { real bias; // how likely is the agent to pick right when the previous rate has no information (50-50)? real beta; // how strongly is previous rate impacting the decision? } // The model to be estimated. model { // The prior for theta is a uniform distribution between 0 and 1 target += normal_lpdf(bias | 0, 1); target += normal_lpdf(beta | 0, .3); // The model consists of a binomial distributions with a rate theta target += bernoulli_logit_lpmf(h | bias + beta * memory); } &quot; write_stan_file( stan_model, dir = &quot;stan/&quot;, basename = &quot;W3_MemoryBernoulli.stan&quot;) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W3_MemoryBernoulli.stan&quot; ## Specify where the model is file &lt;- file.path(&quot;stan/W3_MemoryBernoulli.stan&quot;) mod &lt;- cmdstan_model(file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) # The following command calls Stan with specific options. samples &lt;- mod$sample( data = data, seed = 123, chains = 1, parallel_chains = 2, threads_per_chain = 2, iter_warmup = 1000, iter_sampling = 1000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99, ) ## Running MCMC with 1 chain, with 2 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. samples$summary() ## # A tibble: 3 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lp__ -65.1 -64.8 0.984 0.762 -67.0 -64.1 0.999 388. 371. ## 2 bias 1.12 1.13 0.306 0.318 0.631 1.63 1.00 268. 501. ## 3 beta 0.141 0.128 0.291 0.287 -0.333 0.625 1.00 225. 182. We can see that the model has now estimated both the bias and the role of previous memory. Bias should reflect the bias in the setup (0.8 which in log odds is 1.39), and the beta coefficient for memory (roughly 0). More on quality checks of the models in the next chapter. "],["practical-exercise-4---model-quality-checks.html", "Chapter 5 Practical exercise 4 - Model quality checks 5.1 Generating and plotting additional variables 5.2 Now we can better assess the prior 5.3 as well as how the model was predicting the outcome to be before and after seeing the data.", " Chapter 5 Practical exercise 4 - Model quality checks This document covers: - generating and plotting priors (against posteriors) - generating and plotting predictive checks (prior and posterior ones) - prior sensitivity checks 5.1 Generating and plotting additional variables As we try to understand our model, we might want to plot how the prior relates to the posterior, or - in other words, what has the model learned from looking at the data? We can do so by overlaying the prior and the posterior distributions, what is also called a “prior - posterior update check”. Stan does not automatically save the prior distribution, so we need to tell it to generate and save prior distributions in a convenient place so we can easily plot or use them at will from R. Luckily, Stan gives us a dedicated code chunk to do that: the generated quantities chunk. As before, we need to define the kind of variable we want to save, and then how to generate it. If we take the example of the random agent (with a bias), we have one parameter: theta. We can then generate theta according to the prior in generated quantities. While we are at this, we can also generate a nicer version of the posterior estimate for the theta parameter, now in probability scale (instead of log odds). However, prior and posterior estimates are not always the most immediate thing to understand. For instance, we might have trouble having a good grasp for how the uncertainty in the estimate will play out on 120 trials, or 6 trials, or however many trials we are planning for our experiment. Luckily, we can ask Stan to run predictions from either priors or posteriors, or both: given the priors how many trials will have “right hand” choice? and given the posterior estimates? As we use complex models, the relation between prior/posterior estimates and predictions becomes less and less intuitive. Simulating their implications for the outcomes - also called prior/posterior predictive checks - becomes a very useful tool to adjust our priors and their uncertainty so that they reflect what we know of the outcome scale; as well as to assess whether the model (and its posterior estimates) can appropriately describe the data we observe, or there’s some bias there. More discussion of this can be found at https://4ccoxau.github.io/PriorsWorkshop/. stan_model &lt;- &quot; // This model infers a random bias from a sequences of 1s and 0s (right and left hand choices) // The input (data) for the model. n of trials and the sequence of choices (right as 1, left as 0) data { int&lt;lower=1&gt; n; // n of trials array[n] int h; // sequence of choices (right as 1, left as 0) as long as n } // The parameters that the model needs to estimate (theta) parameters { real theta; // note it is unbounded as we now work on log odds } // The model to be estimated (a bernoulli, parameter theta, prior on the theta) model { // The prior for theta on a log odds scale is a normal distribution with a mean of 0 and a sd of 1. // This covers most of the probability space between 0 and 1, after being converted to probability. target += normal_lpdf(theta | 0, 1); // The model consists of a bernoulli distribution (binomial w 1 trial only) with a rate theta, // note we specify it uses a logit link (theta is in logodds) target += bernoulli_logit_lpmf(h | theta); } generated quantities{ real&lt;lower=0, upper=1&gt; theta_prior; // theta prior parameter, on a prob scale (0-1) real&lt;lower=0, upper=1&gt; theta_posterior; // theta posterior parameter, on a prob scale (0-1) int&lt;lower=0, upper=n&gt; prior_preds; // distribution of right hand choices according to the prior int&lt;lower=0, upper=n&gt; posterior_preds; // distribution of right hand choices according to the posterior theta_prior = inv_logit(normal_rng(0,1)); // generating the prior on a log odds scale and converting theta_posterior = inv_logit(theta); // converting the posterior estimate from log odds to prob. prior_preds = binomial_rng(n, theta_prior); posterior_preds = binomial_rng(n, inv_logit(theta)); } &quot; write_stan_file( stan_model, dir = &quot;stan/&quot;, basename = &quot;W4_SimpleBernoulli_logodds.stan&quot;) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W4_SimpleBernoulli_logodds.stan&quot; ## With the logit format ## Specify where the model is file &lt;- file.path(&quot;stan/W4_SimpleBernoulli_logodds.stan&quot;) mod &lt;- cmdstan_model(file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) ## Model executable is up to date! d1 &lt;- d %&gt;% subset(noise == 0 &amp; rate == 0.8) ## Create the data. N.B. note the two variables have different lengths: 1 for n, n for h. data &lt;- list( n = 120, # n of trials h = d1$choice # sequence of choices (h stands for hand) ) # The following command calls Stan with specific options. samples &lt;- mod$sample( data = data, seed = 123, chains = 2, parallel_chains = 2, threads_per_chain = 2, iter_warmup = 1000, iter_sampling = 2000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99, ) ## Running MCMC with 2 parallel chains, with 2 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Chain 2 finished in 0.1 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.1 seconds. ## Total execution time: 0.1 seconds. draws_df &lt;- as_draws_df(samples$draws()) 5.2 Now we can better assess the prior # Now let&#39;s plot the density for theta (prior and posterior) ggplot(draws_df) + geom_density(aes(theta_posterior), fill = &quot;blue&quot;, alpha = 0.3) + geom_density(aes(theta_prior), fill = &quot;red&quot;, alpha = 0.3) + geom_vline(xintercept = 0.8, linetype = &quot;dashed&quot;, color = &quot;black&quot;, size = 1.5) + xlab(&quot;Rate&quot;) + ylab(&quot;Estimate Densities&quot;) + theme_classic() 5.3 as well as how the model was predicting the outcome to be before and after seeing the data. ggplot(draws_df) + geom_histogram(aes(prior_preds), color = &quot;darkblue&quot;, fill = &quot;blue&quot;, alpha = 0.3) + xlab(&quot;Predicted heads out of 120 trials&quot;) + ylab(&quot;Posterior Density&quot;) + theme_classic() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ggplot(draws_df) + geom_histogram(aes(posterior_preds), color = &quot;darkblue&quot;, fill = &quot;blue&quot;, alpha = 0.3, bins = 90) + geom_point(x = sum(data$h), y = 0, color = &quot;red&quot;, shape = 17, size = 5) + xlab(&quot;Predicted heads out of 120 trials&quot;) + ylab(&quot;Posterior Density&quot;) + theme_classic() ggplot(draws_df) + geom_histogram(aes(prior_preds), color = &quot;lightblue&quot;, fill = &quot;blue&quot;, alpha = 0.3, bins = 90) + geom_histogram(aes(posterior_preds), color = &quot;darkblue&quot;, fill = &quot;blue&quot;, alpha = 0.3, bins = 90) + geom_point(x = sum(data$h), y = 0, color = &quot;red&quot;, shape = 17, size = 5) + xlab(&quot;Predicted heads out of 120 trials&quot;) + ylab(&quot;Posterior Density&quot;) + theme_classic() "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
