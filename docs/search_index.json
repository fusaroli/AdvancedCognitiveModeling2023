[["index.html", "12-CategorizationModelsPrototypes Chapter 1 Advanced Cognitive Modeling 1.1 Course Philosophy and Approach 1.2 Course Structure and Learning Path 1.3 Prerequisites and Preparation 1.4 Course Resources 1.5 About These Notes", " 12-CategorizationModelsPrototypes Riccardo Fusaroli 2025-03-21 Chapter 1 Advanced Cognitive Modeling These course notes support the Advanced Cognitive Modeling course taught in the Master’s program in Cognitive Science at Aarhus University. The course represents a journey into how we can understand cognitive processes through the formalization and implementation of hypothesized mechanisms, their testing and validation. 1.1 Course Philosophy and Approach Advanced cognitive modeling focuses on three interrelated objectives that shape how we approach the modeling of cognitive processes: The first objective centers on understanding the thought process behind model development. Rather than simply providing a toolbox of existing scripts, we explore how cognitive models are conceptualized and constructed from the ground up. This approach ensures you develop the skills to create novel models for unique research questions. The second objective emphasizes mastering the Bayesian workflow essential for robust model development. This workflow encompasses simulation design, prior assessment, parameter recovery testing, and thorough model fit evaluation. These skills ensure your models are not just theoretically sound but also practically reliable and generalize way beyond cognitive modeling. The third objective focuses on developing advanced probabilistic modeling capabilities. Through hands-on experience with Stan, you will learn to implement increasingly sophisticated models while maintaining scientific rigor. 1.2 Course Structure and Learning Path The course follows a carefully structured progression that builds your modeling capabilities step by step: After a deepdive into the physics of pizza ovens, we begin with simple scenarios that introduce fundamental modeling concepts. Each subsequent chapter introduces new modeling techniques while building upon previous knowledge. This cumulative approach ensures you develop a deep understanding of both basic principles and advanced applications. The chapters include theoretical discussions paired with practical coding exercises. During practical sessions, we work with real datasets, design models collaboratively, and implement them using modern statistical tools. This hands-on approach provides ample opportunity for questions and exploration. The course schedule maintains flexibility to adapt to the collective learning pace of each cohort. While we have clear learning objectives, we ensure everyone develops a solid foundation before moving to more advanced topics. 1.3 Prerequisites and Preparation To make the most of this course, students should prepare their technical environment and review fundamental concepts: Software Requirements: - R (version 4.4 or above) - RStudio (version 2024.12.0 or above) - brms package with proper configuration - cmdstanr package with complete installation Technical Prerequisites: - Working knowledge of R programming - Basic understanding of Bayesian statistics - Familiarity with cognitive science fundamentals Additional Resources: - Introduction to R and tidyverse: https://r4ds.had.co.nz/ - A condensed Bayesian statistics primer (by Chris Cox and me): https://4ccoxau.github.io/PriorsWorkshop/ 1.4 Course Resources The course materials include: - Lecture notes and presentations - Practical exercise guides - Example code and solutions - Additional readings and references For comprehensive information: - Course syllabus: [TBA] - Lecture videos: [TBA] 1.5 About These Notes These notes represent an evolving resource that builds upon previous iterations of the course while incorporating new developments in the field. They are designed to serve both as a learning guide during the course and as a reference for your future research endeavors. knitr::opts_chunk$set( warning = FALSE, # Suppress warnings message = FALSE, # Suppress package loading messages echo = TRUE, # Show R code fig.width = 8, # Set default figure width fig.height = 5, # Set default figure height fig.align = &#39;center&#39;, # Center figures out.width = &quot;80%&quot;, # Make figures 80% of text width dpi = 300 # Set high resolution for figures ) "],["foundations.html", "Chapter 2 Foundations 2.1 From Pizza to Cognitive Models: An Introduction 2.2 Why Start with Pizza? 2.3 Learning Objectives 2.4 Part 1: Exploring the Pizza Stone Temperature Data 2.5 Part 2: Initial Statistical Modeling 2.6 Part 3: Understanding the Physics Model 2.7 Part 4: Implementing the Physics-Based Model 2.8 Part 5: Model Analysis and Practical Applications 2.9 Conclusion: From Pizza to Principles", " Chapter 2 Foundations 2.1 From Pizza to Cognitive Models: An Introduction This chapter introduces core modeling concepts through an unexpected lens: the physics of pizza stone heating. While this might seem far removed from cognitive science, it provides an insightful introduction to the challenges and methodologies of modeling complex phenomena. 2.2 Why Start with Pizza? Do I even need to answer that question? Because pizza, obviously. In any case, understanding how humans think and make decisions is arguably one of the most complex challenges in science. Rather than diving directly into this complexity, we begin with a more tractable problem: modeling how a pizza stone heats up in an oven. This seemingly simple process introduces us to key modeling concepts: The importance of selecting appropriate levels of analysis The role of prior knowledge in model development The challenge of balancing model complexity with practical utility The necessity of rigorous validation approaches Through this concrete example, we can focus on understanding modeling principles without the added complexity of cognitive theory. 2.3 Learning Objectives This first chpater is a bit odd, in that it pushes you straight into the deep waters of a complex example. I don’t expect you to understand all the technicalities. But, by completing this tutorial, you will be able to better grasp the importance of generative modeling, that is, of modeling that is focused on the underlying mechanisms producing the data. On the side you might learn something about how to * Implement physics-based thermal modeling using R and Stan * Apply Bayesian inference to real-world temperature data * Compare different statistical models using posterior predictions * Create professional visualizations of temperature evolution * Make practical predictions about heating times under various conditions Oh, and you’ll probably get hungry as well! Required Packages required_packages &lt;- c( &quot;tidyverse&quot;, # For data manipulation and visualization &quot;brms&quot;, # For Bayesian regression modeling &quot;bayesplot&quot;, # For visualization of Bayesian models &quot;tidybayes&quot;, # For working with Bayesian samples &quot;cmdstanr&quot; # For Stan implementation ) # Install and load packages for (pkg in required_packages) { if (!require(pkg, character.only = TRUE)) { install.packages(pkg) library(pkg, character.only = TRUE) } } 2.4 Part 1: Exploring the Pizza Stone Temperature Data In this study, we collected temperature measurements from a pizza stone in a gas-fired oven using an infrared temperature gun. Three different raters (N, TR, and R) took measurements over time to track how the stone heated up. Understanding how pizza stones heat up is crucial for achieving the perfect pizza crust, as consistent and sufficient stone temperature is essential for proper baking. The measurements were taken as follows: # Load and examine the data data &lt;- tibble( Order = rep(0:18, 3), Seconds = rep(c(0, 175, 278, 333, 443, 568, 731, 773, 851, 912, 980, 1040, 1074, 1124, 1175, 1237, 1298, 1359, 1394), 3), Temperature = c(15.1, 233, 244, 280, 289, 304, 343, NA, 333, 341, 320, 370, 325, 362, 363, 357, 380, 376, 380, 14.5, 139.9, 153, 36.1, 254, 459, 263, 369, rep(NA, 11), 12.9, 149.5, 159, 179.4, 191.7, 201, 210, NA, 256, 257, 281, 293, 297, 309, 318, 321, rep(NA, 3)), Rater = rep(c(&quot;N&quot;, &quot;TR&quot;, &quot;R&quot;), each = 19) ) # Create summary statistics summary_stats &lt;- data %&gt;% group_by(Rater) %&gt;% summarize( n_measurements = sum(!is.na(Temperature)), mean_temp = mean(Temperature, na.rm = TRUE), sd_temp = sd(Temperature, na.rm = TRUE), min_temp = min(Temperature, na.rm = TRUE), max_temp = max(Temperature, na.rm = TRUE) ) # Display summary statistics knitr::kable(summary_stats, digits = 1) Rater n_measurements mean_temp sd_temp min_temp max_temp N 18 312.0 86.4 15.1 380 R 15 229.0 83.9 12.9 321 TR 8 211.1 155.2 14.5 459 2.4.1 Initial Data Visualization Let’s visualize how the temperature evolves over time for each rater: ggplot(data, aes(x = Seconds/60, y = Temperature, color = Rater)) + geom_point(size = 3, alpha = 0.7) + geom_line(alpha = 0.5) + labs( title = &quot;Pizza Stone Temperature Evolution&quot;, subtitle = &quot;Measurements by three different raters&quot;, x = &quot;Time (minutes)&quot;, y = &quot;Temperature (°C)&quot;, color = &quot;Rater&quot; ) + theme_bw() + scale_color_brewer(palette = &quot;Set1&quot;) 2.4.2 Key Observations Several interesting patterns emerge from our data: Heating Patterns: The temperature generally increases over time, but not uniformly. We observe some fluctuations that might be due to: Variation in gas flame intensity Different measurement locations on the stone Measurement technique differences between raters Measurement Patterns by Rater Rater N maintained consistent measurements throughout the experiment Rater TR shows more variability and fewer total measurements Rater R shows a more gradual temperature increase pattern Missing Data: Some measurements are missing (NA values), particularly in the later time points for Rater TR. This is common in real-world data collection and needs to be considered in our analysis. Let’s examine the rate of temperature change: # Calculate temperature change rate data_with_rate &lt;- data %&gt;% group_by(Rater) %&gt;% arrange(Seconds) %&gt;% mutate( temp_change = (Temperature - lag(Temperature)) / (Seconds - lag(Seconds)) * 60, minutes = Seconds/60 ) %&gt;% filter(!is.na(temp_change)) # Visualize temperature change rate ggplot(data_with_rate, aes(x = minutes, y = temp_change, color = Rater)) + geom_point() + geom_smooth(se = FALSE, span = 0.75) + labs( title = &quot;Rate of Temperature Change Over Time&quot;, subtitle = &quot;Degrees Celsius per minute&quot;, x = &quot;Time (minutes)&quot;, y = &quot;Temperature Change Rate (°C/min)&quot;, color = &quot;Rater&quot; ) + theme_bw() + scale_color_brewer(palette = &quot;Set1&quot;) This visualization reveals that the heating rate is highest in the first few minutes and gradually decreases as the stone temperature approaches the oven temperature. This aligns with Newton’s Law of Cooling/Heating, which we will explore in the next section. 2.5 Part 2: Initial Statistical Modeling Before developing our physics-based model, let’s explore how standard statistical approaches perform in modeling our temperature data. We’ll implement two types of models using the brms package: a linear mixed-effects model and a lognormal mixed-effects model. Both models will account for variations between raters. 2.5.1 Model Setup and Priors First, let’s ensure we have a directory for our models and set up our computational parameters: # Create models directory if it doesn&#39;t exist dir.create(&quot;models&quot;, showWarnings = FALSE) # Define computational parameters mc_settings &lt;- list( chains = 2, iter = 6000, seed = 123, backend = &quot;cmdstanr&quot; ) 2.5.2 Linear Mixed-Effects Model We begin with a linear mixed-effects model, which assumes that temperature increases linearly with time but allows for different patterns across raters. This model includes both fixed effects (overall time trend) and random effects (rater-specific variations). # Define priors for linear model linear_priors &lt;- c( prior(normal(15, 20), class = &quot;Intercept&quot;), # Centered around room temperature prior(normal(0, 1), class = &quot;b&quot;), # Expected temperature change per second prior(normal(0, 100), class = &quot;sigma&quot;), # Residual variation prior(normal(0, 100), class = &quot;sd&quot;), # Random effects variation prior(lkj(3), class = &quot;cor&quot;) # Random effects correlation ) # Fit linear mixed-effects model linear_model &lt;- brm( Temperature ~ Seconds + (1 + Seconds | Rater), data = data, family = gaussian, prior = linear_priors, chains = mc_settings$chains, iter = mc_settings$iter, seed = mc_settings$seed, backend = mc_settings$backend, file = &quot;models/01_pizza_linear_model&quot;, cores = 2, adapt_delta = 0.99, max_treedepth = 20 ) # Display model summary summary(linear_model) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Temperature ~ Seconds + (1 + Seconds | Rater) ## Data: data (Number of observations: 41) ## Draws: 2 chains, each with iter = 6000; warmup = 3000; thin = 1; ## total post-warmup draws = 6000 ## ## Multilevel Hyperparameters: ## ~Rater (Number of levels: 3) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 88.18 49.29 14.36 210.34 1.00 1804 1750 ## sd(Seconds) 0.70 0.57 0.15 2.34 1.00 1226 1398 ## cor(Intercept,Seconds) -0.03 0.38 -0.71 0.67 1.00 2106 3251 ## ## Regression Coefficients: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 75.42 71.38 -110.73 194.77 1.00 1461 1136 ## Seconds -0.08 0.11 -0.25 0.19 1.00 1406 1133 ## ## Further Distributional Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 58.36 7.51 46.01 75.32 1.00 3735 3662 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). # Generate predictions linear_preds &lt;- fitted( linear_model, newdata = data, probs = c(0.025, 0.975) ) %&gt;% as_tibble() %&gt;% bind_cols(data) 2.5.3 Lognormal Mixed-Effects Model The lognormal model accounts for the fact that temperature changes might be proportional rather than additive, and ensures predictions cannot go below zero (I don’t bring my oven out in the freezing cold!). # Define priors for lognormal model lognormal_priors &lt;- c( prior(normal(2.7, 1), class = &quot;Intercept&quot;), # Log scale for room temperature prior(normal(0, 0.01), class = &quot;b&quot;), # Expected log-scale change per second prior(normal(0, 1), class = &quot;sigma&quot;), # Log-scale residual variation prior(normal(0, 1), class = &quot;sd&quot;), # Random effects variation prior(lkj(3), class = &quot;cor&quot;) # Random effects correlation ) # Fit lognormal mixed-effects model lognormal_model &lt;- brm( Temperature ~ Seconds + (1 + Seconds | Rater), data = data, family = lognormal, prior = lognormal_priors, chains = mc_settings$chains, cores = 2, adapt_delta = 0.99, max_treedepth = 20, iter = mc_settings$iter, seed = mc_settings$seed, backend = mc_settings$backend, file = &quot;models/01_pizza_lognormal_model&quot; ) # Generate predictions lognormal_preds &lt;- fitted( lognormal_model, newdata = data, probs = c(0.025, 0.975) ) %&gt;% as_tibble() %&gt;% bind_cols(data) 2.5.4 Model Comparison and Visualization Let’s compare how these models fit our data: # Compare models using LOO model_comparison &lt;- loo_compare( loo(linear_model), loo(lognormal_model) ) # Create comparison plot ggplot() + # Raw data points geom_point(data = data, aes(x = Seconds/60, y = Temperature, color = Rater), alpha = 0.5) + # Linear model predictions geom_line(data = linear_preds, aes(x = Seconds/60, y = Estimate, linetype = &quot;Linear&quot;), color = &quot;blue&quot;) + geom_ribbon(data = linear_preds, aes(x = Seconds/60, ymin = Q2.5, ymax = Q97.5), fill = &quot;blue&quot;, alpha = 0.1) + # Lognormal model predictions geom_line(data = lognormal_preds, aes(x = Seconds/60, y = Estimate, linetype = &quot;Lognormal&quot;), color = &quot;red&quot;) + geom_ribbon(data = lognormal_preds, aes(x = Seconds/60, ymin = Q2.5, ymax = Q97.5), fill = &quot;red&quot;, alpha = 0.1) + # Formatting facet_wrap(~Rater) + labs( title = &quot;Comparison of Statistical Models&quot;, subtitle = &quot;Linear vs Lognormal Mixed-Effects Models&quot;, x = &quot;Time (minutes)&quot;, y = &quot;Temperature (°C)&quot;, linetype = &quot;Model Type&quot; ) + theme_bw() # Create comparison plot but capping the y axis ggplot() + # Raw data points geom_point(data = data, aes(x = Seconds/60, y = Temperature, color = Rater), alpha = 0.5) + # Linear model predictions geom_line(data = linear_preds, aes(x = Seconds/60, y = Estimate, linetype = &quot;Linear&quot;), color = &quot;blue&quot;) + geom_ribbon(data = linear_preds, aes(x = Seconds/60, ymin = Q2.5, ymax = Q97.5), fill = &quot;blue&quot;, alpha = 0.1) + # Lognormal model predictions geom_line(data = lognormal_preds, aes(x = Seconds/60, y = Estimate, linetype = &quot;Lognormal&quot;), color = &quot;red&quot;) + geom_ribbon(data = lognormal_preds, aes(x = Seconds/60, ymin = Q2.5, ymax = Q97.5), fill = &quot;red&quot;, alpha = 0.1) + ylim(0, 1000) + # Formatting facet_wrap(~Rater) + labs( title = &quot;Comparison of Statistical Models&quot;, subtitle = &quot;Linear vs Lognormal Mixed-Effects Models&quot;, x = &quot;Time (minutes)&quot;, y = &quot;Temperature (°C)&quot;, linetype = &quot;Model Type&quot; ) + theme_bw() 2.5.5 Model Assessment I have seen worse models in my time, but they do seem to have important issues: The linear mixed-effects model assumes a constant rate of temperature change, which we can see is not at all accurate. The actual temperature increase is fast at the beginning and appears to slow down over time, particularly at higher temperatures. While this model has the advantage of simplicity, it is not likely to produce accurate predictions as it seem to fail to capture the underlying physics of heat transfer. The lognormal mixed-effects model is completely off. Further, the models produce some divergences, which is often a sign that they are not well suited to the data. I suggest that the issue is that neither model incorporates our knowledge of heat transfer physics, which suggests an exponential approach to equilibrium temperature. This limitation motivates our next section, where we’ll develop a physics-based model. 2.6 Part 3: Understanding the Physics Model Temperature evolution in a pizza stone follows Newton’s Law of Cooling/Heating. We’ll start by exploring this physical model before applying it to real data. 2.6.1 The Basic Temperature Evolution Equation The temperature evolution of a pizza stone in a gas-fired oven is governed by the heat diffusion equation, which describes how heat flows through solid materials: \\[\\rho c_p \\frac{\\partial T}{\\partial t} = k\\nabla^2T + Q\\] where: \\(\\rho\\) represents the stone’s density (kg/m³) \\(c_p\\) denotes specific heat capacity (J/kg·K) \\(T\\) is temperature (K) \\(t\\) represents time (s) \\(k\\) is thermal conductivity (W/m·K) \\(\\nabla^2\\) is the Laplacian operator \\(Q\\) represents heat input from the oven (W/m³) While this equation provides a complete description of heat flow, we can significantly simplify our analysis by applying the lumped capacitance model. This simplification assumes that the temperature throughout the pizza stone remains uniform at any given time - not perfect, but a reasonable assumption given the stone’s relatively thin profile and good thermal conductivity. This approach reduces our model to: \\[\\frac{dT}{dt} = \\frac{hA}{mc_p}(T_{\\infty} - T)\\] where: \\(h\\) is the heat transfer coefficient (W/m²·K) \\(A\\) is the surface area exposed to heat (m²) \\(m\\) is the stone’s mass (kg) \\(T_{\\infty}\\) is the oven temperature (K) This simplified equation relates the rate of temperature change to the difference between the current stone temperature T and the flame temperature T∞. The coefficient h represents the heat transfer coefficient between the flame and stone, A is the stone’s surface area exposed to heat, m is its mass, and cp remains the specific heat capacity. To solve this differential equation, we begin by separating variables: \\[\\frac{dT}{T_{\\infty} - T} = \\left(\\frac{hA}{mc_p}\\right)dt\\] Integration of both sides yields: \\[-\\ln|T_{\\infty} - T| = \\left(\\frac{hA}{mc_p}\\right)t + C\\] where C is an integration constant. Using the initial condition \\(T = T_i\\) at \\(t = 0\\), we can determine the integration constant: \\[C = -\\ln|T_{\\infty} - T_i|\\] Substituting this back and solving for temperature gives us: \\[T = T_{\\infty} + (T_i - T_{\\infty})\\exp\\left(-\\frac{hA}{mc_p}t\\right)\\] For practical reasons, we combine physical parameters into a single coefficient \\(\\theta\\): \\[HOT = \\frac{hA}{mc_p}\\] Giving our working equation: \\[T = T_{\\infty} + (T_i - T_{\\infty})\\exp(-HOT * t)\\] This equation retains the essential physics while providing a practical model for analyzing our experimental data. The HOT coefficient encapsulates the combined effects of heat transfer efficiency, stone geometry, and material properties into a single parameter that determines how quickly the stone approaches the flame temperature. 2.7 Part 4: Implementing the Physics-Based Model Having established the theoretical foundation for our heat transfer model, we now move to its practical implementation. We will use Stan to create a Bayesian implementation of our physics-based model, allowing us to account for measurement uncertainty and variation between raters. First, we prepare our data for the Stan model. Our model requires initial temperatures, time measurements, and observed temperatures from each rater: # Create data structure for Stan stan_data &lt;- list( N = nrow(data %&gt;% filter(!is.na(Temperature))), time = data %&gt;% filter(!is.na(Temperature)) %&gt;% pull(Seconds), temp = data %&gt;% filter(!is.na(Temperature)) %&gt;% pull(Temperature), n_raters = 3, rater = as.numeric(factor(data %&gt;% filter(!is.na(Temperature)) %&gt;% pull(Rater))), Ti = c(100, 100, 100), # Initial temperature estimates Tinf = 450 # Flame temperature estimate ) Next, we implement our physics-based model in Stan. The model incorporates our derived equation while allowing for rater-specific heating coefficients: stan_code &lt;- &quot; data { int&lt;lower=0&gt; N; // Number of observations vector[N] time; // Time points vector[N] temp; // Observed temperatures int&lt;lower=0&gt; n_raters; // Number of raters array[N] int&lt;lower=1,upper=n_raters&gt; rater; // Rater indices vector[n_raters] Ti; // Initial temperatures real Tinf; // Flame temperature } parameters { vector&lt;lower=0&gt;[n_raters] HOT; // Heating coefficients vector&lt;lower=0&gt;[n_raters] sigma; // Measurement error } model { vector[N] mu; // Physics-based temperature prediction for (i in 1:N) { mu[i] = Tinf + (Ti[rater[i]] - Tinf) * exp(-HOT[rater[i]] * time[i]); } // Prior distributions target += normal_lpdf(HOT | 0.005, 0.005); // Prior for heating rate target += exponential_lpdf(sigma | 1); // Prior for measurement error // Likelihood target += normal_lpdf(temp | mu, sigma[rater]); } &quot; # Save the model writeLines(stan_code, &quot;models/pizza_physics_model.stan&quot;) # Compile and fit the model mod &lt;- cmdstan_model(&quot;models/pizza_physics_model.stan&quot;) fit &lt;- mod$sample( data = stan_data, seed = 123, chains = 2, parallel_chains = 2 ) ## Running MCMC with 2 parallel chains... ## ## Chain 1 Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1 Iteration: 100 / 2000 [ 5%] (Warmup) ## Chain 1 Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1 Iteration: 300 / 2000 [ 15%] (Warmup) ## Chain 1 Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1 Iteration: 500 / 2000 [ 25%] (Warmup) ## Chain 1 Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1 Iteration: 700 / 2000 [ 35%] (Warmup) ## Chain 1 Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1 Iteration: 900 / 2000 [ 45%] (Warmup) ## Chain 1 Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1 Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1 Iteration: 1100 / 2000 [ 55%] (Sampling) ## Chain 1 Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1 Iteration: 1300 / 2000 [ 65%] (Sampling) ## Chain 1 Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1 Iteration: 1500 / 2000 [ 75%] (Sampling) ## Chain 1 Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1 Iteration: 1700 / 2000 [ 85%] (Sampling) ## Chain 1 Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1 Iteration: 1900 / 2000 [ 95%] (Sampling) ## Chain 1 Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2 Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2 Iteration: 100 / 2000 [ 5%] (Warmup) ## Chain 2 Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2 Iteration: 300 / 2000 [ 15%] (Warmup) ## Chain 2 Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2 Iteration: 500 / 2000 [ 25%] (Warmup) ## Chain 2 Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2 Iteration: 700 / 2000 [ 35%] (Warmup) ## Chain 2 Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2 Iteration: 900 / 2000 [ 45%] (Warmup) ## Chain 2 Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2 Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2 Iteration: 1100 / 2000 [ 55%] (Sampling) ## Chain 2 Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2 Iteration: 1300 / 2000 [ 65%] (Sampling) ## Chain 2 Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2 Iteration: 1500 / 2000 [ 75%] (Sampling) ## Chain 2 Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2 Iteration: 1700 / 2000 [ 85%] (Sampling) ## Chain 2 Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2 Iteration: 1900 / 2000 [ 95%] (Sampling) ## Chain 2 Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1 finished in 0.1 seconds. ## Chain 2 finished in 0.1 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.1 seconds. ## Total execution time: 0.2 seconds. The Stan implementation translates our mathematical model into a computational framework. We assign informative priors to our parameters based on physical understanding: the heating coefficient (HOT) is expected to be small but positive, while measurement error (sigma) follows an exponential distribution to ensure positivity while allowing for varying levels of uncertainty between raters. To visualize our model’s predictions and assess its performance, we extract posterior samples and generate predictions across our time range: # Extract draws post &lt;- as_draws_df(fit$draws()) %&gt;% dplyr::select(starts_with(&quot;HOT&quot;), starts_with(&quot;sigma&quot;)) %&gt;% slice_sample(n = 100) # Create prediction grid pred_data &lt;- tidyr::crossing( time = seq(0, max(stan_data$time), length.out = 100), rater = 1:stan_data$n_raters ) %&gt;% mutate( Ti = stan_data$Ti[rater], Tinf = stan_data$Tinf ) # Generate predictions pred_matrix &lt;- matrix(NA, nrow = nrow(pred_data), ncol = 100) for (i in 1:nrow(pred_data)) { pred_matrix[i,] &lt;- with(pred_data[i,], Tinf + (Ti - Tinf) * exp(-as.matrix(post)[,rater] * time)) } # Summarize predictions predictions &lt;- pred_data %&gt;% mutate( mean = rowMeans(pred_matrix), lower = apply(pred_matrix, 1, quantile, 0.025), upper = apply(pred_matrix, 1, quantile, 0.975) ) # Create visualization ggplot(predictions, aes(x = time/60)) + geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) + geom_line(aes(y = mean)) + geom_point( data = data %&gt;% filter(!is.na(Temperature)) %&gt;% mutate(rater = case_when( Rater == &quot;N&quot; ~ 1, Rater == &quot;TR&quot; ~ 2, Rater == &quot;R&quot; ~ 3 )), aes(x = Seconds/60, y = Temperature) ) + facet_wrap(~rater, labeller = labeller(rater = c( &quot;1&quot; = &quot;Rater N&quot;, &quot;2&quot; = &quot;Rater TR&quot;, &quot;3&quot; = &quot;Rater R&quot; ))) + labs( title = &quot;Physics-Based Model Predictions&quot;, x = &quot;Time (minutes)&quot;, y = &quot;Temperature (°C)&quot; ) + theme_bw() Our implementation combines the theoretical understanding developed in Part 3 with practical considerations for real-world data analysis. The model accounts for measurement uncertainty while maintaining the fundamental physics of heat transfer, providing a robust framework for understanding pizza stone temperature evolution. 2.8 Part 5: Model Analysis and Practical Applications Having implemented our physics-based model, we can now analyze its predictions and develop practical insights for pizza stone temperature management. A key question for pizza making is how long it takes to reach optimal cooking temperatures under different conditions. We begin by creating a function that calculates the time needed to reach a target temperature: time_to_temp &lt;- function(target_temp, HOT, Ti, Tinf) { # Solve: target = Tinf + (Ti - Tinf) * exp(-HOT * t) # for t t = -1/HOT * log((target_temp - Tinf)/(Ti - Tinf)) return(t/60) # Convert seconds to minutes } To understand heating times across different oven conditions, we examine how varying flame temperatures affect the time needed to reach pizza-making temperatures. We extract the heating coefficients from our fitted model and analyze temperature scenarios: # Extract HOT samples from our posterior hot_samples &lt;- as_draws_df(fit$draws()) %&gt;% dplyr::select(starts_with(&quot;HOT&quot;)) # Create prediction grid for different flame temperatures pred_data &lt;- tidyr::crossing( Tinf = seq(450, 1200, by = 50), # Range of flame temperatures rater = 1:3 ) %&gt;% mutate( Ti = stan_data$Ti[rater], target_temp = 400 # Target temperature for pizza cooking ) # Calculate heating times across conditions n_samples &lt;- 100 time_preds &lt;- map_dfr(1:nrow(pred_data), function(i) { times &lt;- sapply(1:n_samples, function(j) { hot &lt;- hot_samples[j, paste0(&quot;HOT[&quot;, pred_data$rater[i], &quot;]&quot;)][[1]] time_to_temp( pred_data$target_temp[i], hot, pred_data$Ti[i], pred_data$Tinf[i] ) }) data.frame( rater = pred_data$rater[i], Tinf = pred_data$Tinf[i], mean_time = mean(times), lower = quantile(times, 0.025), upper = quantile(times, 0.975) ) }) # Visualize heating time predictions ggplot(time_preds, aes(x = Tinf)) + geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) + geom_line(aes(y = mean_time)) + facet_wrap(~rater, labeller = labeller(rater = c( &quot;1&quot; = &quot;Rater N&quot;, &quot;2&quot; = &quot;Rater TR&quot;, &quot;3&quot; = &quot;Rater R&quot; ))) + labs( title = &quot;Time Required to Reach Pizza-Making Temperature&quot;, subtitle = &quot;Target temperature: 400°C&quot;, x = &quot;Flame Temperature (°C)&quot;, y = &quot;Minutes to reach target&quot; ) + theme_bw() Our analysis reveals several important insights for practical pizza making. First, the heating time decreases nonlinearly with flame temperature, showing diminishing returns at very high temperatures. We can also observe differences between raters in their measured heating times. These variations likely stem from differences in measurement technique and location on the stone, highlighting the importance of consistent temperature monitoring practices. For practical application, we can provide specific heating guidelines based on our model. At a typical flame temperature of 800°C, the model predicts it will take approximately 20-30 minutes to reach optimal pizza-making temperature, assuming room temperature start. However, this time can vary significantly based on: Initial stone temperature Flame temperature and consistency Environmental conditions. Can we really wait that long? 2.9 Conclusion: From Pizza to Principles The journey from modeling a heating pizza stone to understanding cognitive processes might seem unusual, but it illustrates fundamental principles that will guide us throughout this course. Through this seemingly simple physics problem, we have encountered the core challenges that cognitive scientists face daily. Just relying on standard statistical models is not enough. We need to understand the underlying generative processes. We discovered how choosing the right level of analysis shapes our understanding - just as we simplified complex heat equations into workable models, cognitive scientists must decide which aspects of the mental processes to model explicitly and which to abstract. We learned that even well-understood physical processes require careful statistical treatment, foreshadowing the challenges we will face with more complex cognitive phenomena. The pizza stone experiment also demonstrated the importance of rigorous methodology. We saw how multiple measurements from different raters revealed variability in our data, leading us to consider measurement error and individual differences - themes that will become crucial when studying human behavior. Our exploration of different statistical approaches, from simple linear models to more sophisticated Bayesian frameworks, established a foundation for the modeling techniques we will develop throughout this course. Perhaps most importantly, this chapter starts showing that successful modeling requires balancing competing demands. We must weigh theoretical complexity against practical utility, statistical sophistication against interpretability, and mathematical elegance against real-world applicability. These trade-offs will become even more prominent as we move into modeling cognitive processes. As we progress through this course, we will encounter increasingly complex cognitive phenomena. The principles we learned here - careful data collection, thoughtful model specification, rigorous validation, and balanced interpretation - will serve as our guide. While human cognition presents challenges far beyond those of heating pizza stones, the fundamental approach remains the same: start with clear observations, build theoretically motivated models, and test them systematically against data. In the next chapter, we will begin applying these principles directly to cognitive processes, starting with simple decision-making tasks. The mathematical tools and statistical frameworks introduced here will provide the foundation for understanding how humans process information and make choices. Finally, I hope you are hungry now. I know I am. Let’s go and make some pizza! "],["building-models-of-strategic-decision-making.html", "Chapter 3 Building Models of Strategic Decision-Making 3.1 Learning goals 3.2 Introduction 3.3 The Matching Pennies Game 3.4 Game Structure 3.5 Empirical Investigation 3.6 Empirical explorations 3.7 Notes from previous years 3.8 Building Formal Models 3.9 Cognitive constraints 3.10 Continuity between models 3.11 Mixture of strategies 3.12 Differences from more traditional (general linear model-based) approaches", " Chapter 3 Building Models of Strategic Decision-Making 3.1 Learning goals Becoming more aware of the issue involved in theory building (and assessment); Identifying a small set of verbal models that we can then formalize in mathematical cognitive models and algorithms for simulations and model fitting. 3.2 Introduction In order to do computational models we need a phenomenon to study (and ideally some data), throughout the course you will be asked undergo several experiments, which provides specific behaviors to model. The matching pennies game provides a fun starting point for exploring cognitive modeling. This simple game allows us to examine how humans make decisions in strategic situations, while introducing fundamental concepts in model development and validation. Through this chapter, we will progress from observing actual gameplay behavior to developing formal models that capture decision-making processes. 3.3 The Matching Pennies Game In the matching pennies game, two players engage in a series of choices. One player attempts to match the other’s choice, while the other player aims to achieve a mismatch, and they repeatedly play with each other. This is a prototypical example of interacting behaviors that are usually tackled by game theory, and bring up issues of theory of mind and recursivity. For an introduction see the paper: Waade, Peter T., et al. “Introducing tomsup: Theory of mind simulations using Python.” Behavior Research Methods 55.5 (2023): 2197-2231. 3.4 Game Structure The game proceeds as follows: Two players sit facing each other Each round, both players choose either “left” or “right” to indicate where they believe a penny is hidden The matcher wins by choosing the same hand as their opponent The hider wins by choosing the opposite hand Points are awarded: +1 for winning, -1 for losing Repeat This simple structure creates a rich environment for studying decision-making strategies, learning, and adaptation. 3.5 Empirical Investigation 3.5.1 Data Collection Protocol If you are attending my class you have been (or will be) asked to participate in a matching pennies game. This game provides the foundation for our modeling efforts. By observing gameplay and collecting data, we can develop models that capture the cognitive processes underlying decision-making in strategic situations. Participants play 30 rounds as the matcher and 30 rounds as the hider, allowing us to observe behavior in both roles. While playing, participants track their scores, which can provide quantitative data for later analysis. Participants are also asked to reflect on their strategies and the strategies they believe their opponents are using, as that provides valuable materials to build models on. 3.5.2 Initial Observations Through the careful observation and discussion of gameplay we do in class, several patterns typically emerge. For instance, players often demonstrate strategic adaptation, adjusting their choices based on their opponent’s previous moves. They may attempt to identify patterns in their opponent’s behavior while trying to make their own choices less predictable. The tension between exploitation of perceived patterns and maintenance of unpredictability creates fascinating dynamics for modeling. 3.6 Empirical explorations Below you can observe how a previous year of CogSci did against bots (computational agents) playing according to different strategies. Look at the plots below, where the x axes indicate trial, the y axes how many points the CogSci’ers scored (0 being chance, negative means being completely owned by the bots, positive owning the bot) and the different colors indicate different strategies employed by the bots. Strategy “-2” was a Win-Stay-Lose-Shift bot: when it got a +1, it repeated its previous move (e.g. right if it had just played right), otherwise it would perform the opposite move (e.g. left if it had just played right). Strategy “-1” was a biased Nash both, playing “right” 80% of the time. Strategy “0” indicates a reinforcement learning bot; “1” a bot assuming you were playing according to a reinforcement learning strategy and trying to infer your learning and temperature parameters; “2” a bot assuming you were following strategy “1” and trying to accordingly infer your parameters. library(tidyverse) d &lt;- read_csv(&quot;data/MP_MSc_CogSci22.csv&quot;) %&gt;% mutate(BotStrategy = as.factor(BotStrategy)) d$Role &lt;- ifelse(d$Role == 0, &quot;Matcher&quot;, &quot;Hider&quot;) ggplot(d, aes(Trial, Payoff, group = BotStrategy, color = BotStrategy)) + geom_smooth(se = F) + theme_classic() + facet_wrap(.~Role) That doesn’t look too good, ah? What about individual variability? In the plot below we indicate the score of each of the former students, against the different bots. d1 &lt;- d %&gt;% group_by(ID, BotStrategy) %&gt;% dplyr::summarize(Score = sum(Payoff)) ggplot(d1, aes(BotStrategy, Score, label = ID)) + geom_point(aes(color = ID)) + geom_boxplot(alpha = 0.3) + theme_classic() Now, let’s take a bit of group discussion. Get together in groups, and discuss which strategies and cognitive processes might underlie your and the agents’ behaviors in the game. One thing to keep in mind is what a model is: a simplification that can help us make sense of the world. In other words, any behavior is incredibly complex and involves many complex cognitive mechanisms. So start simple, and if you think it’s too simple, progressively add simple components. Once your study group has discussed a few (during the PE), let’s discuss them. 3.7 Notes from previous years 3.7.1 From Observation to Theory The transition from observing gameplay to building formal models requires careful consideration of multiple factors. We must identify which aspects of behavior to model explicitly while deciding which details can be abstracted away. 3.7.2 Core Modeling Considerations When developing models of matching pennies behavior, we must address several key questions: What information do players use to make decisions? How do players integrate past experiences with current choices? What role does randomness play in decision-making? How do players adapt their strategies over time? Are there notions and models from previous cognitive science courses that can help us understand the behavior? These questions guide our model development process, helping us move from verbal theories to mathematical formulations. 3.7.3 The distinction between participant and researcher perspectives As participants we might not be aware of the strategy we use, or we might believe something erroneous. The exercise here is to act as researchers: what are the principles underlying the participants’ behaviors, no matter what the participants know or believe? Note that talking to participants and being participants helps developing ideas, but it’s not the end point of the process. Also note that as cognitive scientists we can rely on what we have learned about cognitive processes (e.g. memory). Another important component of the distinction is that participants leave in a rich world: they rely on facial expressions and bodily posture, the switch strategies, etc. On the other hand, the researcher is trying to identify one or few at most “simple” strategies. Rich bodily interactions and mixtures or sequences of multiple strategies are not a good place to start modeling. These aspects are a poor starting point for building your first model, and are often pretty difficult to fit to empirical data. Nevertheless, they are important intuitions that the researcher should (eventually?) accommodate. 3.8 Building Formal Models Based on observed behavior patterns and theoretical considerations, we can develop several candidate models of decision-making in the matching pennies game. 3.8.1 Random Choice Model The simplest model assumes players make choices randomly, independent of history or context. Players might simply be randomly choosing “head” or “tail” independently on the opponent’s choices and of how well they are doing. Choices could be fully at random (50% “head”, 50% “tail”) or biased (e.g. 60% “head”, 40% tail). While this may seem overly simplistic, it provides an important baseline for comparison and introduces key concepts in model specification. 3.8.2 Immediate reaction (Win-Stay-Lose-Shift) Another simple strategy is simply to follow the previous choice: if it was successful keep it, if not change it. This strategy is also called Win-Stay-Lose-Shift (WSLS). The model can be formalized as: \\[P(a_t = a_{t-1}) = \\begin{cases} p_w &amp; \\text{if win at } t-1 \\ 1 - p_l &amp; \\text{if loss at } t-1 \\end{cases}\\] where \\(a_t\\) represents the action at time \\(t\\), and \\(p_w\\) and \\(p_l\\) are the probabilities of staying after wins and losses respectively. Alternatively, one could do the opposite: Win-Shift-Lose-Stay. 3.8.3 Keep track of the bias (perfect memory) A more sophisticated approach considers how players track and respond to their opponent’s choice patterns. This model maintains a running estimate of the opponent’s choice probabilities and updates these estimates based on observed choices. 3.8.4 Keep track of the bias (imperfect memory) A player could not be able to keep in mind all previous trials, or decide to forget old trials, in case the biase shifts over time. So we could use only the last n trials, or do a weighted mean with weigths proportional to temporal closeness (the more recent, the higher the weight). 3.8.5 Reinforcement learning Since there is a lot of leeway in how much memory we should keep of previous trials, we could also use a model that explicitly estimates how much players are learning on a trial by trial basis (high learning, low memory; low learning, high memory). This is the model of reinforcement learning, which we will deal with in future chapters. Shortly described, reinforcement learning assumes that each choice has a possible reward (probability of winning) and at every trial given the feedback received updates the expected value of the choice taken. The update depends on the prediction error (difference between expected and actual reward) and the learning rate. 3.8.6 k-ToM Reinforcement learning is a neat model, but can be problematic when playing against other agents: what the game is really about is not assessing the probability of the opponent choosing “head” generalizing from their past choices, but predicting what they will do. This requires making an explicit model of how the opponent chooses. k-ToM models will be dealt with in future chapters, but can be here anticipated as models assuming that the opponent follows a random bias (0-ToM), or models us as following a random bias (1-ToM), or models us modeling them as following a random bias (2-ToM), etc. 3.8.7 Other possible strategies Many additional strategies can be generated by combining former strategies. Generating random output is hard, so if we want to confuse the opponent, we could act first choosing tail 8 times, and then switching to a WSLS strategy for 4 trials, and then choosing head 4 times. Or implementing any of the previous strategies and doing the opposite “to mess with the opponent”. 3.9 Cognitive constraints As we discuss strategies, we can also identify several cognitive constraints that we know from former studies: in particular, memory, perseveration, and errors. 3.9.1 Memory Humans have limited memory and a tendency to forget that is roughly exponential. Models assuming perfect memory for longer stretches of trials are unrealistic. We could for instance use the exponential decay of memory to create weights following the same curve in the “keeping track of bias” models. Roughly, this is what reinforcement learning is doing via the learning rate parameter. 3.9.2 Perseveration Winning choice is not changed. People tend to have a tendency to perseverate with “good” choices independently of which other strategy they might be using. 3.9.3 Errors Humans make mistakes, get distracted, push the wrong button, forget to check whether they won or lost before. So a realistic model of what happens in these games should contain a certain chance of making a mistake. E.g. a 10% chance that any choice will be perfectly random instead of following the strategy. Such random deviations from the strategy might also be conceptualized as explorations: keeping the door open to the strategy not being optimal and therefore testing other choices. For instance, one could have an imperfect WSLS where the probability of staying if winning (or shifting if losing) is only 80% and not 100%. Further, these deviations could be asymmetric, with the probability of staying if winning is 80% and of shifting if losing is 100%; for instance if negative and positive feedback are perceived asymmetrically. 3.10 Continuity between models Many of these models are simply extreme cases of others. For instance, WSLS is a reinforcement learning model with an extreme learning rate (reward replaces the formerly expected value without any moderation), which is also a memory model with a memory of 1 previous trial. k-ToM builds on reinforcement learning: at level 1 assumes the other is a RL agent. 3.11 Mixture of strategies We discussed that there are techniques to consider the data generated by a mixture of models: estimating the probability that they are generated by model 1 or 2 or n. This probability can then be conditioned, according to our research question, to group (are people w schizophrenia more likely to employ model 1) or ID (are different participants using different models), or condition, or… We discussed that we often need lots of data to disambiguate between models, so conditioning e.g. on trial would in practice almost (?) never work. 3.12 Differences from more traditional (general linear model-based) approaches In a more traditional approach we would carefully set up the experiment to discriminate between hypotheses. For instance, if the hypothesis is that humans deploy ToM only when playing against intentional agents, we can set agents with increasing levels of k-ToM against humans, set up two framings (this is a human playing hide and seek, this is a slot machine), and assess whether humans perform differently. E.g. whether they perform better when thinking it’s a human. We analyze performance e.g. as binary outcome on a trial by trial base and condition its rate on framing and complexity. If framing makes a difference in the expected direction, we are good. If we do this properly, thanks to the clever experimental designs we set up, we can discriminate between hypotheses. And that is good. However, cognitive modeling opens additional opportunities. For instance, we can actually reconstruct which level of recursion the participants are enacting and if it changes over time. This might be very useful in the experimental setup, and crucial in more observational setups. Cognitive modeling also allows us to discriminate between different cognitive components more difficult to assess by looking at performance only. For instance, why are participants performing less optimally when facing a supposedly non-intentional agent? Is their learning rate different? Is their estimate of volatility different? In other setups, e.g. a gambling context, we might observe that some participants (e.g. parkinson’s patients) are gambling away much. Is this due to changes in their risk-seeking propensities, loss aversion, or changes in the ability to actually learn the reward structure? Experimental setups help, but cognitive modeling can provide more nuanced and direct evidence. "],["from-verbal-descriptions-to-formal-models.html", "Chapter 4 From verbal descriptions to formal models 4.1 Learning Goals 4.2 The Value of Formalization 4.3 Defining general conditions 4.4 Implementing a random agent 4.5 Implementing a Win-Stay-Lose-Shift agent 4.6 Now we scale it up 4.7 Conclusion", " Chapter 4 From verbal descriptions to formal models This chapter bridges the gap between verbal theories and computational implementations of cognitive models. Building on our observations of the matching pennies game, we now develop precise mathematical formulations that can generate testable predictions. 4.1 Learning Goals After completing this chapter, you will be able to: Transform verbal descriptions of decision-making strategies into precise mathematical formulations, which implications can be more easily explored and that can be empirically tested Create computational implementations of these mathematical models as agent-based models in R Generate and analyze simulated data to understand model behavior under different conditions 4.2 The Value of Formalization Moving from verbal to formal models represents a crucial step in cognitive science. When we describe behavior in words, ambiguities often remain hidden. For instance, a verbal description might state that players “tend to repeat successful choices.” But what exactly constitutes “tend to”? How strongly should past successes influence future choices? Mathematical formalization forces us to be precise about these specifications. By computationally implementing the our models, we are forced to make them very explicit in their assumptions; we become able to simulate the models in a variety of different situations and therefore better understand their implications So, what we’ll do throughout the chapter is to: choose two of the models and formalize them, that is, produce an algorithm that enacts the strategy, so we can simulate them. implement the algorithms as functions: getting an input and producing an output, so we can more easily implement them across various contexts (e.g. varying amount of trials, input, etc). See R4DataScience, if you need a refresher: https://r4ds.had.co.nz/functions.html implement a Random Bias agent (choosing “head” 70% of the times) and get your agents to play against it for 120 trials (and save the data) implement a Win-Stay-Lose-Shift agent (keeping the same choice if it won, changing it if it lost) and do the same. scale up the simulation: have 100 agents for each of your strategy playing against both Random Bias and Win-Stay-Lose-Shift and save their data. figure out a good way to visualize the data to assess which strategy performs better, whether that changes over time and generally explore what the agents are doing. 4.3 Defining general conditions pacman::p_load(tidyverse, patchwork) # Number of trials per simulation trials &lt;- 120 # Number of agents to simulate agents &lt;- 100 # Optional: Set random seed for reproducibility # set.seed(123) 4.4 Implementing a random agent Remember a random agent is an agent that picks at random between “right” and “left” independently on what the opponent is doing. A random agent might be perfectly random (50% chance of choosing “right”, same for “left”) or biased. The variable “rate” determines the rate of choosing “right”. rate &lt;- 0.5 RandomAgent &lt;- rbinom(trials, 1, rate) # we simply sample randomly from a binomial # Now let&#39;s plot how it&#39;s choosing d1 &lt;- tibble(trial = seq(trials), choice = RandomAgent) p1 &lt;- ggplot(d1, aes(trial, choice)) + geom_line() + labs( title = &quot;Random Agent Behavior (rate 0.5)&quot;, x = &quot;Trial Number&quot;, y = &quot;Choice (0/1)&quot; ) + theme_classic() p1 # What if we were to compare it to an agent being biased? rate &lt;- 0.8 RandomAgent &lt;- rbinom(trials, 1, rate) # we simply sample randomly from a binomial # Now let&#39;s plot how it&#39;s choosing d2 &lt;- tibble(trial = seq(trials), choice = RandomAgent) p2 &lt;- ggplot(d2, aes(trial, choice)) + geom_line() + labs( title = &quot;Biased Random Agent Behavior&quot;, x = &quot;Trial Number&quot;, y = &quot;Choice (0/1)&quot; ) + theme_classic() p1 + p2 print(&quot;This first visualization shows the behavior of a purely random agent - one that chooses between options with equal probability (rate = 0.5). Looking at the jagged line jumping between 0 and 1, we can see that the agent&#39;s choices appear truly random, with no discernible pattern. This represents what we might expect from a player who is deliberately trying to be unpredictable in the matching pennies game. However, this raw choice plot can be hard to interpret. A more informative way to look at the agent&#39;s behavior is to examine how its average rate of choosing option 1 evolves over time:&quot;) ## [1] &quot;This first visualization shows the behavior of a purely random agent - one that chooses between options with equal probability (rate = 0.5). Looking at the jagged line jumping between 0 and 1, we can see that the agent&#39;s choices appear truly random, with no discernible pattern. This represents what we might expect from a player who is deliberately trying to be unpredictable in the matching pennies game.\\nHowever, this raw choice plot can be hard to interpret. A more informative way to look at the agent&#39;s behavior is to examine how its average rate of choosing option 1 evolves over time:&quot; # Tricky to see, let&#39;s try writing the cumulative rate: d1$cumulativerate &lt;- cumsum(d1$choice) / seq_along(d1$choice) d2$cumulativerate &lt;- cumsum(d2$choice) / seq_along(d2$choice) p3 &lt;- ggplot(d1, aes(trial, cumulativerate)) + geom_line() + ylim(0,1) + labs( title = &quot;Random Agent Behavior&quot;, x = &quot;Trial Number&quot;, y = &quot;Cumulative probability of choosing 1 (0-1)&quot; ) + theme_classic() p4 &lt;- ggplot(d2, aes(trial, cumulativerate)) + geom_line() + labs( title = &quot;Random Agent Behavior&quot;, x = &quot;Trial Number&quot;, y = &quot;Cumulative probability of choosing 1 (0-1)&quot; ) + ylim(0,1) + theme_classic() p3 + p4 print(&quot;This cumulative rate plot helps us better understand the agent&#39;s overall tendencies. For a truly random agent, we expect this line to converge toward 0.5 as the number of trials increases. Early fluctuations away from 0.5 are possible due to random chance, but with more trials, these fluctuations tend to even out. When we compare agents with different underlying biases (rate = 0.5 vs rate = 0.8):&quot;) ## [1] &quot;This cumulative rate plot helps us better understand the agent&#39;s overall tendencies. For a truly random agent, we expect this line to converge toward 0.5 as the number of trials increases. Early fluctuations away from 0.5 are possible due to random chance, but with more trials, these fluctuations tend to even out.\\nWhen we compare agents with different underlying biases (rate = 0.5 vs rate = 0.8):&quot; ## Now in the same plot d1$rate &lt;- 0.5 d2$rate &lt;- 0.8 d &lt;- rbind(d1,d2) %&gt;% mutate(rate = as.factor(rate)) p5 &lt;- ggplot(d, aes(trial, cumulativerate, color = rate, group = rate)) + geom_line() + labs( title = &quot;Random Agents Behavior&quot;, x = &quot;Trial Number&quot;, y = &quot;Cumulative probability of choosing 1 (0-1)&quot; ) + ylim(0,1) + theme_classic() p5 print(&quot;We can clearly see how bias affects choice behavior. The unbiased agent (rate = 0.5) stabilizes around choosing each option equally often, while the biased agent (rate = 0.8) shows a strong preference for option 1, choosing it approximately 80% of the time. This comparison helps us understand how we might detect biases in real players&#39; behavior - consistent deviation from 50-50 choice proportions could indicate an underlying preference or strategy.&quot;) ## [1] &quot;We can clearly see how bias affects choice behavior. The unbiased agent (rate = 0.5) stabilizes around choosing each option equally often, while the biased agent (rate = 0.8) shows a strong preference for option 1, choosing it approximately 80% of the time. This comparison helps us understand how we might detect biases in real players&#39; behavior - consistent deviation from 50-50 choice proportions could indicate an underlying preference or strategy.&quot; # Now as a function #&#39; Create a random decision-making agent #&#39; @param input Vector of previous choices (not used but included for API consistency) #&#39; @param rate Probability of choosing option 1 (default: 0.5 for unbiased) #&#39; @return Vector of binary choices #&#39; @examples #&#39; # Create unbiased random agent for 10 trials #&#39; choices &lt;- RandomAgent_f(rep(1,10), 0.5) RandomAgent_f &lt;- function(input, rate = 0.5) { # Input validation if (!is.numeric(rate) || rate &lt; 0 || rate &gt; 1) { stop(&quot;Rate must be a probability between 0 and 1&quot;) } n &lt;- length(input) choice &lt;- rbinom(n, 1, rate) return(choice) } input &lt;- rep(1,trials) # it doesn&#39;t matter, it&#39;s not taken into account choice &lt;- RandomAgent_f(input, rate) d3 &lt;- tibble(trial = seq(trials), choice) ggplot(d3, aes(trial, choice)) + geom_line() + labs( title = &quot;Random Agent Behavior&quot;, x = &quot;Trial Number&quot;, y = &quot;Cumulative probability of choosing 1 (0-1)&quot; ) + theme_classic() ## What if there&#39;s noise? RandomAgentNoise_f &lt;- function(input, rate, noise){ n &lt;- length(input) choice &lt;- rbinom(n, 1, rate) if (rbinom(1, 1, noise) == 1) {choice = rbinom(1,1,0.5)} return(choice) } 4.5 Implementing a Win-Stay-Lose-Shift agent #&#39; Create a Win-Stay-Lose-Shift decision-making agent #&#39; @param prevChoice Previous choice made by the agent (0 or 1) #&#39; @param feedback Success of previous choice (1 for win, 0 for loss) #&#39; @param noise Optional probability of random choice (default: 0) #&#39; @return Next choice (0 or 1) #&#39; @examples #&#39; # Basic WSLS decision after a win #&#39; next_choice &lt;- WSLSAgent_f(prevChoice = 1, feedback = 1) WSLSAgent_f &lt;- function(prevChoice, feedback, noise = 0) { # Input validation if (!is.numeric(prevChoice) || !prevChoice %in% c(0,1)) { stop(&quot;Previous choice must be 0 or 1&quot;) } if (!is.numeric(feedback) || !feedback %in% c(0,1)) { stop(&quot;Feedback must be 0 or 1&quot;) } if (!is.numeric(noise) || noise &lt; 0 || noise &gt; 1) { stop(&quot;Noise must be a probability between 0 and 1&quot;) } # Core WSLS logic choice &lt;- if (feedback == 1) { prevChoice # Stay with previous choice if won } else { 1 - prevChoice # Switch to opposite choice if lost } # Apply noise if specified if (noise &gt; 0 &amp;&amp; runif(1) &lt; noise) { choice &lt;- sample(c(0,1), 1) } return(choice) } WSLSAgentNoise_f &lt;- function(prevChoice, Feedback, noise){ if (Feedback == 1) { choice = prevChoice } else if (Feedback == 0) { choice = 1 - prevChoice } if (rbinom(1, 1, noise) == 1) {choice &lt;- rbinom(1, 1, .5)} return(choice) } WSLSAgent &lt;- WSLSAgent_f(1, 0) # Against a random agent Self &lt;- rep(NA, trials) Other &lt;- rep(NA, trials) Self[1] &lt;- RandomAgent_f(1, 0.5) Other &lt;- RandomAgent_f(seq(trials), rate) for (i in 2:trials) { if (Self[i - 1] == Other[i - 1]) { Feedback = 1 } else {Feedback = 0} Self[i] &lt;- WSLSAgent_f(Self[i - 1], Feedback) } sum(Self == Other) ## [1] 85 df &lt;- tibble(Self, Other, trial = seq(trials), Feedback = as.numeric(Self == Other)) ggplot(df) + theme_classic() + geom_line(color = &quot;red&quot;, aes(trial, Self)) + geom_line(color = &quot;blue&quot;, aes(trial, Other)) + labs( title = &quot;WSLS Agent (red) vs Biased Random Opponent (blue)&quot;, x = &quot;Trial Number&quot;, y = &quot;Choice (0/1)&quot;, color = &quot;Agent Type&quot; ) ggplot(df) + theme_classic() + geom_line(color = &quot;red&quot;, aes(trial, Feedback)) + geom_line(color = &quot;blue&quot;, aes(trial, 1 - Feedback)) + labs( title = &quot;WSLS Agent (red) vs Biased Random Opponent (blue)&quot;, x = &quot;Trial Number&quot;, y = &quot;Feedback received (0/1)&quot;, color = &quot;Agent Type&quot; ) print(&quot;These plots compare how a Win-Stay-Lose-Shift (WSLS) agent performs against different opponents. The red line shows the WSLS agent&#39;s choices, while the blue line shows the opponent&#39;s choices. When playing against a biased random opponent, we can see clearer patterns in the WSLS agent&#39;s behavior as it responds to wins and losses. Against another WSLS agent, the interaction becomes more complex, as each agent is trying to adapt to the other&#39;s adaptations. This kind of visualization helps us understand how different strategies might interact in actual gameplay.&quot;) ## [1] &quot;These plots compare how a Win-Stay-Lose-Shift (WSLS) agent performs against different opponents. The red line shows the WSLS agent&#39;s choices, while the blue line shows the opponent&#39;s choices. When playing against a biased random opponent, we can see clearer patterns in the WSLS agent&#39;s behavior as it responds to wins and losses. Against another WSLS agent, the interaction becomes more complex, as each agent is trying to adapt to the other&#39;s adaptations. This kind of visualization helps us understand how different strategies might interact in actual gameplay.&quot; df$cumulativerateSelf &lt;- cumsum(df$Feedback) / seq_along(df$Feedback) df$cumulativerateOther &lt;- cumsum(1 - df$Feedback) / seq_along(df$Feedback) ggplot(df) + theme_classic() + geom_line(color = &quot;red&quot;, aes(trial, cumulativerateSelf)) + geom_line(color = &quot;blue&quot;, aes(trial, cumulativerateOther)) + labs( title = &quot;WSLS Agent (red) vs Biased Random Opponent (blue)&quot;, x = &quot;Trial Number&quot;, y = &quot;Cumulative probability of choosing 1 (0-1)&quot;, color = &quot;Agent Type&quot; ) # Against a Win-Stay-Lose Shift Self &lt;- rep(NA, trials) Other &lt;- rep(NA, trials) Self[1] &lt;- RandomAgent_f(1, 0.5) Other[1] &lt;- RandomAgent_f(1, 0.5) for (i in 2:trials) { if (Self[i - 1] == Other[i - 1]) { Feedback = 1 } else {Feedback = 0} Self[i] &lt;- WSLSAgent_f(Self[i - 1], Feedback) Other[i] &lt;- WSLSAgent_f(Other[i - 1], 1 - Feedback) } sum(Self == Other) ## [1] 60 df &lt;- tibble(Self, Other, trial = seq(trials), Feedback = as.numeric(Self == Other)) ggplot(df) + theme_classic() + geom_line(color = &quot;red&quot;, aes(trial, Self)) + geom_line(color = &quot;blue&quot;, aes(trial, Other)) ggplot(df) + theme_classic() + geom_line(color = &quot;red&quot;, aes(trial, Feedback)) + geom_line(color = &quot;blue&quot;, aes(trial, 1 - Feedback)) df$cumulativerateSelf &lt;- cumsum(df$Feedback) / seq_along(df$Feedback) df$cumulativerateOther &lt;- cumsum(1 - df$Feedback) / seq_along(df$Feedback) ggplot(df) + theme_classic() + geom_line(color = &quot;red&quot;, aes(trial, cumulativerateSelf)) + geom_line(color = &quot;blue&quot;, aes(trial, cumulativerateOther)) print(&quot;This cumulative performance plot reveals the overall effectiveness of the WSLS strategy. By tracking the running average of successes, we can see whether the strategy leads to above-chance performance in the long run. When playing against a biased random opponent, the WSLS agent can potentially exploit the opponent&#39;s predictable tendencies, though success depends on how strong and consistent the opponent&#39;s bias is. When we pit the WSLS agent against another WSLS agent, the dynamics become more complex. Both agents are now trying to adapt to each other&#39;s adaptations, creating a more sophisticated strategic interaction. The resulting behavior often shows interesting patterns of mutual adaptation, where each agent&#39;s attempts to exploit the other&#39;s strategy leads to evolving patterns of play.&quot;) ## [1] &quot;This cumulative performance plot reveals the overall effectiveness of the WSLS strategy. By tracking the running average of successes, we can see whether the strategy leads to above-chance performance in the long run. When playing against a biased random opponent, the WSLS agent can potentially exploit the opponent&#39;s predictable tendencies, though success depends on how strong and consistent the opponent&#39;s bias is.\\nWhen we pit the WSLS agent against another WSLS agent, the dynamics become more complex. Both agents are now trying to adapt to each other&#39;s adaptations, creating a more sophisticated strategic interaction. The resulting behavior often shows interesting patterns of mutual adaptation, where each agent&#39;s attempts to exploit the other&#39;s strategy leads to evolving patterns of play.&quot; 4.6 Now we scale it up trials = 120 agents = 100 # WSLS vs agents with varying rates for (rate in seq(from = 0.5, to = 1, by = 0.05)) { for (agent in seq(agents)) { Self &lt;- rep(NA, trials) Other &lt;- rep(NA, trials) Self[1] &lt;- RandomAgent_f(1, 0.5) Other &lt;- RandomAgent_f(seq(trials), rate) for (i in 2:trials) { if (Self[i - 1] == Other[i - 1]) { Feedback = 1 } else {Feedback = 0} Self[i] &lt;- WSLSAgent_f(Self[i - 1], Feedback) } temp &lt;- tibble(Self, Other, trial = seq(trials), Feedback = as.numeric(Self == Other), agent, rate) if (agent == 1 &amp; rate == 0.5) {df &lt;- temp} else {df &lt;- bind_rows(df, temp)} } } ## WSLS with another WSLS for (agent in seq(agents)) { Self &lt;- rep(NA, trials) Other &lt;- rep(NA, trials) Self[1] &lt;- RandomAgent_f(1, 0.5) Other[1] &lt;- RandomAgent_f(1, 0.5) for (i in 2:trials) { if (Self[i - 1] == Other[i - 1]) { Feedback = 1 } else {Feedback = 0} Self[i] &lt;- WSLSAgent_f(Self[i - 1], Feedback) Other[i] &lt;- WSLSAgent_f(Other[i - 1], 1 - Feedback) } temp &lt;- tibble(Self, Other, trial = seq(trials), Feedback = as.numeric(Self == Other), agent, rate) if (agent == 1 ) {df1 &lt;- temp} else {df1 &lt;- bind_rows(df1, temp)} } 4.6.1 And we visualize it ggplot(df, aes(trial, Feedback, group = rate, color = rate)) + geom_smooth(se = F) + theme_classic() We can see that the bigger the bias in the random agent, the bigger the performance in the WSLS (the higher the chances the random agent picks the same hand more than once in a row). Now it’s your turn to follow a similar process for your 2 chosen strategies. 4.7 Conclusion Moving from verbal descriptions to formal computational models represents a crucial step in cognitive science. Through our work with the matching pennies game, we have seen how this transformation process requires careful consideration of theoretical assumptions, mathematical precision, and practical implementation details. The development of formal models forces us to be explicit about mechanisms that might remain ambiguous in verbal descriptions. When we state that an agent “learns from experience” or “responds to patterns,” we must specify exactly how these processes work. This precision not only clarifies our theoretical understanding but also enables rigorous empirical testing. Our implementation of different agent types - from simple random choice to more sophisticated strategies - demonstrates how computational modeling can reveal surprising implications of seemingly straightforward theories. Through simulation, we discovered that even basic strategies can produce complex patterns of behavior, especially when agents interact with each other over multiple trials. Perhaps most importantly, this chapter has established a foundational workflow for cognitive modeling: begin with careful observation, think carefully and develop precise mathematical formulations, implement these as computational models, and validate predictions against data. Don’t be afraid to make mistakes, or rethink your strategy and iterate the modeling process. This systematic approach will serve as our template as we progress to more complex cognitive phenomena in subsequent chapters. While our matching pennies models may seem simple compared to the rich complexity of human cognition, they exemplify the essential principles of good modeling practice: clarity of assumptions, precision in implementation, and rigorous validation against empirical data. These principles will guide our exploration of more sophisticated cognitive models throughout this course. For more advanced examples of models that can underly behavior in the Matching Pennies game check: Chapter 12 on reinforcement learning. the paper by Waade et al mentioned at the beginning of the chapter. "],["from-simulation-to-model-fitting.html", "Chapter 5 From simulation to model fitting 5.1 Learning Goals 5.2 The Challenge of Model Fitting 5.3 Simulating data 5.4 Building our basic model in Stan 5.5 Parameter recovery 5.6 The memory model: conditioning theta 5.7 Memory agent with internal parameter 5.8 Relationship to Rescorla-Wagner 5.9 Bayesian memory agent 5.10 Conclusion: From Simple Models to Complex Cognitive Processes", " Chapter 5 From simulation to model fitting This chapter introduces essential techniques for moving from theoretical models to empirical validation. Building on our implementation of decision-making agents, we now tackle the challenge of determining whether these models accurately describe observed behavior. 5.1 Learning Goals After completing this chapter, you will be able to: Design and implement Bayesian parameter estimation for cognitive models using Stan Create and interpret prior and posterior predictive checks to validate model behavior Evaluate model quality through systematic parameter recovery studies 5.2 The Challenge of Model Fitting Understanding human behavior requires more than just implementing plausible models - we must determine whether these models actually capture meaningful empirical patterns. Consider our biased agent model that tends to favor one choice over another. While we can specify different levels of bias in our simulations, real-world application requires determining what bias values best explain observed behavior, and for instance whether a pharmacological manipulation can affect the bias. Bayesian inference provides a powerful framework for this challenge. It allows us to: Express our prior beliefs about reasonable parameter values Update these beliefs based on observed data Quantify uncertainty in our parameter estimates Generate predictions that account for parameter uncertainty 5.3 Simulating data As usual we start with simulated data, where we know the underlying mechanisms and parameter values. Simulated data are rarely enough (empirical data often offer unexpected challenges), but they are a great starting point to stress test your model: does the model reconstruct the right parameter values? Does it reproduce the overall patterns in the data? Here we build a new simulation of random agents with bias and noise. The code and visualization is really nothing different from last chapter. # Set this to TRUE when you want to regenerate all simulation results # Otherwise, existing results will be loaded regenerate_simulations &lt;- FALSE pacman::p_load(tidyverse, here, posterior, cmdstanr, brms, tidybayes) trials &lt;- 120 RandomAgentNoise_f &lt;- function(rate, noise) { choice &lt;- rbinom(1, 1, rate) # generating noiseless choices if (rbinom(1, 1, noise) == 1) { choice = rbinom(1, 1, 0.5) # introducing noise } return(choice) } # Check if the simulation data file exists already sim_data_file &lt;- &quot;simdata/W3_randomnoise.csv&quot; if (regenerate_simulations || !file.exists(sim_data_file)) { # Generate new simulation data d &lt;- NULL for (noise in seq(0, 0.5, 0.1)) { # looping through noise levels for (rate in seq(0, 1, 0.1)) { # looping through rate levels randomChoice &lt;- rep(NA, trials) for (t in seq(trials)) { # looping through trials (to make it homologous to more reactive models) randomChoice[t] &lt;- RandomAgentNoise_f(rate, noise) } temp &lt;- tibble(trial = seq(trials), choice = randomChoice, rate, noise) temp$cumulativerate &lt;- cumsum(temp$choice) / seq_along(temp$choice) if (exists(&quot;d&quot;)) { d &lt;- rbind(d, temp) } else{ d &lt;- temp } } } # Save the simulation data write_csv(d, sim_data_file) cat(&quot;Generated new simulation data and saved to&quot;, sim_data_file, &quot;\\n&quot;) } else { # Load existing simulation data d &lt;- read_csv(sim_data_file) cat(&quot;Loaded existing simulation data from&quot;, sim_data_file, &quot;\\n&quot;) } ## Loaded existing simulation data from simdata/W3_randomnoise.csv # Now we visualize it p1 &lt;- ggplot(d, aes(trial, cumulativerate, group = rate, color = rate)) + geom_line() + geom_hline(yintercept = 0.5, linetype = &quot;dashed&quot;) + ylim(0,1) + facet_wrap(.~noise) + theme_classic() p1 5.4 Building our basic model in Stan N.B. Refer to the video and slides for the step by step build-up of the Stan code. Now we subset to a simple case, no noise and rate of 0.8, to focus on the Stan model. We make it into the right format for Stan, build the Stan model, and fit it. 5.4.1 Data Here we define the data and format it for Stan. Stan likes data as a list. Why a list? Well, dataframes (now tibbles) are amazing. But they have a big drawback: they require each variable to have the same length. Lists do not have that limitation, they are more flexible. So, lists. We’ll have to learn how to live with them. d1 &lt;- d %&gt;% subset(noise == 0 &amp; rate == 0.8) ## Create the data. N.B. note the two variables have different lengths: 1 for n, n for h. data_biased &lt;- list( n = 120, # n of trials h = d1$choice # sequence of choices (h stands for hand) ) 5.4.2 Model We write the stan code within the R code (so I can show it to you more easily), then we save it as a stan file, which can be loaded at a later stage in order to compile it. [Missing: more info on compiling etc.] Remember that the minimal Stan model requires 3 chunks, one specifying the data it will need as input; one specifying the parameters to be estimated; one specifying the model within which the parameters appear, and the priors for those parameters. stan_model &lt;- &quot; // This model infers a random bias from a sequences of 1s and 0s (right and left hand choices) // The input (data) for the model. n of trials and the sequence of choices (right as 1, left as 0) data { int&lt;lower=1&gt; n; // n of trials array[n] int h; // sequence of choices (right as 1, left as 0) as long as n } // The parameters that the model needs to estimate (theta) parameters { real&lt;lower=0, upper=1&gt; theta; // rate or theta is a probability and therefore bound between 0 and 1 } // The model to be estimated (a bernoulli, parameter theta, prior on the theta) model { // The prior for theta is a beta distribution alpha of 1, beta of 1, equivalent to a uniform between 0 and 1 target += beta_lpdf(theta | 1, 1); // N.B. you could also define the parameters of the priors as variables to be found in the data // target += beta_lpdf(theta | beta_alpha, beta_beta); BUT remember to add beta_alpha and beta_beta to the data list // The model consists of a bernoulli distribution (binomial w 1 trial only) with a rate theta target += bernoulli_lpmf(h | theta); } &quot; write_stan_file( stan_model, dir = &quot;stan/&quot;, basename = &quot;W3_SimpleBernoulli.stan&quot;) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W3_SimpleBernoulli.stan&quot; 5.4.3 Compiling and fitting the model ## Specify where the model is file &lt;- file.path(&quot;stan/W3_SimpleBernoulli.stan&quot;) # File path for saved model model_file &lt;- &quot;simmodels/W3_SimpleBernoulli.rds&quot; # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(model_file)) { # Compile the model mod &lt;- cmdstan_model(file, # this specifies we can parallelize the gradient estimations on multiple cores cpp_options = list(stan_threads = TRUE), # this is a trick to make it faster stanc_options = list(&quot;O1&quot;)) # The following command calls Stan with specific options. samples_biased &lt;- mod$sample( data = data_biased, # the data :-) seed = 123, # a seed, so I always get the same results chains = 2, # how many chains should I fit (to check whether they give the same results) parallel_chains = 2, # how many of the chains can be run in parallel? threads_per_chain = 2, # distribute gradient estimations within chain across multiple cores iter_warmup = 1000, # warmup iterations through which hyperparameters (steps and step length) are adjusted iter_sampling = 2000, # total number of iterations refresh = 0, # how often to show that iterations have been run max_treedepth = 20, # how many steps in the future to check to avoid u-turns adapt_delta = 0.99, # how high a learning rate to adjust hyperparameters during warmup ) # Save the fitted model samples_biased$save_object(file = model_file) cat(&quot;Generated new model fit and saved to&quot;, model_file, &quot;\\n&quot;) } else { # Load existing results cat(&quot;Loading biased model samples...\\n&quot;) samples_biased &lt;- readRDS(model_file) cat(&quot;Available parameters:&quot;, paste(colnames(as_draws_df(samples_biased$draws())), collapse=&quot;, &quot;), &quot;\\n&quot;) cat(&quot;Loaded existing model fit from&quot;, model_file, &quot;\\n&quot;) } ## Loading biased model samples... ## Available parameters: lp__, theta, .chain, .iteration, .draw ## Loaded existing model fit from simmodels/W3_SimpleBernoulli.rds samples_biased$summary() # summarize the model ## # A tibble: 2 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lp__ -56.6 -56.3 0.814 0.336 -58.2 -56.0 1.00 1258. 951. ## 2 theta 0.828 0.830 0.0356 0.0359 0.768 0.884 1.00 893. 969. 5.4.4 Assessing model quality Then we need to look more in the details at the quality of the estimation: * the markov chains * how the prior and the posterior estimates relate to each other (whether the prior is constraining the posterior estimate) # Check if samples_biased exists if (!exists(&quot;samples_biased&quot;)) { model_file &lt;- &quot;simmodels/W3_SimpleBernoulli.rds&quot; if (file.exists(model_file)) { cat(&quot;Loading biased model samples...\\n&quot;) samples_biased &lt;- readRDS(model_file) cat(&quot;Available parameters:&quot;, paste(colnames(as_draws_df(samples_biased$draws())), collapse=&quot;, &quot;), &quot;\\n&quot;) } else { cat(&quot;Model file not found. Set regenerate_simulations=TRUE to create it.\\n&quot;) # Provide dummy data or skip the remaining code knitr::knit_exit() } } # Extract posterior samples and include sampling of the prior: draws_df_biased &lt;- as_draws_df(samples_biased$draws()) # Explicitly extract parameters theta_param &lt;- draws_df_biased$theta cat(&quot;Successfully extracted theta parameter with&quot;, length(theta_param), &quot;values\\n&quot;) ## Successfully extracted theta parameter with 4000 values # Checking the model&#39;s chains ggplot(draws_df_biased, aes(.iteration, theta, group = .chain, color = .chain)) + geom_line() + theme_classic() # add a prior for theta (ugly, but we&#39;ll do better soon) draws_df_biased &lt;- draws_df_biased %&gt;% mutate( theta_prior = rbeta(nrow(draws_df_biased), 1, 1) ) # Now let&#39;s plot the density for theta (prior and posterior) ggplot(draws_df_biased) + geom_density(aes(theta), fill = &quot;blue&quot;, alpha = 0.3) + geom_density(aes(theta_prior), fill = &quot;red&quot;, alpha = 0.3) + geom_vline(xintercept = 0.8, linetype = &quot;dashed&quot;, color = &quot;black&quot;, linewidth = 1.5) + xlab(&quot;Rate&quot;) + ylab(&quot;Posterior Density&quot;) + theme_classic() As we can see from the posterior estimates and the prior posterior update check, our model is doing a decent job. It doesn’t exactly reconstruct the rate of 0.8, but 0.755 is pretty close and 0.8 is included within the credible interval. Now we build the same model, but using the log odds scale for the theta parameter, which will become useful later when we condition theta on variables and build multilevel models (as we can do what we want in a log odds space and it will always be bound between 0 and 1). stan_model &lt;- &quot; // This model infers a random bias from a sequences of 1s and 0s (right and left hand choices) // The input (data) for the model. n of trials and the sequence of choices (right as 1, left as 0) data { int&lt;lower=1&gt; n; // n of trials array[n] int h; // sequence of choices (right as 1, left as 0) as long as n } // The parameters that the model needs to estimate (theta) parameters { real theta; // note it is unbounded as we now work on log odds } // The model to be estimated (a bernoulli, parameter theta, prior on the theta) model { // The prior for theta on a log odds scale is a normal distribution with a mean of 0 and a sd of 1. // This covers most of the probability space between 0 and 1, after being converted to probability. target += normal_lpdf(theta | 0, 1); // as before the parameters of the prior could be fed as variables // target += normal_lpdf(theta | normal_mu, normal_sigma); // The model consists of a bernoulli distribution (binomial w 1 trial only) with a rate theta, // note we specify it uses a logit link (theta is in logodds) target += bernoulli_logit_lpmf(h | theta); } &quot; write_stan_file( stan_model, dir = &quot;stan/&quot;, basename = &quot;W3_SimpleBernoulli_logodds.stan&quot;) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W3_SimpleBernoulli_logodds.stan&quot; ## With the logit format ## Specify where the model is file &lt;- file.path(&quot;stan/W3_SimpleBernoulli_logodds.stan&quot;) # File path for saved model model_file &lt;- &quot;simmodels/W3_SimpleBernoulli_logodds.rds&quot; # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(model_file)) { # Compile the model mod &lt;- cmdstan_model(file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) # The following command calls Stan with specific options. samples_biased_logodds &lt;- mod$sample( data = data_biased, seed = 123, chains = 2, parallel_chains = 2, threads_per_chain = 2, iter_warmup = 1000, iter_sampling = 2000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99, ) # Save the fitted model samples_biased_logodds$save_object(file = model_file) cat(&quot;Generated new model fit and saved to&quot;, model_file, &quot;\\n&quot;) } else { # Load existing results cat(&quot;Loading biased model (log-odds) samples...\\n&quot;) samples_biased_logodds &lt;- readRDS(model_file) cat(&quot;Available parameters:&quot;, paste(colnames(as_draws_df(samples_biased_logodds$draws())), collapse = &quot;, &quot;), &quot;\\n&quot;) cat(&quot;Loaded existing model fit from&quot;, model_file, &quot;\\n&quot;) } ## Loading biased model (log-odds) samples... ## Available parameters: lp__, theta, .chain, .iteration, .draw ## Loaded existing model fit from simmodels/W3_SimpleBernoulli_logodds.rds 5.4.5 Summarizing the results if (!exists(&quot;samples_biased_logodds&quot;)) { cat(&quot;Loading biased model (log-odds) samples...\\n&quot;) samples_biased_logodds &lt;- readRDS(&quot;simmodels/W3_SimpleBernoulli_logodds.rds&quot;) cat(&quot;Available parameters:&quot;, paste(colnames(as_draws_df(samples_biased_logodds$draws())), collapse = &quot;, &quot;), &quot;\\n&quot;) } # Extract posterior samples and include sampling of the prior: draws_df_biased_logodds &lt;- as_draws_df(samples_biased_logodds$draws()) # Explicitly extract theta parameter theta_param_logodds &lt;- draws_df_biased_logodds$theta cat(&quot;Successfully extracted theta parameter with&quot;, length(theta_param_logodds), &quot;values\\n&quot;) ## Successfully extracted theta parameter with 4000 values ggplot(draws_df_biased_logodds, aes(.iteration, theta, group = .chain, color = .chain)) + geom_line() + theme_classic() # add a prior for theta (ugly, but we&#39;ll do better soon) draws_df_biased_logodds &lt;- draws_df_biased_logodds %&gt;% mutate( theta_prior = rnorm(nrow(draws_df_biased_logodds), 0, 1) ) # Now let&#39;s plot the density for theta (prior and posterior) ggplot(draws_df_biased_logodds) + geom_density(aes(theta), fill = &quot;blue&quot;, alpha = 0.3) + geom_density(aes(theta_prior), fill = &quot;red&quot;, alpha = 0.3) + geom_vline(xintercept = 1.38, linetype = &quot;dashed&quot;, color = &quot;black&quot;, size = 1.5) + xlab(&quot;Rate&quot;) + ylab(&quot;Posterior Density&quot;) + theme_classic() # Summary samples_biased_logodds$summary() ## # A tibble: 2 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lp__ -56.7 -56.4 0.757 0.294 -58.2 -56.2 1.00 1036. 945. ## 2 theta 1.53 1.52 0.234 0.226 1.14 1.92 1.00 1076. 878. We can see that the results are very similar. 5.5 Parameter recovery Now that we see that the model works in one case, we can run it through all possible rate and noise levels in the simulation. Here we’ll implement a better approach using parallelization, which is more efficient for complex models. To parallelize, we rely on furrr, a neat R package that distributes parallel operations across cores. This approach becomes crucial with more complex models. First we need to define the function that will define the operations to be run on each core separately, here we simulate the data according to a seed, a n of trials, a rate and a noise, and then we fit the model to them. Second, we need to create a tibble of the seeds, n of trials, rate and noise values that should be simulated. Third, we use future_pmap_dfr to run the function on each row of the tibble above separately on a different core. Note that I set the system to split across 4 parallel cores (to work on my computer without clogging it). Do change it according to the system you are using. Note that if you have 40 “jobs” (rows of the tibble, sets of parameter values to run), using e.g. 32 cores will not substantially speed things more than using 20. # File path for saved recovery results recovery_file &lt;- &quot;simdata/W3_recoverydf_parallel.csv&quot; # Check if we need to run the parameter recovery if (regenerate_simulations || !file.exists(recovery_file)) { # Load necessary packages for parallelization pacman::p_load(future, purrr, furrr) # Set up parallel processing (adjust workers based on your system) plan(multisession, workers = 4) # Define the function that will be run on each core separately sim_d_and_fit &lt;- function(seed, trials, rateLvl, noiseLvl) { # Generate random choices randomChoice &lt;- rep(NA, trials) for (t in seq(trials)) { randomChoice[t] &lt;- RandomAgentNoise_f(rateLvl, noiseLvl) } # Create data for Stan data &lt;- list( n = trials, h = randomChoice ) # Compile the model file &lt;- file.path(&quot;stan/W3_SimpleBernoulli_logodds.stan&quot;) mod &lt;- cmdstan_model(file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) # Fit the model samples_recovery &lt;- mod$sample( data = data, seed = 1000, chains = 1, parallel_chains = 1, threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 2000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99, ) # Extract results draws_df_recovery &lt;- as_draws_df(samples_recovery$draws()) # Check for parameter name cat(&quot;Parameters available:&quot;, paste(colnames(draws_df_recovery), collapse=&quot;, &quot;), &quot;\\n&quot;) # Extract theta parameter theta_param &lt;- draws_df_recovery$theta temp &lt;- tibble(biasEst = inv_logit_scaled(theta_param), biasTrue = rateLvl, noise = noiseLvl) return(temp) } # Create a tibble with all parameter combinations param_combinations &lt;- tibble( rateLvl = rep(seq(0, 1, 0.1), each = 6), noiseLvl = rep(seq(0, 0.5, 0.1), 11), seed = 1000, trials = 120 ) # Run the function on each parameter combination in parallel recovery_df &lt;- future_pmap_dfr( param_combinations, sim_d_and_fit, .options = furrr_options(seed = TRUE) ) # Save results for future use write_csv(recovery_df, recovery_file) cat(&quot;Generated new parameter recovery results and saved to&quot;, recovery_file, &quot;\\n&quot;) } else { # Load existing results recovery_df &lt;- read_csv(recovery_file) cat(&quot;Loaded existing parameter recovery results from&quot;, recovery_file, &quot;\\n&quot;) } ## Loaded existing parameter recovery results from simdata/W3_recoverydf_parallel.csv # Visualize the parameter recovery results ggplot(recovery_df, aes(biasTrue, biasEst)) + geom_point(alpha = 0.1) + geom_smooth() + facet_wrap(.~noise) + theme_classic() There’s much to be said about the final plot, but for now let’s just say that it looks good. We can reconstruct in a nice ordered way true rate values. However, our ability to do so decreases with the increase in noise. So far no surprises. Wait, you say, shouldn’t we actually model the generative process, that is, include noise in the Stan model? Gold star, there! But let’s wait a bit before we get there, we’ll need mixture models. 5.6 The memory model: conditioning theta Now that we fitted the base model, we can move onto more complex models. For instance a memory model (including all previous trials). Here we rely on a generalized linear model kind of thinking: the theta is the expression of a linear model (bias + b1 * PreviousRate). To make the variable more intuitive we code previous rate - which is bound to a probability 0-1 space - into log-odds via a logit link/transformation. In this way a previous rate with more left than right choices will result in a negative value, thereby decreasing our propensity to choose right; and one with more right than left choices will result in a positive value, thereby increasing our propensity to choose right. # We subset to only include no noise and a specific rate d1 &lt;- d %&gt;% subset(noise == 0 &amp; rate == 0.8) %&gt;% rename(Other = choice) %&gt;% mutate(cumulativerate = lag(cumulativerate, 1)) d1$cumulativerate[1] &lt;- 0.5 # no prior info at first trial d1$cumulativerate[d1$cumulativerate == 0] &lt;- 0.01 d1$cumulativerate[d1$cumulativerate == 1] &lt;- 0.99 # Now we create the memory agent with a coefficient of 1 (in log odds) MemoryAgent_f &lt;- function(bias, beta, cumulativerate){ choice = rbinom(1, 1, inv_logit_scaled(bias + beta * logit_scaled(cumulativerate))) return(choice) } d1$Self[1] &lt;- RandomAgentNoise_f(0.5, 0) for (i in 2:trials) { d1$Self[i] &lt;- MemoryAgent_f(bias = 0, beta = 1, d1$cumulativerate[i]) } ## Create the data data_memory &lt;- list( n = 120, h = d1$Self, memory = d1$cumulativerate # this creates the new parameter: the rate of right hands so far in log-odds ) stan_model &lt;- &quot; // The input (data) for the model. n of trials and h for (right and left) hand data { int&lt;lower=1&gt; n; array[n] int h; vector[n] memory; // here we add the new variable between 0.01 and .99 } // The parameters accepted by the model. parameters { real bias; // how likely is the agent to pick right when the previous rate has no information (50-50)? real beta; // how strongly is previous rate impacting the decision? } // The model to be estimated. model { // priors target += normal_lpdf(bias | 0, .3); target += normal_lpdf(beta | 0, .5); // model target += bernoulli_logit_lpmf(h | bias + beta * logit(memory)); } &quot; write_stan_file( stan_model, dir = &quot;stan/&quot;, basename = &quot;W3_MemoryBernoulli.stan&quot;) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W3_MemoryBernoulli.stan&quot; ## Specify where the model is file &lt;- file.path(&quot;stan/W3_MemoryBernoulli.stan&quot;) # File path for saved model model_file_memory &lt;- &quot;simmodels/W3_MemoryBernoulli.rds&quot; # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(model_file_memory)) { # Compile the model mod_memory &lt;- cmdstan_model(file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) # The following command calls Stan with specific options. samples_memory &lt;- mod_memory$sample( data = data_memory, seed = 123, chains = 2, parallel_chains = 2, threads_per_chain = 2, iter_warmup = 1000, iter_sampling = 1000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99, ) # Save the fitted model samples_memory$save_object(file = model_file_memory) cat(&quot;Generated new model fit and saved to&quot;, model_file_memory, &quot;\\n&quot;) } else { # Load existing results cat(&quot;Loading memory model samples...\\n&quot;) samples_memory &lt;- readRDS(model_file_memory) cat(&quot;Available parameters:&quot;, paste(colnames(as_draws_df(samples_memory$draws())), collapse=&quot;, &quot;), &quot;\\n&quot;) cat(&quot;Loaded existing model fit from&quot;, model_file_memory, &quot;\\n&quot;) } ## Loading memory model samples... ## Available parameters: lp__, bias, beta, .chain, .iteration, .draw ## Loaded existing model fit from simmodels/W3_MemoryBernoulli.rds 5.6.1 Summarizing the results # Check if samples_memory exists if (!exists(&quot;samples_memory&quot;)) { cat(&quot;Loading memory model samples...\\n&quot;) samples_memory &lt;- readRDS(&quot;simmodels/W3_MemoryBernoulli.rds&quot;) cat(&quot;Available parameters:&quot;, paste(colnames(as_draws_df(samples_memory$draws())), collapse = &quot;, &quot;), &quot;\\n&quot;) } # Extract posterior samples and include sampling of the prior: draws_df_memory &lt;- as_draws_df(samples_memory$draws()) # Explicitly extract parameters bias_param &lt;- draws_df_memory$bias beta_param &lt;- draws_df_memory$beta cat(&quot;Successfully extracted&quot;, length(bias_param), &quot;values for bias parameter\\n&quot;) ## Successfully extracted 2000 values for bias parameter cat(&quot;Successfully extracted&quot;, length(beta_param), &quot;values for beta parameter\\n&quot;) ## Successfully extracted 2000 values for beta parameter # Trace plot for bias ggplot(draws_df_memory, aes(.iteration, bias, group = .chain, color = .chain)) + geom_line() + labs(title = &quot;Trace plot for bias parameter&quot;) + theme_classic() # Trace plot for beta ggplot(draws_df_memory, aes(.iteration, beta, group = .chain, color = .chain)) + geom_line() + labs(title = &quot;Trace plot for beta parameter&quot;) + theme_classic() # add prior distributions draws_df_memory &lt;- draws_df_memory %&gt;% mutate( bias_prior = rnorm(nrow(draws_df_memory), 0, .3), beta_prior = rnorm(nrow(draws_df_memory), 0, .5) ) # Now let&#39;s plot the density for bias (prior and posterior) ggplot(draws_df_memory) + geom_density(aes(bias), fill = &quot;blue&quot;, alpha = 0.3) + geom_density(aes(bias_prior), fill = &quot;red&quot;, alpha = 0.3) + geom_vline(xintercept = 0, linetype = &quot;dashed&quot;, color = &quot;black&quot;, size = 1.5) + labs(title = &quot;Prior-Posterior Update for Bias Parameter&quot;, subtitle = &quot;Blue: posterior, Red: prior, Dashed: true value&quot;) + xlab(&quot;Bias&quot;) + ylab(&quot;Posterior Density&quot;) + theme_classic() # Now let&#39;s plot the density for beta (prior and posterior) ggplot(draws_df_memory) + geom_density(aes(beta), fill = &quot;blue&quot;, alpha = 0.3) + geom_density(aes(beta_prior), fill = &quot;red&quot;, alpha = 0.3) + geom_vline(xintercept = 1, linetype = &quot;dashed&quot;, color = &quot;black&quot;, size = 1.5) + labs(title = &quot;Prior-Posterior Update for Beta Parameter&quot;, subtitle = &quot;Blue: posterior, Red: prior, Dashed: true value&quot;) + xlab(&quot;Beta&quot;) + ylab(&quot;Posterior Density&quot;) + theme_classic() # Print summary samples_memory$summary() ## # A tibble: 3 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lp__ -42.1 -41.8 1.03 0.769 -44.1 -41.1 1.00 691. 619. ## 2 bias 0.235 0.239 0.292 0.292 -0.248 0.706 1.01 659. 795. ## 3 beta 0.966 0.957 0.213 0.215 0.631 1.32 1.00 620. 645. We can see that the model has now estimated both the bias and the role of previous memory. Bias should reflect the bias in the setup (0.5 which in log odds is 0), and the beta coefficient for memory (roughly 1). More on the quality checks of the models in the next chapter. 5.7 Memory agent with internal parameter So far we behaved like in GLM: we keep feeding to the model an external variable of memory, but what if we coded memory as an internal parameter? This opens up to further possibilities to model how long memory is kept and weighted by distance from the current moment, etc. ## Create the data data &lt;- list( n = 120, h = d1$Self, other = d1$Other ) stan_model &lt;- &quot; // Memory-based choice model with prior and posterior predictions data { int&lt;lower=1&gt; n; array[n] int h; array[n] int other; } parameters { real bias; real beta; } transformed parameters { vector[n] memory; for (trial in 1:n) { if (trial == 1) { memory[trial] = 0.5; } if (trial &lt; n) { memory[trial + 1] = memory[trial] + ((other[trial] - memory[trial]) / (trial + 1)); if (memory[trial + 1] == 0) { memory[trial + 1] = 0.01; } if (memory[trial + 1] == 1) { memory[trial + 1] = 0.99; } } } } model { // Priors target += normal_lpdf(bias | 0, .3); target += normal_lpdf(beta | 0, .5); // Likelihood for (trial in 1:n) { target += bernoulli_logit_lpmf(h[trial] | bias + beta * logit(memory[trial])); } } generated quantities { // Generate prior samples real bias_prior = normal_rng(0, .3); real beta_prior = normal_rng(0, .5); // Variables for predictions array[n] int prior_preds; array[n] int posterior_preds; vector[n] memory_prior; vector[n] log_lik; // Generate predictions at different memory levels array[3] real memory_levels = {0.2, 0.5, 0.8}; // Low, neutral, and high memory array[3] int prior_preds_memory; array[3] int posterior_preds_memory; // Generate predictions from prior for each memory level for (i in 1:3) { real logit_memory = logit(memory_levels[i]); prior_preds_memory[i] = bernoulli_logit_rng(bias_prior + beta_prior * logit_memory); posterior_preds_memory[i] = bernoulli_logit_rng(bias + beta * logit_memory); } // Generate predictions from prior memory_prior[1] = 0.5; for (trial in 1:n) { if (trial == 1) { prior_preds[trial] = bernoulli_logit_rng(bias_prior + beta_prior * logit(memory_prior[trial])); } else { memory_prior[trial] = memory_prior[trial-1] + ((other[trial-1] - memory_prior[trial-1]) / trial); if (memory_prior[trial] == 0) { memory_prior[trial] = 0.01; } if (memory_prior[trial] == 1) { memory_prior[trial] = 0.99; } prior_preds[trial] = bernoulli_logit_rng(bias_prior + beta_prior * logit(memory_prior[trial])); } } // Generate predictions from posterior for (trial in 1:n) { posterior_preds[trial] = bernoulli_logit_rng(bias + beta * logit(memory[trial])); log_lik[trial] = bernoulli_logit_lpmf(h[trial] | bias + beta * logit(memory[trial])); } } &quot; write_stan_file( stan_model, dir = &quot;stan/&quot;, basename = &quot;W3_InternalMemory.stan&quot;) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W3_InternalMemory.stan&quot; ## Specify where the model is file &lt;- file.path(&quot;stan/W3_InternalMemory.stan&quot;) # File path for saved model model_file &lt;- &quot;simmodels/W3_InternalMemory.rds&quot; # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(model_file)) { # Compile the model mod &lt;- cmdstan_model(file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) # The following command calls Stan with specific options. samples_memory_internal &lt;- mod$sample( data = data, seed = 123, chains = 1, parallel_chains = 2, threads_per_chain = 2, iter_warmup = 1000, iter_sampling = 1000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99, ) # Save the fitted model samples_memory_internal$save_object(file = model_file) cat(&quot;Generated new model fit and saved to&quot;, model_file, &quot;\\n&quot;) } else { # Load existing results samples_memory_internal &lt;- readRDS(model_file) cat(&quot;Loaded existing model fit from&quot;, model_file, &quot;\\n&quot;) } ## Loaded existing model fit from simmodels/W3_InternalMemory.rds draws_df &lt;- as_draws_df(samples_memory_internal$draws()) # 1. Check chain convergence # Plot traces for main parameters mcmc_trace(draws_df, pars = c(&quot;bias&quot;, &quot;beta&quot;)) + theme_minimal() + ggtitle(&quot;Parameter Traces Across Chains&quot;) # Plot rank histograms to check mixing mcmc_rank_hist(draws_df, pars = c(&quot;bias&quot;, &quot;beta&quot;)) # 2. Prior-Posterior Update Check p1 &lt;- ggplot() + geom_density(data = draws_df, aes(bias, fill = &quot;Posterior&quot;), alpha = 0.5) + geom_density(data = draws_df, aes(bias_prior, fill = &quot;Prior&quot;), alpha = 0.5) + geom_vline(xintercept = 0, linetype = &quot;dashed&quot;) + scale_fill_manual(values = c(&quot;Prior&quot; = &quot;red&quot;, &quot;Posterior&quot; = &quot;blue&quot;)) + theme_minimal() + ggtitle(&quot;Prior-Posterior Update: Bias Parameter&quot;) p2 &lt;- ggplot() + geom_density(data = draws_df, aes(beta, fill = &quot;Posterior&quot;), alpha = 0.5) + geom_density(data = draws_df, aes(beta_prior, fill = &quot;Prior&quot;), alpha = 0.5) + geom_vline(xintercept = 1, linetype = &quot;dashed&quot;) + scale_fill_manual(values = c(&quot;Prior&quot; = &quot;red&quot;, &quot;Posterior&quot; = &quot;blue&quot;)) + theme_minimal() + ggtitle(&quot;Prior-Posterior Update: Beta Parameter&quot;) p3 &lt;- ggplot() + geom_point(data = draws_df, aes(bias, beta), alpha = 0.5) + theme_minimal() + ggtitle(&quot;Correlation&quot;) p1 + p2 + p3 # First let&#39;s properly extract and organize our posterior predictions posterior_predictions &lt;- draws_df %&gt;% dplyr::select(starts_with(&quot;posterior_preds[&quot;)) %&gt;% # Select all posterior prediction columns pivot_longer(everything(), names_to = &quot;trial&quot;, values_to = &quot;prediction&quot;) %&gt;% # Clean up the trial number from the Stan array notation mutate(trial = as.numeric(str_extract(trial, &quot;\\\\d+&quot;))) # Calculate summary statistics for posterior predictions posterior_summary &lt;- posterior_predictions %&gt;% group_by(trial) %&gt;% summarise( mean = mean(prediction), lower = quantile(prediction, 0.025), upper = quantile(prediction, 0.975) ) # Do the same for prior predictions prior_predictions &lt;- draws_df %&gt;% dplyr::select(starts_with(&quot;prior_preds[&quot;)) %&gt;% pivot_longer(everything(), names_to = &quot;trial&quot;, values_to = &quot;prediction&quot;) %&gt;% mutate(trial = as.numeric(str_extract(trial, &quot;\\\\d+&quot;))) prior_summary &lt;- prior_predictions %&gt;% group_by(trial) %&gt;% summarise( mean = mean(prediction), lower = quantile(prediction, 0.025), upper = quantile(prediction, 0.975) ) # Now let&#39;s create our visualization # First the prior predictive check p4 &lt;- ggplot() + # Add prior prediction interval geom_ribbon(data = prior_summary, aes(x = trial, ymin = lower, ymax = upper), alpha = 0.2, fill = &quot;red&quot;) + # Add mean prior prediction geom_line(data = prior_summary, aes(x = trial, y = mean), color = &quot;red&quot;) + # Add actual data points geom_point(data = tibble(trial = 1:length(data$h), choice = data$h), aes(x = trial, y = choice), alpha = 0.5) + labs(title = &quot;Prior Predictive Check&quot;, x = &quot;Trial&quot;, y = &quot;Choice (0/1)&quot;) + theme_minimal() # Then the posterior predictive check p5 &lt;- ggplot() + # Add posterior prediction interval geom_ribbon(data = posterior_summary, aes(x = trial, ymin = lower, ymax = upper), alpha = 0.2, fill = &quot;blue&quot;) + # Add mean posterior prediction geom_line(data = posterior_summary, aes(x = trial, y = mean), color = &quot;blue&quot;) + # Add actual data points geom_point(data = tibble(trial = 1:length(data$h), choice = data$h), aes(x = trial, y = choice), alpha = 0.5) + labs(title = &quot;Posterior Predictive Check&quot;, x = &quot;Trial&quot;, y = &quot;Choice (0/1)&quot;) + theme_minimal() # Display plots side by side p4 + p5 # First, let&#39;s calculate the total number of 1s predicted in each posterior sample posterior_totals &lt;- draws_df %&gt;% dplyr::select(starts_with(&quot;posterior_preds[&quot;)) %&gt;% # Sum across rows to get total 1s per sample mutate(total_ones = rowSums(.)) # Do the same for prior predictions prior_totals &lt;- draws_df %&gt;% dplyr::select(starts_with(&quot;prior_preds[&quot;)) %&gt;% mutate(total_ones = rowSums(.)) # Calculate actual number of 1s in the data actual_ones &lt;- sum(data$h) # Create visualization comparing distributions ggplot() + # Prior predictive distribution geom_histogram(data = prior_totals, aes(x = total_ones, fill = &quot;Prior&quot;), alpha = 0.3) + # Posterior predictive distribution geom_histogram(data = posterior_totals, aes(x = total_ones, fill = &quot;Posterior&quot;), alpha = 0.3) + # Vertical line for actual data geom_vline(xintercept = actual_ones, linetype = &quot;dashed&quot;, color = &quot;black&quot;, size = 1) + # Aesthetics scale_fill_manual(values = c(&quot;Prior&quot; = &quot;red&quot;, &quot;Posterior&quot; = &quot;blue&quot;), name = &quot;Distribution&quot;) + labs(title = &quot;Distribution of Predicted Successes (1s) out of 120 Trials&quot;, subtitle = &quot;Comparing Prior, Posterior and Actual Data&quot;, x = &quot;Number of 1s&quot;, y = &quot;Density&quot;) + theme_minimal() + # Add annotation for actual value annotate(&quot;text&quot;, x = actual_ones, y = 0, label = paste(&quot;Actual:&quot;, actual_ones), vjust = -0.5) # Let&#39;s also print summary statistics prior_summary &lt;- prior_totals %&gt;% summarise( mean = mean(total_ones), sd = sd(total_ones), q025 = quantile(total_ones, 0.025), q975 = quantile(total_ones, 0.975) ) posterior_summary &lt;- posterior_totals %&gt;% summarise( mean = mean(total_ones), sd = sd(total_ones), q025 = quantile(total_ones, 0.025), q975 = quantile(total_ones, 0.975) ) print(&quot;Prior predictive summary:&quot;) ## [1] &quot;Prior predictive summary:&quot; print(prior_summary) ## # A tibble: 1 × 4 ## mean sd q025 q975 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 60.6 23.9 17 102 print(&quot;Posterior predictive summary:&quot;) ## [1] &quot;Posterior predictive summary:&quot; print(posterior_summary) ## # A tibble: 1 × 4 ## mean sd q025 q975 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 105. 4.64 95 113 # First let&#39;s calculate predicted probabilities for each draw and memory level predicted_probs &lt;- draws_df %&gt;% mutate( # Calculate probability of choosing right for each memory level # using the logistic function on our parameter estimates prob_low = inv_logit_scaled(bias + beta * logit_scaled(0.2)), prob_mid = inv_logit_scaled(bias + beta * logit_scaled(0.5)), prob_high = inv_logit_scaled(bias + beta * logit_scaled(0.8)) ) %&gt;% # Reshape to long format for easier plotting pivot_longer( cols = starts_with(&quot;prob_&quot;), names_to = &quot;memory_level&quot;, values_to = &quot;probability&quot; ) %&gt;% mutate( memory_value = case_when( memory_level == &quot;prob_low&quot; ~ 0.2, memory_level == &quot;prob_mid&quot; ~ 0.5, memory_level == &quot;prob_high&quot; ~ 0.8 ) ) # Do the same for prior predictions prior_probs &lt;- draws_df %&gt;% mutate( prob_low = inv_logit_scaled(bias_prior + beta_prior * logit_scaled(0.2)), prob_mid = inv_logit_scaled(bias_prior + beta_prior * logit_scaled(0.5)), prob_high = inv_logit_scaled(bias_prior + beta_prior * logit_scaled(0.8)) ) %&gt;% pivot_longer( cols = starts_with(&quot;prob_&quot;), names_to = &quot;memory_level&quot;, values_to = &quot;probability&quot; ) %&gt;% mutate( memory_value = case_when( memory_level == &quot;prob_low&quot; ~ 0.2, memory_level == &quot;prob_mid&quot; ~ 0.5, memory_level == &quot;prob_high&quot; ~ 0.8 ) ) # Create visualization with density plots p1 &lt;- ggplot() + # Add prior distributions geom_density(data = prior_probs, aes(x = probability, fill = &quot;Prior&quot;), alpha = 0.3) + # Add posterior distributions geom_density(data = predicted_probs, aes(x = probability, fill = &quot;Posterior&quot;), alpha = 0.3) + # Separate by memory level facet_wrap(~memory_value, labeller = labeller(memory_value = c( &quot;0.2&quot; = &quot;Low Memory (20% Right)&quot;, &quot;0.5&quot; = &quot;Neutral Memory (50% Right)&quot;, &quot;0.8&quot; = &quot;High Memory (80% Right)&quot; ))) + # Aesthetics scale_fill_manual(values = c(&quot;Prior&quot; = &quot;red&quot;, &quot;Posterior&quot; = &quot;blue&quot;), name = &quot;Distribution&quot;) + labs(title = &quot;Distribution of Predicted Probabilities at Different Memory Levels&quot;, x = &quot;Probability of Choosing Right&quot;, y = &quot;Density&quot;) + theme_minimal() # Alternative visualization using violin plots p2 &lt;- ggplot() + # Add prior distributions geom_violin(data = prior_probs, aes(x = factor(memory_value), y = probability, fill = &quot;Prior&quot;), alpha = 0.3, position = position_dodge(width = 0.5)) + # Add posterior distributions geom_violin(data = predicted_probs, aes(x = factor(memory_value), y = probability, fill = &quot;Posterior&quot;), alpha = 0.3, position = position_dodge(width = 0.5)) + # Aesthetics scale_fill_manual(values = c(&quot;Prior&quot; = &quot;red&quot;, &quot;Posterior&quot; = &quot;blue&quot;), name = &quot;Distribution&quot;) + scale_x_discrete(labels = c(&quot;Low\\n(20% Right)&quot;, &quot;Neutral\\n(50% Right)&quot;, &quot;High\\n(80% Right)&quot;)) + labs(title = &quot;Distribution of Predicted Probabilities by Memory Level&quot;, x = &quot;Memory Level&quot;, y = &quot;Probability of Choosing Right&quot;) + theme_minimal() # Display both visualizations p1 / p2 # 4. Check for divergences # Extract divergent transitions n_div &lt;- sum(draws_df$.divergent) print(paste(&quot;Number of divergent transitions:&quot;, n_div)) ## [1] &quot;Number of divergent transitions: 0&quot; Now that we know how to model memory as an internal state, we can play with making the update discount the past, setting a parameter that indicates after how many trials memory is lost, etc. 5.7.1 Trying out a more complex memory model, with a rate of forgetting that exponentially discounts the past stan_model &lt;- &quot; // The input (data) for the model. n of trials and h for (right and left) hand data { int&lt;lower=1&gt; n; array[n] int h; array[n] int other; } // The parameters accepted by the model. parameters { real bias; // how likely is the agent to pick right when the previous rate has no information (50-50)? real beta; // how strongly is previous rate impacting the decision? real&lt;lower=0, upper=1&gt; forgetting; } // The model to be estimated. model { vector[n] memory; // Priors target += beta_lpdf(forgetting | 1, 1); target += normal_lpdf(bias | 0, .3); target += normal_lpdf(beta | 0, .5); // Model, looping to keep track of memory for (trial in 1:n) { if (trial == 1) { memory[trial] = 0.5; } target += bernoulli_logit_lpmf(h[trial] | bias + beta * logit(memory[trial])); if (trial &lt; n){ memory[trial + 1] = (1 - forgetting) * memory[trial] + forgetting * other[trial]; if (memory[trial + 1] == 0){memory[trial + 1] = 0.01;} if (memory[trial + 1] == 1){memory[trial + 1] = 0.99;} } } } &quot; write_stan_file( stan_model, dir = &quot;stan/&quot;, basename = &quot;W3_InternalMemory2.stan&quot;) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W3_InternalMemory2.stan&quot; ## Specify where the model is file &lt;- file.path(&quot;stan/W3_InternalMemory2.stan&quot;) # File path for saved model model_file &lt;- &quot;simmodels/W3_InternalMemory2.rds&quot; # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(model_file)) { # Compile the model mod &lt;- cmdstan_model(file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) # The following command calls Stan with specific options. samples_memory_forgetting &lt;- mod$sample( data = data, seed = 123, chains = 1, parallel_chains = 2, threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 1000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99, ) # Save the fitted model samples_memory_forgetting$save_object(file = model_file) cat(&quot;Generated new model fit and saved to&quot;, model_file, &quot;\\n&quot;) } else { # Load existing results samples_memory_forgetting &lt;- readRDS(model_file) cat(&quot;Loaded existing model fit from&quot;, model_file, &quot;\\n&quot;) } ## Loaded existing model fit from simmodels/W3_InternalMemory2.rds samples_memory_forgetting$summary() ## # A tibble: 4 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lp__ -46.7 -46.4 1.19 1.01 -49.0 -45.3 0.999 356. 497. ## 2 bias 0.503 0.499 0.260 0.277 0.0958 0.937 1.01 293. 349. ## 3 beta 0.833 0.840 0.259 0.247 0.366 1.27 1.00 272. 274. ## 4 forgetting 0.191 0.166 0.109 0.0725 0.0809 0.381 1.00 312. 277. The memory model we’ve implemented can be seen as part of a broader family of models that track and update beliefs based on incoming evidence. Let’s explore how it relates to some key frameworks. 5.7.2 Connection to Kalman Filters Our memory model updates beliefs about the probability of right-hand choices using a weighted average of past observations. This is conceptually similar to how a Kalman filter works, though simpler: Kalman filters maintain both an estimate and uncertainty about that estimate They optimally weight new evidence based on relative uncertainty Our model uses a fixed weighting scheme (1/trial or the forgetting parameter) The key difference is that Kalman filters dynamically adjust how much they learn from new evidence based on uncertainty, while our model uses a fixed learning scheme. 5.8 Relationship to Rescorla-Wagner The Rescorla-Wagner model of learning follows the form: V(t+1) = V(t) + α(λ - V(t)) where: V(t) is the current estimate α is the learning rate λ is the observed outcome (λ - V(t)) is the prediction error Our memory model with forgetting parameter follows a very similar structure: memory(t+1) = (1-forgetting) * memory(t) + forgetting * outcome(t) This can be rewritten as: memory(t+1) = memory(t) + forgetting * (outcome(t) - memory(t)) Making the parallel clear: our forgetting parameter acts as the learning rate α in Rescorla-Wagner. 5.8.1 Connection to Hierarchical Gaussian Filter (HGF) The HGF extends these ideas by: Tracking beliefs at multiple levels Allowing learning rates to vary over time Explicitly modeling environmental volatility Our model could be seen as the simplest case of an HGF where: We only track one level (probability of right-hand choice) Have a fixed learning rate (forgetting parameter) Don’t explicitly model environmental volatility 5.8.2 Implications for Model Development Understanding these relationships helps us think about how models relate to each other and to extend our model: We could add uncertainty estimates to get Kalman-like behavior We could make the forgetting parameter dynamic to capture changing learning rates We could add multiple levels to track both immediate probabilities and longer-term trends Each extension would make the model more flexible but also more complex to fit to data. The choice depends on our specific research questions and available data. 5.9 Bayesian memory agent We can also model the memory agent in a Bayesian framework. This allows us to model the agent as (optimally) estimating a possible distribution of rates from the other’s behavior and keep all the uncertainty. stan_model &lt;- &quot; data { int&lt;lower=1&gt; n; // number of trials array[n] int h; // agent&#39;s choices (0 or 1) array[n] int other; // other player&#39;s choices (0 or 1) } parameters { real&lt;lower=0&gt; alpha_prior; // Prior alpha parameter real&lt;lower=0&gt; beta_prior; // Prior beta parameter } transformed parameters { vector[n] alpha; // Alpha parameter at each trial vector[n] beta; // Beta parameter at each trial vector[n] rate; // Expected rate at each trial // Initialize with prior alpha[1] = alpha_prior; beta[1] = beta_prior; rate[1] = alpha[1] / (alpha[1] + beta[1]); // Sequential updating of Beta distribution for(t in 2:n) { // Update Beta parameters based on previous observation alpha[t] = alpha[t-1] + other[t-1]; beta[t] = beta[t-1] + (1 - other[t-1]); // Calculate expected rate rate[t] = alpha[t] / (alpha[t] + beta[t]); } } model { // Priors on hyperparameters target += gamma_lpdf(alpha_prior | 2, 1); target += gamma_lpdf(beta_prior | 2, 1); // Agent&#39;s choices follow current rate estimates for(t in 1:n) { target += bernoulli_lpmf(h[t] | rate[t]); } } generated quantities { array[n] int prior_preds; array[n] int posterior_preds; real initial_rate = alpha_prior / (alpha_prior + beta_prior); // Prior predictions use initial rate for(t in 1:n) { prior_preds[t] = bernoulli_rng(initial_rate); } // Posterior predictions use sequentially updated rates for(t in 1:n) { posterior_preds[t] = bernoulli_rng(rate[t]); } } &quot; write_stan_file( stan_model, dir = &quot;stan/&quot;, basename = &quot;W3_BayesianMemory.stan&quot;) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W3_BayesianMemory.stan&quot; ## Specify where the model is file &lt;- file.path(&quot;stan/W3_BayesianMemory.stan&quot;) # File path for saved model model_file &lt;- &quot;simmodels/W3_BayesianMemory.rds&quot; # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(model_file)) { # Compile the model mod &lt;- cmdstan_model(file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) # The following command calls Stan with specific options. samples_memory_bayes &lt;- mod$sample( data = data, seed = 123, chains = 1, parallel_chains = 2, threads_per_chain = 2, iter_warmup = 1000, iter_sampling = 1000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99, ) # Save the fitted model samples_memory_bayes$save_object(file = model_file) cat(&quot;Generated new model fit and saved to&quot;, model_file, &quot;\\n&quot;) } else { # Load existing results samples_memory_bayes &lt;- readRDS(model_file) cat(&quot;Loaded existing model fit from&quot;, model_file, &quot;\\n&quot;) } ## Loaded existing model fit from simmodels/W3_BayesianMemory.rds samples_memory_bayes$summary() ## # A tibble: 604 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lp__ -43.0 -42.8 0.898 0.724 -44.6 -42.1 0.999 271. 615. ## 2 alpha_prior 2.99 2.60 1.76 1.67 0.794 6.37 1.00 463. 368. ## 3 beta_prior 0.591 0.497 0.415 0.348 0.120 1.35 1.01 550. 529. ## 4 alpha[1] 2.99 2.60 1.76 1.67 0.794 6.37 1.00 463. 368. ## 5 alpha[2] 3.99 3.60 1.76 1.67 1.79 7.37 1.00 463. 368. ## 6 alpha[3] 4.99 4.60 1.76 1.67 2.79 8.37 1.00 463. 368. ## 7 alpha[4] 5.99 5.60 1.76 1.67 3.79 9.37 1.00 463. 368. ## 8 alpha[5] 6.99 6.60 1.76 1.67 4.79 10.4 1.00 463. 368. ## 9 alpha[6] 7.99 7.60 1.76 1.67 5.79 11.4 1.00 463. 368. ## 10 alpha[7] 8.99 8.60 1.76 1.67 6.79 12.4 1.00 463. 368. ## # ℹ 594 more rows # Extract draws draws_df &lt;- as_draws_df(samples_memory_bayes$draws()) # First let&#39;s look at the priors ggplot(draws_df) + geom_density(aes(alpha_prior), fill = &quot;blue&quot;, alpha = 0.3) + geom_density(aes(beta_prior), fill = &quot;red&quot;, alpha = 0.3) + theme_classic() + labs(title = &quot;Prior Distributions&quot;, x = &quot;Parameter Value&quot;, y = &quot;Density&quot;) # Now let&#39;s look at how the rate evolves over trials # First melt the rate values across trials into long format rate_df &lt;- draws_df %&gt;% dplyr::select(starts_with(&quot;rate[&quot;)) %&gt;% pivot_longer(everything(), names_to = &quot;trial&quot;, values_to = &quot;rate&quot;, names_pattern = &quot;rate\\\\[(\\\\d+)\\\\]&quot;) %&gt;% mutate(trial = as.numeric(trial)) # Calculate summary statistics for each trial rate_summary &lt;- rate_df %&gt;% group_by(trial) %&gt;% summarise( mean_rate = mean(rate), lower = quantile(rate, 0.025), upper = quantile(rate, 0.975) ) plot_data &lt;- tibble(trial = seq(120), choices = data$other) # Plot the evolution of rate estimates ggplot(rate_summary, aes(x = trial)) + geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) + geom_line(aes(y = mean_rate), color = &quot;blue&quot;) + # Add true data points geom_line(data = plot_data, aes(x = trial, y = choices), color = &quot;orange&quot;, alpha = 0.5) + theme_classic() + labs(title = &quot;Evolution of Rate Estimates&quot;, x = &quot;Trial&quot;, y = &quot;Rate&quot;, subtitle = &quot;Blue line: posterior mean, Gray band: 95% CI&quot;) + ylim(0, 1) # Let&#39;s also look at the correlation between alpha and beta parameters ggplot(draws_df) + geom_point(aes(alpha_prior, beta_prior), alpha = 0.1) + theme_classic() + labs(title = &quot;Correlation between Alpha and Beta Parameters&quot;, x = &quot;Alpha&quot;, y = &quot;Beta&quot;) 5.10 Conclusion: From Simple Models to Complex Cognitive Processes Throughout this chapter, we’ve progressed from basic parameter estimation to increasingly sophisticated models of decision-making. We began with a simple biased agent model, demonstrating how Bayesian inference allows us to recover underlying parameters from observed behavior. We saw how we can transform parameters from one scale to another - here from probability-scale to log-odds parameterizations -, thus gaining flexibility that will prove valuable for more complex models. The transition to memory-based models illustrated how we can incorporate psychological theory into our statistical framework. We explored different approaches to modeling memory - from treating it as an external predictor to implementing it as an internal state variable that evolves over time. The final exploration of exponential forgetting demonstrated how we can capture more nuanced cognitive processes while maintaining mathematical tractability. This progression sets the stage for Chapter 12, where we’ll explore how these memory updating mechanisms relate to reinforcement learning models. The exponential discounting of past events we implemented here represents a simplified version of the learning mechanisms we’ll encounter in reinforcement learning. Several key principles emerged that will guide our future modeling work: The importance of systematic model validation through parameter recovery studies and prior-posterior checks. These techniques help ensure our models can meaningfully capture the processes we aim to study. The value of starting simple and gradually adding complexity. Each model we implemented built upon previous ones, allowing us to understand the impact of new components while maintaining a solid foundation. This principle will become particularly important when we tackle reinforcement learning models, where multiple parameters interact in complex ways to produce learning behavior. The relationship between mathematical convenience and psychological reality. The log-odds transformation, for instance, provides both computational benefits and psychological insights about how humans might represent probabilities. Similarly, the memory updating rules we explored here foreshadow the prediction error calculations central to reinforcement learning and relates very tightly to other popular models like the Kalman filter and the Hierarchical Gaussian Filter. In the next chapters, we will build upon these foundations to tackle even more sophisticated cognitive models. Chapter 5 will introduce multilevel modeling, allowing us to capture individual differences while maintaining population-level insights. This will set the stage for exploring how different individuals might employ different strategies or show varying levels of memory decay in their decision-making processes. These individual differences become again relevant in future models where parameters like learning rate, or bias for social information can vary substantially across individuals. "],["model-quality-assessment.html", "Chapter 6 Model Quality Assessment 6.1 Introduction 6.2 Generating and plotting additional variables 6.3 Assessing priors 6.4 Prior Predictive Checks 6.5 Posterior Predictive Checks 6.6 Prior sensitivity analysis 6.7 The memory model 6.8 Prior sensitivity check for the memory model 6.9 Conclusion", " Chapter 6 Model Quality Assessment 6.1 Introduction Building computational models is only the first step in understanding cognitive processes. We must rigorously evaluate whether our models actually capture meaningful patterns in behavior and provide reliable insights. This chapter introduces systematic approaches for assessing model quality, focusing on techniques that help us understand both the strengths and limitations of our cognitive models. This document covers: - generating and plotting priors (against posteriors) - generating and plotting predictive checks (prior and posterior ones) - prior sensitivity checks [I SHOULD RESTRUCTURE THE DOCUMENT SO THAT PRIOR PREDICTIVE CHECKS COME BEFORE PRIOR / POSTERIOR UPDATE CHECKS] 6.2 Generating and plotting additional variables As we try to understand our model, we might want to plot how the prior relates to the posterior, or - in other words, what has the model learned from looking at the data? We can do so by overlaying the prior and the posterior distributions, what is also called a “prior - posterior update check”. Stan does not automatically save the prior distribution, so we need to tell it to generate and save prior distributions in a convenient place so we can easily plot or use them at will from R. Luckily, Stan gives us a dedicated code chunk to do that: the generated quantities chunk. As before, we need to define the kind of variable we want to save, and then how to generate it. If we take the example of the random agent (with a bias), we have one parameter: theta. We can then generate theta according to the prior in generated quantities. While we are at this, we can also generate a nicer version of the posterior estimate for the theta parameter, now in probability scale (instead of log odds). However, prior and posterior estimates are not always the most immediate thing to understand. For instance, we might have trouble having a good grasp for how the uncertainty in the estimate will play out on 120 trials, or 6 trials, or however many trials we are planning for our experiment. Luckily, we can ask Stan to run predictions from either priors or posteriors, or both: given the priors how many trials will have “right hand” choice? and given the posterior estimates? As we use complex models, the relation between prior/posterior estimates and predictions becomes less and less intuitive. Simulating their implications for the outcomes - also called prior/posterior predictive checks - becomes a very useful tool to adjust our priors and their uncertainty so that they reflect what we know of the outcome scale; as well as to assess whether the model (and its posterior estimates) can appropriately describe the data we observe, or there’s some bias there. More discussion of this can be found at https://4ccoxau.github.io/PriorsWorkshop/. pacman::p_load(tidyverse, here, posterior, cmdstanr, brms, tidybayes) d &lt;- read_csv(&quot;simdata/W3_randomnoise.csv&quot;) stan_model &lt;- &quot; // This model infers a random bias from a sequences of 1s and 0s (right and left hand choices) // The input (data) for the model. n of trials and the sequence of choices (right as 1, left as 0) data { int&lt;lower=1&gt; n; // n of trials array[n] int h; // sequence of choices (right as 1, left as 0) as long as n } // The parameters that the model needs to estimate (theta) parameters { real theta; // note it is unbounded as we now work on log odds } // The model to be estimated (a bernoulli, parameter theta, prior on the theta) model { // The prior for theta on a log odds scale is a normal distribution with a mean of 0 and a sd of 1. // This covers most of the probability space between 0 and 1, after being converted to probability. target += normal_lpdf(theta | 0, 1); // The model consists of a bernoulli distribution (binomial w 1 trial only) with a rate theta, // note we specify it uses a logit link (theta is in logodds) target += bernoulli_logit_lpmf(h | theta); } generated quantities{ real&lt;lower=0, upper=1&gt; theta_prior; // theta prior parameter, on a prob scale (0-1) real&lt;lower=0, upper=1&gt; theta_posterior; // theta posterior parameter, on a prob scale (0-1) int&lt;lower=0, upper=n&gt; prior_preds; // distribution of right hand choices according to the prior int&lt;lower=0, upper=n&gt; posterior_preds; // distribution of right hand choices according to the posterior theta_prior = inv_logit(normal_rng(0,1)); // generating the prior on a log odds scale and converting theta_posterior = inv_logit(theta); // converting the posterior estimate from log odds to prob. prior_preds = binomial_rng(n, theta_prior); posterior_preds = binomial_rng(n, inv_logit(theta)); } &quot; write_stan_file( stan_model, dir = &quot;stan/&quot;, basename = &quot;W5_SimpleBernoulli_logodds.stan&quot;) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W5_SimpleBernoulli_logodds.stan&quot; ## With the logit format ## Specify where the model is file &lt;- file.path(&quot;stan/W5_SimpleBernoulli_logodds.stan&quot;) mod &lt;- cmdstan_model(file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) d1 &lt;- d %&gt;% subset(noise == 0 &amp; rate == 0.8) ## Create the data. N.B. note the two variables have different lengths: 1 for n, n for h. data &lt;- list( n = 120, # n of trials h = d1$choice # sequence of choices (h stands for hand) ) # The following command calls Stan with specific options. samples &lt;- mod$sample( data = data, seed = 123, chains = 2, parallel_chains = 2, threads_per_chain = 2, iter_warmup = 1000, iter_sampling = 2000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99, ) ## Running MCMC with 2 parallel chains, with 2 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Chain 2 finished in 0.1 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.1 seconds. ## Total execution time: 0.2 seconds. draws_df &lt;- as_draws_df(samples$draws()) 6.3 Assessing priors # Now let&#39;s plot the density for theta (prior and posterior) ggplot(draws_df) + geom_histogram(aes(theta_posterior), fill = &quot;blue&quot;, alpha = 0.3) + geom_histogram(aes(theta_prior), fill = &quot;red&quot;, alpha = 0.3) + geom_vline(xintercept = 0.8, linetype = &quot;dashed&quot;, color = &quot;black&quot;, size = 1.5) + xlab(&quot;Rate&quot;) + ylab(&quot;Estimate Densities&quot;) + theme_classic() 6.4 Prior Predictive Checks Prior predictive checks involve simulating data from our model using only the prior distributions, before seeing any actual data. This helps us understand what kinds of patterns our model assumes are possible before we begin fitting to real observations. These predictions should be assessed for: Plausible ranges of behavior Appropriate levels of uncertainty Preservation of known constraints Coverage of theoretically important patterns 6.5 Posterior Predictive Checks After fitting our models, posterior predictive checks help us determine whether the fitted model can reproduce key patterns in our observed data. We generate new data using parameters sampled from the posterior distribution and compare these simulations to our actual observations. For decision-making models, important patterns to check include: Overall choice proportions Sequential dependencies in choices Learning curves Response to feedback Individual differences in strategies ggplot(draws_df) + geom_histogram(aes(prior_preds), color = &quot;darkblue&quot;, fill = &quot;blue&quot;, alpha = 0.3) + xlab(&quot;Predicted choices of right out of 120 trials&quot;) + ylab(&quot;Posterior Density&quot;) + theme_classic() ggplot(draws_df) + geom_histogram(aes(posterior_preds), color = &quot;darkblue&quot;, fill = &quot;blue&quot;, alpha = 0.3, bins = 90) + geom_point(x = sum(data$h), y = 0, color = &quot;red&quot;, shape = 17, size = 5) + xlab(&quot;Predicted choices of right out of 120 trials&quot;) + ylab(&quot;Posterior Density&quot;) + theme_classic() ggplot(draws_df) + geom_histogram(aes(prior_preds), color = &quot;lightblue&quot;, fill = &quot;blue&quot;, alpha = 0.3, bins = 90) + geom_histogram(aes(posterior_preds), color = &quot;darkblue&quot;, fill = &quot;blue&quot;, alpha = 0.3, bins = 90) + geom_point(x = sum(data$h), y = 0, color = &quot;red&quot;, shape = 17, size = 5) + xlab(&quot;Predicted choices of right out of 120 trials&quot;) + ylab(&quot;Posterior Density&quot;) + theme_classic() 6.6 Prior sensitivity analysis ## Now we adding different priors for theta prior_mean &lt;- seq(-3, 3, .5) prior_sd &lt;- seq(0.1, 1, 0.1) priors &lt;- expand.grid(prior_mean, prior_sd) priors &lt;- tibble(prior_mean = priors$Var1, prior_sd = priors$Var2) stan_model &lt;- &quot; // The input (data) for the model data { int&lt;lower=1&gt; n; array[n] int h; real prior_mean; real&lt;lower=0&gt; prior_sd; } // The parameters accepted by the model. parameters { real theta; } // The model to be estimated. model { // Prior target += normal_lpdf(theta | prior_mean, prior_sd); // Model target += bernoulli_logit_lpmf(h | theta); } generated quantities{ real&lt;lower=0, upper=1&gt; theta_prior; real&lt;lower=0, upper=1&gt; theta_posterior; int&lt;lower=0, upper=n&gt; prior_preds; int&lt;lower=0, upper=n&gt; posterior_preds; theta_prior = inv_logit(normal_rng(0,1)); theta_posterior = inv_logit(theta); prior_preds = binomial_rng(n, theta_prior); posterior_preds = binomial_rng(n, inv_logit(theta)); } &quot; write_stan_file( stan_model, dir = &quot;stan/&quot;, basename = &quot;W4_PriorBernoulli.stan&quot;) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W4_PriorBernoulli.stan&quot; file &lt;- file.path(&quot;stan/W4_PriorBernoulli.stan&quot;) mod &lt;- cmdstan_model(file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) dd &lt;- d %&gt;% subset(noise == 0.1 &amp; rate == 0.8) pacman::p_load(future, purrr, furrr) plan(multisession, workers = 4) sim_d_and_fit &lt;- function(prior_mean, prior_sd) { data &lt;- list( n = nrow(dd), h = dd$choice, prior_mean = prior_mean, prior_sd = prior_sd ) samples &lt;- mod$sample( data = data, seed = 1000, chains = 1, parallel_chains = 1, threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 2000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99, ) draws_df &lt;- as_draws_df(samples$draws()) temp &lt;- tibble(theta_prior = draws_df$theta_prior, theta_posterior = draws_df$theta_posterior, prior_preds = draws_df$prior_preds, posterior_preds = draws_df$posterior_preds, prior_mean = prior_mean, prior_sd = prior_sd) return(temp) } # Commenting this out to ensure faster compiling time for the book. Uncomment to run the code recovery_df &lt;- future_pmap_dfr(priors, sim_d_and_fit, .options = furrr_options(seed = TRUE)) ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. write_csv(recovery_df, &quot;simdata/W5_priorSensitivityRecovery.csv&quot;) Now we load the data and plot it recovery_df &lt;- read_csv(&quot;simdata/W5_priorSensitivityRecovery.csv&quot;) ggplot(recovery_df, aes(prior_mean, theta_posterior)) + geom_point(alpha = 0.1) + geom_hline(yintercept = 0.8, color = &quot;red&quot;) + geom_smooth() + facet_wrap(.~prior_sd) + theme_classic() 6.7 The memory model We can do the same for the memory model: generate prior distributions to overlay to the posteriors (prior-posterior update checks), generate predicted outcomes based on the priors (prior predictive checks) and on the posteriors (posterior predictive checks). N.B. prior and posterior predictions now depend on the value on memory. I identified 3 meaningful values for the memory value (e.g. 0.5, 0.7, 0.9) and used those to generate 3 prior and posterior predictive checks. # We subset to only include no noise and a specific rate d1 &lt;- d %&gt;% subset(noise == 0 &amp; rate == 0.8) %&gt;% rename(Other = choice) %&gt;% mutate(cumulativerate = lag(cumulativerate, 1)) d1$cumulativerate[1] &lt;- 0.5 # no prior info at first trial d1$cumulativerate[d1$cumulativerate == 0] &lt;- 0.01 d1$cumulativerate[d1$cumulativerate == 1] &lt;- 0.99 # Now we create the memory agent with a coefficient of 0.9 bias = 0 beta = 0.9 MemoryAgent_f &lt;- function(bias, beta, cumulativerate){ choice = rbinom(1, 1, inv_logit_scaled(bias + beta * logit_scaled(cumulativerate))) return(choice) } d1$Self[1] &lt;- RandomAgentNoise_f(0.5, 0) for (i in 1:trials) { d1$Self[i] &lt;- MemoryAgent_f(bias, beta, d1$cumulativerate[i]) } ## Create the data. data &lt;- list( n = 120, h = d1$Self, other = d1$Other ) stan_model &lt;- &quot; // The input (data) for the model. n of trials and h for (right and left) hand data { int&lt;lower=1&gt; n; array[n] int h; array[n] int other; } // The parameters accepted by the model. parameters { real bias; // how likely is the agent to pick right when the previous rate has no information (50-50)? real beta; // how strongly is previous rate impacting the decision? } transformed parameters{ vector[n] memory; for (trial in 1:n){ if (trial == 1) { memory[trial] = 0.5; } if (trial &lt; n){ memory[trial + 1] = memory[trial] + ((other[trial] - memory[trial]) / trial); if (memory[trial + 1] == 0){memory[trial + 1] = 0.01;} if (memory[trial + 1] == 1){memory[trial + 1] = 0.99;} } } } // The model to be estimated. model { // Priors target += normal_lpdf(bias | 0, .3); target += normal_lpdf(beta | 0, .5); // Model, looping to keep track of memory for (trial in 1:n) { target += bernoulli_logit_lpmf(h[trial] | bias + beta * logit(memory[trial])); } } generated quantities{ real bias_prior; real beta_prior; int&lt;lower=0, upper=n&gt; prior_preds5; int&lt;lower=0, upper=n&gt; post_preds5; int&lt;lower=0, upper=n&gt; prior_preds7; int&lt;lower=0, upper=n&gt; post_preds7; int&lt;lower=0, upper=n&gt; prior_preds9; int&lt;lower=0, upper=n&gt; post_preds9; bias_prior = normal_rng(0, 0.3); beta_prior = normal_rng(0, 0.5); prior_preds5 = binomial_rng(n, inv_logit(bias_prior + beta_prior * logit(0.5))); prior_preds7 = binomial_rng(n, inv_logit(bias_prior + beta_prior * logit(0.7))); prior_preds9 = binomial_rng(n, inv_logit(bias_prior + beta_prior * logit(0.9))); post_preds5 = binomial_rng(n, inv_logit(bias + beta * logit(0.5))); post_preds7 = binomial_rng(n, inv_logit(bias + beta * logit(0.7))); post_preds9 = binomial_rng(n, inv_logit(bias + beta * logit(0.9))); } &quot; write_stan_file( stan_model, dir = &quot;stan/&quot;, basename = &quot;W5_MemoryBernoulli.stan&quot;) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W5_MemoryBernoulli.stan&quot; ## Specify where the model is file &lt;- file.path(&quot;stan/W5_MemoryBernoulli.stan&quot;) mod &lt;- cmdstan_model(file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) # The following command calls Stan with specific options. samples &lt;- mod$sample( data = data, seed = 123, chains = 1, parallel_chains = 2, threads_per_chain = 2, iter_warmup = 1000, iter_sampling = 1000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99, ) ## Running MCMC with 1 chain, with 2 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. samples$summary() ## # A tibble: 131 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lp__ -49.3 -49.0 0.975 0.683 -51.3 -48.4 1.00 336. 433. ## 2 bias 0.108 0.101 0.270 0.251 -0.318 0.573 1.00 363. 258. ## 3 beta 0.870 0.874 0.184 0.179 0.578 1.17 1.00 378. 466. ## 4 memory[1] 0.5 0.5 0 0 0.5 0.5 NA NA NA ## 5 memory[2] 0.99 0.99 0 0 0.99 0.99 NA NA NA ## 6 memory[3] 0.995 0.995 0 0 0.995 0.995 NA NA NA ## 7 memory[4] 0.997 0.997 0 0 0.997 0.997 NA NA NA ## 8 memory[5] 0.998 0.998 0 0 0.998 0.998 NA NA NA ## 9 memory[6] 0.998 0.998 0 0 0.998 0.998 NA NA NA ## 10 memory[7] 0.998 0.998 0 0 0.998 0.998 NA NA NA ## # ℹ 121 more rows # Extract posterior samples and include sampling of the prior: draws_df &lt;- as_draws_df(samples$draws()) # Now let&#39;s plot the density for bias (prior and posterior) ggplot(draws_df) + geom_density(aes(bias), fill = &quot;blue&quot;, alpha = 0.3) + geom_density(aes(bias_prior), fill = &quot;red&quot;, alpha = 0.3) + geom_vline(xintercept = 0, size = 2) + xlab(&quot;Bias&quot;) + ylab(&quot;Posterior Density&quot;) + theme_classic() ggplot(draws_df) + geom_density(aes(beta), fill = &quot;blue&quot;, alpha = 0.3) + geom_density(aes(beta_prior), fill = &quot;red&quot;, alpha = 0.3) + geom_vline(xintercept = 0.9, size = 2) + xlab(&quot;MemoryBeta&quot;) + ylab(&quot;Posterior Density&quot;) + theme_classic() ggplot(draws_df) + geom_histogram(aes(`prior_preds5`), color = &quot;yellow&quot;, fill = &quot;lightyellow&quot;, alpha = 0.2) + geom_histogram(aes(`prior_preds7`), color = &quot;green&quot;, fill = &quot;lightgreen&quot;, alpha = 0.2) + geom_histogram(aes(`prior_preds9`), color = &quot;blue&quot;, fill = &quot;lightblue&quot;, alpha = 0.2) + xlab(&quot;Predicted choices of right out of 120 trials&quot;) + ylab(&quot;Posterior Density&quot;) + theme_classic() ggplot(draws_df) + geom_histogram(aes(`post_preds5`), color = &quot;yellow&quot;, fill = &quot;lightyellow&quot;, alpha = 0.3, bins = 90) + geom_histogram(aes(`post_preds7`), color = &quot;green&quot;, fill = &quot;lightgreen&quot;, alpha = 0.3, bins = 90) + geom_histogram(aes(`post_preds9`), color = &quot;blue&quot;, fill = &quot;lightblue&quot;, alpha = 0.3, bins = 90) + #geom_point(x = sum(data$h), y = 0, color = &quot;red&quot;, shape = 17, size = 5) + xlab(&quot;Predicted choices of right out of 120 trials&quot;) + ylab(&quot;Posterior Density&quot;) + theme_classic() ggplot(draws_df) + geom_histogram(aes(`prior_preds5`), color = &quot;lightblue&quot;, fill = &quot;blue&quot;, alpha = 0.3, bins = 90) + geom_histogram(aes(`post_preds5`), color = &quot;darkblue&quot;, fill = &quot;blue&quot;, alpha = 0.3, bins = 90) + xlab(&quot;Predicted choices of right out of 120 trials&quot;) + ylab(&quot;Posterior Density&quot;) + theme_classic() 6.8 Prior sensitivity check for the memory model ## Now we adding different priors for theta prior_mean_bias &lt;- 0 prior_sd_bias &lt;- seq(0.1, 0.5, 0.1) prior_mean_beta &lt;- 0 prior_sd_beta &lt;- seq(0.1, 0.5, 0.1) priors &lt;- tibble(expand.grid(tibble(prior_mean_bias, prior_sd_bias, prior_mean_beta, prior_sd_beta))) stan_model &lt;- &quot; // The input (data) for the model data { int&lt;lower=1&gt; n; array[n] int h; array[n] int other; real prior_mean_bias; real&lt;lower=0&gt; prior_sd_bias; real prior_mean_beta; real&lt;lower=0&gt; prior_sd_beta; } // The parameters accepted by the model. parameters { real bias; // how likely is the agent to pick right when the previous rate has no information (50-50)? real beta; // how strongly is previous rate impacting the decision? } transformed parameters{ vector[n] memory; for (trial in 1:n){ if (trial == 1) { memory[trial] = 0.5; } if (trial &lt; n){ memory[trial + 1] = memory[trial] + ((other[trial] - memory[trial]) / trial); if (memory[trial + 1] == 0){memory[trial + 1] = 0.01;} if (memory[trial + 1] == 1){memory[trial + 1] = 0.99;} } } } // The model to be estimated. model { // The priors target += normal_lpdf(bias | prior_mean_bias, prior_sd_bias); target += normal_lpdf(beta | prior_mean_beta, prior_sd_beta); // The model target += bernoulli_logit_lpmf(h | bias + beta * logit(memory)); } generated quantities{ real bias_prior; real beta_prior; int&lt;lower=0, upper=n&gt; prior_preds5; int&lt;lower=0, upper=n&gt; post_preds5; int&lt;lower=0, upper=n&gt; prior_preds7; int&lt;lower=0, upper=n&gt; post_preds7; int&lt;lower=0, upper=n&gt; prior_preds9; int&lt;lower=0, upper=n&gt; post_preds9; bias_prior = normal_rng(prior_mean_bias, prior_sd_bias); beta_prior = normal_rng(prior_mean_beta, prior_sd_beta); prior_preds5 = binomial_rng(n, inv_logit(bias_prior + beta_prior * logit(0.5))); prior_preds7 = binomial_rng(n, inv_logit(bias_prior + beta_prior * logit(0.7))); prior_preds9 = binomial_rng(n, inv_logit(bias_prior + beta_prior * logit(0.9))); post_preds5 = binomial_rng(n, inv_logit(bias + beta * logit(0.5))); post_preds7 = binomial_rng(n, inv_logit(bias + beta * logit(0.7))); post_preds9 = binomial_rng(n, inv_logit(bias + beta * logit(0.9))); } &quot; write_stan_file( stan_model, dir = &quot;stan/&quot;, basename = &quot;W5_PriorMemory.stan&quot;) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W5_PriorMemory.stan&quot; file &lt;- file.path(&quot;stan/W5_PriorMemory.stan&quot;) mod &lt;- cmdstan_model(file, cpp_options = list(stan_threads = TRUE)) dd &lt;- d %&gt;% subset(noise == 0.1 &amp; rate == 0.8) %&gt;% mutate(memory = lag(cumulativerate, 1)) dd$memory[1] &lt;- 0.5 pacman::p_load(future, purrr, furrr) plan(multisession, workers = 4) sim_d_and_fit &lt;- function(prior_mean_bias, prior_sd_bias, prior_mean_beta, prior_sd_beta) { data &lt;- list( n = nrow(d1), h = d1$Self, other = d1$Other, prior_mean_bias = prior_mean_bias, prior_sd_bias = prior_sd_bias, prior_mean_beta = prior_mean_beta, prior_sd_beta = prior_sd_beta ) samples &lt;- mod$sample( data = data, seed = 1000, chains = 1, parallel_chains = 1, threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 2000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99, ) draws_df &lt;- as_draws_df(samples$draws()) temp &lt;- tibble(bias_prior = draws_df$bias_prior, beta_prior = draws_df$beta_prior, bias_posterior = draws_df$bias, beta_posterior = draws_df$beta, prior_preds5 = draws_df$prior_preds5, prior_preds7 = draws_df$prior_preds7, prior_preds9 = draws_df$prior_preds9, posterior_preds5 = draws_df$post_preds5, posterior_preds7 = draws_df$post_preds7, posterior_preds9 = draws_df$post_preds9, prior_mean_bias = prior_mean_bias, prior_sd_bias = prior_sd_bias, prior_mean_beta = prior_mean_beta, prior_sd_beta = prior_sd_beta) return(temp) } # Commenting this out to ensure the book compiles faster. Uncomment to run the code. recovery_df &lt;- future_pmap_dfr(priors, sim_d_and_fit, .options = furrr_options(seed = TRUE)) ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. write_csv(recovery_df, &quot;simdata/W5_MemoryPriorSensitivity.csv&quot;) ggplot(recovery_df, aes(prior_sd_beta, beta_posterior)) + geom_point(alpha = 0.1) + geom_hline(yintercept = 0.8, color = &quot;red&quot;) + geom_smooth(method = lm) + facet_wrap(.~prior_sd_bias) + theme_classic() 6.9 Conclusion Rigorous model assessment is essential for developing reliable insights into cognitive processes. The techniques covered in this chapter provide a systematic framework for validating our models and understanding their limitations. As we move forward to more complex models incorporating individual differences and learning mechanisms, these quality checks become increasingly important for ensuring our conclusions are well-supported by the evidence. In the next chapter, we’ll build on these foundations as we explore multilevel modeling approaches that can capture individual differences while maintaining population-level insights. "],["individual-differences-in-cognitive-strategies-multilevel-modeling.html", "Chapter 7 Individual Differences in Cognitive Strategies (Multilevel modeling) 7.1 Introduction 7.2 Learning Objectives 7.3 The Value of Multilevel Modeling 7.4 Graphical Model Visualization 7.5 Generating the agents 7.6 Plotting the agents 7.7 Coding the multilevel agents 7.8 Multilevel Random Agent Model 7.9 Let’s look at individuals 7.10 Prior sensitivity checks 7.11 Parameter recovery 7.12 Multilevel Memory Agent Model 7.13 Comparing Pooling Approaches 7.14 Comparing Pooling Approaches 7.15 Multilevel Modeling Cheatsheet 7.16 Conclusion: The Power and Challenges of Multilevel Modeling 7.17 Exercises (just some ideas)", " Chapter 7 Individual Differences in Cognitive Strategies (Multilevel modeling) 7.1 Introduction Our exploration of decision-making models has so far focused on single agents or averaged behavior across many agents. However, cognitive science consistentlyreveals that individuals differ systematically in how they approach tasks and process information. Some people may be more risk-averse, have better memory, learn faster, or employ entirely different strategies than others. This chapter introduces multilevel modeling as a powerful framework for capturing these individual differences while still identifying population-level patterns. Multilevel modeling (also called hierarchical modeling) provides a powerful framework for addressing this challenge. It allows us to simultaneously: Capture individual differences across participants Identify population-level patterns that generalize across individuals Improve estimates for individuals with limited data by leveraging information from the group Consider our matching pennies game: different players might vary in their strategic sophistication, memory capacity, or learning rates. Some may show strong biases toward particular choices while others adapt more flexibly to their opponents. Multilevel modeling allows us to capture these variations while still understanding what patterns hold across the population. Consider our matching pennies game: players might vary in their strategic sophistication, memory capacity, or learning rates. Some may show strong biases toward particular choices while others adapt more flexibly to their opponents. Multilevel modeling allows us to quantify these variations while still understanding what patterns hold across the population. 7.2 Learning Objectives After completing this chapter, you will be able to: Understand how multilevel modeling balances individual and group-level information Distinguish between complete pooling, no pooling, and partial pooling approaches to modeling group and individual variation Use different parameterizations to improve model efficiency Evaluate model quality through systematic parameter recovery studies Apply multilevel modeling techniques to cognitive science questions 7.3 The Value of Multilevel Modeling Traditional approaches to handling individual differences often force a choice between two extremes: 7.3.1 Complete Pooling Treats all participants as identical by averaging or combining their data Estimates a single set of parameters for the entire group Ignores individual differences entirely Example: Fitting a single model to all participants’ data combined 7.3.2 No Pooling Analyzes each participant completely separately Estimates separate parameters for each individual Fails to leverage information shared across participants and can lead to unstable estimates Example: Fitting separate models to each participant’s data Multilevel modeling offers a middle ground through partial pooling. Individual estimates are informed by both individual-level data and the overall population distribution. 7.3.3 Partial Pooling (Multilevel Modeling) Individual parameters are treated as coming from a group-level distribution Estimates are informed by both individual data and the population distribution Creates a balance between individual and group information Example: Hierarchical Bayesian model with parameters at both individual and group levels This partial pooling approach is particularly valuable when: Data per individual is limited (e.g., few trials per participant) Individual differences are meaningful but not completely independent We want to make predictions about new individuals from the same population 7.4 Graphical Model Visualization Before diving into code, let’s understand the structure of our multilevel models using graphical model notation. These diagrams help visualize how parameters relate to each other and to the observed data. 7.4.1 Biased Agent Model In this model, each agent has an individual bias parameter (θ) that determines their probability of choosing “right” (1) versus “left” (0). We are now conceptualizing our agents as being part of (sampled from) a more general population. This general population is characterized by a population level average parameter value (e.g. a general bias of 0.8 as we all like right hands more) and a certain variation in the population (e.g. a standard deviation of 0.1, as we are all a bit different from each other). Each biased agent’s bias is then sampled from that distribution. The key elements are: Population parameters: μ_θ (mean bias) and σ_θ (standard deviation of bias) Individual parameters: θ_i (bias for agent i) Observed data: y_it (choice for agent i on trial t) 7.4.2 Memory Agent Model This model is more complex, with each agent having two parameters: a baseline bias (α) and a memory sensitivity parameter (β). The key elements are: Population parameters: μ_α, σ_α, μ_β, σ_β (means and standard deviations) Individual parameters: α_i (bias for agent i), β_i (memory sensitivity for agent i) Transformed variables: m_it (memory state for agent i on trial t) Observed data: y_it (choice for agent i on trial t) These graphical models help us understand how information flows in our models and guide our implementation in Stan. Again, it’s practical to work in log odds. Why? Well, it’s not unconceivable that an agent would be 3 sd from the mean. So a biased agent could have a rate of 0.8 + 3 * 0.1, which gives a rate of 1.1. It’s kinda impossible to choose 110% of the time the right hand. We want an easy way to avoid these situations without too carefully tweaking our parameters, or including exception statements (e.g. if rate &gt; 1, then rate = 1). Conversion to log odds is again a wonderful way to work in a boundless space, and in the last step shrinking everything back to 0-1 probability space. N.B. we model all agents with some added noise as we assume it cannot be eliminated from empirical studies. pacman::p_load(tidyverse, here, posterior, cmdstanr, brms, tidybayes, patchwork, bayesplot, furrr, LaplacesDemon) # Population-level parameters agents &lt;- 100 # Number of agents to simulate trials &lt;- 120 # Number of trials per agent noise &lt;- 0 # Base noise level (probability of random choice) # Biased agent population parameters rateM &lt;- 1.386 # Population mean of bias (log-odds scale, ~0.8 in probability) rateSD &lt;- 0.65 # Population SD of bias (log-odds scale, ~0.1 in probability) # Memory agent population parameters biasM &lt;- 0 # Population mean of baseline bias (log-odds scale) biasSD &lt;- 0.1 # Population SD of baseline bias (log-odds scale) betaM &lt;- 1.5 # Population mean of memory sensitivity (log-odds scale) betaSD &lt;- 0.3 # Population SD of memory sensitivity (log-odds scale) # For reference, convert log-odds parameters to probability scale cat(&quot;Biased agent population mean (probability scale):&quot;, round(plogis(rateM), 2), &quot;\\n&quot;) ## Biased agent population mean (probability scale): 0.8 cat(&quot;Approximate biased agent population SD (probability scale):&quot;, round(0.1, 2), &quot;\\n&quot;) ## Approximate biased agent population SD (probability scale): 0.1 # Random agent function: makes choices based on bias parameter # Parameters: # rate: log-odds of choosing option 1 (&quot;right&quot;) # noise: probability of making a random choice # Returns: binary choice (0 or 1) RandomAgentNoise_f &lt;- function(rate, noise) { # Generate choice based on agent&#39;s bias parameter (on log-odds scale) choice &lt;- rbinom(1, 1, plogis(rate)) # With probability &#39;noise&#39;, override choice with random 50/50 selection if (rbinom(1, 1, noise) == 1) { choice = rbinom(1, 1, 0.5) } return(choice) } # Memory agent function: makes choices based on opponent&#39;s historical choices # Parameters: # bias: baseline tendency to choose option 1 (log-odds scale) # beta: sensitivity to memory (how strongly past choices affect decisions) # otherRate: opponent&#39;s observed rate of choosing option 1 (probability scale) # noise: probability of making a random choice # Returns: binary choice (0 or 1) MemoryAgentNoise_f &lt;- function(bias, beta, otherRate, noise) { # Calculate choice probability based on memory of opponent&#39;s choices # Higher beta means agent responds more strongly to opponent&#39;s pattern choice_prob &lt;- inv_logit_scaled(bias + beta * logit_scaled(otherRate)) # Generate choice choice &lt;- rbinom(1, 1, choice_prob) # With probability &#39;noise&#39;, override choice with random 50/50 selection if (rbinom(1, 1, noise) == 1) { choice = rbinom(1, 1, 0.5) } return(choice) } 7.5 Generating the agents [MISSING: PARALLELIZE] # Function to simulate one agent&#39;s behavior simulate_agent &lt;- function(agent_id, population_params, n_trials, noise_level) { # Sample agent-specific parameters from population distributions rate &lt;- rnorm(1, population_params$rateM, population_params$rateSD) bias &lt;- rnorm(1, population_params$biasM, population_params$biasSD) beta &lt;- rnorm(1, population_params$betaM, population_params$betaSD) # Initialize choice vectors randomChoice &lt;- rep(NA, n_trials) memoryChoice &lt;- rep(NA, n_trials) # Generate choices for each trial for (trial in 1:n_trials) { # Random agent makes choice based on bias parameter randomChoice[trial] &lt;- RandomAgentNoise_f(rate, noise_level) # Memory agent uses history of random agent&#39;s choices if (trial == 1) { # First trial: no history, so use 50/50 chance memoryChoice[trial] &lt;- rbinom(1, 1, 0.5) } else { # Later trials: use memory of previous random agent choices memoryChoice[trial] &lt;- MemoryAgentNoise_f( bias, beta, mean(randomChoice[1:trial], na.rm = TRUE), # Current memory noise_level ) } } # Create tibble with all agent data return(tibble( agent = agent_id, trial = seq(n_trials), randomChoice, trueRate = rate, # Store true parameter values for later validation memoryChoice, noise = noise_level, rateM = population_params$rateM, rateSD = population_params$rateSD, bias = bias, beta = beta, biasM = population_params$biasM, biasSD = population_params$biasSD, betaM = population_params$betaM, betaSD = population_params$betaSD )) } # Population parameters bundled in a list population_params &lt;- list( rateM = rateM, rateSD = rateSD, biasM = biasM, biasSD = biasSD, betaM = betaM, betaSD = betaSD ) # Simulate all agents (in a real application, consider using purrr::map functions) d &lt;- NULL for (agent_id in 1:agents) { agent_data &lt;- simulate_agent(agent_id, population_params, trials, noise) if (agent_id == 1) { d &lt;- agent_data } else { d &lt;- rbind(d, agent_data) } } # Calculate running statistics for each agent d &lt;- d %&gt;% group_by(agent) %&gt;% mutate( # Cumulative proportions of choices (shows learning/strategy over time) randomRate = cumsum(randomChoice) / seq_along(randomChoice), memoryRate = cumsum(memoryChoice) / seq_along(memoryChoice) ) # Display information about the simulated dataset cat(&quot;Generated data for&quot;, agents, &quot;agents with&quot;, trials, &quot;trials each\\n&quot;) ## Generated data for 100 agents with 120 trials each cat(&quot;Total observations:&quot;, nrow(d), &quot;\\n&quot;) ## Total observations: 12000 # Show a small sample of the data head(d, 5) ## # A tibble: 5 × 16 ## # Groups: agent [1] ## agent trial randomChoice trueRate memoryChoice noise rateM rateSD bias beta biasM ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 0.862 0 0 1.39 0.65 0.159 1.13 0 ## 2 1 2 1 0.862 1 0 1.39 0.65 0.159 1.13 0 ## 3 1 3 0 0.862 1 0 1.39 0.65 0.159 1.13 0 ## 4 1 4 1 0.862 1 0 1.39 0.65 0.159 1.13 0 ## 5 1 5 0 0.862 0 0 1.39 0.65 0.159 1.13 0 ## # ℹ 5 more variables: biasSD &lt;dbl&gt;, betaM &lt;dbl&gt;, betaSD &lt;dbl&gt;, randomRate &lt;dbl&gt;, ## # memoryRate &lt;dbl&gt; 7.6 Plotting the agents # Create plot themes that we&#39;ll reuse custom_theme &lt;- theme_classic() + theme( legend.position = &quot;top&quot;, plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5, size = 9) ) # Plot 1: Trajectories of randomRate for all agents p1 &lt;- ggplot(d, aes(x = trial, y = randomRate)) + geom_line(aes(group = agent, color = &quot;Individual Agents&quot;), alpha = 0.25) + # Individual agents geom_smooth(aes(color = &quot;Average&quot;), se = TRUE, size = 1.2) + # Group average geom_hline(yintercept = 0.5, linetype = &quot;dashed&quot;) + ylim(0, 1) + labs( title = &quot;Random Agent Behavior&quot;, subtitle = &quot;Cumulative proportion of &#39;right&#39; choices over trials&quot;, x = &quot;Trial Number&quot;, y = &quot;Proportion of Right Choices&quot;, color = NULL ) + scale_color_manual(values = c(&quot;Individual Agents&quot; = &quot;gray50&quot;, &quot;Average&quot; = &quot;blue&quot;)) + custom_theme # Plot 2: Trajectories of memoryRate for all agents p2 &lt;- ggplot(d, aes(x = trial, y = memoryRate)) + geom_line(alpha = 0.15, aes(color = &quot;Individual Agents&quot;, group = agent)) + # Individual agents geom_smooth(aes(color = &quot;Average&quot;), se = TRUE, size = 1.2) + # Group average geom_hline(yintercept = 0.5, linetype = &quot;dashed&quot;) + ylim(0, 1) + labs( title = &quot;Memory Agent Behavior&quot;, subtitle = &quot;Cumulative proportion of &#39;right&#39; choices over trials&quot;, x = &quot;Trial Number&quot;, y = &quot;Proportion of Right Choices&quot;, color = NULL ) + scale_color_manual(values = c(&quot;Individual Agents&quot; = &quot;gray50&quot;, &quot;Average&quot; = &quot;darkred&quot;)) + custom_theme # Display plots side by side p1 + p2 # Plot 3-5: Correlation between random and memory agent behavior at different timepoints # These show how well memory agents track random agents&#39; behavior over time p3 &lt;- d %&gt;% filter(trial == 10) %&gt;% ggplot(aes(randomRate, memoryRate)) + geom_point(alpha = 0.6) + geom_smooth(method = &quot;lm&quot;, color = &quot;blue&quot;) + geom_abline(intercept = 0, slope = 1, color = &quot;red&quot;, linetype = &quot;dashed&quot;) + coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) + labs( title = &quot;After 10 Trials&quot;, x = &quot;Random Agent Rate&quot;, y = &quot;Memory Agent Rate&quot; ) + custom_theme p4 &lt;- d %&gt;% filter(trial == 60) %&gt;% ggplot(aes(randomRate, memoryRate)) + geom_point(alpha = 0.6) + geom_smooth(method = &quot;lm&quot;, color = &quot;blue&quot;) + geom_abline(intercept = 0, slope = 1, color = &quot;red&quot;, linetype = &quot;dashed&quot;) + coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) + labs( title = &quot;After 60 Trials&quot;, x = &quot;Random Agent Rate&quot;, y = &quot;Memory Agent Rate&quot; ) + custom_theme p5 &lt;- d %&gt;% filter(trial == 120) %&gt;% ggplot(aes(randomRate, memoryRate)) + geom_point(alpha = 0.6) + geom_smooth(method = &quot;lm&quot;, color = &quot;blue&quot;) + geom_abline(intercept = 0, slope = 1, color = &quot;red&quot;, linetype = &quot;dashed&quot;) + coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) + labs( title = &quot;After 120 Trials&quot;, x = &quot;Random Agent Rate&quot;, y = &quot;Memory Agent Rate&quot; ) + custom_theme # Display plots in a single row p3 + p4 + p5 + plot_layout(guides = &quot;collect&quot;) + plot_annotation( title = &quot;Memory Agents&#39; Adaptation to Random Agents Over Time&quot;, subtitle = &quot;Red line: perfect tracking; Blue line: actual relationship&quot;, theme = theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) ) # Plot 6-8: Correlation between true rate parameter and observed rate # These show how well we can recover the underlying rate parameter p6 &lt;- d %&gt;% filter(trial == 10) %&gt;% ggplot(aes(inv_logit_scaled(trueRate), randomRate)) + geom_point(alpha = 0.6) + geom_smooth(method = &quot;lm&quot;, color = &quot;blue&quot;) + geom_abline(intercept = 0, slope = 1, color = &quot;red&quot;, linetype = &quot;dashed&quot;) + coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) + labs( title = &quot;After 10 Trials&quot;, x = &quot;True Rate Parameter&quot;, y = &quot;Observed Rate&quot; ) + custom_theme p7 &lt;- d %&gt;% filter(trial == 60) %&gt;% ggplot(aes(inv_logit_scaled(trueRate), randomRate)) + geom_point(alpha = 0.6) + geom_smooth(method = &quot;lm&quot;, color = &quot;blue&quot;) + geom_abline(intercept = 0, slope = 1, color = &quot;red&quot;, linetype = &quot;dashed&quot;) + coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) + labs( title = &quot;After 60 Trials&quot;, x = &quot;True Rate Parameter&quot;, y = &quot;Observed Rate&quot; ) + custom_theme p8 &lt;- d %&gt;% filter(trial == 120) %&gt;% ggplot(aes(inv_logit_scaled(trueRate), randomRate)) + geom_point(alpha = 0.6) + geom_smooth(method = &quot;lm&quot;, color = &quot;blue&quot;) + geom_abline(intercept = 0, slope = 1, color = &quot;red&quot;, linetype = &quot;dashed&quot;) + coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) + labs( title = &quot;After 120 Trials&quot;, x = &quot;True Rate Parameter&quot;, y = &quot;Observed Rate&quot; ) + custom_theme # Display plots in a single row p6 + p7 + p8 + plot_layout(guides = &quot;collect&quot;) + plot_annotation( title = &quot;Parameter Recovery: True vs. Observed Rates Over Time&quot;, subtitle = &quot;Red line: perfect recovery; Blue line: actual relationship&quot;, theme = theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) ) Note that as the n of trials increases, the memory model matches the random model better and better 7.7 Coding the multilevel agents 7.7.1 Multilevel random Remember that the simulated parameters are: * biasM &lt;- 0 * biasSD &lt;- 0.1 * betaM &lt;- 1.5 * betaSD &lt;- 0.3 Prep the data # For multilevel models, we need to reshape our data into matrices # where rows are trials and columns are agents # Function to create matrices from our long-format data create_stan_data &lt;- function(data, agent_type) { # Select relevant choice column based on agent type choice_col &lt;- ifelse(agent_type == &quot;random&quot;, &quot;randomChoice&quot;, &quot;memoryChoice&quot;) other_col &lt;- ifelse(agent_type == &quot;random&quot;, &quot;memoryChoice&quot;, &quot;randomChoice&quot;) # Create choice matrix choice_data &lt;- data %&gt;% dplyr::select(agent, trial, all_of(choice_col)) %&gt;% pivot_wider( names_from = agent, values_from = all_of(choice_col), names_prefix = &quot;agent_&quot; ) %&gt;% dplyr::select(-trial) %&gt;% as.matrix() # Create other-choice matrix (used for memory agent) other_data &lt;- data %&gt;% dplyr::select(agent, trial, all_of(other_col)) %&gt;% pivot_wider( names_from = agent, values_from = all_of(other_col), names_prefix = &quot;agent_&quot; ) %&gt;% dplyr::select(-trial) %&gt;% as.matrix() # Return data as a list ready for Stan return(list( trials = trials, agents = agents, h = choice_data, other = other_data )) } # Create data for random agent model data_random &lt;- create_stan_data(d, &quot;random&quot;) # Create data for memory agent model data_memory &lt;- create_stan_data(d, &quot;memory&quot;) # Display dimensions of our data matrices cat(&quot;Random agent matrix dimensions:&quot;, dim(data_random$h), &quot;\\n&quot;) ## Random agent matrix dimensions: 120 100 cat(&quot;Memory agent matrix dimensions:&quot;, dim(data_memory$h), &quot;\\n&quot;) ## Memory agent matrix dimensions: 120 100 7.8 Multilevel Random Agent Model Our first multilevel model focuses on the biased random agent. For each agent, we’ll estimate an individual bias parameter (theta) that determines their probability of choosing “right” versus “left”. These individual parameters will be modeled as coming from a population distribution with mean thetaM and standard deviation thetaSD. This approach balances two sources of information: 1. The agent’s individual choice patterns 2. The overall population distribution of bias parameters The model implements the following hierarchical structure: Population level: θᵐ ~ Normal(0, 1), θˢᵈ ~ Normal⁺(0, 0.3) Individual level: θᵢ ~ Normal(θᵐ, θˢᵈ) Data level: yᵢₜ ~ Bernoulli(logit⁻¹(θᵢ)) Let’s implement this in Stan: # Stan model for multilevel random agent stan_model &lt;- &quot; /* Multilevel Bernoulli Model * This model infers agent-specific choice biases from sequences of binary choices (0/1) * The model assumes each agent has their own bias (theta) drawn from a population distribution */ functions { // Generate random numbers from truncated normal distribution real normal_lb_rng(real mu, real sigma, real lb) { real p = normal_cdf(lb | mu, sigma); // cdf for bounds real u = uniform_rng(p, 1); return (sigma * inv_Phi(u)) + mu; // inverse cdf for value } } data { int&lt;lower=1&gt; trials; // Number of trials per agent int&lt;lower=1&gt; agents; // Number of agents array[trials, agents] int&lt;lower=0, upper=1&gt; h; // Choice data: 0 or 1 for each trial/agent } parameters { real thetaM; // Population-level mean bias (log-odds scale) real&lt;lower=0&gt; thetaSD; // Population-level SD of bias array[agents] real theta; // Agent-specific biases (log-odds scale) } model { // Population-level priors target += normal_lpdf(thetaM | 0, 1); // Prior for population mean target += normal_lpdf(thetaSD | 0, 0.3) // Half-normal prior for population SD - normal_lccdf(0 | 0, 0.3); // Adjustment for truncation at 0 // Agent-level model target += normal_lpdf(theta | thetaM, thetaSD); // Agent biases drawn from population // Likelihood for observed choices for (i in 1:agents) { target += bernoulli_logit_lpmf(h[,i] | theta[i]); // Choice likelihood } } generated quantities { // Prior predictive samples real thetaM_prior = normal_rng(0, 1); real&lt;lower=0&gt; thetaSD_prior = normal_lb_rng(0, 0.3, 0); real&lt;lower=0, upper=1&gt; theta_prior = inv_logit(normal_rng(thetaM_prior, thetaSD_prior)); // Posterior predictive samples real&lt;lower=0, upper=1&gt; theta_posterior = inv_logit(normal_rng(thetaM, thetaSD)); // Predictive simulations int&lt;lower=0, upper=trials&gt; prior_preds = binomial_rng(trials, inv_logit(thetaM_prior)); int&lt;lower=0, upper=trials&gt; posterior_preds = binomial_rng(trials, inv_logit(thetaM)); // Convert parameters to probability scale for easier interpretation real&lt;lower=0, upper=1&gt; thetaM_prob = inv_logit(thetaM); } &quot; write_stan_file( stan_model, dir = &quot;stan/&quot;, basename = &quot;W6_MultilevelBias.stan&quot;) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W6_MultilevelBias.stan&quot; # File path for saved model model_file &lt;- &quot;simmodels/W6_MultilevelBias.RDS&quot; # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(model_file)) { file &lt;- file.path(&quot;stan/W6_MultilevelBias.stan&quot;) mod &lt;- cmdstan_model(file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) # Check if we need to rerun the simulation samples &lt;- mod$sample( data = data_random, seed = 123, chains = 2, parallel_chains = 2, threads_per_chain = 2, iter_warmup = 2000, iter_sampling = 2000, refresh = 500, max_treedepth = 20, adapt_delta = 0.99, ) samples$save_object(file = model_file) cat(&quot;Generated new model fit and saved to&quot;, model_file, &quot;\\n&quot;) } else { # Load existing results samples &lt;- readRDS(model_file) cat(&quot;Loaded existing model fit from&quot;, model_file, &quot;\\n&quot;) } ## Loaded existing model fit from simmodels/W6_MultilevelBias.RDS 7.8.1 Assessing multilevel random agents Besides the usual prior predictive checks, prior posterior update checks, posterior predictive checks, based on the population level estimates; we also want to plot at least a few of the single agents to assess how well the model is doing for them. [MISSING: PLOT MODEL ESTIMATES AGAINST N OF HEADS BY PARTICIPANT] # Load the model results samples &lt;- readRDS(&quot;simmodels/W6_MultilevelBias.RDS&quot;) # Display summary statistics for key parameters samples$summary(c(&quot;thetaM&quot;, &quot;thetaSD&quot;, &quot;thetaM_prob&quot;)) ## # A tibble: 3 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 thetaM 1.35 1.35 0.0677 0.0673 1.24 1.46 1.00 5172. 3042. ## 2 thetaSD 0.638 0.635 0.0516 0.0510 0.559 0.727 1.00 3636. 2796. ## 3 thetaM_prob 0.793 0.794 0.0111 0.0110 0.775 0.811 1.00 5172. 3042. # Extract posterior draws for analysis draws_df &lt;- as_draws_df(samples$draws()) # Create a function for standard diagnostic plots plot_diagnostics &lt;- function(parameter_name, true_value = NULL, prior_name = paste0(parameter_name, &quot;_prior&quot;)) { # Trace plot to check mixing and convergence p1 &lt;- ggplot(draws_df, aes(.iteration, .data[[parameter_name]], group = .chain, color = as.factor(.chain))) + geom_line(alpha = 0.5) + labs(title = paste(&quot;Trace Plot for&quot;, parameter_name), x = &quot;Iteration&quot;, y = parameter_name, color = &quot;Chain&quot;) + theme_classic() # Prior-posterior update plot p2 &lt;- ggplot(draws_df) + geom_histogram(aes(.data[[parameter_name]]), fill = &quot;blue&quot;, alpha = 0.3) + geom_histogram(aes(.data[[prior_name]]), fill = &quot;red&quot;, alpha = 0.3) # Add true value if provided if (!is.null(true_value)) { p2 &lt;- p2 + geom_vline(xintercept = true_value, linetype = &quot;dashed&quot;, size = 1) } p2 &lt;- p2 + labs(title = paste(&quot;Prior-Posterior Update for&quot;, parameter_name), subtitle = &quot;Blue: posterior, Red: prior, Dashed: true value&quot;, x = parameter_name, y = &quot;Density&quot;) + theme_classic() # Return both plots return(p1 + p2) } # Plot diagnostics for population mean pop_mean_plots &lt;- plot_diagnostics(&quot;thetaM&quot;, rateM) # Plot diagnostics for population SD pop_sd_plots &lt;- plot_diagnostics(&quot;thetaSD&quot;, rateSD) # Display diagnostic plots pop_mean_plots pop_sd_plots # Create predictive check plots # Prior predictive check p1 &lt;- ggplot(draws_df) + geom_histogram(aes(prior_preds), bins = 30, fill = &quot;blue&quot;, alpha = 0.3, color = &quot;darkblue&quot;) + labs(title = &quot;Prior Predictive Check&quot;, subtitle = &quot;Distribution of predicted &#39;right&#39; choices out of 120 trials&quot;, x = &quot;Number of Right Choices&quot;, y = &quot;Count&quot;) + theme_classic() # Posterior predictive check p2 &lt;- ggplot(draws_df) + geom_histogram(aes(posterior_preds), bins = 30, fill = &quot;green&quot;, alpha = 0.3, color = &quot;darkgreen&quot;) + geom_histogram(aes(prior_preds), bins = 30, fill = &quot;blue&quot;, alpha = 0.1, color = &quot;darkblue&quot;) + labs(title = &quot;Prior vs Posterior Predictive Check&quot;, subtitle = &quot;Green: posterior predictions, Blue: prior predictions&quot;, x = &quot;Number of Right Choices&quot;, y = &quot;Count&quot;) + theme_classic() # Average observed choices per agent agent_means &lt;- colMeans(data_random$h) observed_counts &lt;- agent_means * trials # Add observed counts to posterior predictive plot p3 &lt;- ggplot(draws_df) + geom_histogram(aes(posterior_preds), bins = 30, fill = &quot;green&quot;, alpha = 0.3, color = &quot;darkgreen&quot;) + geom_histogram(data = tibble(observed = observed_counts), aes(observed), bins = 30, fill = &quot;red&quot;, alpha = 0.3, color = &quot;darkred&quot;) + labs(title = &quot;Posterior Predictions vs Observed Data&quot;, subtitle = &quot;Green: posterior predictions, Red: actual observed counts&quot;, x = &quot;Number of Right Choices&quot;, y = &quot;Count&quot;) + theme_classic() # Display predictive check plots p1 + p2 + p3 7.9 Let’s look at individuals # Extract individual parameter estimates # Sample 10 random agents to examine more closely sample_agents &lt;- sample(1:agents, 10) # Extract posterior samples for each agent&#39;s theta parameter theta_samples &lt;- matrix(NA, nrow = nrow(draws_df), ncol = agents) for (a in 1:agents) { # Convert from log-odds to probability scale for ease of interpretation theta_samples[, a] &lt;- inv_logit_scaled(draws_df[[paste0(&quot;theta[&quot;, a, &quot;]&quot;)]]) } # Calculate true rates dddd &lt;- unique(d[,c(&quot;agent&quot;, &quot;trueRate&quot;)]) %&gt;% mutate(trueRate = inv_logit_scaled(trueRate)) # Create a dataframe with the true rates and empirical rates agent_comparison &lt;- tibble( agent = 1:agents, true_rate = dddd$trueRate, # True rate used in simulation empirical_rate = colMeans(data_random$h) # Observed proportion of right choices ) # Calculate summary statistics for posterior estimates theta_summaries &lt;- tibble( agent = 1:agents, mean = colMeans(theta_samples), lower_95 = apply(theta_samples, 2, quantile, 0.025), upper_95 = apply(theta_samples, 2, quantile, 0.975) ) %&gt;% # Join with true values for comparison left_join(agent_comparison, by = &quot;agent&quot;) %&gt;% # Calculate error metrics mutate( abs_error = abs(mean - true_rate), in_interval = true_rate &gt;= lower_95 &amp; true_rate &lt;= upper_95, rel_error = abs_error / true_rate ) # Plot 1: Posterior distributions for sample agents posterior_samples &lt;- tibble( agent = rep(sample_agents, each = nrow(draws_df)), sample_idx = rep(1:nrow(draws_df), times = length(sample_agents)), estimated_rate = as.vector(theta_samples[, sample_agents]) ) p1 &lt;- ggplot() + # Add density plot for posterior distribution of rates geom_density(data = posterior_samples, aes(x = estimated_rate, group = agent), fill = &quot;skyblue&quot;, alpha = 0.5) + # Add vertical line for true rate geom_vline(data = agent_comparison %&gt;% filter(agent %in% sample_agents), aes(xintercept = true_rate), color = &quot;red&quot;, size = 1) + # Add vertical line for empirical rate geom_vline(data = agent_comparison %&gt;% filter(agent %in% sample_agents), aes(xintercept = empirical_rate), color = &quot;green4&quot;, size = 1, linetype = &quot;dashed&quot;) + # Facet by agent facet_wrap(~agent, scales = &quot;free_y&quot;) + # Add formatting labs(title = &quot;Individual Agent Parameter Estimation&quot;, subtitle = &quot;Blue density: Posterior distribution\\nRed line: True rate\\nGreen dashed line: Empirical rate&quot;, x = &quot;Rate Parameter (Probability Scale)&quot;, y = &quot;Density&quot;) + theme_classic() + theme(strip.background = element_rect(fill = &quot;white&quot;), strip.text = element_text(face = &quot;bold&quot;)) + xlim(0, 1) # Plot 2: Parameter recovery for all agents p2 &lt;- ggplot(theta_summaries, aes(x = true_rate, y = mean)) + geom_point(aes(color = in_interval), size = 3, alpha = 0.7) + geom_errorbar(aes(ymin = lower_95, ymax = upper_95, color = in_interval), width = 0.01, alpha = 0.3) + geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;, color = &quot;black&quot;) + geom_smooth(method = &quot;lm&quot;, color = &quot;blue&quot;, se = FALSE, linetype = &quot;dotted&quot;) + scale_color_manual(values = c(&quot;TRUE&quot; = &quot;darkgreen&quot;, &quot;FALSE&quot; = &quot;red&quot;)) + labs(title = &quot;Parameter Recovery Performance&quot;, subtitle = &quot;Each point represents one agent; error bars show 95% credible intervals&quot;, x = &quot;True Rate&quot;, y = &quot;Estimated Rate&quot;, color = &quot;True Value in\\n95% Interval&quot;) + theme_classic() + xlim(0, 1) + ylim(0, 1) # Calculate parameter estimation metrics error_metrics &lt;- tibble( agent = 1:agents, true_rate = dddd$trueRate, empirical_rate = colMeans(data_random$h), estimated_rate = colMeans(theta_samples), lower_95 = apply(theta_samples, 2, quantile, 0.025), upper_95 = apply(theta_samples, 2, quantile, 0.975), absolute_error = abs(estimated_rate - true_rate), relative_error = absolute_error / true_rate, in_interval = true_rate &gt;= lower_95 &amp; true_rate &lt;= upper_95 ) # Create direct comparison plot p3 &lt;- ggplot(error_metrics, aes(x = agent)) + geom_errorbar(aes(ymin = lower_95, ymax = upper_95), width = 0.2, color = &quot;blue&quot;, alpha = 0.5) + geom_point(aes(y = estimated_rate), color = &quot;blue&quot;, size = 3) + geom_point(aes(y = true_rate), color = &quot;red&quot;, shape = 4, size = 3, stroke = 2, alpha = 0.3) + geom_point(aes(y = empirical_rate), color = &quot;green4&quot;, shape = 1, size = 3, stroke = 2, alpha = 0.3) + labs(title = &quot;Parameter Estimation Accuracy by Agent&quot;, subtitle = &quot;Blue points and bars: Estimated rate with 95% credible interval\\nRed X: True rate used in simulation\\nGreen circle: Empirical rate from observed data&quot;, x = &quot;Agent ID&quot;, y = &quot;Rate&quot;) + theme_classic() # Plot 4: Shrinkage visualization # Calculate population mean estimate pop_mean &lt;- mean(draws_df$thetaM_prob) # Add shrinkage information to the data shrinkage_data &lt;- theta_summaries %&gt;% mutate( # Distance from empirical to population mean (shows shrinkage direction) empirical_to_pop = empirical_rate - pop_mean, # Distance from estimate to empirical (shows amount of shrinkage) estimate_to_empirical = mean - empirical_rate, # Ratio of distances (shrinkage proportion) shrinkage_ratio = 1 - (abs(mean - pop_mean) / abs(empirical_rate - pop_mean)), # Define number of trials for sizing points n_trials = trials ) p4 &lt;- ggplot(shrinkage_data, aes(x = empirical_rate, y = mean)) + # Reference line for no shrinkage geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;, color = &quot;gray50&quot;) + # Horizontal line for population mean geom_hline(yintercept = pop_mean, linetype = &quot;dotted&quot;, color = &quot;blue&quot;) + # Points for each agent geom_point(aes(size = n_trials, color = abs(shrinkage_ratio)), alpha = 0.7) + # Connect points to their empirical values to show shrinkage geom_segment(aes(xend = empirical_rate, yend = empirical_rate, color = abs(shrinkage_ratio)), alpha = 0.3) + scale_color_gradient(low = &quot;yellow&quot;, high = &quot;red&quot;) + labs(title = &quot;Shrinkage Effects in Multilevel Modeling&quot;, subtitle = &quot;Points above diagonal shrink down, points below shrink up;\\nBlue dotted line: population mean estimate&quot;, x = &quot;Empirical Rate (Observed Proportion)&quot;, y = &quot;Posterior Mean Estimate&quot;, color = &quot;Shrinkage\\nMagnitude&quot;, size = &quot;Number of\\nTrials&quot;) + theme_classic() + xlim(0, 1) + ylim(0, 1) # Display plots p1 p2 + p3 p4 7.10 Prior sensitivity checks # File path for saved sensitivity results sensitivity_results_file &lt;- &quot;simdata/W6_sensitivity_results.csv&quot; # Check if we need to rerun the analysis if (regenerate_simulations || !file.exists(sensitivity_results_file)) { # Define a range of prior specifications to test prior_settings &lt;- expand_grid( prior_mean_theta = c(-1, 0, 1), # Different prior means for population rate prior_sd_theta = c(0.5, 1, 2), # Different prior SDs for population rate prior_scale_theta_sd = c(0.1, 0.3, 0.5, 1) # Different scales for the SD hyperprior ) # First, create a single Stan model that accepts prior hyperparameters as data stan_code &lt;- &quot; functions { real normal_lb_rng(real mu, real sigma, real lb) { real p = normal_cdf(lb | mu, sigma); real u = uniform_rng(p, 1); return (sigma * inv_Phi(u)) + mu; } } data { int&lt;lower=1&gt; trials; int&lt;lower=1&gt; agents; array[trials, agents] int&lt;lower=0, upper=1&gt; h; real prior_mean_theta; real&lt;lower=0&gt; prior_sd_theta; real&lt;lower=0&gt; prior_scale_theta_sd; } parameters { real thetaM; real&lt;lower=0&gt; thetaSD; array[agents] real theta; } model { // Population-level priors with specified hyperparameters target += normal_lpdf(thetaM | prior_mean_theta, prior_sd_theta); target += normal_lpdf(thetaSD | 0, prior_scale_theta_sd) - normal_lccdf(0 | 0, prior_scale_theta_sd); // Agent-level model target += normal_lpdf(theta | thetaM, thetaSD); // Likelihood for (i in 1:agents) { target += bernoulli_logit_lpmf(h[,i] | theta[i]); } } generated quantities { real thetaM_prob = inv_logit(thetaM); } &quot; # Write model to file writeLines(stan_code, &quot;stan/sensitivity_model.stan&quot;) # Compile the model once mod_sensitivity &lt;- cmdstan_model(&quot;stan/sensitivity_model.stan&quot;, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) # Function to fit model with specified priors fit_with_priors &lt;- function(prior_mean_theta, prior_sd_theta, prior_scale_theta_sd) { # Create data with prior specifications data_with_priors &lt;- c(data_random, list( prior_mean_theta = prior_mean_theta, prior_sd_theta = prior_sd_theta, prior_scale_theta_sd = prior_scale_theta_sd )) # Fit model fit &lt;- mod_sensitivity$sample( data = data_with_priors, seed = 123, chains = 1, parallel_chains = 1, threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 1000, refresh = 0 ) # Extract posterior summaries summary_df &lt;- fit$summary(c(&quot;thetaM&quot;, &quot;thetaSD&quot;, &quot;thetaM_prob&quot;)) # Return results return(tibble( prior_mean_theta = prior_mean_theta, prior_sd_theta = prior_sd_theta, prior_scale_theta_sd = prior_scale_theta_sd, est_thetaM = summary_df$mean[summary_df$variable == &quot;thetaM&quot;], est_thetaSD = summary_df$mean[summary_df$variable == &quot;thetaSD&quot;], est_thetaM_prob = summary_df$mean[summary_df$variable == &quot;thetaM_prob&quot;] )) } # Run parallel analysis library(furrr) plan(multisession, workers = 4) # Using fewer workers to reduce resource use sensitivity_results &lt;- future_pmap_dfr( prior_settings, function(prior_mean_theta, prior_sd_theta, prior_scale_theta_sd) { fit_with_priors(prior_mean_theta, prior_sd_theta, prior_scale_theta_sd) }, .options = furrr_options(seed = TRUE) ) # Save results for future use write_csv(sensitivity_results, sensitivity_results_file) cat(&quot;Generated new sensitivity analysis results and saved to&quot;, sensitivity_results_file, &quot;\\n&quot;) } else { # Load existing results sensitivity_results &lt;- read_csv(sensitivity_results_file) cat(&quot;Loaded existing sensitivity analysis results from&quot;, sensitivity_results_file, &quot;\\n&quot;) } ## Loaded existing sensitivity analysis results from simdata/W6_sensitivity_results.csv # Plot for population mean estimate p1 &lt;- ggplot(sensitivity_results, aes(x = prior_mean_theta, y = est_thetaM_prob, color = factor(prior_scale_theta_sd))) + geom_point(size = 3) + geom_hline(yintercept = inv_logit_scaled(d$rateM), linetype = &quot;dashed&quot;) + labs(title = &quot;Sensitivity of Population Mean Parameter&quot;, subtitle = &quot;Dashed line shows average empirical rate across participants&quot;, x = &quot;Prior Mean, SD for Population Parameter&quot;, y = &quot;Estimated Population Mean (probability scale)&quot;, color = &quot;Prior Scale for\\nPopulation SD&quot;) + theme_classic() + ylim(0.7, 0.9) + facet_wrap(.~prior_sd_theta) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Plot for population SD parameter p2 &lt;- ggplot(sensitivity_results, aes(x = prior_mean_theta, y = est_thetaSD, color = factor(prior_scale_theta_sd))) + geom_point(size = 3) + geom_hline(yintercept = d$rateSD, linetype = &quot;dashed&quot;) + labs(title = &quot;Sensitivity of Population Variance Parameter&quot;, x = &quot;Prior Mean, SD for Population Parameter&quot;, y = &quot;Estimated Population SD&quot;, color = &quot;Prior Scale for\\nPopulation SD&quot;) + theme_classic() + ylim(0.5, 0.7) + facet_wrap(.~prior_sd_theta) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Display plots p1 / p2 7.11 Parameter recovery # File path for saved recovery results recovery_results_file &lt;- &quot;simdata/W6_recovery_results.csv&quot; # Check if we need to rerun the analysis if (regenerate_simulations || !file.exists(recovery_results_file)) { # Function to simulate data and recover parameters recover_parameters &lt;- function(true_thetaM, true_thetaSD, n_agents, n_trials) { # Generate agent-specific true rates agent_thetas &lt;- rnorm(n_agents, true_thetaM, true_thetaSD) # Generate choice data sim_data &lt;- matrix(NA, nrow = n_trials, ncol = n_agents) for (a in 1:n_agents) { sim_data[,a] &lt;- rbinom(n_trials, 1, inv_logit_scaled(agent_thetas[a])) } # Prepare data for Stan stan_data &lt;- list( trials = n_trials, agents = n_agents, h = sim_data, prior_mean_theta = 0, # Using neutral priors prior_sd_theta = 1, prior_scale_theta_sd = 0.3 ) # Compile the model once mod_sensitivity &lt;- cmdstan_model(&quot;stan/sensitivity_model.stan&quot;, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) # Fit model fit &lt;- mod_sensitivity$sample( data = stan_data, seed = 123, chains = 1, parallel_chains = 1, threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 1000, refresh = 0 ) # Extract estimates summary_df &lt;- fit$summary(c(&quot;thetaM&quot;, &quot;thetaSD&quot;)) # Return comparison of true vs. estimated parameters return(tibble( true_thetaM = true_thetaM, true_thetaSD = true_thetaSD, n_agents = n_agents, n_trials = n_trials, est_thetaM = summary_df$mean[summary_df$variable == &quot;thetaM&quot;], est_thetaSD = summary_df$mean[summary_df$variable == &quot;thetaSD&quot;] )) } # Create parameter grid for recovery study recovery_settings &lt;- expand_grid( true_thetaM = c(-1, 0, 1), true_thetaSD = c(0.1, 0.3, 0.5, 0.7), n_agents = c(20, 50), n_trials = c(60, 120) ) # Run parameter recovery study (this can be very time-consuming) recovery_results &lt;- pmap_dfr( recovery_settings, function(true_thetaM, true_thetaSD, n_agents, n_trials) { recover_parameters(true_thetaM, true_thetaSD, n_agents, n_trials) } ) # Save results for future use write_csv(recovery_results, recovery_results_file) cat(&quot;Generated new parameter recovery results and saved to&quot;, recovery_results_file, &quot;\\n&quot;) } else { # Load existing results recovery_results &lt;- read_csv(recovery_results_file) cat(&quot;Loaded existing parameter recovery results from&quot;, recovery_results_file, &quot;\\n&quot;) } ## Loaded existing parameter recovery results from simdata/W6_recovery_results.csv # Visualize parameter recovery results p1 &lt;- ggplot(recovery_results, aes(x = true_thetaM, y = est_thetaM, color = factor(true_thetaSD))) + geom_point(size = 3) + geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;) + facet_grid(n_agents ~ n_trials, labeller = labeller( n_agents = function(x) paste0(&quot;Agents: &quot;, x), n_trials = function(x) paste0(&quot;Trials: &quot;, x) )) + labs(title = &quot;Recovery of Population Mean Parameter&quot;, x = &quot;True Value&quot;, y = &quot;Estimated Value&quot;, color = &quot;True Population SD&quot;) + theme_classic() p2 &lt;- ggplot(recovery_results, aes(x = true_thetaSD, y = est_thetaSD, color = factor(true_thetaM))) + geom_point(size = 3) + geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;) + facet_grid(n_agents ~ n_trials, labeller = labeller( n_agents = function(x) paste0(&quot;Agents: &quot;, x), n_trials = function(x) paste0(&quot;Trials: &quot;, x) )) + labs(title = &quot;Recovery of Population SD Parameter&quot;, x = &quot;True Value&quot;, y = &quot;Estimated Value&quot;, color = &quot;True Population Mean&quot;) + theme_classic() # Display parameter recovery plots p1 / p2 7.12 Multilevel Memory Agent Model Now we’ll implement a more complex model for the memory agent. This model has two parameters per agent: bias: baseline tendency to choose “right” (log-odds scale) beta: sensitivity to the memory of opponent’s past choices Like the random agent model, we’ll use a multilevel structure where individual parameters come from population distributions. However, this model presents some additional challenges: We need to handle two parameters per agent We need to track and update memory states during the model The hierarchical structure is more complex The hierarchical structure for this model is: Population level: μ_bias ~ Normal(0, 1), σ_bias ~ Normal⁺(0, 0.3) μ_beta ~ Normal(0, 0.3), σ_beta ~ Normal⁺(0, 0.3) Individual level: bias_i ~ Normal(μ_bias, σ_bias) beta_i ~ Normal(μ_beta, σ_beta) Transformed variables: memory_it = updated based on opponent’s choices Data level: y_it ~ Bernoulli(logit⁻¹(bias_i + beta_i * logit(memory_it))) Let’s implement this model. [MISSING: DAGS] Code, compile and fit the model # Stan model for multilevel memory agent with centered parameterization stan_model &lt;- &quot; // Multilevel Memory Agent Model (Centered Parameterization) // functions{ real normal_lb_rng(real mu, real sigma, real lb) { real p = normal_cdf(lb | mu, sigma); // cdf for bounds real u = uniform_rng(p, 1); return (sigma * inv_Phi(u)) + mu; // inverse cdf for value } } // The input data for the model data { int&lt;lower = 1&gt; trials; // Number of trials per agent int&lt;lower = 1&gt; agents; // Number of agents array[trials, agents] int h; // Memory agent choices array[trials, agents] int other; // Opponent (random agent) choices } // Parameters to be estimated parameters { // Population-level parameters real biasM; // Mean of baseline bias real&lt;lower = 0&gt; biasSD; // SD of baseline bias real betaM; // Mean of memory sensitivity real&lt;lower = 0&gt; betaSD; // SD of memory sensitivity // Individual-level parameters array[agents] real bias; // Individual baseline bias parameters array[agents] real beta; // Individual memory sensitivity parameters } // Transformed parameters (derived quantities) transformed parameters { // Memory state for each agent and trial array[trials, agents] real memory; // Calculate memory states based on opponent&#39;s choices for (agent in 1:agents){ // Initial memory state (no prior information) memory[1, agent] = 0.5; for (trial in 1:trials){ // Update memory based on opponent&#39;s choices if (trial &lt; trials){ // Simple averaging memory update memory[trial + 1, agent] = memory[trial, agent] + ((other[trial, agent] - memory[trial, agent]) / trial); // Handle edge cases to avoid numerical issues if (memory[trial + 1, agent] == 0){memory[trial + 1, agent] = 0.01;} if (memory[trial + 1, agent] == 1){memory[trial + 1, agent] = 0.99;} } } } } // Model definition model { // Population-level priors target += normal_lpdf(biasM | 0, 1); target += normal_lpdf(biasSD | 0, .3) - normal_lccdf(0 | 0, .3); // Half-normal target += normal_lpdf(betaM | 0, .3); target += normal_lpdf(betaSD | 0, .3) - normal_lccdf(0 | 0, .3); // Half-normal // Individual-level priors target += normal_lpdf(bias | biasM, biasSD); target += normal_lpdf(beta | betaM, betaSD); // Likelihood for (agent in 1:agents) { for (trial in 1:trials) { target += bernoulli_logit_lpmf(h[trial,agent] | bias[agent] + logit(memory[trial, agent]) * beta[agent]); } } } // Generated quantities for model checking and predictions generated quantities{ // Prior samples for checking real biasM_prior; real&lt;lower=0&gt; biasSD_prior; real betaM_prior; real&lt;lower=0&gt; betaSD_prior; real bias_prior; real beta_prior; // Predictive simulations with different memory values int&lt;lower=0, upper = trials&gt; prior_preds0; // No memory effect (memory=0) int&lt;lower=0, upper = trials&gt; prior_preds1; // Neutral memory (memory=0.5) int&lt;lower=0, upper = trials&gt; prior_preds2; // Strong memory (memory=1) int&lt;lower=0, upper = trials&gt; posterior_preds0; int&lt;lower=0, upper = trials&gt; posterior_preds1; int&lt;lower=0, upper = trials&gt; posterior_preds2; // Individual-level predictions (for each agent) array[agents] int&lt;lower=0, upper = trials&gt; posterior_predsID0; array[agents] int&lt;lower=0, upper = trials&gt; posterior_predsID1; array[agents] int&lt;lower=0, upper = trials&gt; posterior_predsID2; // Generate prior samples biasM_prior = normal_rng(0,1); biasSD_prior = normal_lb_rng(0,0.3,0); betaM_prior = normal_rng(0,1); betaSD_prior = normal_lb_rng(0,0.3,0); bias_prior = normal_rng(biasM_prior, biasSD_prior); beta_prior = normal_rng(betaM_prior, betaSD_prior); // Prior predictive checks with different memory values prior_preds0 = binomial_rng(trials, inv_logit(bias_prior + 0 * beta_prior)); prior_preds1 = binomial_rng(trials, inv_logit(bias_prior + 1 * beta_prior)); prior_preds2 = binomial_rng(trials, inv_logit(bias_prior + 2 * beta_prior)); // Posterior predictive checks with different memory values posterior_preds0 = binomial_rng(trials, inv_logit(biasM + 0 * betaM)); posterior_preds1 = binomial_rng(trials, inv_logit(biasM + 1 * betaM)); posterior_preds2 = binomial_rng(trials, inv_logit(biasM + 2 * betaM)); // Individual-level predictions for (agent in 1:agents){ posterior_predsID0[agent] = binomial_rng(trials, inv_logit(bias[agent] + 0 * beta[agent])); posterior_predsID1[agent] = binomial_rng(trials, inv_logit(bias[agent] + 1 * beta[agent])); posterior_predsID2[agent] = binomial_rng(trials, inv_logit(bias[agent] + 2 * beta[agent])); } } &quot; write_stan_file( stan_model, dir = &quot;stan/&quot;, basename = &quot;W6_MultilevelMemory.stan&quot;) # File path for saved model model_file &lt;- &quot;simmodels/W6_MultilevelMemory_centered.RDS&quot; # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(model_file)) { file &lt;- file.path(&quot;stan/W6_MultilevelMemory.stan&quot;) mod &lt;- cmdstan_model(file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) # Sample from the posterior distribution samples_mlvl &lt;- mod$sample( data = data_memory, seed = 123, chains = 2, parallel_chains = 2, threads_per_chain = 1, iter_warmup = 2000, iter_sampling = 2000, refresh = 500, max_treedepth = 20, adapt_delta = 0.99 ) # Save the model results samples_mlvl$save_object(file = model_file) cat(&quot;Generated new model fit and saved to&quot;, model_file, &quot;\\n&quot;) } else { # Load existing results samples_mlvl &lt;- readRDS(model_file) cat(&quot;Loaded existing model fit from&quot;, model_file, &quot;\\n&quot;) } 7.12.1 Assessing multilevel memory # Check if samples_biased exists if (!exists(&quot;samples_mlvl&quot;)) { cat(&quot;Loading multilevel model samples...\\n&quot;) samples_mlvl &lt;- readRDS(&quot;simmodels/W6_MultilevelMemory_centered.RDS&quot;) cat(&quot;Available parameters:&quot;, paste(colnames(as_draws_df(samples_mlvl$draws())), collapse = &quot;, &quot;), &quot;\\n&quot;) } ## Loading multilevel model samples... ## Available parameters: lp__, biasM, biasSD, betaM, betaSD, bias[1], bias[2], bias[3], bias[4], bias[5], bias[6], bias[7], bias[8], bias[9], bias[10], bias[11], bias[12], bias[13], bias[14], bias[15], bias[16], bias[17], bias[18], bias[19], bias[20], bias[21], bias[22], bias[23], bias[24], bias[25], bias[26], bias[27], bias[28], bias[29], bias[30], bias[31], bias[32], bias[33], bias[34], bias[35], bias[36], bias[37], bias[38], bias[39], bias[40], bias[41], bias[42], bias[43], bias[44], bias[45], bias[46], bias[47], bias[48], bias[49], bias[50], bias[51], bias[52], bias[53], bias[54], bias[55], bias[56], bias[57], bias[58], bias[59], bias[60], bias[61], bias[62], bias[63], bias[64], bias[65], bias[66], bias[67], bias[68], bias[69], bias[70], bias[71], bias[72], bias[73], bias[74], bias[75], bias[76], bias[77], bias[78], bias[79], bias[80], bias[81], bias[82], bias[83], bias[84], bias[85], bias[86], bias[87], bias[88], bias[89], bias[90], bias[91], bias[92], bias[93], bias[94], bias[95], bias[96], bias[97], bias[98], bias[99], bias[100], beta[1], beta[2], beta[3], beta[4], beta[5], beta[6], beta[7], beta[8], beta[9], beta[10], beta[11], beta[12], beta[13], beta[14], beta[15], beta[16], beta[17], beta[18], beta[19], beta[20], beta[21], beta[22], beta[23], beta[24], beta[25], beta[26], beta[27], beta[28], beta[29], beta[30], beta[31], beta[32], beta[33], beta[34], beta[35], beta[36], beta[37], beta[38], beta[39], beta[40], beta[41], beta[42], beta[43], beta[44], beta[45], beta[46], beta[47], beta[48], beta[49], beta[50], beta[51], beta[52], beta[53], beta[54], beta[55], beta[56], beta[57], beta[58], beta[59], beta[60], beta[61], beta[62], beta[63], beta[64], beta[65], beta[66], beta[67], beta[68], beta[69], beta[70], beta[71], beta[72], beta[73], beta[74], beta[75], beta[76], beta[77], beta[78], beta[79], beta[80], beta[81], beta[82], beta[83], beta[84], beta[85], beta[86], beta[87], beta[88], beta[89], beta[90], beta[91], beta[92], beta[93], beta[94], beta[95], beta[96], beta[97], beta[98], beta[99], beta[100], memory[1,1], memory[2,1], memory[3,1], memory[4,1], memory[5,1], memory[6,1], memory[7,1], memory[8,1], memory[9,1], memory[10,1], memory[11,1], memory[12,1], memory[13,1], memory[14,1], memory[15,1], memory[16,1], memory[17,1], memory[18,1], memory[19,1], memory[20,1], memory[21,1], memory[22,1], memory[23,1], memory[24,1], memory[25,1], memory[26,1], memory[27,1], memory[28,1], memory[29,1], memory[30,1], memory[31,1], memory[32,1], memory[33,1], memory[34,1], memory[35,1], memory[36,1], memory[37,1], memory[38,1], memory[39,1], memory[40,1], memory[41,1], memory[42,1], memory[43,1], memory[44,1], memory[45,1], memory[46,1], memory[47,1], memory[48,1], memory[49,1], memory[50,1], memory[51,1], memory[52,1], memory[53,1], memory[54,1], memory[55,1], memory[56,1], memory[57,1], memory[58,1], memory[59,1], memory[60,1], memory[61,1], memory[62,1], memory[63,1], memory[64,1], memory[65,1], memory[66,1], memory[67,1], memory[68,1], memory[69,1], memory[70,1], memory[71,1], memory[72,1], memory[73,1], memory[74,1], memory[75,1], memory[76,1], memory[77,1], memory[78,1], memory[79,1], memory[80,1], memory[81,1], memory[82,1], memory[83,1], memory[84,1], memory[85,1], memory[86,1], memory[87,1], memory[88,1], memory[89,1], memory[90,1], memory[91,1], memory[92,1], memory[93,1], memory[94,1], memory[95,1], memory[96,1], memory[97,1], memory[98,1], memory[99,1], memory[100,1], memory[101,1], memory[102,1], memory[103,1], memory[104,1], memory[105,1], memory[106,1], memory[107,1], memory[108,1], memory[109,1], memory[110,1], memory[111,1], memory[112,1], memory[113,1], memory[114,1], memory[115,1], memory[116,1], memory[117,1], memory[118,1], memory[119,1], memory[120,1], memory[1,2], memory[2,2], memory[3,2], memory[4,2], memory[5,2], memory[6,2], memory[7,2], memory[8,2], memory[9,2], memory[10,2], memory[11,2], memory[12,2], memory[13,2], memory[14,2], memory[15,2], memory[16,2], memory[17,2], memory[18,2], memory[19,2], memory[20,2], memory[21,2], memory[22,2], memory[23,2], memory[24,2], memory[25,2], memory[26,2], memory[27,2], memory[28,2], memory[29,2], memory[30,2], memory[31,2], memory[32,2], memory[33,2], memory[34,2], memory[35,2], memory[36,2], memory[37,2], memory[38,2], memory[39,2], memory[40,2], memory[41,2], memory[42,2], memory[43,2], memory[44,2], memory[45,2], memory[46,2], memory[47,2], memory[48,2], memory[49,2], memory[50,2], memory[51,2], memory[52,2], memory[53,2], memory[54,2], memory[55,2], memory[56,2], memory[57,2], memory[58,2], memory[59,2], memory[60,2], memory[61,2], memory[62,2], memory[63,2], memory[64,2], memory[65,2], memory[66,2], memory[67,2], memory[68,2], memory[69,2], memory[70,2], memory[71,2], memory[72,2], memory[73,2], memory[74,2], memory[75,2], memory[76,2], memory[77,2], memory[78,2], memory[79,2], memory[80,2], memory[81,2], memory[82,2], memory[83,2], memory[84,2], memory[85,2], memory[86,2], memory[87,2], memory[88,2], memory[89,2], memory[90,2], memory[91,2], memory[92,2], memory[93,2], memory[94,2], memory[95,2], memory[96,2], memory[97,2], memory[98,2], memory[99,2], memory[100,2], memory[101,2], memory[102,2], memory[103,2], memory[104,2], memory[105,2], memory[106,2], memory[107,2], memory[108,2], memory[109,2], memory[110,2], memory[111,2], memory[112,2], memory[113,2], memory[114,2], memory[115,2], memory[116,2], memory[117,2], memory[118,2], memory[119,2], memory[120,2], memory[1,3], memory[2,3], memory[3,3], memory[4,3], memory[5,3], memory[6,3], memory[7,3], memory[8,3], memory[9,3], memory[10,3], memory[11,3], memory[12,3], memory[13,3], memory[14,3], memory[15,3], memory[16,3], memory[17,3], memory[18,3], memory[19,3], memory[20,3], memory[21,3], memory[22,3], memory[23,3], memory[24,3], memory[25,3], memory[26,3], memory[27,3], memory[28,3], memory[29,3], memory[30,3], memory[31,3], memory[32,3], memory[33,3], memory[34,3], memory[35,3], memory[36,3], memory[37,3], memory[38,3], memory[39,3], memory[40,3], memory[41,3], memory[42,3], memory[43,3], memory[44,3], memory[45,3], memory[46,3], memory[47,3], memory[48,3], memory[49,3], memory[50,3], memory[51,3], memory[52,3], memory[53,3], memory[54,3], memory[55,3], memory[56,3], memory[57,3], memory[58,3], memory[59,3], memory[60,3], memory[61,3], memory[62,3], memory[63,3], memory[64,3], memory[65,3], memory[66,3], memory[67,3], memory[68,3], memory[69,3], memory[70,3], memory[71,3], memory[72,3], memory[73,3], memory[74,3], memory[75,3], memory[76,3], memory[77,3], memory[78,3], memory[79,3], memory[80,3], memory[81,3], memory[82,3], memory[83,3], memory[84,3], memory[85,3], memory[86,3], memory[87,3], memory[88,3], memory[89,3], memory[90,3], memory[91,3], memory[92,3], memory[93,3], memory[94,3], memory[95,3], memory[96,3], memory[97,3], memory[98,3], memory[99,3], memory[100,3], memory[101,3], memory[102,3], memory[103,3], memory[104,3], memory[105,3], memory[106,3], memory[107,3], memory[108,3], memory[109,3], memory[110,3], memory[111,3], memory[112,3], memory[113,3], memory[114,3], memory[115,3], memory[116,3], memory[117,3], memory[118,3], memory[119,3], memory[120,3], memory[1,4], memory[2,4], memory[3,4], memory[4,4], memory[5,4], memory[6,4], memory[7,4], memory[8,4], memory[9,4], memory[10,4], memory[11,4], memory[12,4], memory[13,4], memory[14,4], memory[15,4], memory[16,4], memory[17,4], memory[18,4], memory[19,4], memory[20,4], memory[21,4], memory[22,4], memory[23,4], memory[24,4], memory[25,4], memory[26,4], memory[27,4], memory[28,4], memory[29,4], memory[30,4], memory[31,4], memory[32,4], memory[33,4], memory[34,4], memory[35,4], memory[36,4], memory[37,4], memory[38,4], memory[39,4], memory[40,4], memory[41,4], memory[42,4], memory[43,4], memory[44,4], memory[45,4], memory[46,4], memory[47,4], memory[48,4], memory[49,4], memory[50,4], memory[51,4], memory[52,4], memory[53,4], memory[54,4], memory[55,4], memory[56,4], memory[57,4], memory[58,4], memory[59,4], memory[60,4], memory[61,4], memory[62,4], memory[63,4], memory[64,4], memory[65,4], memory[66,4], memory[67,4], memory[68,4], memory[69,4], memory[70,4], memory[71,4], memory[72,4], memory[73,4], memory[74,4], memory[75,4], memory[76,4], memory[77,4], memory[78,4], memory[79,4], memory[80,4], memory[81,4], memory[82,4], memory[83,4], memory[84,4], memory[85,4], memory[86,4], memory[87,4], memory[88,4], memory[89,4], memory[90,4], memory[91,4], memory[92,4], memory[93,4], memory[94,4], memory[95,4], memory[96,4], memory[97,4], memory[98,4], memory[99,4], memory[100,4], memory[101,4], memory[102,4], memory[103,4], memory[104,4], memory[105,4], memory[106,4], memory[107,4], memory[108,4], memory[109,4], memory[110,4], memory[111,4], memory[112,4], memory[113,4], memory[114,4], memory[115,4], memory[116,4], memory[117,4], memory[118,4], memory[119,4], memory[120,4], memory[1,5], memory[2,5], memory[3,5], memory[4,5], memory[5,5], memory[6,5], memory[7,5], memory[8,5], memory[9,5], memory[10,5], memory[11,5], memory[12,5], memory[13,5], memory[14,5], memory[15,5], memory[16,5], memory[17,5], memory[18,5], memory[19,5], memory[20,5], memory[21,5], memory[22,5], memory[23,5], memory[24,5], memory[25,5], memory[26,5], memory[27,5], memory[28,5], memory[29,5], memory[30,5], memory[31,5], memory[32,5], memory[33,5], memory[34,5], memory[35,5], memory[36,5], memory[37,5], memory[38,5], memory[39,5], memory[40,5], memory[41,5], memory[42,5], memory[43,5], memory[44,5], memory[45,5], memory[46,5], memory[47,5], memory[48,5], memory[49,5], memory[50,5], memory[51,5], memory[52,5], memory[53,5], memory[54,5], memory[55,5], memory[56,5], memory[57,5], memory[58,5], memory[59,5], memory[60,5], memory[61,5], memory[62,5], memory[63,5], memory[64,5], memory[65,5], memory[66,5], memory[67,5], memory[68,5], memory[69,5], memory[70,5], memory[71,5], memory[72,5], memory[73,5], memory[74,5], memory[75,5], memory[76,5], memory[77,5], memory[78,5], memory[79,5], memory[80,5], memory[81,5], memory[82,5], memory[83,5], memory[84,5], memory[85,5], memory[86,5], memory[87,5], memory[88,5], memory[89,5], memory[90,5], memory[91,5], memory[92,5], memory[93,5], memory[94,5], memory[95,5], memory[96,5], memory[97,5], memory[98,5], memory[99,5], memory[100,5], memory[101,5], memory[102,5], memory[103,5], memory[104,5], memory[105,5], memory[106,5], memory[107,5], memory[108,5], memory[109,5], memory[110,5], memory[111,5], memory[112,5], memory[113,5], memory[114,5], memory[115,5], memory[116,5], memory[117,5], memory[118,5], memory[119,5], memory[120,5], memory[1,6], memory[2,6], memory[3,6], memory[4,6], memory[5,6], memory[6,6], memory[7,6], memory[8,6], memory[9,6], memory[10,6], memory[11,6], memory[12,6], memory[13,6], memory[14,6], memory[15,6], memory[16,6], memory[17,6], memory[18,6], memory[19,6], memory[20,6], memory[21,6], memory[22,6], memory[23,6], memory[24,6], memory[25,6], memory[26,6], memory[27,6], memory[28,6], memory[29,6], memory[30,6], memory[31,6], memory[32,6], memory[33,6], memory[34,6], memory[35,6], memory[36,6], memory[37,6], memory[38,6], memory[39,6], memory[40,6], memory[41,6], memory[42,6], memory[43,6], memory[44,6], memory[45,6], memory[46,6], memory[47,6], memory[48,6], memory[49,6], memory[50,6], memory[51,6], memory[52,6], memory[53,6], memory[54,6], memory[55,6], memory[56,6], memory[57,6], memory[58,6], memory[59,6], memory[60,6], memory[61,6], memory[62,6], memory[63,6], memory[64,6], memory[65,6], memory[66,6], memory[67,6], memory[68,6], memory[69,6], memory[70,6], memory[71,6], memory[72,6], memory[73,6], memory[74,6], memory[75,6], memory[76,6], memory[77,6], memory[78,6], memory[79,6], memory[80,6], memory[81,6], memory[82,6], memory[83,6], memory[84,6], memory[85,6], memory[86,6], memory[87,6], memory[88,6], memory[89,6], memory[90,6], memory[91,6], memory[92,6], memory[93,6], memory[94,6], memory[95,6], memory[96,6], memory[97,6], memory[98,6], memory[99,6], memory[100,6], memory[101,6], memory[102,6], memory[103,6], memory[104,6], memory[105,6], memory[106,6], memory[107,6], memory[108,6], memory[109,6], memory[110,6], memory[111,6], memory[112,6], memory[113,6], memory[114,6], memory[115,6], memory[116,6], memory[117,6], memory[118,6], memory[119,6], memory[120,6], memory[1,7], memory[2,7], memory[3,7], memory[4,7], memory[5,7], memory[6,7], memory[7,7], memory[8,7], memory[9,7], memory[10,7], memory[11,7], memory[12,7], memory[13,7], memory[14,7], memory[15,7], memory[16,7], memory[17,7], memory[18,7], memory[19,7], memory[20,7], memory[21,7], memory[22,7], memory[23,7], memory[24,7], memory[25,7], memory[26,7], memory[27,7], memory[28,7], memory[29,7], memory[30,7], memory[31,7], memory[32,7], memory[33,7], memory[34,7], memory[35,7], memory[36,7], memory[37,7], memory[38,7], memory[39,7], memory[40,7], memory[41,7], memory[42,7], memory[43,7], memory[44,7], memory[45,7], memory[46,7], memory[47,7], memory[48,7], memory[49,7], memory[50,7], memory[51,7], memory[52,7], memory[53,7], memory[54,7], memory[55,7], memory[56,7], memory[57,7], memory[58,7], memory[59,7], memory[60,7], memory[61,7], memory[62,7], memory[63,7], memory[64,7], memory[65,7], memory[66,7], memory[67,7], memory[68,7], memory[69,7], memory[70,7], memory[71,7], memory[72,7], memory[73,7], memory[74,7], memory[75,7], memory[76,7], memory[77,7], memory[78,7], memory[79,7], memory[80,7], memory[81,7], memory[82,7], memory[83,7], memory[84,7], memory[85,7], memory[86,7], memory[87,7], memory[88,7], memory[89,7], memory[90,7], memory[91,7], memory[92,7], memory[93,7], memory[94,7], memory[95,7], memory[96,7], memory[97,7], memory[98,7], memory[99,7], memory[100,7], memory[101,7], memory[102,7], memory[103,7], memory[104,7], memory[105,7], memory[106,7], memory[107,7], memory[108,7], memory[109,7], memory[110,7], memory[111,7], memory[112,7], memory[113,7], memory[114,7], memory[115,7], memory[116,7], memory[117,7], memory[118,7], memory[119,7], memory[120,7], memory[1,8], memory[2,8], memory[3,8], memory[4,8], memory[5,8], memory[6,8], memory[7,8], memory[8,8], memory[9,8], memory[10,8], memory[11,8], memory[12,8], memory[13,8], memory[14,8], memory[15,8], memory[16,8], memory[17,8], memory[18,8], memory[19,8], memory[20,8], memory[21,8], memory[22,8], memory[23,8], memory[24,8], memory[25,8], memory[26,8], memory[27,8], memory[28,8], memory[29,8], memory[30,8], memory[31,8], memory[32,8], memory[33,8], memory[34,8], memory[35,8], memory[36,8], memory[37,8], memory[38,8], memory[39,8], memory[40,8], memory[41,8], memory[42,8], memory[43,8], memory[44,8], memory[45,8], memory[46,8], memory[47,8], memory[48,8], memory[49,8], memory[50,8], memory[51,8], memory[52,8], memory[53,8], memory[54,8], memory[55,8], memory[56,8], memory[57,8], memory[58,8], memory[59,8], memory[60,8], memory[61,8], memory[62,8], memory[63,8], memory[64,8], memory[65,8], memory[66,8], memory[67,8], memory[68,8], memory[69,8], memory[70,8], memory[71,8], memory[72,8], memory[73,8], memory[74,8], memory[75,8], memory[76,8], memory[77,8], memory[78,8], memory[79,8], memory[80,8], memory[81,8], memory[82,8], memory[83,8], memory[84,8], memory[85,8], memory[86,8], memory[87,8], memory[88,8], memory[89,8], memory[90,8], memory[91,8], memory[92,8], memory[93,8], memory[94,8], memory[95,8], memory[96,8], memory[97,8], memory[98,8], memory[99,8], memory[100,8], memory[101,8], memory[102,8], memory[103,8], memory[104,8], memory[105,8], memory[106,8], memory[107,8], memory[108,8], memory[109,8], memory[110,8], memory[111,8], memory[112,8], memory[113,8], memory[114,8], memory[115,8], memory[116,8], memory[117,8], memory[118,8], memory[119,8], memory[120,8], memory[1,9], memory[2,9], memory[3,9], memory[4,9], memory[5,9], memory[6,9], memory[7,9], memory[8,9], memory[9,9], memory[10,9], memory[11,9], memory[12,9], memory[13,9], memory[14,9], memory[15,9], memory[16,9], memory[17,9], memory[18,9], memory[19,9], memory[20,9], memory[21,9], memory[22,9], memory[23,9], memory[24,9], memory[25,9], memory[26,9], memory[27,9], memory[28,9], memory[29,9], memory[30,9], memory[31,9], memory[32,9], memory[33,9], memory[34,9], memory[35,9], memory[36,9], memory[37,9], memory[38,9], memory[39,9], memory[40,9], memory[41,9], memory[42,9], memory[43,9], memory[44,9], memory[45,9], memory[46,9], memory[47,9], memory[48,9], memory[49,9], memory[50,9], memory[51,9], memory[52,9], memory[53,9], memory[54,9], memory[55,9], memory[56,9], memory[57,9], memory[58,9], memory[59,9], memory[60,9], memory[61,9], memory[62,9], memory[63,9], memory[64,9], memory[65,9], memory[66,9], memory[67,9], memory[68,9], memory[69,9], memory[70,9], memory[71,9], memory[72,9], memory[73,9], memory[74,9], memory[75,9], memory[76,9], memory[77,9], memory[78,9], memory[79,9], memory[80,9], memory[81,9], memory[82,9], memory[83,9], memory[84,9], memory[85,9], memory[86,9], memory[87,9], memory[88,9], memory[89,9], memory[90,9], memory[91,9], memory[92,9], memory[93,9], memory[94,9], memory[95,9], memory[96,9], memory[97,9], memory[98,9], memory[99,9], memory[100,9], memory[101,9], memory[102,9], memory[103,9], memory[104,9], memory[105,9], memory[106,9], memory[107,9], memory[108,9], memory[109,9], memory[110,9], memory[111,9], memory[112,9], memory[113,9], memory[114,9], memory[115,9], memory[116,9], memory[117,9], memory[118,9], memory[119,9], memory[120,9], memory[1,10], memory[2,10], memory[3,10], memory[4,10], memory[5,10], memory[6,10], memory[7,10], memory[8,10], memory[9,10], memory[10,10], memory[11,10], memory[12,10], memory[13,10], memory[14,10], memory[15,10], memory[16,10], memory[17,10], memory[18,10], memory[19,10], memory[20,10], memory[21,10], memory[22,10], memory[23,10], memory[24,10], memory[25,10], memory[26,10], memory[27,10], memory[28,10], memory[29,10], memory[30,10], memory[31,10], memory[32,10], memory[33,10], memory[34,10], memory[35,10], memory[36,10], memory[37,10], memory[38,10], memory[39,10], memory[40,10], memory[41,10], memory[42,10], memory[43,10], memory[44,10], memory[45,10], memory[46,10], memory[47,10], memory[48,10], memory[49,10], memory[50,10], memory[51,10], memory[52,10], memory[53,10], memory[54,10], memory[55,10], memory[56,10], memory[57,10], memory[58,10], memory[59,10], memory[60,10], memory[61,10], memory[62,10], memory[63,10], memory[64,10], memory[65,10], memory[66,10], memory[67,10], memory[68,10], memory[69,10], memory[70,10], memory[71,10], memory[72,10], memory[73,10], memory[74,10], memory[75,10], memory[76,10], memory[77,10], memory[78,10], memory[79,10], memory[80,10], memory[81,10], memory[82,10], memory[83,10], memory[84,10], memory[85,10], memory[86,10], memory[87,10], memory[88,10], memory[89,10], memory[90,10], memory[91,10], memory[92,10], memory[93,10], memory[94,10], memory[95,10], memory[96,10], memory[97,10], memory[98,10], memory[99,10], memory[100,10], memory[101,10], memory[102,10], memory[103,10], memory[104,10], memory[105,10], memory[106,10], memory[107,10], memory[108,10], memory[109,10], memory[110,10], memory[111,10], memory[112,10], memory[113,10], memory[114,10], memory[115,10], memory[116,10], memory[117,10], memory[118,10], memory[119,10], memory[120,10], memory[1,11], memory[2,11], memory[3,11], memory[4,11], memory[5,11], memory[6,11], memory[7,11], memory[8,11], memory[9,11], memory[10,11], memory[11,11], memory[12,11], memory[13,11], memory[14,11], memory[15,11], memory[16,11], memory[17,11], memory[18,11], memory[19,11], memory[20,11], memory[21,11], memory[22,11], memory[23,11], memory[24,11], memory[25,11], memory[26,11], memory[27,11], memory[28,11], memory[29,11], memory[30,11], memory[31,11], memory[32,11], memory[33,11], memory[34,11], memory[35,11], memory[36,11], memory[37,11], memory[38,11], memory[39,11], memory[40,11], memory[41,11], memory[42,11], memory[43,11], memory[44,11], memory[45,11], memory[46,11], memory[47,11], memory[48,11], memory[49,11], memory[50,11], memory[51,11], memory[52,11], memory[53,11], memory[54,11], memory[55,11], memory[56,11], memory[57,11], memory[58,11], memory[59,11], memory[60,11], memory[61,11], memory[62,11], memory[63,11], memory[64,11], memory[65,11], memory[66,11], memory[67,11], memory[68,11], memory[69,11], memory[70,11], memory[71,11], memory[72,11], memory[73,11], memory[74,11], memory[75,11], memory[76,11], memory[77,11], memory[78,11], memory[79,11], memory[80,11], memory[81,11], memory[82,11], memory[83,11], memory[84,11], memory[85,11], memory[86,11], memory[87,11], memory[88,11], memory[89,11], memory[90,11], memory[91,11], memory[92,11], memory[93,11], memory[94,11], memory[95,11], memory[96,11], memory[97,11], memory[98,11], memory[99,11], memory[100,11], memory[101,11], memory[102,11], memory[103,11], memory[104,11], memory[105,11], memory[106,11], memory[107,11], memory[108,11], memory[109,11], memory[110,11], memory[111,11], memory[112,11], memory[113,11], memory[114,11], memory[115,11], memory[116,11], memory[117,11], memory[118,11], memory[119,11], memory[120,11], memory[1,12], memory[2,12], memory[3,12], memory[4,12], memory[5,12], memory[6,12], memory[7,12], memory[8,12], memory[9,12], memory[10,12], memory[11,12], memory[12,12], memory[13,12], memory[14,12], memory[15,12], memory[16,12], memory[17,12], memory[18,12], memory[19,12], memory[20,12], memory[21,12], memory[22,12], memory[23,12], memory[24,12], memory[25,12], memory[26,12], memory[27,12], memory[28,12], memory[29,12], memory[30,12], memory[31,12], memory[32,12], memory[33,12], memory[34,12], memory[35,12], memory[36,12], memory[37,12], memory[38,12], memory[39,12], memory[40,12], memory[41,12], memory[42,12], memory[43,12], memory[44,12], memory[45,12], memory[46,12], memory[47,12], memory[48,12], memory[49,12], memory[50,12], memory[51,12], memory[52,12], memory[53,12], memory[54,12], memory[55,12], memory[56,12], memory[57,12], memory[58,12], memory[59,12], memory[60,12], memory[61,12], memory[62,12], memory[63,12], memory[64,12], memory[65,12], memory[66,12], memory[67,12], memory[68,12], memory[69,12], memory[70,12], memory[71,12], memory[72,12], memory[73,12], memory[74,12], memory[75,12], memory[76,12], memory[77,12], memory[78,12], memory[79,12], memory[80,12], memory[81,12], memory[82,12], memory[83,12], memory[84,12], memory[85,12], memory[86,12], memory[87,12], memory[88,12], memory[89,12], memory[90,12], memory[91,12], memory[92,12], memory[93,12], memory[94,12], memory[95,12], memory[96,12], memory[97,12], memory[98,12], memory[99,12], memory[100,12], memory[101,12], memory[102,12], memory[103,12], memory[104,12], memory[105,12], memory[106,12], memory[107,12], memory[108,12], memory[109,12], memory[110,12], memory[111,12], memory[112,12], memory[113,12], memory[114,12], memory[115,12], memory[116,12], memory[117,12], memory[118,12], memory[119,12], memory[120,12], memory[1,13], memory[2,13], memory[3,13], memory[4,13], memory[5,13], memory[6,13], memory[7,13], memory[8,13], memory[9,13], memory[10,13], memory[11,13], memory[12,13], memory[13,13], memory[14,13], memory[15,13], memory[16,13], memory[17,13], memory[18,13], memory[19,13], memory[20,13], memory[21,13], memory[22,13], memory[23,13], memory[24,13], memory[25,13], memory[26,13], memory[27,13], memory[28,13], memory[29,13], memory[30,13], memory[31,13], memory[32,13], memory[33,13], memory[34,13], memory[35,13], memory[36,13], memory[37,13], memory[38,13], memory[39,13], memory[40,13], memory[41,13], memory[42,13], memory[43,13], memory[44,13], memory[45,13], memory[46,13], memory[47,13], memory[48,13], memory[49,13], memory[50,13], memory[51,13], memory[52,13], memory[53,13], memory[54,13], memory[55,13], memory[56,13], memory[57,13], memory[58,13], memory[59,13], memory[60,13], memory[61,13], memory[62,13], memory[63,13], memory[64,13], memory[65,13], memory[66,13], memory[67,13], memory[68,13], memory[69,13], memory[70,13], memory[71,13], memory[72,13], memory[73,13], memory[74,13], memory[75,13], memory[76,13], memory[77,13], memory[78,13], memory[79,13], memory[80,13], memory[81,13], memory[82,13], memory[83,13], memory[84,13], memory[85,13], memory[86,13], memory[87,13], memory[88,13], memory[89,13], memory[90,13], memory[91,13], memory[92,13], memory[93,13], memory[94,13], memory[95,13], memory[96,13], memory[97,13], memory[98,13], memory[99,13], memory[100,13], memory[101,13], memory[102,13], memory[103,13], memory[104,13], memory[105,13], memory[106,13], memory[107,13], memory[108,13], memory[109,13], memory[110,13], memory[111,13], memory[112,13], memory[113,13], memory[114,13], memory[115,13], memory[116,13], memory[117,13], memory[118,13], memory[119,13], memory[120,13], memory[1,14], memory[2,14], memory[3,14], memory[4,14], memory[5,14], memory[6,14], memory[7,14], memory[8,14], memory[9,14], memory[10,14], memory[11,14], memory[12,14], memory[13,14], memory[14,14], memory[15,14], memory[16,14], memory[17,14], memory[18,14], memory[19,14], memory[20,14], memory[21,14], memory[22,14], memory[23,14], memory[24,14], memory[25,14], memory[26,14], memory[27,14], memory[28,14], memory[29,14], memory[30,14], memory[31,14], memory[32,14], memory[33,14], memory[34,14], memory[35,14], memory[36,14], memory[37,14], memory[38,14], memory[39,14], memory[40,14], memory[41,14], memory[42,14], memory[43,14], memory[44,14], memory[45,14], memory[46,14], memory[47,14], memory[48,14], memory[49,14], memory[50,14], memory[51,14], memory[52,14], memory[53,14], memory[54,14], memory[55,14], memory[56,14], memory[57,14], memory[58,14], memory[59,14], memory[60,14], memory[61,14], memory[62,14], memory[63,14], memory[64,14], memory[65,14], memory[66,14], memory[67,14], memory[68,14], memory[69,14], memory[70,14], memory[71,14], memory[72,14], memory[73,14], memory[74,14], memory[75,14], memory[76,14], memory[77,14], memory[78,14], memory[79,14], memory[80,14], memory[81,14], memory[82,14], memory[83,14], memory[84,14], memory[85,14], memory[86,14], memory[87,14], memory[88,14], memory[89,14], memory[90,14], memory[91,14], memory[92,14], memory[93,14], memory[94,14], memory[95,14], memory[96,14], memory[97,14], memory[98,14], memory[99,14], memory[100,14], memory[101,14], memory[102,14], memory[103,14], memory[104,14], memory[105,14], memory[106,14], memory[107,14], memory[108,14], memory[109,14], memory[110,14], memory[111,14], memory[112,14], memory[113,14], memory[114,14], memory[115,14], memory[116,14], memory[117,14], memory[118,14], memory[119,14], memory[120,14], memory[1,15], memory[2,15], memory[3,15], memory[4,15], memory[5,15], memory[6,15], memory[7,15], memory[8,15], memory[9,15], memory[10,15], memory[11,15], memory[12,15], memory[13,15], memory[14,15], memory[15,15], memory[16,15], memory[17,15], memory[18,15], memory[19,15], memory[20,15], memory[21,15], memory[22,15], memory[23,15], memory[24,15], memory[25,15], memory[26,15], memory[27,15], memory[28,15], memory[29,15], memory[30,15], memory[31,15], memory[32,15], memory[33,15], memory[34,15], memory[35,15], memory[36,15], memory[37,15], memory[38,15], memory[39,15], memory[40,15], memory[41,15], memory[42,15], memory[43,15], memory[44,15], memory[45,15], memory[46,15], memory[47,15], memory[48,15], memory[49,15], memory[50,15], memory[51,15], memory[52,15], memory[53,15], memory[54,15], memory[55,15], memory[56,15], memory[57,15], memory[58,15], memory[59,15], memory[60,15], memory[61,15], memory[62,15], memory[63,15], memory[64,15], memory[65,15], memory[66,15], memory[67,15], memory[68,15], memory[69,15], memory[70,15], memory[71,15], memory[72,15], memory[73,15], memory[74,15], memory[75,15], memory[76,15], memory[77,15], memory[78,15], memory[79,15], memory[80,15], memory[81,15], memory[82,15], memory[83,15], memory[84,15], memory[85,15], memory[86,15], memory[87,15], memory[88,15], memory[89,15], memory[90,15], memory[91,15], memory[92,15], memory[93,15], memory[94,15], memory[95,15], memory[96,15], memory[97,15], memory[98,15], memory[99,15], memory[100,15], memory[101,15], memory[102,15], memory[103,15], memory[104,15], memory[105,15], memory[106,15], memory[107,15], memory[108,15], memory[109,15], memory[110,15], memory[111,15], memory[112,15], memory[113,15], memory[114,15], memory[115,15], memory[116,15], memory[117,15], memory[118,15], memory[119,15], memory[120,15], memory[1,16], memory[2,16], memory[3,16], memory[4,16], memory[5,16], memory[6,16], memory[7,16], memory[8,16], memory[9,16], memory[10,16], memory[11,16], memory[12,16], memory[13,16], memory[14,16], memory[15,16], memory[16,16], memory[17,16], memory[18,16], memory[19,16], memory[20,16], memory[21,16], memory[22,16], memory[23,16], memory[24,16], memory[25,16], memory[26,16], memory[27,16], memory[28,16], memory[29,16], memory[30,16], memory[31,16], memory[32,16], memory[33,16], memory[34,16], memory[35,16], memory[36,16], memory[37,16], memory[38,16], memory[39,16], memory[40,16], memory[41,16], memory[42,16], memory[43,16], memory[44,16], memory[45,16], memory[46,16], memory[47,16], memory[48,16], memory[49,16], memory[50,16], memory[51,16], memory[52,16], memory[53,16], memory[54,16], memory[55,16], memory[56,16], memory[57,16], memory[58,16], memory[59,16], memory[60,16], memory[61,16], memory[62,16], memory[63,16], memory[64,16], memory[65,16], memory[66,16], memory[67,16], memory[68,16], memory[69,16], memory[70,16], memory[71,16], memory[72,16], memory[73,16], memory[74,16], memory[75,16], memory[76,16], memory[77,16], memory[78,16], memory[79,16], memory[80,16], memory[81,16], memory[82,16], memory[83,16], memory[84,16], memory[85,16], memory[86,16], memory[87,16], memory[88,16], memory[89,16], memory[90,16], memory[91,16], memory[92,16], memory[93,16], memory[94,16], memory[95,16], memory[96,16], memory[97,16], memory[98,16], memory[99,16], memory[100,16], memory[101,16], memory[102,16], memory[103,16], memory[104,16], memory[105,16], memory[106,16], memory[107,16], memory[108,16], memory[109,16], memory[110,16], memory[111,16], memory[112,16], memory[113,16], memory[114,16], memory[115,16], memory[116,16], memory[117,16], memory[118,16], memory[119,16], memory[120,16], memory[1,17], memory[2,17], memory[3,17], memory[4,17], memory[5,17], memory[6,17], memory[7,17], memory[8,17], memory[9,17], memory[10,17], memory[11,17], memory[12,17], memory[13,17], memory[14,17], memory[15,17], memory[16,17], memory[17,17], memory[18,17], memory[19,17], memory[20,17], memory[21,17], memory[22,17], memory[23,17], memory[24,17], memory[25,17], memory[26,17], memory[27,17], memory[28,17], memory[29,17], memory[30,17], memory[31,17], memory[32,17], memory[33,17], memory[34,17], memory[35,17], memory[36,17], memory[37,17], memory[38,17], memory[39,17], memory[40,17], memory[41,17], memory[42,17], memory[43,17], memory[44,17], memory[45,17], memory[46,17], memory[47,17], memory[48,17], memory[49,17], memory[50,17], memory[51,17], memory[52,17], memory[53,17], memory[54,17], memory[55,17], memory[56,17], memory[57,17], memory[58,17], memory[59,17], memory[60,17], memory[61,17], memory[62,17], memory[63,17], memory[64,17], memory[65,17], memory[66,17], memory[67,17], memory[68,17], memory[69,17], memory[70,17], memory[71,17], memory[72,17], memory[73,17], memory[74,17], memory[75,17], memory[76,17], memory[77,17], memory[78,17], memory[79,17], memory[80,17], memory[81,17], memory[82,17], memory[83,17], memory[84,17], memory[85,17], memory[86,17], memory[87,17], memory[88,17], memory[89,17], memory[90,17], memory[91,17], memory[92,17], memory[93,17], memory[94,17], memory[95,17], memory[96,17], memory[97,17], memory[98,17], memory[99,17], memory[100,17], memory[101,17], memory[102,17], memory[103,17], memory[104,17], memory[105,17], memory[106,17], memory[107,17], memory[108,17], memory[109,17], memory[110,17], memory[111,17], memory[112,17], memory[113,17], memory[114,17], memory[115,17], memory[116,17], memory[117,17], memory[118,17], memory[119,17], memory[120,17], memory[1,18], memory[2,18], memory[3,18], memory[4,18], memory[5,18], memory[6,18], memory[7,18], memory[8,18], memory[9,18], memory[10,18], memory[11,18], memory[12,18], memory[13,18], memory[14,18], memory[15,18], memory[16,18], memory[17,18], memory[18,18], memory[19,18], memory[20,18], memory[21,18], memory[22,18], memory[23,18], memory[24,18], memory[25,18], memory[26,18], memory[27,18], memory[28,18], memory[29,18], memory[30,18], memory[31,18], memory[32,18], memory[33,18], memory[34,18], memory[35,18], memory[36,18], memory[37,18], memory[38,18], memory[39,18], memory[40,18], memory[41,18], memory[42,18], memory[43,18], memory[44,18], memory[45,18], memory[46,18], memory[47,18], memory[48,18], memory[49,18], memory[50,18], memory[51,18], memory[52,18], memory[53,18], memory[54,18], memory[55,18], memory[56,18], memory[57,18], memory[58,18], memory[59,18], memory[60,18], memory[61,18], memory[62,18], memory[63,18], memory[64,18], memory[65,18], memory[66,18], memory[67,18], memory[68,18], memory[69,18], memory[70,18], memory[71,18], memory[72,18], memory[73,18], memory[74,18], memory[75,18], memory[76,18], memory[77,18], memory[78,18], memory[79,18], memory[80,18], memory[81,18], memory[82,18], memory[83,18], memory[84,18], memory[85,18], memory[86,18], memory[87,18], memory[88,18], memory[89,18], memory[90,18], memory[91,18], memory[92,18], memory[93,18], memory[94,18], memory[95,18], memory[96,18], memory[97,18], memory[98,18], memory[99,18], memory[100,18], memory[101,18], memory[102,18], memory[103,18], memory[104,18], memory[105,18], memory[106,18], memory[107,18], memory[108,18], memory[109,18], memory[110,18], memory[111,18], memory[112,18], memory[113,18], memory[114,18], memory[115,18], memory[116,18], memory[117,18], memory[118,18], memory[119,18], memory[120,18], memory[1,19], memory[2,19], memory[3,19], memory[4,19], memory[5,19], memory[6,19], memory[7,19], memory[8,19], memory[9,19], memory[10,19], memory[11,19], memory[12,19], memory[13,19], memory[14,19], memory[15,19], memory[16,19], memory[17,19], memory[18,19], memory[19,19], memory[20,19], memory[21,19], memory[22,19], memory[23,19], memory[24,19], memory[25,19], memory[26,19], memory[27,19], memory[28,19], memory[29,19], memory[30,19], memory[31,19], memory[32,19], memory[33,19], memory[34,19], memory[35,19], memory[36,19], memory[37,19], memory[38,19], memory[39,19], memory[40,19], memory[41,19], memory[42,19], memory[43,19], memory[44,19], memory[45,19], memory[46,19], memory[47,19], memory[48,19], memory[49,19], memory[50,19], memory[51,19], memory[52,19], memory[53,19], memory[54,19], memory[55,19], memory[56,19], memory[57,19], memory[58,19], memory[59,19], memory[60,19], memory[61,19], memory[62,19], memory[63,19], memory[64,19], memory[65,19], memory[66,19], memory[67,19], memory[68,19], memory[69,19], memory[70,19], memory[71,19], memory[72,19], memory[73,19], memory[74,19], memory[75,19], memory[76,19], memory[77,19], memory[78,19], memory[79,19], memory[80,19], memory[81,19], memory[82,19], memory[83,19], memory[84,19], memory[85,19], memory[86,19], memory[87,19], memory[88,19], memory[89,19], memory[90,19], memory[91,19], memory[92,19], memory[93,19], memory[94,19], memory[95,19], memory[96,19], memory[97,19], memory[98,19], memory[99,19], memory[100,19], memory[101,19], memory[102,19], memory[103,19], memory[104,19], memory[105,19], memory[106,19], memory[107,19], memory[108,19], memory[109,19], memory[110,19], memory[111,19], memory[112,19], memory[113,19], memory[114,19], memory[115,19], memory[116,19], memory[117,19], memory[118,19], memory[119,19], memory[120,19], memory[1,20], memory[2,20], memory[3,20], memory[4,20], memory[5,20], memory[6,20], memory[7,20], memory[8,20], memory[9,20], memory[10,20], memory[11,20], memory[12,20], memory[13,20], memory[14,20], memory[15,20], memory[16,20], memory[17,20], memory[18,20], memory[19,20], memory[20,20], memory[21,20], memory[22,20], memory[23,20], memory[24,20], memory[25,20], memory[26,20], memory[27,20], memory[28,20], memory[29,20], memory[30,20], memory[31,20], memory[32,20], memory[33,20], memory[34,20], memory[35,20], memory[36,20], memory[37,20], memory[38,20], memory[39,20], memory[40,20], memory[41,20], memory[42,20], memory[43,20], memory[44,20], memory[45,20], memory[46,20], memory[47,20], memory[48,20], memory[49,20], memory[50,20], memory[51,20], memory[52,20], memory[53,20], memory[54,20], memory[55,20], memory[56,20], memory[57,20], memory[58,20], memory[59,20], memory[60,20], memory[61,20], memory[62,20], memory[63,20], memory[64,20], memory[65,20], memory[66,20], memory[67,20], memory[68,20], memory[69,20], memory[70,20], memory[71,20], memory[72,20], memory[73,20], memory[74,20], memory[75,20], memory[76,20], memory[77,20], memory[78,20], memory[79,20], memory[80,20], memory[81,20], memory[82,20], memory[83,20], memory[84,20], memory[85,20], memory[86,20], memory[87,20], memory[88,20], memory[89,20], memory[90,20], memory[91,20], memory[92,20], memory[93,20], memory[94,20], memory[95,20], memory[96,20], memory[97,20], memory[98,20], memory[99,20], memory[100,20], memory[101,20], memory[102,20], memory[103,20], memory[104,20], memory[105,20], memory[106,20], memory[107,20], memory[108,20], memory[109,20], memory[110,20], memory[111,20], memory[112,20], memory[113,20], memory[114,20], memory[115,20], memory[116,20], memory[117,20], memory[118,20], memory[119,20], memory[120,20], memory[1,21], memory[2,21], memory[3,21], memory[4,21], memory[5,21], memory[6,21], memory[7,21], memory[8,21], memory[9,21], memory[10,21], memory[11,21], memory[12,21], memory[13,21], memory[14,21], memory[15,21], memory[16,21], memory[17,21], memory[18,21], memory[19,21], memory[20,21], memory[21,21], memory[22,21], memory[23,21], memory[24,21], memory[25,21], memory[26,21], memory[27,21], memory[28,21], memory[29,21], memory[30,21], memory[31,21], memory[32,21], memory[33,21], memory[34,21], memory[35,21], memory[36,21], memory[37,21], memory[38,21], memory[39,21], memory[40,21], memory[41,21], memory[42,21], memory[43,21], memory[44,21], memory[45,21], memory[46,21], memory[47,21], memory[48,21], memory[49,21], memory[50,21], memory[51,21], memory[52,21], memory[53,21], memory[54,21], memory[55,21], memory[56,21], memory[57,21], memory[58,21], memory[59,21], memory[60,21], memory[61,21], memory[62,21], memory[63,21], memory[64,21], memory[65,21], memory[66,21], memory[67,21], memory[68,21], memory[69,21], memory[70,21], memory[71,21], memory[72,21], memory[73,21], memory[74,21], memory[75,21], memory[76,21], memory[77,21], memory[78,21], memory[79,21], memory[80,21], memory[81,21], memory[82,21], memory[83,21], memory[84,21], memory[85,21], memory[86,21], memory[87,21], memory[88,21], memory[89,21], memory[90,21], memory[91,21], memory[92,21], memory[93,21], memory[94,21], memory[95,21], memory[96,21], memory[97,21], memory[98,21], memory[99,21], memory[100,21], memory[101,21], memory[102,21], memory[103,21], memory[104,21], memory[105,21], memory[106,21], memory[107,21], memory[108,21], memory[109,21], memory[110,21], memory[111,21], memory[112,21], memory[113,21], memory[114,21], memory[115,21], memory[116,21], memory[117,21], memory[118,21], memory[119,21], memory[120,21], memory[1,22], memory[2,22], memory[3,22], memory[4,22], memory[5,22], memory[6,22], memory[7,22], memory[8,22], memory[9,22], memory[10,22], memory[11,22], memory[12,22], memory[13,22], memory[14,22], memory[15,22], memory[16,22], memory[17,22], memory[18,22], memory[19,22], memory[20,22], memory[21,22], memory[22,22], memory[23,22], memory[24,22], memory[25,22], memory[26,22], memory[27,22], memory[28,22], memory[29,22], memory[30,22], memory[31,22], memory[32,22], memory[33,22], memory[34,22], memory[35,22], memory[36,22], memory[37,22], memory[38,22], memory[39,22], memory[40,22], memory[41,22], memory[42,22], memory[43,22], memory[44,22], memory[45,22], memory[46,22], memory[47,22], memory[48,22], memory[49,22], memory[50,22], memory[51,22], memory[52,22], memory[53,22], memory[54,22], memory[55,22], memory[56,22], memory[57,22], memory[58,22], memory[59,22], memory[60,22], memory[61,22], memory[62,22], memory[63,22], memory[64,22], memory[65,22], memory[66,22], memory[67,22], memory[68,22], memory[69,22], memory[70,22], memory[71,22], memory[72,22], memory[73,22], memory[74,22], memory[75,22], memory[76,22], memory[77,22], memory[78,22], memory[79,22], memory[80,22], memory[81,22], memory[82,22], memory[83,22], memory[84,22], memory[85,22], memory[86,22], memory[87,22], memory[88,22], memory[89,22], memory[90,22], memory[91,22], memory[92,22], memory[93,22], memory[94,22], memory[95,22], memory[96,22], memory[97,22], memory[98,22], memory[99,22], memory[100,22], memory[101,22], memory[102,22], memory[103,22], memory[104,22], memory[105,22], memory[106,22], memory[107,22], memory[108,22], memory[109,22], memory[110,22], memory[111,22], memory[112,22], memory[113,22], memory[114,22], memory[115,22], memory[116,22], memory[117,22], memory[118,22], memory[119,22], memory[120,22], memory[1,23], memory[2,23], memory[3,23], memory[4,23], memory[5,23], memory[6,23], memory[7,23], memory[8,23], memory[9,23], memory[10,23], memory[11,23], memory[12,23], memory[13,23], memory[14,23], memory[15,23], memory[16,23], memory[17,23], memory[18,23], memory[19,23], memory[20,23], memory[21,23], memory[22,23], memory[23,23], memory[24,23], memory[25,23], memory[26,23], memory[27,23], memory[28,23], memory[29,23], memory[30,23], memory[31,23], memory[32,23], memory[33,23], memory[34,23], memory[35,23], memory[36,23], memory[37,23], memory[38,23], memory[39,23], memory[40,23], memory[41,23], memory[42,23], memory[43,23], memory[44,23], memory[45,23], memory[46,23], memory[47,23], memory[48,23], memory[49,23], memory[50,23], memory[51,23], memory[52,23], memory[53,23], memory[54,23], memory[55,23], memory[56,23], memory[57,23], memory[58,23], memory[59,23], memory[60,23], memory[61,23], memory[62,23], memory[63,23], memory[64,23], memory[65,23], memory[66,23], memory[67,23], memory[68,23], memory[69,23], memory[70,23], memory[71,23], memory[72,23], memory[73,23], memory[74,23], memory[75,23], memory[76,23], memory[77,23], memory[78,23], memory[79,23], memory[80,23], memory[81,23], memory[82,23], memory[83,23], memory[84,23], memory[85,23], memory[86,23], memory[87,23], memory[88,23], memory[89,23], memory[90,23], memory[91,23], memory[92,23], memory[93,23], memory[94,23], memory[95,23], memory[96,23], memory[97,23], memory[98,23], memory[99,23], memory[100,23], memory[101,23], memory[102,23], memory[103,23], memory[104,23], memory[105,23], memory[106,23], memory[107,23], memory[108,23], memory[109,23], memory[110,23], memory[111,23], memory[112,23], memory[113,23], memory[114,23], memory[115,23], memory[116,23], memory[117,23], memory[118,23], memory[119,23], memory[120,23], memory[1,24], memory[2,24], memory[3,24], memory[4,24], memory[5,24], memory[6,24], memory[7,24], memory[8,24], memory[9,24], memory[10,24], memory[11,24], memory[12,24], memory[13,24], memory[14,24], memory[15,24], memory[16,24], memory[17,24], memory[18,24], memory[19,24], memory[20,24], memory[21,24], memory[22,24], memory[23,24], memory[24,24], memory[25,24], memory[26,24], memory[27,24], memory[28,24], memory[29,24], memory[30,24], memory[31,24], memory[32,24], memory[33,24], memory[34,24], memory[35,24], memory[36,24], memory[37,24], memory[38,24], memory[39,24], memory[40,24], memory[41,24], memory[42,24], memory[43,24], memory[44,24], memory[45,24], memory[46,24], memory[47,24], memory[48,24], memory[49,24], memory[50,24], memory[51,24], memory[52,24], memory[53,24], memory[54,24], memory[55,24], memory[56,24], memory[57,24], memory[58,24], memory[59,24], memory[60,24], memory[61,24], memory[62,24], memory[63,24], memory[64,24], memory[65,24], memory[66,24], memory[67,24], memory[68,24], memory[69,24], memory[70,24], memory[71,24], memory[72,24], memory[73,24], memory[74,24], memory[75,24], memory[76,24], memory[77,24], memory[78,24], memory[79,24], memory[80,24], memory[81,24], memory[82,24], memory[83,24], memory[84,24], memory[85,24], memory[86,24], memory[87,24], memory[88,24], memory[89,24], memory[90,24], memory[91,24], memory[92,24], memory[93,24], memory[94,24], memory[95,24], memory[96,24], memory[97,24], memory[98,24], memory[99,24], memory[100,24], memory[101,24], memory[102,24], memory[103,24], memory[104,24], memory[105,24], memory[106,24], memory[107,24], memory[108,24], memory[109,24], memory[110,24], memory[111,24], memory[112,24], memory[113,24], memory[114,24], memory[115,24], memory[116,24], memory[117,24], memory[118,24], memory[119,24], memory[120,24], memory[1,25], memory[2,25], memory[3,25], memory[4,25], memory[5,25], memory[6,25], memory[7,25], memory[8,25], memory[9,25], memory[10,25], memory[11,25], memory[12,25], memory[13,25], memory[14,25], memory[15,25], memory[16,25], memory[17,25], memory[18,25], memory[19,25], memory[20,25], memory[21,25], memory[22,25], memory[23,25], memory[24,25], memory[25,25], memory[26,25], memory[27,25], memory[28,25], memory[29,25], memory[30,25], memory[31,25], memory[32,25], memory[33,25], memory[34,25], memory[35,25], memory[36,25], memory[37,25], memory[38,25], memory[39,25], memory[40,25], memory[41,25], memory[42,25], memory[43,25], memory[44,25], memory[45,25], memory[46,25], memory[47,25], memory[48,25], memory[49,25], memory[50,25], memory[51,25], memory[52,25], memory[53,25], memory[54,25], memory[55,25], memory[56,25], memory[57,25], memory[58,25], memory[59,25], memory[60,25], memory[61,25], memory[62,25], memory[63,25], memory[64,25], memory[65,25], memory[66,25], memory[67,25], memory[68,25], memory[69,25], memory[70,25], memory[71,25], memory[72,25], memory[73,25], memory[74,25], memory[75,25], memory[76,25], memory[77,25], memory[78,25], memory[79,25], memory[80,25], memory[81,25], memory[82,25], memory[83,25], memory[84,25], memory[85,25], memory[86,25], memory[87,25], memory[88,25], memory[89,25], memory[90,25], memory[91,25], memory[92,25], memory[93,25], memory[94,25], memory[95,25], memory[96,25], memory[97,25], memory[98,25], memory[99,25], memory[100,25], memory[101,25], memory[102,25], memory[103,25], memory[104,25], memory[105,25], memory[106,25], memory[107,25], memory[108,25], memory[109,25], memory[110,25], memory[111,25], memory[112,25], memory[113,25], memory[114,25], memory[115,25], memory[116,25], memory[117,25], memory[118,25], memory[119,25], memory[120,25], memory[1,26], memory[2,26], memory[3,26], memory[4,26], memory[5,26], memory[6,26], memory[7,26], memory[8,26], memory[9,26], memory[10,26], memory[11,26], memory[12,26], memory[13,26], memory[14,26], memory[15,26], memory[16,26], memory[17,26], memory[18,26], memory[19,26], memory[20,26], memory[21,26], memory[22,26], memory[23,26], memory[24,26], memory[25,26], memory[26,26], memory[27,26], memory[28,26], memory[29,26], memory[30,26], memory[31,26], memory[32,26], memory[33,26], memory[34,26], memory[35,26], memory[36,26], memory[37,26], memory[38,26], memory[39,26], memory[40,26], memory[41,26], memory[42,26], memory[43,26], memory[44,26], memory[45,26], memory[46,26], memory[47,26], memory[48,26], memory[49,26], memory[50,26], memory[51,26], memory[52,26], memory[53,26], memory[54,26], memory[55,26], memory[56,26], memory[57,26], memory[58,26], memory[59,26], memory[60,26], memory[61,26], memory[62,26], memory[63,26], memory[64,26], memory[65,26], memory[66,26], memory[67,26], memory[68,26], memory[69,26], memory[70,26], memory[71,26], memory[72,26], memory[73,26], memory[74,26], memory[75,26], memory[76,26], memory[77,26], memory[78,26], memory[79,26], memory[80,26], memory[81,26], memory[82,26], memory[83,26], memory[84,26], memory[85,26], memory[86,26], memory[87,26], memory[88,26], memory[89,26], memory[90,26], memory[91,26], memory[92,26], memory[93,26], memory[94,26], memory[95,26], memory[96,26], memory[97,26], memory[98,26], memory[99,26], memory[100,26], memory[101,26], memory[102,26], memory[103,26], memory[104,26], memory[105,26], memory[106,26], memory[107,26], memory[108,26], memory[109,26], memory[110,26], memory[111,26], memory[112,26], memory[113,26], memory[114,26], memory[115,26], memory[116,26], memory[117,26], memory[118,26], memory[119,26], memory[120,26], memory[1,27], memory[2,27], memory[3,27], memory[4,27], memory[5,27], memory[6,27], memory[7,27], memory[8,27], memory[9,27], memory[10,27], memory[11,27], memory[12,27], memory[13,27], memory[14,27], memory[15,27], memory[16,27], memory[17,27], memory[18,27], memory[19,27], memory[20,27], memory[21,27], memory[22,27], memory[23,27], memory[24,27], memory[25,27], memory[26,27], memory[27,27], memory[28,27], memory[29,27], memory[30,27], memory[31,27], memory[32,27], memory[33,27], memory[34,27], memory[35,27], memory[36,27], memory[37,27], memory[38,27], memory[39,27], memory[40,27], memory[41,27], memory[42,27], memory[43,27], memory[44,27], memory[45,27], memory[46,27], memory[47,27], memory[48,27], memory[49,27], memory[50,27], memory[51,27], memory[52,27], memory[53,27], memory[54,27], memory[55,27], memory[56,27], memory[57,27], memory[58,27], memory[59,27], memory[60,27], memory[61,27], memory[62,27], memory[63,27], memory[64,27], memory[65,27], memory[66,27], memory[67,27], memory[68,27], memory[69,27], memory[70,27], memory[71,27], memory[72,27], memory[73,27], memory[74,27], memory[75,27], memory[76,27], memory[77,27], memory[78,27], memory[79,27], memory[80,27], memory[81,27], memory[82,27], memory[83,27], memory[84,27], memory[85,27], memory[86,27], memory[87,27], memory[88,27], memory[89,27], memory[90,27], memory[91,27], memory[92,27], memory[93,27], memory[94,27], memory[95,27], memory[96,27], memory[97,27], memory[98,27], memory[99,27], memory[100,27], memory[101,27], memory[102,27], memory[103,27], memory[104,27], memory[105,27], memory[106,27], memory[107,27], memory[108,27], memory[109,27], memory[110,27], memory[111,27], memory[112,27], memory[113,27], memory[114,27], memory[115,27], memory[116,27], memory[117,27], memory[118,27], memory[119,27], memory[120,27], memory[1,28], memory[2,28], memory[3,28], memory[4,28], memory[5,28], memory[6,28], memory[7,28], memory[8,28], memory[9,28], memory[10,28], memory[11,28], memory[12,28], memory[13,28], memory[14,28], memory[15,28], memory[16,28], memory[17,28], memory[18,28], memory[19,28], memory[20,28], memory[21,28], memory[22,28], memory[23,28], memory[24,28], memory[25,28], memory[26,28], memory[27,28], memory[28,28], memory[29,28], memory[30,28], memory[31,28], memory[32,28], memory[33,28], memory[34,28], memory[35,28], memory[36,28], memory[37,28], memory[38,28], memory[39,28], memory[40,28], memory[41,28], memory[42,28], memory[43,28], memory[44,28], memory[45,28], memory[46,28], memory[47,28], memory[48,28], memory[49,28], memory[50,28], memory[51,28], memory[52,28], memory[53,28], memory[54,28], memory[55,28], memory[56,28], memory[57,28], memory[58,28], memory[59,28], memory[60,28], memory[61,28], memory[62,28], memory[63,28], memory[64,28], memory[65,28], memory[66,28], memory[67,28], memory[68,28], memory[69,28], memory[70,28], memory[71,28], memory[72,28], memory[73,28], memory[74,28], memory[75,28], memory[76,28], memory[77,28], memory[78,28], memory[79,28], memory[80,28], memory[81,28], memory[82,28], memory[83,28], memory[84,28], memory[85,28], memory[86,28], memory[87,28], memory[88,28], memory[89,28], memory[90,28], memory[91,28], memory[92,28], memory[93,28], memory[94,28], memory[95,28], memory[96,28], memory[97,28], memory[98,28], memory[99,28], memory[100,28], memory[101,28], memory[102,28], memory[103,28], memory[104,28], memory[105,28], memory[106,28], memory[107,28], memory[108,28], memory[109,28], memory[110,28], memory[111,28], memory[112,28], memory[113,28], memory[114,28], memory[115,28], memory[116,28], memory[117,28], memory[118,28], memory[119,28], memory[120,28], memory[1,29], memory[2,29], memory[3,29], memory[4,29], memory[5,29], memory[6,29], memory[7,29], memory[8,29], memory[9,29], memory[10,29], memory[11,29], memory[12,29], memory[13,29], memory[14,29], memory[15,29], memory[16,29], memory[17,29], memory[18,29], memory[19,29], memory[20,29], memory[21,29], memory[22,29], memory[23,29], memory[24,29], memory[25,29], memory[26,29], memory[27,29], memory[28,29], memory[29,29], memory[30,29], memory[31,29], memory[32,29], memory[33,29], memory[34,29], memory[35,29], memory[36,29], memory[37,29], memory[38,29], memory[39,29], memory[40,29], memory[41,29], memory[42,29], memory[43,29], memory[44,29], memory[45,29], memory[46,29], memory[47,29], memory[48,29], memory[49,29], memory[50,29], memory[51,29], memory[52,29], memory[53,29], memory[54,29], memory[55,29], memory[56,29], memory[57,29], memory[58,29], memory[59,29], memory[60,29], memory[61,29], memory[62,29], memory[63,29], memory[64,29], memory[65,29], memory[66,29], memory[67,29], memory[68,29], memory[69,29], memory[70,29], memory[71,29], memory[72,29], memory[73,29], memory[74,29], memory[75,29], memory[76,29], memory[77,29], memory[78,29], memory[79,29], memory[80,29], memory[81,29], memory[82,29], memory[83,29], memory[84,29], memory[85,29], memory[86,29], memory[87,29], memory[88,29], memory[89,29], memory[90,29], memory[91,29], memory[92,29], memory[93,29], memory[94,29], memory[95,29], memory[96,29], memory[97,29], memory[98,29], memory[99,29], memory[100,29], memory[101,29], memory[102,29], memory[103,29], memory[104,29], memory[105,29], memory[106,29], memory[107,29], memory[108,29], memory[109,29], memory[110,29], memory[111,29], memory[112,29], memory[113,29], memory[114,29], memory[115,29], memory[116,29], memory[117,29], memory[118,29], memory[119,29], memory[120,29], memory[1,30], memory[2,30], memory[3,30], memory[4,30], memory[5,30], memory[6,30], memory[7,30], memory[8,30], memory[9,30], memory[10,30], memory[11,30], memory[12,30], memory[13,30], memory[14,30], memory[15,30], memory[16,30], memory[17,30], memory[18,30], memory[19,30], memory[20,30], memory[21,30], memory[22,30], memory[23,30], memory[24,30], memory[25,30], memory[26,30], memory[27,30], memory[28,30], memory[29,30], memory[30,30], memory[31,30], memory[32,30], memory[33,30], memory[34,30], memory[35,30], memory[36,30], memory[37,30], memory[38,30], memory[39,30], memory[40,30], memory[41,30], memory[42,30], memory[43,30], memory[44,30], memory[45,30], memory[46,30], memory[47,30], memory[48,30], memory[49,30], memory[50,30], memory[51,30], memory[52,30], memory[53,30], memory[54,30], memory[55,30], memory[56,30], memory[57,30], memory[58,30], memory[59,30], memory[60,30], memory[61,30], memory[62,30], memory[63,30], memory[64,30], memory[65,30], memory[66,30], memory[67,30], memory[68,30], memory[69,30], memory[70,30], memory[71,30], memory[72,30], memory[73,30], memory[74,30], memory[75,30], memory[76,30], memory[77,30], memory[78,30], memory[79,30], memory[80,30], memory[81,30], memory[82,30], memory[83,30], memory[84,30], memory[85,30], memory[86,30], memory[87,30], memory[88,30], memory[89,30], memory[90,30], memory[91,30], memory[92,30], memory[93,30], memory[94,30], memory[95,30], memory[96,30], memory[97,30], memory[98,30], memory[99,30], memory[100,30], memory[101,30], memory[102,30], memory[103,30], memory[104,30], memory[105,30], memory[106,30], memory[107,30], memory[108,30], memory[109,30], memory[110,30], memory[111,30], memory[112,30], memory[113,30], memory[114,30], memory[115,30], memory[116,30], memory[117,30], memory[118,30], memory[119,30], memory[120,30], memory[1,31], memory[2,31], memory[3,31], memory[4,31], memory[5,31], memory[6,31], memory[7,31], memory[8,31], memory[9,31], memory[10,31], memory[11,31], memory[12,31], memory[13,31], memory[14,31], memory[15,31], memory[16,31], memory[17,31], memory[18,31], memory[19,31], memory[20,31], memory[21,31], memory[22,31], memory[23,31], memory[24,31], memory[25,31], memory[26,31], memory[27,31], memory[28,31], memory[29,31], memory[30,31], memory[31,31], memory[32,31], memory[33,31], memory[34,31], memory[35,31], memory[36,31], memory[37,31], memory[38,31], memory[39,31], memory[40,31], memory[41,31], memory[42,31], memory[43,31], memory[44,31], memory[45,31], memory[46,31], memory[47,31], memory[48,31], memory[49,31], memory[50,31], memory[51,31], memory[52,31], memory[53,31], memory[54,31], memory[55,31], memory[56,31], memory[57,31], memory[58,31], memory[59,31], memory[60,31], memory[61,31], memory[62,31], memory[63,31], memory[64,31], memory[65,31], memory[66,31], memory[67,31], memory[68,31], memory[69,31], memory[70,31], memory[71,31], memory[72,31], memory[73,31], memory[74,31], memory[75,31], memory[76,31], memory[77,31], memory[78,31], memory[79,31], memory[80,31], memory[81,31], memory[82,31], memory[83,31], memory[84,31], memory[85,31], memory[86,31], memory[87,31], memory[88,31], memory[89,31], memory[90,31], memory[91,31], memory[92,31], memory[93,31], memory[94,31], memory[95,31], memory[96,31], memory[97,31], memory[98,31], memory[99,31], memory[100,31], memory[101,31], memory[102,31], memory[103,31], memory[104,31], memory[105,31], memory[106,31], memory[107,31], memory[108,31], memory[109,31], memory[110,31], memory[111,31], memory[112,31], memory[113,31], memory[114,31], memory[115,31], memory[116,31], memory[117,31], memory[118,31], memory[119,31], memory[120,31], memory[1,32], memory[2,32], memory[3,32], memory[4,32], memory[5,32], memory[6,32], memory[7,32], memory[8,32], memory[9,32], memory[10,32], memory[11,32], memory[12,32], memory[13,32], memory[14,32], memory[15,32], memory[16,32], memory[17,32], memory[18,32], memory[19,32], memory[20,32], memory[21,32], memory[22,32], memory[23,32], memory[24,32], memory[25,32], memory[26,32], memory[27,32], memory[28,32], memory[29,32], memory[30,32], memory[31,32], memory[32,32], memory[33,32], memory[34,32], memory[35,32], memory[36,32], memory[37,32], memory[38,32], memory[39,32], memory[40,32], memory[41,32], memory[42,32], memory[43,32], memory[44,32], memory[45,32], memory[46,32], memory[47,32], memory[48,32], memory[49,32], memory[50,32], memory[51,32], memory[52,32], memory[53,32], memory[54,32], memory[55,32], memory[56,32], memory[57,32], memory[58,32], memory[59,32], memory[60,32], memory[61,32], memory[62,32], memory[63,32], memory[64,32], memory[65,32], memory[66,32], memory[67,32], memory[68,32], memory[69,32], memory[70,32], memory[71,32], memory[72,32], memory[73,32], memory[74,32], memory[75,32], memory[76,32], memory[77,32], memory[78,32], memory[79,32], memory[80,32], memory[81,32], memory[82,32], memory[83,32], memory[84,32], memory[85,32], memory[86,32], memory[87,32], memory[88,32], memory[89,32], memory[90,32], memory[91,32], memory[92,32], memory[93,32], memory[94,32], memory[95,32], memory[96,32], memory[97,32], memory[98,32], memory[99,32], memory[100,32], memory[101,32], memory[102,32], memory[103,32], memory[104,32], memory[105,32], memory[106,32], memory[107,32], memory[108,32], memory[109,32], memory[110,32], memory[111,32], memory[112,32], memory[113,32], memory[114,32], memory[115,32], memory[116,32], memory[117,32], memory[118,32], memory[119,32], memory[120,32], memory[1,33], memory[2,33], memory[3,33], memory[4,33], memory[5,33], memory[6,33], memory[7,33], memory[8,33], memory[9,33], memory[10,33], memory[11,33], memory[12,33], memory[13,33], memory[14,33], memory[15,33], memory[16,33], memory[17,33], memory[18,33], memory[19,33], memory[20,33], memory[21,33], memory[22,33], memory[23,33], memory[24,33], memory[25,33], memory[26,33], memory[27,33], memory[28,33], memory[29,33], memory[30,33], memory[31,33], memory[32,33], memory[33,33], memory[34,33], memory[35,33], memory[36,33], memory[37,33], memory[38,33], memory[39,33], memory[40,33], memory[41,33], memory[42,33], memory[43,33], memory[44,33], memory[45,33], memory[46,33], memory[47,33], memory[48,33], memory[49,33], memory[50,33], memory[51,33], memory[52,33], memory[53,33], memory[54,33], memory[55,33], memory[56,33], memory[57,33], memory[58,33], memory[59,33], memory[60,33], memory[61,33], memory[62,33], memory[63,33], memory[64,33], memory[65,33], memory[66,33], memory[67,33], memory[68,33], memory[69,33], memory[70,33], memory[71,33], memory[72,33], memory[73,33], memory[74,33], memory[75,33], memory[76,33], memory[77,33], memory[78,33], memory[79,33], memory[80,33], memory[81,33], memory[82,33], memory[83,33], memory[84,33], memory[85,33], memory[86,33], memory[87,33], memory[88,33], memory[89,33], memory[90,33], memory[91,33], memory[92,33], memory[93,33], memory[94,33], memory[95,33], memory[96,33], memory[97,33], memory[98,33], memory[99,33], memory[100,33], memory[101,33], memory[102,33], memory[103,33], memory[104,33], memory[105,33], memory[106,33], memory[107,33], memory[108,33], memory[109,33], memory[110,33], memory[111,33], memory[112,33], memory[113,33], memory[114,33], memory[115,33], memory[116,33], memory[117,33], memory[118,33], memory[119,33], memory[120,33], memory[1,34], memory[2,34], memory[3,34], memory[4,34], memory[5,34], memory[6,34], memory[7,34], memory[8,34], memory[9,34], memory[10,34], memory[11,34], memory[12,34], memory[13,34], memory[14,34], memory[15,34], memory[16,34], memory[17,34], memory[18,34], memory[19,34], memory[20,34], memory[21,34], memory[22,34], memory[23,34], memory[24,34], memory[25,34], memory[26,34], memory[27,34], memory[28,34], memory[29,34], memory[30,34], memory[31,34], memory[32,34], memory[33,34], memory[34,34], memory[35,34], memory[36,34], memory[37,34], memory[38,34], memory[39,34], memory[40,34], memory[41,34], memory[42,34], memory[43,34], memory[44,34], memory[45,34], memory[46,34], memory[47,34], memory[48,34], memory[49,34], memory[50,34], memory[51,34], memory[52,34], memory[53,34], memory[54,34], memory[55,34], memory[56,34], memory[57,34], memory[58,34], memory[59,34], memory[60,34], memory[61,34], memory[62,34], memory[63,34], memory[64,34], memory[65,34], memory[66,34], memory[67,34], memory[68,34], memory[69,34], memory[70,34], memory[71,34], memory[72,34], memory[73,34], memory[74,34], memory[75,34], memory[76,34], memory[77,34], memory[78,34], memory[79,34], memory[80,34], memory[81,34], memory[82,34], memory[83,34], memory[84,34], memory[85,34], memory[86,34], memory[87,34], memory[88,34], memory[89,34], memory[90,34], memory[91,34], memory[92,34], memory[93,34], memory[94,34], memory[95,34], memory[96,34], memory[97,34], memory[98,34], memory[99,34], memory[100,34], memory[101,34], memory[102,34], memory[103,34], memory[104,34], memory[105,34], memory[106,34], memory[107,34], memory[108,34], memory[109,34], memory[110,34], memory[111,34], memory[112,34], memory[113,34], memory[114,34], memory[115,34], memory[116,34], memory[117,34], memory[118,34], memory[119,34], memory[120,34], memory[1,35], memory[2,35], memory[3,35], memory[4,35], memory[5,35], memory[6,35], memory[7,35], memory[8,35], memory[9,35], memory[10,35], memory[11,35], memory[12,35], memory[13,35], memory[14,35], memory[15,35], memory[16,35], memory[17,35], memory[18,35], memory[19,35], memory[20,35], memory[21,35], memory[22,35], memory[23,35], memory[24,35], memory[25,35], memory[26,35], memory[27,35], memory[28,35], memory[29,35], memory[30,35], memory[31,35], memory[32,35], memory[33,35], memory[34,35], memory[35,35], memory[36,35], memory[37,35], memory[38,35], memory[39,35], memory[40,35], memory[41,35], memory[42,35], memory[43,35], memory[44,35], memory[45,35], memory[46,35], memory[47,35], memory[48,35], memory[49,35], memory[50,35], memory[51,35], memory[52,35], memory[53,35], memory[54,35], memory[55,35], memory[56,35], memory[57,35], memory[58,35], memory[59,35], memory[60,35], memory[61,35], memory[62,35], memory[63,35], memory[64,35], memory[65,35], memory[66,35], memory[67,35], memory[68,35], memory[69,35], memory[70,35], memory[71,35], memory[72,35], memory[73,35], memory[74,35], memory[75,35], memory[76,35], memory[77,35], memory[78,35], memory[79,35], memory[80,35], memory[81,35], memory[82,35], memory[83,35], memory[84,35], memory[85,35], memory[86,35], memory[87,35], memory[88,35], memory[89,35], memory[90,35], memory[91,35], memory[92,35], memory[93,35], memory[94,35], memory[95,35], memory[96,35], memory[97,35], memory[98,35], memory[99,35], memory[100,35], memory[101,35], memory[102,35], memory[103,35], memory[104,35], memory[105,35], memory[106,35], memory[107,35], memory[108,35], memory[109,35], memory[110,35], memory[111,35], memory[112,35], memory[113,35], memory[114,35], memory[115,35], memory[116,35], memory[117,35], memory[118,35], memory[119,35], memory[120,35], memory[1,36], memory[2,36], memory[3,36], memory[4,36], memory[5,36], memory[6,36], memory[7,36], memory[8,36], memory[9,36], memory[10,36], memory[11,36], memory[12,36], memory[13,36], memory[14,36], memory[15,36], memory[16,36], memory[17,36], memory[18,36], memory[19,36], memory[20,36], memory[21,36], memory[22,36], memory[23,36], memory[24,36], memory[25,36], memory[26,36], memory[27,36], memory[28,36], memory[29,36], memory[30,36], memory[31,36], memory[32,36], memory[33,36], memory[34,36], memory[35,36], memory[36,36], memory[37,36], memory[38,36], memory[39,36], memory[40,36], memory[41,36], memory[42,36], memory[43,36], memory[44,36], memory[45,36], memory[46,36], memory[47,36], memory[48,36], memory[49,36], memory[50,36], memory[51,36], memory[52,36], memory[53,36], memory[54,36], memory[55,36], memory[56,36], memory[57,36], memory[58,36], memory[59,36], memory[60,36], memory[61,36], memory[62,36], memory[63,36], memory[64,36], memory[65,36], memory[66,36], memory[67,36], memory[68,36], memory[69,36], memory[70,36], memory[71,36], memory[72,36], memory[73,36], memory[74,36], memory[75,36], memory[76,36], memory[77,36], memory[78,36], memory[79,36], memory[80,36], memory[81,36], memory[82,36], memory[83,36], memory[84,36], memory[85,36], memory[86,36], memory[87,36], memory[88,36], memory[89,36], memory[90,36], memory[91,36], memory[92,36], memory[93,36], memory[94,36], memory[95,36], memory[96,36], memory[97,36], memory[98,36], memory[99,36], memory[100,36], memory[101,36], memory[102,36], memory[103,36], memory[104,36], memory[105,36], memory[106,36], memory[107,36], memory[108,36], memory[109,36], memory[110,36], memory[111,36], memory[112,36], memory[113,36], memory[114,36], memory[115,36], memory[116,36], memory[117,36], memory[118,36], memory[119,36], memory[120,36], memory[1,37], memory[2,37], memory[3,37], memory[4,37], memory[5,37], memory[6,37], memory[7,37], memory[8,37], memory[9,37], memory[10,37], memory[11,37], memory[12,37], memory[13,37], memory[14,37], memory[15,37], memory[16,37], memory[17,37], memory[18,37], memory[19,37], memory[20,37], memory[21,37], memory[22,37], memory[23,37], memory[24,37], memory[25,37], memory[26,37], memory[27,37], memory[28,37], memory[29,37], memory[30,37], memory[31,37], memory[32,37], memory[33,37], memory[34,37], memory[35,37], memory[36,37], memory[37,37], memory[38,37], memory[39,37], memory[40,37], memory[41,37], memory[42,37], memory[43,37], memory[44,37], memory[45,37], memory[46,37], memory[47,37], memory[48,37], memory[49,37], memory[50,37], memory[51,37], memory[52,37], memory[53,37], memory[54,37], memory[55,37], memory[56,37], memory[57,37], memory[58,37], memory[59,37], memory[60,37], memory[61,37], memory[62,37], memory[63,37], memory[64,37], memory[65,37], memory[66,37], memory[67,37], memory[68,37], memory[69,37], memory[70,37], memory[71,37], memory[72,37], memory[73,37], memory[74,37], memory[75,37], memory[76,37], memory[77,37], memory[78,37], memory[79,37], memory[80,37], memory[81,37], memory[82,37], memory[83,37], memory[84,37], memory[85,37], memory[86,37], memory[87,37], memory[88,37], memory[89,37], memory[90,37], memory[91,37], memory[92,37], memory[93,37], memory[94,37], memory[95,37], memory[96,37], memory[97,37], memory[98,37], memory[99,37], memory[100,37], memory[101,37], memory[102,37], memory[103,37], memory[104,37], memory[105,37], memory[106,37], memory[107,37], memory[108,37], memory[109,37], memory[110,37], memory[111,37], memory[112,37], memory[113,37], memory[114,37], memory[115,37], memory[116,37], memory[117,37], memory[118,37], memory[119,37], memory[120,37], memory[1,38], memory[2,38], memory[3,38], memory[4,38], memory[5,38], memory[6,38], memory[7,38], memory[8,38], memory[9,38], memory[10,38], memory[11,38], memory[12,38], memory[13,38], memory[14,38], memory[15,38], memory[16,38], memory[17,38], memory[18,38], memory[19,38], memory[20,38], memory[21,38], memory[22,38], memory[23,38], memory[24,38], memory[25,38], memory[26,38], memory[27,38], memory[28,38], memory[29,38], memory[30,38], memory[31,38], memory[32,38], memory[33,38], memory[34,38], memory[35,38], memory[36,38], memory[37,38], memory[38,38], memory[39,38], memory[40,38], memory[41,38], memory[42,38], memory[43,38], memory[44,38], memory[45,38], memory[46,38], memory[47,38], memory[48,38], memory[49,38], memory[50,38], memory[51,38], memory[52,38], memory[53,38], memory[54,38], memory[55,38], memory[56,38], memory[57,38], memory[58,38], memory[59,38], memory[60,38], memory[61,38], memory[62,38], memory[63,38], memory[64,38], memory[65,38], memory[66,38], memory[67,38], memory[68,38], memory[69,38], memory[70,38], memory[71,38], memory[72,38], memory[73,38], memory[74,38], memory[75,38], memory[76,38], memory[77,38], memory[78,38], memory[79,38], memory[80,38], memory[81,38], memory[82,38], memory[83,38], memory[84,38], memory[85,38], memory[86,38], memory[87,38], memory[88,38], memory[89,38], memory[90,38], memory[91,38], memory[92,38], memory[93,38], memory[94,38], memory[95,38], memory[96,38], memory[97,38], memory[98,38], memory[99,38], memory[100,38], memory[101,38], memory[102,38], memory[103,38], memory[104,38], memory[105,38], memory[106,38], memory[107,38], memory[108,38], memory[109,38], memory[110,38], memory[111,38], memory[112,38], memory[113,38], memory[114,38], memory[115,38], memory[116,38], memory[117,38], memory[118,38], memory[119,38], memory[120,38], memory[1,39], memory[2,39], memory[3,39], memory[4,39], memory[5,39], memory[6,39], memory[7,39], memory[8,39], memory[9,39], memory[10,39], memory[11,39], memory[12,39], memory[13,39], memory[14,39], memory[15,39], memory[16,39], memory[17,39], memory[18,39], memory[19,39], memory[20,39], memory[21,39], memory[22,39], memory[23,39], memory[24,39], memory[25,39], memory[26,39], memory[27,39], memory[28,39], memory[29,39], memory[30,39], memory[31,39], memory[32,39], memory[33,39], memory[34,39], memory[35,39], memory[36,39], memory[37,39], memory[38,39], memory[39,39], memory[40,39], memory[41,39], memory[42,39], memory[43,39], memory[44,39], memory[45,39], memory[46,39], memory[47,39], memory[48,39], memory[49,39], memory[50,39], memory[51,39], memory[52,39], memory[53,39], memory[54,39], memory[55,39], memory[56,39], memory[57,39], memory[58,39], memory[59,39], memory[60,39], memory[61,39], memory[62,39], memory[63,39], memory[64,39], memory[65,39], memory[66,39], memory[67,39], memory[68,39], memory[69,39], memory[70,39], memory[71,39], memory[72,39], memory[73,39], memory[74,39], memory[75,39], memory[76,39], memory[77,39], memory[78,39], memory[79,39], memory[80,39], memory[81,39], memory[82,39], memory[83,39], memory[84,39], memory[85,39], memory[86,39], memory[87,39], memory[88,39], memory[89,39], memory[90,39], memory[91,39], memory[92,39], memory[93,39], memory[94,39], memory[95,39], memory[96,39], memory[97,39], memory[98,39], memory[99,39], memory[100,39], memory[101,39], memory[102,39], memory[103,39], memory[104,39], memory[105,39], memory[106,39], memory[107,39], memory[108,39], memory[109,39], memory[110,39], memory[111,39], memory[112,39], memory[113,39], memory[114,39], memory[115,39], memory[116,39], memory[117,39], memory[118,39], memory[119,39], memory[120,39], memory[1,40], memory[2,40], memory[3,40], memory[4,40], memory[5,40], memory[6,40], memory[7,40], memory[8,40], memory[9,40], memory[10,40], memory[11,40], memory[12,40], memory[13,40], memory[14,40], memory[15,40], memory[16,40], memory[17,40], memory[18,40], memory[19,40], memory[20,40], memory[21,40], memory[22,40], memory[23,40], memory[24,40], memory[25,40], memory[26,40], memory[27,40], memory[28,40], memory[29,40], memory[30,40], memory[31,40], memory[32,40], memory[33,40], memory[34,40], memory[35,40], memory[36,40], memory[37,40], memory[38,40], memory[39,40], memory[40,40], memory[41,40], memory[42,40], memory[43,40], memory[44,40], memory[45,40], memory[46,40], memory[47,40], memory[48,40], memory[49,40], memory[50,40], memory[51,40], memory[52,40], memory[53,40], memory[54,40], memory[55,40], memory[56,40], memory[57,40], memory[58,40], memory[59,40], memory[60,40], memory[61,40], memory[62,40], memory[63,40], memory[64,40], memory[65,40], memory[66,40], memory[67,40], memory[68,40], memory[69,40], memory[70,40], memory[71,40], memory[72,40], memory[73,40], memory[74,40], memory[75,40], memory[76,40], memory[77,40], memory[78,40], memory[79,40], memory[80,40], memory[81,40], memory[82,40], memory[83,40], memory[84,40], memory[85,40], memory[86,40], memory[87,40], memory[88,40], memory[89,40], memory[90,40], memory[91,40], memory[92,40], memory[93,40], memory[94,40], memory[95,40], memory[96,40], memory[97,40], memory[98,40], memory[99,40], memory[100,40], memory[101,40], memory[102,40], memory[103,40], memory[104,40], memory[105,40], memory[106,40], memory[107,40], memory[108,40], memory[109,40], memory[110,40], memory[111,40], memory[112,40], memory[113,40], memory[114,40], memory[115,40], memory[116,40], memory[117,40], memory[118,40], memory[119,40], memory[120,40], memory[1,41], memory[2,41], memory[3,41], memory[4,41], memory[5,41], memory[6,41], memory[7,41], memory[8,41], memory[9,41], memory[10,41], memory[11,41], memory[12,41], memory[13,41], memory[14,41], memory[15,41], memory[16,41], memory[17,41], memory[18,41], memory[19,41], memory[20,41], memory[21,41], memory[22,41], memory[23,41], memory[24,41], memory[25,41], memory[26,41], memory[27,41], memory[28,41], memory[29,41], memory[30,41], memory[31,41], memory[32,41], memory[33,41], memory[34,41], memory[35,41], memory[36,41], memory[37,41], memory[38,41], memory[39,41], memory[40,41], memory[41,41], memory[42,41], memory[43,41], memory[44,41], memory[45,41], memory[46,41], memory[47,41], memory[48,41], memory[49,41], memory[50,41], memory[51,41], memory[52,41], memory[53,41], memory[54,41], memory[55,41], memory[56,41], memory[57,41], memory[58,41], memory[59,41], memory[60,41], memory[61,41], memory[62,41], memory[63,41], memory[64,41], memory[65,41], memory[66,41], memory[67,41], memory[68,41], memory[69,41], memory[70,41], memory[71,41], memory[72,41], memory[73,41], memory[74,41], memory[75,41], memory[76,41], memory[77,41], memory[78,41], memory[79,41], memory[80,41], memory[81,41], memory[82,41], memory[83,41], memory[84,41], memory[85,41], memory[86,41], memory[87,41], memory[88,41], memory[89,41], memory[90,41], memory[91,41], memory[92,41], memory[93,41], memory[94,41], memory[95,41], memory[96,41], memory[97,41], memory[98,41], memory[99,41], memory[100,41], memory[101,41], memory[102,41], memory[103,41], memory[104,41], memory[105,41], memory[106,41], memory[107,41], memory[108,41], memory[109,41], memory[110,41], memory[111,41], memory[112,41], memory[113,41], memory[114,41], memory[115,41], memory[116,41], memory[117,41], memory[118,41], memory[119,41], memory[120,41], memory[1,42], memory[2,42], memory[3,42], memory[4,42], memory[5,42], memory[6,42], memory[7,42], memory[8,42], memory[9,42], memory[10,42], memory[11,42], memory[12,42], memory[13,42], memory[14,42], memory[15,42], memory[16,42], memory[17,42], memory[18,42], memory[19,42], memory[20,42], memory[21,42], memory[22,42], memory[23,42], memory[24,42], memory[25,42], memory[26,42], memory[27,42], memory[28,42], memory[29,42], memory[30,42], memory[31,42], memory[32,42], memory[33,42], memory[34,42], memory[35,42], memory[36,42], memory[37,42], memory[38,42], memory[39,42], memory[40,42], memory[41,42], memory[42,42], memory[43,42], memory[44,42], memory[45,42], memory[46,42], memory[47,42], memory[48,42], memory[49,42], memory[50,42], memory[51,42], memory[52,42], memory[53,42], memory[54,42], memory[55,42], memory[56,42], memory[57,42], memory[58,42], memory[59,42], memory[60,42], memory[61,42], memory[62,42], memory[63,42], memory[64,42], memory[65,42], memory[66,42], memory[67,42], memory[68,42], memory[69,42], memory[70,42], memory[71,42], memory[72,42], memory[73,42], memory[74,42], memory[75,42], memory[76,42], memory[77,42], memory[78,42], memory[79,42], memory[80,42], memory[81,42], memory[82,42], memory[83,42], memory[84,42], memory[85,42], memory[86,42], memory[87,42], memory[88,42], memory[89,42], memory[90,42], memory[91,42], memory[92,42], memory[93,42], memory[94,42], memory[95,42], memory[96,42], memory[97,42], memory[98,42], memory[99,42], memory[100,42], memory[101,42], memory[102,42], memory[103,42], memory[104,42], memory[105,42], memory[106,42], memory[107,42], memory[108,42], memory[109,42], memory[110,42], memory[111,42], memory[112,42], memory[113,42], memory[114,42], memory[115,42], memory[116,42], memory[117,42], memory[118,42], memory[119,42], memory[120,42], memory[1,43], memory[2,43], memory[3,43], memory[4,43], memory[5,43], memory[6,43], memory[7,43], memory[8,43], memory[9,43], memory[10,43], memory[11,43], memory[12,43], memory[13,43], memory[14,43], memory[15,43], memory[16,43], memory[17,43], memory[18,43], memory[19,43], memory[20,43], memory[21,43], memory[22,43], memory[23,43], memory[24,43], memory[25,43], memory[26,43], memory[27,43], memory[28,43], memory[29,43], memory[30,43], memory[31,43], memory[32,43], memory[33,43], memory[34,43], memory[35,43], memory[36,43], memory[37,43], memory[38,43], memory[39,43], memory[40,43], memory[41,43], memory[42,43], memory[43,43], memory[44,43], memory[45,43], memory[46,43], memory[47,43], memory[48,43], memory[49,43], memory[50,43], memory[51,43], memory[52,43], memory[53,43], memory[54,43], memory[55,43], memory[56,43], memory[57,43], memory[58,43], memory[59,43], memory[60,43], memory[61,43], memory[62,43], memory[63,43], memory[64,43], memory[65,43], memory[66,43], memory[67,43], memory[68,43], memory[69,43], memory[70,43], memory[71,43], memory[72,43], memory[73,43], memory[74,43], memory[75,43], memory[76,43], memory[77,43], memory[78,43], memory[79,43], memory[80,43], memory[81,43], memory[82,43], memory[83,43], memory[84,43], memory[85,43], memory[86,43], memory[87,43], memory[88,43], memory[89,43], memory[90,43], memory[91,43], memory[92,43], memory[93,43], memory[94,43], memory[95,43], memory[96,43], memory[97,43], memory[98,43], memory[99,43], memory[100,43], memory[101,43], memory[102,43], memory[103,43], memory[104,43], memory[105,43], memory[106,43], memory[107,43], memory[108,43], memory[109,43], memory[110,43], memory[111,43], memory[112,43], memory[113,43], memory[114,43], memory[115,43], memory[116,43], memory[117,43], memory[118,43], memory[119,43], memory[120,43], memory[1,44], memory[2,44], memory[3,44], memory[4,44], memory[5,44], memory[6,44], memory[7,44], memory[8,44], memory[9,44], memory[10,44], memory[11,44], memory[12,44], memory[13,44], memory[14,44], memory[15,44], memory[16,44], memory[17,44], memory[18,44], memory[19,44], memory[20,44], memory[21,44], memory[22,44], memory[23,44], memory[24,44], memory[25,44], memory[26,44], memory[27,44], memory[28,44], memory[29,44], memory[30,44], memory[31,44], memory[32,44], memory[33,44], memory[34,44], memory[35,44], memory[36,44], memory[37,44], memory[38,44], memory[39,44], memory[40,44], memory[41,44], memory[42,44], memory[43,44], memory[44,44], memory[45,44], memory[46,44], memory[47,44], memory[48,44], memory[49,44], memory[50,44], memory[51,44], memory[52,44], memory[53,44], memory[54,44], memory[55,44], memory[56,44], memory[57,44], memory[58,44], memory[59,44], memory[60,44], memory[61,44], memory[62,44], memory[63,44], memory[64,44], memory[65,44], memory[66,44], memory[67,44], memory[68,44], memory[69,44], memory[70,44], memory[71,44], memory[72,44], memory[73,44], memory[74,44], memory[75,44], memory[76,44], memory[77,44], memory[78,44], memory[79,44], memory[80,44], memory[81,44], memory[82,44], memory[83,44], memory[84,44], memory[85,44], memory[86,44], memory[87,44], memory[88,44], memory[89,44], memory[90,44], memory[91,44], memory[92,44], memory[93,44], memory[94,44], memory[95,44], memory[96,44], memory[97,44], memory[98,44], memory[99,44], memory[100,44], memory[101,44], memory[102,44], memory[103,44], memory[104,44], memory[105,44], memory[106,44], memory[107,44], memory[108,44], memory[109,44], memory[110,44], memory[111,44], memory[112,44], memory[113,44], memory[114,44], memory[115,44], memory[116,44], memory[117,44], memory[118,44], memory[119,44], memory[120,44], memory[1,45], memory[2,45], memory[3,45], memory[4,45], memory[5,45], memory[6,45], memory[7,45], memory[8,45], memory[9,45], memory[10,45], memory[11,45], memory[12,45], memory[13,45], memory[14,45], memory[15,45], memory[16,45], memory[17,45], memory[18,45], memory[19,45], memory[20,45], memory[21,45], memory[22,45], memory[23,45], memory[24,45], memory[25,45], memory[26,45], memory[27,45], memory[28,45], memory[29,45], memory[30,45], memory[31,45], memory[32,45], memory[33,45], memory[34,45], memory[35,45], memory[36,45], memory[37,45], memory[38,45], memory[39,45], memory[40,45], memory[41,45], memory[42,45], memory[43,45], memory[44,45], memory[45,45], memory[46,45], memory[47,45], memory[48,45], memory[49,45], memory[50,45], memory[51,45], memory[52,45], memory[53,45], memory[54,45], memory[55,45], memory[56,45], memory[57,45], memory[58,45], memory[59,45], memory[60,45], memory[61,45], memory[62,45], memory[63,45], memory[64,45], memory[65,45], memory[66,45], memory[67,45], memory[68,45], memory[69,45], memory[70,45], memory[71,45], memory[72,45], memory[73,45], memory[74,45], memory[75,45], memory[76,45], memory[77,45], memory[78,45], memory[79,45], memory[80,45], memory[81,45], memory[82,45], memory[83,45], memory[84,45], memory[85,45], memory[86,45], memory[87,45], memory[88,45], memory[89,45], memory[90,45], memory[91,45], memory[92,45], memory[93,45], memory[94,45], memory[95,45], memory[96,45], memory[97,45], memory[98,45], memory[99,45], memory[100,45], memory[101,45], memory[102,45], memory[103,45], memory[104,45], memory[105,45], memory[106,45], memory[107,45], memory[108,45], memory[109,45], memory[110,45], memory[111,45], memory[112,45], memory[113,45], memory[114,45], memory[115,45], memory[116,45], memory[117,45], memory[118,45], memory[119,45], memory[120,45], memory[1,46], memory[2,46], memory[3,46], memory[4,46], memory[5,46], memory[6,46], memory[7,46], memory[8,46], memory[9,46], memory[10,46], memory[11,46], memory[12,46], memory[13,46], memory[14,46], memory[15,46], memory[16,46], memory[17,46], memory[18,46], memory[19,46], memory[20,46], memory[21,46], memory[22,46], memory[23,46], memory[24,46], memory[25,46], memory[26,46], memory[27,46], memory[28,46], memory[29,46], memory[30,46], memory[31,46], memory[32,46], memory[33,46], memory[34,46], memory[35,46], memory[36,46], memory[37,46], memory[38,46], memory[39,46], memory[40,46], memory[41,46], memory[42,46], memory[43,46], memory[44,46], memory[45,46], memory[46,46], memory[47,46], memory[48,46], memory[49,46], memory[50,46], memory[51,46], memory[52,46], memory[53,46], memory[54,46], memory[55,46], memory[56,46], memory[57,46], memory[58,46], memory[59,46], memory[60,46], memory[61,46], memory[62,46], memory[63,46], memory[64,46], memory[65,46], memory[66,46], memory[67,46], memory[68,46], memory[69,46], memory[70,46], memory[71,46], memory[72,46], memory[73,46], memory[74,46], memory[75,46], memory[76,46], memory[77,46], memory[78,46], memory[79,46], memory[80,46], memory[81,46], memory[82,46], memory[83,46], memory[84,46], memory[85,46], memory[86,46], memory[87,46], memory[88,46], memory[89,46], memory[90,46], memory[91,46], memory[92,46], memory[93,46], memory[94,46], memory[95,46], memory[96,46], memory[97,46], memory[98,46], memory[99,46], memory[100,46], memory[101,46], memory[102,46], memory[103,46], memory[104,46], memory[105,46], memory[106,46], memory[107,46], memory[108,46], memory[109,46], memory[110,46], memory[111,46], memory[112,46], memory[113,46], memory[114,46], memory[115,46], memory[116,46], memory[117,46], memory[118,46], memory[119,46], memory[120,46], memory[1,47], memory[2,47], memory[3,47], memory[4,47], memory[5,47], memory[6,47], memory[7,47], memory[8,47], memory[9,47], memory[10,47], memory[11,47], memory[12,47], memory[13,47], memory[14,47], memory[15,47], memory[16,47], memory[17,47], memory[18,47], memory[19,47], memory[20,47], memory[21,47], memory[22,47], memory[23,47], memory[24,47], memory[25,47], memory[26,47], memory[27,47], memory[28,47], memory[29,47], memory[30,47], memory[31,47], memory[32,47], memory[33,47], memory[34,47], memory[35,47], memory[36,47], memory[37,47], memory[38,47], memory[39,47], memory[40,47], memory[41,47], memory[42,47], memory[43,47], memory[44,47], memory[45,47], memory[46,47], memory[47,47], memory[48,47], memory[49,47], memory[50,47], memory[51,47], memory[52,47], memory[53,47], memory[54,47], memory[55,47], memory[56,47], memory[57,47], memory[58,47], memory[59,47], memory[60,47], memory[61,47], memory[62,47], memory[63,47], memory[64,47], memory[65,47], memory[66,47], memory[67,47], memory[68,47], memory[69,47], memory[70,47], memory[71,47], memory[72,47], memory[73,47], memory[74,47], memory[75,47], memory[76,47], memory[77,47], memory[78,47], memory[79,47], memory[80,47], memory[81,47], memory[82,47], memory[83,47], memory[84,47], memory[85,47], memory[86,47], memory[87,47], memory[88,47], memory[89,47], memory[90,47], memory[91,47], memory[92,47], memory[93,47], memory[94,47], memory[95,47], memory[96,47], memory[97,47], memory[98,47], memory[99,47], memory[100,47], memory[101,47], memory[102,47], memory[103,47], memory[104,47], memory[105,47], memory[106,47], memory[107,47], memory[108,47], memory[109,47], memory[110,47], memory[111,47], memory[112,47], memory[113,47], memory[114,47], memory[115,47], memory[116,47], memory[117,47], memory[118,47], memory[119,47], memory[120,47], memory[1,48], memory[2,48], memory[3,48], memory[4,48], memory[5,48], memory[6,48], memory[7,48], memory[8,48], memory[9,48], memory[10,48], memory[11,48], memory[12,48], memory[13,48], memory[14,48], memory[15,48], memory[16,48], memory[17,48], memory[18,48], memory[19,48], memory[20,48], memory[21,48], memory[22,48], memory[23,48], memory[24,48], memory[25,48], memory[26,48], memory[27,48], memory[28,48], memory[29,48], memory[30,48], memory[31,48], memory[32,48], memory[33,48], memory[34,48], memory[35,48], memory[36,48], memory[37,48], memory[38,48], memory[39,48], memory[40,48], memory[41,48], memory[42,48], memory[43,48], memory[44,48], memory[45,48], memory[46,48], memory[47,48], memory[48,48], memory[49,48], memory[50,48], memory[51,48], memory[52,48], memory[53,48], memory[54,48], memory[55,48], memory[56,48], memory[57,48], memory[58,48], memory[59,48], memory[60,48], memory[61,48], memory[62,48], memory[63,48], memory[64,48], memory[65,48], memory[66,48], memory[67,48], memory[68,48], memory[69,48], memory[70,48], memory[71,48], memory[72,48], memory[73,48], memory[74,48], memory[75,48], memory[76,48], memory[77,48], memory[78,48], memory[79,48], memory[80,48], memory[81,48], memory[82,48], memory[83,48], memory[84,48], memory[85,48], memory[86,48], memory[87,48], memory[88,48], memory[89,48], memory[90,48], memory[91,48], memory[92,48], memory[93,48], memory[94,48], memory[95,48], memory[96,48], memory[97,48], memory[98,48], memory[99,48], memory[100,48], memory[101,48], memory[102,48], memory[103,48], memory[104,48], memory[105,48], memory[106,48], memory[107,48], memory[108,48], memory[109,48], memory[110,48], memory[111,48], memory[112,48], memory[113,48], memory[114,48], memory[115,48], memory[116,48], memory[117,48], memory[118,48], memory[119,48], memory[120,48], memory[1,49], memory[2,49], memory[3,49], memory[4,49], memory[5,49], memory[6,49], memory[7,49], memory[8,49], memory[9,49], memory[10,49], memory[11,49], memory[12,49], memory[13,49], memory[14,49], memory[15,49], memory[16,49], memory[17,49], memory[18,49], memory[19,49], memory[20,49], memory[21,49], memory[22,49], memory[23,49], memory[24,49], memory[25,49], memory[26,49], memory[27,49], memory[28,49], memory[29,49], memory[30,49], memory[31,49], memory[32,49], memory[33,49], memory[34,49], memory[35,49], memory[36,49], memory[37,49], memory[38,49], memory[39,49], memory[40,49], memory[41,49], memory[42,49], memory[43,49], memory[44,49], memory[45,49], memory[46,49], memory[47,49], memory[48,49], memory[49,49], memory[50,49], memory[51,49], memory[52,49], memory[53,49], memory[54,49], memory[55,49], memory[56,49], memory[57,49], memory[58,49], memory[59,49], memory[60,49], memory[61,49], memory[62,49], memory[63,49], memory[64,49], memory[65,49], memory[66,49], memory[67,49], memory[68,49], memory[69,49], memory[70,49], memory[71,49], memory[72,49], memory[73,49], memory[74,49], memory[75,49], memory[76,49], memory[77,49], memory[78,49], memory[79,49], memory[80,49], memory[81,49], memory[82,49], memory[83,49], memory[84,49], memory[85,49], memory[86,49], memory[87,49], memory[88,49], memory[89,49], memory[90,49], memory[91,49], memory[92,49], memory[93,49], memory[94,49], memory[95,49], memory[96,49], memory[97,49], memory[98,49], memory[99,49], memory[100,49], memory[101,49], memory[102,49], memory[103,49], memory[104,49], memory[105,49], memory[106,49], memory[107,49], memory[108,49], memory[109,49], memory[110,49], memory[111,49], memory[112,49], memory[113,49], memory[114,49], memory[115,49], memory[116,49], memory[117,49], memory[118,49], memory[119,49], memory[120,49], memory[1,50], memory[2,50], memory[3,50], memory[4,50], memory[5,50], memory[6,50], memory[7,50], memory[8,50], memory[9,50], memory[10,50], memory[11,50], memory[12,50], memory[13,50], memory[14,50], memory[15,50], memory[16,50], memory[17,50], memory[18,50], memory[19,50], memory[20,50], memory[21,50], memory[22,50], memory[23,50], memory[24,50], memory[25,50], memory[26,50], memory[27,50], memory[28,50], memory[29,50], memory[30,50], memory[31,50], memory[32,50], memory[33,50], memory[34,50], memory[35,50], memory[36,50], memory[37,50], memory[38,50], memory[39,50], memory[40,50], memory[41,50], memory[42,50], memory[43,50], memory[44,50], memory[45,50], memory[46,50], memory[47,50], memory[48,50], memory[49,50], memory[50,50], memory[51,50], memory[52,50], memory[53,50], memory[54,50], memory[55,50], memory[56,50], memory[57,50], memory[58,50], memory[59,50], memory[60,50], memory[61,50], memory[62,50], memory[63,50], memory[64,50], memory[65,50], memory[66,50], memory[67,50], memory[68,50], memory[69,50], memory[70,50], memory[71,50], memory[72,50], memory[73,50], memory[74,50], memory[75,50], memory[76,50], memory[77,50], memory[78,50], memory[79,50], memory[80,50], memory[81,50], memory[82,50], memory[83,50], memory[84,50], memory[85,50], memory[86,50], memory[87,50], memory[88,50], memory[89,50], memory[90,50], memory[91,50], memory[92,50], memory[93,50], memory[94,50], memory[95,50], memory[96,50], memory[97,50], memory[98,50], memory[99,50], memory[100,50], memory[101,50], memory[102,50], memory[103,50], memory[104,50], memory[105,50], memory[106,50], memory[107,50], memory[108,50], memory[109,50], memory[110,50], memory[111,50], memory[112,50], memory[113,50], memory[114,50], memory[115,50], memory[116,50], memory[117,50], memory[118,50], memory[119,50], memory[120,50], memory[1,51], memory[2,51], memory[3,51], memory[4,51], memory[5,51], memory[6,51], memory[7,51], memory[8,51], memory[9,51], memory[10,51], memory[11,51], memory[12,51], memory[13,51], memory[14,51], memory[15,51], memory[16,51], memory[17,51], memory[18,51], memory[19,51], memory[20,51], memory[21,51], memory[22,51], memory[23,51], memory[24,51], memory[25,51], memory[26,51], memory[27,51], memory[28,51], memory[29,51], memory[30,51], memory[31,51], memory[32,51], memory[33,51], memory[34,51], memory[35,51], memory[36,51], memory[37,51], memory[38,51], memory[39,51], memory[40,51], memory[41,51], memory[42,51], memory[43,51], memory[44,51], memory[45,51], memory[46,51], memory[47,51], memory[48,51], memory[49,51], memory[50,51], memory[51,51], memory[52,51], memory[53,51], memory[54,51], memory[55,51], memory[56,51], memory[57,51], memory[58,51], memory[59,51], memory[60,51], memory[61,51], memory[62,51], memory[63,51], memory[64,51], memory[65,51], memory[66,51], memory[67,51], memory[68,51], memory[69,51], memory[70,51], memory[71,51], memory[72,51], memory[73,51], memory[74,51], memory[75,51], memory[76,51], memory[77,51], memory[78,51], memory[79,51], memory[80,51], memory[81,51], memory[82,51], memory[83,51], memory[84,51], memory[85,51], memory[86,51], memory[87,51], memory[88,51], memory[89,51], memory[90,51], memory[91,51], memory[92,51], memory[93,51], memory[94,51], memory[95,51], memory[96,51], memory[97,51], memory[98,51], memory[99,51], memory[100,51], memory[101,51], memory[102,51], memory[103,51], memory[104,51], memory[105,51], memory[106,51], memory[107,51], memory[108,51], memory[109,51], memory[110,51], memory[111,51], memory[112,51], memory[113,51], memory[114,51], memory[115,51], memory[116,51], memory[117,51], memory[118,51], memory[119,51], memory[120,51], memory[1,52], memory[2,52], memory[3,52], memory[4,52], memory[5,52], memory[6,52], memory[7,52], memory[8,52], memory[9,52], memory[10,52], memory[11,52], memory[12,52], memory[13,52], memory[14,52], memory[15,52], memory[16,52], memory[17,52], memory[18,52], memory[19,52], memory[20,52], memory[21,52], memory[22,52], memory[23,52], memory[24,52], memory[25,52], memory[26,52], memory[27,52], memory[28,52], memory[29,52], memory[30,52], memory[31,52], memory[32,52], memory[33,52], memory[34,52], memory[35,52], memory[36,52], memory[37,52], memory[38,52], memory[39,52], memory[40,52], memory[41,52], memory[42,52], memory[43,52], memory[44,52], memory[45,52], memory[46,52], memory[47,52], memory[48,52], memory[49,52], memory[50,52], memory[51,52], memory[52,52], memory[53,52], memory[54,52], memory[55,52], memory[56,52], memory[57,52], memory[58,52], memory[59,52], memory[60,52], memory[61,52], memory[62,52], memory[63,52], memory[64,52], memory[65,52], memory[66,52], memory[67,52], memory[68,52], memory[69,52], memory[70,52], memory[71,52], memory[72,52], memory[73,52], memory[74,52], memory[75,52], memory[76,52], memory[77,52], memory[78,52], memory[79,52], memory[80,52], memory[81,52], memory[82,52], memory[83,52], memory[84,52], memory[85,52], memory[86,52], memory[87,52], memory[88,52], memory[89,52], memory[90,52], memory[91,52], memory[92,52], memory[93,52], memory[94,52], memory[95,52], memory[96,52], memory[97,52], memory[98,52], memory[99,52], memory[100,52], memory[101,52], memory[102,52], memory[103,52], memory[104,52], memory[105,52], memory[106,52], memory[107,52], memory[108,52], memory[109,52], memory[110,52], memory[111,52], memory[112,52], memory[113,52], memory[114,52], memory[115,52], memory[116,52], memory[117,52], memory[118,52], memory[119,52], memory[120,52], memory[1,53], memory[2,53], memory[3,53], memory[4,53], memory[5,53], memory[6,53], memory[7,53], memory[8,53], memory[9,53], memory[10,53], memory[11,53], memory[12,53], memory[13,53], memory[14,53], memory[15,53], memory[16,53], memory[17,53], memory[18,53], memory[19,53], memory[20,53], memory[21,53], memory[22,53], memory[23,53], memory[24,53], memory[25,53], memory[26,53], memory[27,53], memory[28,53], memory[29,53], memory[30,53], memory[31,53], memory[32,53], memory[33,53], memory[34,53], memory[35,53], memory[36,53], memory[37,53], memory[38,53], memory[39,53], memory[40,53], memory[41,53], memory[42,53], memory[43,53], memory[44,53], memory[45,53], memory[46,53], memory[47,53], memory[48,53], memory[49,53], memory[50,53], memory[51,53], memory[52,53], memory[53,53], memory[54,53], memory[55,53], memory[56,53], memory[57,53], memory[58,53], memory[59,53], memory[60,53], memory[61,53], memory[62,53], memory[63,53], memory[64,53], memory[65,53], memory[66,53], memory[67,53], memory[68,53], memory[69,53], memory[70,53], memory[71,53], memory[72,53], memory[73,53], memory[74,53], memory[75,53], memory[76,53], memory[77,53], memory[78,53], memory[79,53], memory[80,53], memory[81,53], memory[82,53], memory[83,53], memory[84,53], memory[85,53], memory[86,53], memory[87,53], memory[88,53], memory[89,53], memory[90,53], memory[91,53], memory[92,53], memory[93,53], memory[94,53], memory[95,53], memory[96,53], memory[97,53], memory[98,53], memory[99,53], memory[100,53], memory[101,53], memory[102,53], memory[103,53], memory[104,53], memory[105,53], memory[106,53], memory[107,53], memory[108,53], memory[109,53], memory[110,53], memory[111,53], memory[112,53], memory[113,53], memory[114,53], memory[115,53], memory[116,53], memory[117,53], memory[118,53], memory[119,53], memory[120,53], memory[1,54], memory[2,54], memory[3,54], memory[4,54], memory[5,54], memory[6,54], memory[7,54], memory[8,54], memory[9,54], memory[10,54], memory[11,54], memory[12,54], memory[13,54], memory[14,54], memory[15,54], memory[16,54], memory[17,54], memory[18,54], memory[19,54], memory[20,54], memory[21,54], memory[22,54], memory[23,54], memory[24,54], memory[25,54], memory[26,54], memory[27,54], memory[28,54], memory[29,54], memory[30,54], memory[31,54], memory[32,54], memory[33,54], memory[34,54], memory[35,54], memory[36,54], memory[37,54], memory[38,54], memory[39,54], memory[40,54], memory[41,54], memory[42,54], memory[43,54], memory[44,54], memory[45,54], memory[46,54], memory[47,54], memory[48,54], memory[49,54], memory[50,54], memory[51,54], memory[52,54], memory[53,54], memory[54,54], memory[55,54], memory[56,54], memory[57,54], memory[58,54], memory[59,54], memory[60,54], memory[61,54], memory[62,54], memory[63,54], memory[64,54], memory[65,54], memory[66,54], memory[67,54], memory[68,54], memory[69,54], memory[70,54], memory[71,54], memory[72,54], memory[73,54], memory[74,54], memory[75,54], memory[76,54], memory[77,54], memory[78,54], memory[79,54], memory[80,54], memory[81,54], memory[82,54], memory[83,54], memory[84,54], memory[85,54], memory[86,54], memory[87,54], memory[88,54], memory[89,54], memory[90,54], memory[91,54], memory[92,54], memory[93,54], memory[94,54], memory[95,54], memory[96,54], memory[97,54], memory[98,54], memory[99,54], memory[100,54], memory[101,54], memory[102,54], memory[103,54], memory[104,54], memory[105,54], memory[106,54], memory[107,54], memory[108,54], memory[109,54], memory[110,54], memory[111,54], memory[112,54], memory[113,54], memory[114,54], memory[115,54], memory[116,54], memory[117,54], memory[118,54], memory[119,54], memory[120,54], memory[1,55], memory[2,55], memory[3,55], memory[4,55], memory[5,55], memory[6,55], memory[7,55], memory[8,55], memory[9,55], memory[10,55], memory[11,55], memory[12,55], memory[13,55], memory[14,55], memory[15,55], memory[16,55], memory[17,55], memory[18,55], memory[19,55], memory[20,55], memory[21,55], memory[22,55], memory[23,55], memory[24,55], memory[25,55], memory[26,55], memory[27,55], memory[28,55], memory[29,55], memory[30,55], memory[31,55], memory[32,55], memory[33,55], memory[34,55], memory[35,55], memory[36,55], memory[37,55], memory[38,55], memory[39,55], memory[40,55], memory[41,55], memory[42,55], memory[43,55], memory[44,55], memory[45,55], memory[46,55], memory[47,55], memory[48,55], memory[49,55], memory[50,55], memory[51,55], memory[52,55], memory[53,55], memory[54,55], memory[55,55], memory[56,55], memory[57,55], memory[58,55], memory[59,55], memory[60,55], memory[61,55], memory[62,55], memory[63,55], memory[64,55], memory[65,55], memory[66,55], memory[67,55], memory[68,55], memory[69,55], memory[70,55], memory[71,55], memory[72,55], memory[73,55], memory[74,55], memory[75,55], memory[76,55], memory[77,55], memory[78,55], memory[79,55], memory[80,55], memory[81,55], memory[82,55], memory[83,55], memory[84,55], memory[85,55], memory[86,55], memory[87,55], memory[88,55], memory[89,55], memory[90,55], memory[91,55], memory[92,55], memory[93,55], memory[94,55], memory[95,55], memory[96,55], memory[97,55], memory[98,55], memory[99,55], memory[100,55], memory[101,55], memory[102,55], memory[103,55], memory[104,55], memory[105,55], memory[106,55], memory[107,55], memory[108,55], memory[109,55], memory[110,55], memory[111,55], memory[112,55], memory[113,55], memory[114,55], memory[115,55], memory[116,55], memory[117,55], memory[118,55], memory[119,55], memory[120,55], memory[1,56], memory[2,56], memory[3,56], memory[4,56], memory[5,56], memory[6,56], memory[7,56], memory[8,56], memory[9,56], memory[10,56], memory[11,56], memory[12,56], memory[13,56], memory[14,56], memory[15,56], memory[16,56], memory[17,56], memory[18,56], memory[19,56], memory[20,56], memory[21,56], memory[22,56], memory[23,56], memory[24,56], memory[25,56], memory[26,56], memory[27,56], memory[28,56], memory[29,56], memory[30,56], memory[31,56], memory[32,56], memory[33,56], memory[34,56], memory[35,56], memory[36,56], memory[37,56], memory[38,56], memory[39,56], memory[40,56], memory[41,56], memory[42,56], memory[43,56], memory[44,56], memory[45,56], memory[46,56], memory[47,56], memory[48,56], memory[49,56], memory[50,56], memory[51,56], memory[52,56], memory[53,56], memory[54,56], memory[55,56], memory[56,56], memory[57,56], memory[58,56], memory[59,56], memory[60,56], memory[61,56], memory[62,56], memory[63,56], memory[64,56], memory[65,56], memory[66,56], memory[67,56], memory[68,56], memory[69,56], memory[70,56], memory[71,56], memory[72,56], memory[73,56], memory[74,56], memory[75,56], memory[76,56], memory[77,56], memory[78,56], memory[79,56], memory[80,56], memory[81,56], memory[82,56], memory[83,56], memory[84,56], memory[85,56], memory[86,56], memory[87,56], memory[88,56], memory[89,56], memory[90,56], memory[91,56], memory[92,56], memory[93,56], memory[94,56], memory[95,56], memory[96,56], memory[97,56], memory[98,56], memory[99,56], memory[100,56], memory[101,56], memory[102,56], memory[103,56], memory[104,56], memory[105,56], memory[106,56], memory[107,56], memory[108,56], memory[109,56], memory[110,56], memory[111,56], memory[112,56], memory[113,56], memory[114,56], memory[115,56], memory[116,56], memory[117,56], memory[118,56], memory[119,56], memory[120,56], memory[1,57], memory[2,57], memory[3,57], memory[4,57], memory[5,57], memory[6,57], memory[7,57], memory[8,57], memory[9,57], memory[10,57], memory[11,57], memory[12,57], memory[13,57], memory[14,57], memory[15,57], memory[16,57], memory[17,57], memory[18,57], memory[19,57], memory[20,57], memory[21,57], memory[22,57], memory[23,57], memory[24,57], memory[25,57], memory[26,57], memory[27,57], memory[28,57], memory[29,57], memory[30,57], memory[31,57], memory[32,57], memory[33,57], memory[34,57], memory[35,57], memory[36,57], memory[37,57], memory[38,57], memory[39,57], memory[40,57], memory[41,57], memory[42,57], memory[43,57], memory[44,57], memory[45,57], memory[46,57], memory[47,57], memory[48,57], memory[49,57], memory[50,57], memory[51,57], memory[52,57], memory[53,57], memory[54,57], memory[55,57], memory[56,57], memory[57,57], memory[58,57], memory[59,57], memory[60,57], memory[61,57], memory[62,57], memory[63,57], memory[64,57], memory[65,57], memory[66,57], memory[67,57], memory[68,57], memory[69,57], memory[70,57], memory[71,57], memory[72,57], memory[73,57], memory[74,57], memory[75,57], memory[76,57], memory[77,57], memory[78,57], memory[79,57], memory[80,57], memory[81,57], memory[82,57], memory[83,57], memory[84,57], memory[85,57], memory[86,57], memory[87,57], memory[88,57], memory[89,57], memory[90,57], memory[91,57], memory[92,57], memory[93,57], memory[94,57], memory[95,57], memory[96,57], memory[97,57], memory[98,57], memory[99,57], memory[100,57], memory[101,57], memory[102,57], memory[103,57], memory[104,57], memory[105,57], memory[106,57], memory[107,57], memory[108,57], memory[109,57], memory[110,57], memory[111,57], memory[112,57], memory[113,57], memory[114,57], memory[115,57], memory[116,57], memory[117,57], memory[118,57], memory[119,57], memory[120,57], memory[1,58], memory[2,58], memory[3,58], memory[4,58], memory[5,58], memory[6,58], memory[7,58], memory[8,58], memory[9,58], memory[10,58], memory[11,58], memory[12,58], memory[13,58], memory[14,58], memory[15,58], memory[16,58], memory[17,58], memory[18,58], memory[19,58], memory[20,58], memory[21,58], memory[22,58], memory[23,58], memory[24,58], memory[25,58], memory[26,58], memory[27,58], memory[28,58], memory[29,58], memory[30,58], memory[31,58], memory[32,58], memory[33,58], memory[34,58], memory[35,58], memory[36,58], memory[37,58], memory[38,58], memory[39,58], memory[40,58], memory[41,58], memory[42,58], memory[43,58], memory[44,58], memory[45,58], memory[46,58], memory[47,58], memory[48,58], memory[49,58], memory[50,58], memory[51,58], memory[52,58], memory[53,58], memory[54,58], memory[55,58], memory[56,58], memory[57,58], memory[58,58], memory[59,58], memory[60,58], memory[61,58], memory[62,58], memory[63,58], memory[64,58], memory[65,58], memory[66,58], memory[67,58], memory[68,58], memory[69,58], memory[70,58], memory[71,58], memory[72,58], memory[73,58], memory[74,58], memory[75,58], memory[76,58], memory[77,58], memory[78,58], memory[79,58], memory[80,58], memory[81,58], memory[82,58], memory[83,58], memory[84,58], memory[85,58], memory[86,58], memory[87,58], memory[88,58], memory[89,58], memory[90,58], memory[91,58], memory[92,58], memory[93,58], memory[94,58], memory[95,58], memory[96,58], memory[97,58], memory[98,58], memory[99,58], memory[100,58], memory[101,58], memory[102,58], memory[103,58], memory[104,58], memory[105,58], memory[106,58], memory[107,58], memory[108,58], memory[109,58], memory[110,58], memory[111,58], memory[112,58], memory[113,58], memory[114,58], memory[115,58], memory[116,58], memory[117,58], memory[118,58], memory[119,58], memory[120,58], memory[1,59], memory[2,59], memory[3,59], memory[4,59], memory[5,59], memory[6,59], memory[7,59], memory[8,59], memory[9,59], memory[10,59], memory[11,59], memory[12,59], memory[13,59], memory[14,59], memory[15,59], memory[16,59], memory[17,59], memory[18,59], memory[19,59], memory[20,59], memory[21,59], memory[22,59], memory[23,59], memory[24,59], memory[25,59], memory[26,59], memory[27,59], memory[28,59], memory[29,59], memory[30,59], memory[31,59], memory[32,59], memory[33,59], memory[34,59], memory[35,59], memory[36,59], memory[37,59], memory[38,59], memory[39,59], memory[40,59], memory[41,59], memory[42,59], memory[43,59], memory[44,59], memory[45,59], memory[46,59], memory[47,59], memory[48,59], memory[49,59], memory[50,59], memory[51,59], memory[52,59], memory[53,59], memory[54,59], memory[55,59], memory[56,59], memory[57,59], memory[58,59], memory[59,59], memory[60,59], memory[61,59], memory[62,59], memory[63,59], memory[64,59], memory[65,59], memory[66,59], memory[67,59], memory[68,59], memory[69,59], memory[70,59], memory[71,59], memory[72,59], memory[73,59], memory[74,59], memory[75,59], memory[76,59], memory[77,59], memory[78,59], memory[79,59], memory[80,59], memory[81,59], memory[82,59], memory[83,59], memory[84,59], memory[85,59], memory[86,59], memory[87,59], memory[88,59], memory[89,59], memory[90,59], memory[91,59], memory[92,59], memory[93,59], memory[94,59], memory[95,59], memory[96,59], memory[97,59], memory[98,59], memory[99,59], memory[100,59], memory[101,59], memory[102,59], memory[103,59], memory[104,59], memory[105,59], memory[106,59], memory[107,59], memory[108,59], memory[109,59], memory[110,59], memory[111,59], memory[112,59], memory[113,59], memory[114,59], memory[115,59], memory[116,59], memory[117,59], memory[118,59], memory[119,59], memory[120,59], memory[1,60], memory[2,60], memory[3,60], memory[4,60], memory[5,60], memory[6,60], memory[7,60], memory[8,60], memory[9,60], memory[10,60], memory[11,60], memory[12,60], memory[13,60], memory[14,60], memory[15,60], memory[16,60], memory[17,60], memory[18,60], memory[19,60], memory[20,60], memory[21,60], memory[22,60], memory[23,60], memory[24,60], memory[25,60], memory[26,60], memory[27,60], memory[28,60], memory[29,60], memory[30,60], memory[31,60], memory[32,60], memory[33,60], memory[34,60], memory[35,60], memory[36,60], memory[37,60], memory[38,60], memory[39,60], memory[40,60], memory[41,60], memory[42,60], memory[43,60], memory[44,60], memory[45,60], memory[46,60], memory[47,60], memory[48,60], memory[49,60], memory[50,60], memory[51,60], memory[52,60], memory[53,60], memory[54,60], memory[55,60], memory[56,60], memory[57,60], memory[58,60], memory[59,60], memory[60,60], memory[61,60], memory[62,60], memory[63,60], memory[64,60], memory[65,60], memory[66,60], memory[67,60], memory[68,60], memory[69,60], memory[70,60], memory[71,60], memory[72,60], memory[73,60], memory[74,60], memory[75,60], memory[76,60], memory[77,60], memory[78,60], memory[79,60], memory[80,60], memory[81,60], memory[82,60], memory[83,60], memory[84,60], memory[85,60], memory[86,60], memory[87,60], memory[88,60], memory[89,60], memory[90,60], memory[91,60], memory[92,60], memory[93,60], memory[94,60], memory[95,60], memory[96,60], memory[97,60], memory[98,60], memory[99,60], memory[100,60], memory[101,60], memory[102,60], memory[103,60], memory[104,60], memory[105,60], memory[106,60], memory[107,60], memory[108,60], memory[109,60], memory[110,60], memory[111,60], memory[112,60], memory[113,60], memory[114,60], memory[115,60], memory[116,60], memory[117,60], memory[118,60], memory[119,60], memory[120,60], memory[1,61], memory[2,61], memory[3,61], memory[4,61], memory[5,61], memory[6,61], memory[7,61], memory[8,61], memory[9,61], memory[10,61], memory[11,61], memory[12,61], memory[13,61], memory[14,61], memory[15,61], memory[16,61], memory[17,61], memory[18,61], memory[19,61], memory[20,61], memory[21,61], memory[22,61], memory[23,61], memory[24,61], memory[25,61], memory[26,61], memory[27,61], memory[28,61], memory[29,61], memory[30,61], memory[31,61], memory[32,61], memory[33,61], memory[34,61], memory[35,61], memory[36,61], memory[37,61], memory[38,61], memory[39,61], memory[40,61], memory[41,61], memory[42,61], memory[43,61], memory[44,61], memory[45,61], memory[46,61], memory[47,61], memory[48,61], memory[49,61], memory[50,61], memory[51,61], memory[52,61], memory[53,61], memory[54,61], memory[55,61], memory[56,61], memory[57,61], memory[58,61], memory[59,61], memory[60,61], memory[61,61], memory[62,61], memory[63,61], memory[64,61], memory[65,61], memory[66,61], memory[67,61], memory[68,61], memory[69,61], memory[70,61], memory[71,61], memory[72,61], memory[73,61], memory[74,61], memory[75,61], memory[76,61], memory[77,61], memory[78,61], memory[79,61], memory[80,61], memory[81,61], memory[82,61], memory[83,61], memory[84,61], memory[85,61], memory[86,61], memory[87,61], memory[88,61], memory[89,61], memory[90,61], memory[91,61], memory[92,61], memory[93,61], memory[94,61], memory[95,61], memory[96,61], memory[97,61], memory[98,61], memory[99,61], memory[100,61], memory[101,61], memory[102,61], memory[103,61], memory[104,61], memory[105,61], memory[106,61], memory[107,61], memory[108,61], memory[109,61], memory[110,61], memory[111,61], memory[112,61], memory[113,61], memory[114,61], memory[115,61], memory[116,61], memory[117,61], memory[118,61], memory[119,61], memory[120,61], memory[1,62], memory[2,62], memory[3,62], memory[4,62], memory[5,62], memory[6,62], memory[7,62], memory[8,62], memory[9,62], memory[10,62], memory[11,62], memory[12,62], memory[13,62], memory[14,62], memory[15,62], memory[16,62], memory[17,62], memory[18,62], memory[19,62], memory[20,62], memory[21,62], memory[22,62], memory[23,62], memory[24,62], memory[25,62], memory[26,62], memory[27,62], memory[28,62], memory[29,62], memory[30,62], memory[31,62], memory[32,62], memory[33,62], memory[34,62], memory[35,62], memory[36,62], memory[37,62], memory[38,62], memory[39,62], memory[40,62], memory[41,62], memory[42,62], memory[43,62], memory[44,62], memory[45,62], memory[46,62], memory[47,62], memory[48,62], memory[49,62], memory[50,62], memory[51,62], memory[52,62], memory[53,62], memory[54,62], memory[55,62], memory[56,62], memory[57,62], memory[58,62], memory[59,62], memory[60,62], memory[61,62], memory[62,62], memory[63,62], memory[64,62], memory[65,62], memory[66,62], memory[67,62], memory[68,62], memory[69,62], memory[70,62], memory[71,62], memory[72,62], memory[73,62], memory[74,62], memory[75,62], memory[76,62], memory[77,62], memory[78,62], memory[79,62], memory[80,62], memory[81,62], memory[82,62], memory[83,62], memory[84,62], memory[85,62], memory[86,62], memory[87,62], memory[88,62], memory[89,62], memory[90,62], memory[91,62], memory[92,62], memory[93,62], memory[94,62], memory[95,62], memory[96,62], memory[97,62], memory[98,62], memory[99,62], memory[100,62], memory[101,62], memory[102,62], memory[103,62], memory[104,62], memory[105,62], memory[106,62], memory[107,62], memory[108,62], memory[109,62], memory[110,62], memory[111,62], memory[112,62], memory[113,62], memory[114,62], memory[115,62], memory[116,62], memory[117,62], memory[118,62], memory[119,62], memory[120,62], memory[1,63], memory[2,63], memory[3,63], memory[4,63], memory[5,63], memory[6,63], memory[7,63], memory[8,63], memory[9,63], memory[10,63], memory[11,63], memory[12,63], memory[13,63], memory[14,63], memory[15,63], memory[16,63], memory[17,63], memory[18,63], memory[19,63], memory[20,63], memory[21,63], memory[22,63], memory[23,63], memory[24,63], memory[25,63], memory[26,63], memory[27,63], memory[28,63], memory[29,63], memory[30,63], memory[31,63], memory[32,63], memory[33,63], memory[34,63], memory[35,63], memory[36,63], memory[37,63], memory[38,63], memory[39,63], memory[40,63], memory[41,63], memory[42,63], memory[43,63], memory[44,63], memory[45,63], memory[46,63], memory[47,63], memory[48,63], memory[49,63], memory[50,63], memory[51,63], memory[52,63], memory[53,63], memory[54,63], memory[55,63], memory[56,63], memory[57,63], memory[58,63], memory[59,63], memory[60,63], memory[61,63], memory[62,63], memory[63,63], memory[64,63], memory[65,63], memory[66,63], memory[67,63], memory[68,63], memory[69,63], memory[70,63], memory[71,63], memory[72,63], memory[73,63], memory[74,63], memory[75,63], memory[76,63], memory[77,63], memory[78,63], memory[79,63], memory[80,63], memory[81,63], memory[82,63], memory[83,63], memory[84,63], memory[85,63], memory[86,63], memory[87,63], memory[88,63], memory[89,63], memory[90,63], memory[91,63], memory[92,63], memory[93,63], memory[94,63], memory[95,63], memory[96,63], memory[97,63], memory[98,63], memory[99,63], memory[100,63], memory[101,63], memory[102,63], memory[103,63], memory[104,63], memory[105,63], memory[106,63], memory[107,63], memory[108,63], memory[109,63], memory[110,63], memory[111,63], memory[112,63], memory[113,63], memory[114,63], memory[115,63], memory[116,63], memory[117,63], memory[118,63], memory[119,63], memory[120,63], memory[1,64], memory[2,64], memory[3,64], memory[4,64], memory[5,64], memory[6,64], memory[7,64], memory[8,64], memory[9,64], memory[10,64], memory[11,64], memory[12,64], memory[13,64], memory[14,64], memory[15,64], memory[16,64], memory[17,64], memory[18,64], memory[19,64], memory[20,64], memory[21,64], memory[22,64], memory[23,64], memory[24,64], memory[25,64], memory[26,64], memory[27,64], memory[28,64], memory[29,64], memory[30,64], memory[31,64], memory[32,64], memory[33,64], memory[34,64], memory[35,64], memory[36,64], memory[37,64], memory[38,64], memory[39,64], memory[40,64], memory[41,64], memory[42,64], memory[43,64], memory[44,64], memory[45,64], memory[46,64], memory[47,64], memory[48,64], memory[49,64], memory[50,64], memory[51,64], memory[52,64], memory[53,64], memory[54,64], memory[55,64], memory[56,64], memory[57,64], memory[58,64], memory[59,64], memory[60,64], memory[61,64], memory[62,64], memory[63,64], memory[64,64], memory[65,64], memory[66,64], memory[67,64], memory[68,64], memory[69,64], memory[70,64], memory[71,64], memory[72,64], memory[73,64], memory[74,64], memory[75,64], memory[76,64], memory[77,64], memory[78,64], memory[79,64], memory[80,64], memory[81,64], memory[82,64], memory[83,64], memory[84,64], memory[85,64], memory[86,64], memory[87,64], memory[88,64], memory[89,64], memory[90,64], memory[91,64], memory[92,64], memory[93,64], memory[94,64], memory[95,64], memory[96,64], memory[97,64], memory[98,64], memory[99,64], memory[100,64], memory[101,64], memory[102,64], memory[103,64], memory[104,64], memory[105,64], memory[106,64], memory[107,64], memory[108,64], memory[109,64], memory[110,64], memory[111,64], memory[112,64], memory[113,64], memory[114,64], memory[115,64], memory[116,64], memory[117,64], memory[118,64], memory[119,64], memory[120,64], memory[1,65], memory[2,65], memory[3,65], memory[4,65], memory[5,65], memory[6,65], memory[7,65], memory[8,65], memory[9,65], memory[10,65], memory[11,65], memory[12,65], memory[13,65], memory[14,65], memory[15,65], memory[16,65], memory[17,65], memory[18,65], memory[19,65], memory[20,65], memory[21,65], memory[22,65], memory[23,65], memory[24,65], memory[25,65], memory[26,65], memory[27,65], memory[28,65], memory[29,65], memory[30,65], memory[31,65], memory[32,65], memory[33,65], memory[34,65], memory[35,65], memory[36,65], memory[37,65], memory[38,65], memory[39,65], memory[40,65], memory[41,65], memory[42,65], memory[43,65], memory[44,65], memory[45,65], memory[46,65], memory[47,65], memory[48,65], memory[49,65], memory[50,65], memory[51,65], memory[52,65], memory[53,65], memory[54,65], memory[55,65], memory[56,65], memory[57,65], memory[58,65], memory[59,65], memory[60,65], memory[61,65], memory[62,65], memory[63,65], memory[64,65], memory[65,65], memory[66,65], memory[67,65], memory[68,65], memory[69,65], memory[70,65], memory[71,65], memory[72,65], memory[73,65], memory[74,65], memory[75,65], memory[76,65], memory[77,65], memory[78,65], memory[79,65], memory[80,65], memory[81,65], memory[82,65], memory[83,65], memory[84,65], memory[85,65], memory[86,65], memory[87,65], memory[88,65], memory[89,65], memory[90,65], memory[91,65], memory[92,65], memory[93,65], memory[94,65], memory[95,65], memory[96,65], memory[97,65], memory[98,65], memory[99,65], memory[100,65], memory[101,65], memory[102,65], memory[103,65], memory[104,65], memory[105,65], memory[106,65], memory[107,65], memory[108,65], memory[109,65], memory[110,65], memory[111,65], memory[112,65], memory[113,65], memory[114,65], memory[115,65], memory[116,65], memory[117,65], memory[118,65], memory[119,65], memory[120,65], memory[1,66], memory[2,66], memory[3,66], memory[4,66], memory[5,66], memory[6,66], memory[7,66], memory[8,66], memory[9,66], memory[10,66], memory[11,66], memory[12,66], memory[13,66], memory[14,66], memory[15,66], memory[16,66], memory[17,66], memory[18,66], memory[19,66], memory[20,66], memory[21,66], memory[22,66], memory[23,66], memory[24,66], memory[25,66], memory[26,66], memory[27,66], memory[28,66], memory[29,66], memory[30,66], memory[31,66], memory[32,66], memory[33,66], memory[34,66], memory[35,66], memory[36,66], memory[37,66], memory[38,66], memory[39,66], memory[40,66], memory[41,66], memory[42,66], memory[43,66], memory[44,66], memory[45,66], memory[46,66], memory[47,66], memory[48,66], memory[49,66], memory[50,66], memory[51,66], memory[52,66], memory[53,66], memory[54,66], memory[55,66], memory[56,66], memory[57,66], memory[58,66], memory[59,66], memory[60,66], memory[61,66], memory[62,66], memory[63,66], memory[64,66], memory[65,66], memory[66,66], memory[67,66], memory[68,66], memory[69,66], memory[70,66], memory[71,66], memory[72,66], memory[73,66], memory[74,66], memory[75,66], memory[76,66], memory[77,66], memory[78,66], memory[79,66], memory[80,66], memory[81,66], memory[82,66], memory[83,66], memory[84,66], memory[85,66], memory[86,66], memory[87,66], memory[88,66], memory[89,66], memory[90,66], memory[91,66], memory[92,66], memory[93,66], memory[94,66], memory[95,66], memory[96,66], memory[97,66], memory[98,66], memory[99,66], memory[100,66], memory[101,66], memory[102,66], memory[103,66], memory[104,66], memory[105,66], memory[106,66], memory[107,66], memory[108,66], memory[109,66], memory[110,66], memory[111,66], memory[112,66], memory[113,66], memory[114,66], memory[115,66], memory[116,66], memory[117,66], memory[118,66], memory[119,66], memory[120,66], memory[1,67], memory[2,67], memory[3,67], memory[4,67], memory[5,67], memory[6,67], memory[7,67], memory[8,67], memory[9,67], memory[10,67], memory[11,67], memory[12,67], memory[13,67], memory[14,67], memory[15,67], memory[16,67], memory[17,67], memory[18,67], memory[19,67], memory[20,67], memory[21,67], memory[22,67], memory[23,67], memory[24,67], memory[25,67], memory[26,67], memory[27,67], memory[28,67], memory[29,67], memory[30,67], memory[31,67], memory[32,67], memory[33,67], memory[34,67], memory[35,67], memory[36,67], memory[37,67], memory[38,67], memory[39,67], memory[40,67], memory[41,67], memory[42,67], memory[43,67], memory[44,67], memory[45,67], memory[46,67], memory[47,67], memory[48,67], memory[49,67], memory[50,67], memory[51,67], memory[52,67], memory[53,67], memory[54,67], memory[55,67], memory[56,67], memory[57,67], memory[58,67], memory[59,67], memory[60,67], memory[61,67], memory[62,67], memory[63,67], memory[64,67], memory[65,67], memory[66,67], memory[67,67], memory[68,67], memory[69,67], memory[70,67], memory[71,67], memory[72,67], memory[73,67], memory[74,67], memory[75,67], memory[76,67], memory[77,67], memory[78,67], memory[79,67], memory[80,67], memory[81,67], memory[82,67], memory[83,67], memory[84,67], memory[85,67], memory[86,67], memory[87,67], memory[88,67], memory[89,67], memory[90,67], memory[91,67], memory[92,67], memory[93,67], memory[94,67], memory[95,67], memory[96,67], memory[97,67], memory[98,67], memory[99,67], memory[100,67], memory[101,67], memory[102,67], memory[103,67], memory[104,67], memory[105,67], memory[106,67], memory[107,67], memory[108,67], memory[109,67], memory[110,67], memory[111,67], memory[112,67], memory[113,67], memory[114,67], memory[115,67], memory[116,67], memory[117,67], memory[118,67], memory[119,67], memory[120,67], memory[1,68], memory[2,68], memory[3,68], memory[4,68], memory[5,68], memory[6,68], memory[7,68], memory[8,68], memory[9,68], memory[10,68], memory[11,68], memory[12,68], memory[13,68], memory[14,68], memory[15,68], memory[16,68], memory[17,68], memory[18,68], memory[19,68], memory[20,68], memory[21,68], memory[22,68], memory[23,68], memory[24,68], memory[25,68], memory[26,68], memory[27,68], memory[28,68], memory[29,68], memory[30,68], memory[31,68], memory[32,68], memory[33,68], memory[34,68], memory[35,68], memory[36,68], memory[37,68], memory[38,68], memory[39,68], memory[40,68], memory[41,68], memory[42,68], memory[43,68], memory[44,68], memory[45,68], memory[46,68], memory[47,68], memory[48,68], memory[49,68], memory[50,68], memory[51,68], memory[52,68], memory[53,68], memory[54,68], memory[55,68], memory[56,68], memory[57,68], memory[58,68], memory[59,68], memory[60,68], memory[61,68], memory[62,68], memory[63,68], memory[64,68], memory[65,68], memory[66,68], memory[67,68], memory[68,68], memory[69,68], memory[70,68], memory[71,68], memory[72,68], memory[73,68], memory[74,68], memory[75,68], memory[76,68], memory[77,68], memory[78,68], memory[79,68], memory[80,68], memory[81,68], memory[82,68], memory[83,68], memory[84,68], memory[85,68], memory[86,68], memory[87,68], memory[88,68], memory[89,68], memory[90,68], memory[91,68], memory[92,68], memory[93,68], memory[94,68], memory[95,68], memory[96,68], memory[97,68], memory[98,68], memory[99,68], memory[100,68], memory[101,68], memory[102,68], memory[103,68], memory[104,68], memory[105,68], memory[106,68], memory[107,68], memory[108,68], memory[109,68], memory[110,68], memory[111,68], memory[112,68], memory[113,68], memory[114,68], memory[115,68], memory[116,68], memory[117,68], memory[118,68], memory[119,68], memory[120,68], memory[1,69], memory[2,69], memory[3,69], memory[4,69], memory[5,69], memory[6,69], memory[7,69], memory[8,69], memory[9,69], memory[10,69], memory[11,69], memory[12,69], memory[13,69], memory[14,69], memory[15,69], memory[16,69], memory[17,69], memory[18,69], memory[19,69], memory[20,69], memory[21,69], memory[22,69], memory[23,69], memory[24,69], memory[25,69], memory[26,69], memory[27,69], memory[28,69], memory[29,69], memory[30,69], memory[31,69], memory[32,69], memory[33,69], memory[34,69], memory[35,69], memory[36,69], memory[37,69], memory[38,69], memory[39,69], memory[40,69], memory[41,69], memory[42,69], memory[43,69], memory[44,69], memory[45,69], memory[46,69], memory[47,69], memory[48,69], memory[49,69], memory[50,69], memory[51,69], memory[52,69], memory[53,69], memory[54,69], memory[55,69], memory[56,69], memory[57,69], memory[58,69], memory[59,69], memory[60,69], memory[61,69], memory[62,69], memory[63,69], memory[64,69], memory[65,69], memory[66,69], memory[67,69], memory[68,69], memory[69,69], memory[70,69], memory[71,69], memory[72,69], memory[73,69], memory[74,69], memory[75,69], memory[76,69], memory[77,69], memory[78,69], memory[79,69], memory[80,69], memory[81,69], memory[82,69], memory[83,69], memory[84,69], memory[85,69], memory[86,69], memory[87,69], memory[88,69], memory[89,69], memory[90,69], memory[91,69], memory[92,69], memory[93,69], memory[94,69], memory[95,69], memory[96,69], memory[97,69], memory[98,69], memory[99,69], memory[100,69], memory[101,69], memory[102,69], memory[103,69], memory[104,69], memory[105,69], memory[106,69], memory[107,69], memory[108,69], memory[109,69], memory[110,69], memory[111,69], memory[112,69], memory[113,69], memory[114,69], memory[115,69], memory[116,69], memory[117,69], memory[118,69], memory[119,69], memory[120,69], memory[1,70], memory[2,70], memory[3,70], memory[4,70], memory[5,70], memory[6,70], memory[7,70], memory[8,70], memory[9,70], memory[10,70], memory[11,70], memory[12,70], memory[13,70], memory[14,70], memory[15,70], memory[16,70], memory[17,70], memory[18,70], memory[19,70], memory[20,70], memory[21,70], memory[22,70], memory[23,70], memory[24,70], memory[25,70], memory[26,70], memory[27,70], memory[28,70], memory[29,70], memory[30,70], memory[31,70], memory[32,70], memory[33,70], memory[34,70], memory[35,70], memory[36,70], memory[37,70], memory[38,70], memory[39,70], memory[40,70], memory[41,70], memory[42,70], memory[43,70], memory[44,70], memory[45,70], memory[46,70], memory[47,70], memory[48,70], memory[49,70], memory[50,70], memory[51,70], memory[52,70], memory[53,70], memory[54,70], memory[55,70], memory[56,70], memory[57,70], memory[58,70], memory[59,70], memory[60,70], memory[61,70], memory[62,70], memory[63,70], memory[64,70], memory[65,70], memory[66,70], memory[67,70], memory[68,70], memory[69,70], memory[70,70], memory[71,70], memory[72,70], memory[73,70], memory[74,70], memory[75,70], memory[76,70], memory[77,70], memory[78,70], memory[79,70], memory[80,70], memory[81,70], memory[82,70], memory[83,70], memory[84,70], memory[85,70], memory[86,70], memory[87,70], memory[88,70], memory[89,70], memory[90,70], memory[91,70], memory[92,70], memory[93,70], memory[94,70], memory[95,70], memory[96,70], memory[97,70], memory[98,70], memory[99,70], memory[100,70], memory[101,70], memory[102,70], memory[103,70], memory[104,70], memory[105,70], memory[106,70], memory[107,70], memory[108,70], memory[109,70], memory[110,70], memory[111,70], memory[112,70], memory[113,70], memory[114,70], memory[115,70], memory[116,70], memory[117,70], memory[118,70], memory[119,70], memory[120,70], memory[1,71], memory[2,71], memory[3,71], memory[4,71], memory[5,71], memory[6,71], memory[7,71], memory[8,71], memory[9,71], memory[10,71], memory[11,71], memory[12,71], memory[13,71], memory[14,71], memory[15,71], memory[16,71], memory[17,71], memory[18,71], memory[19,71], memory[20,71], memory[21,71], memory[22,71], memory[23,71], memory[24,71], memory[25,71], memory[26,71], memory[27,71], memory[28,71], memory[29,71], memory[30,71], memory[31,71], memory[32,71], memory[33,71], memory[34,71], memory[35,71], memory[36,71], memory[37,71], memory[38,71], memory[39,71], memory[40,71], memory[41,71], memory[42,71], memory[43,71], memory[44,71], memory[45,71], memory[46,71], memory[47,71], memory[48,71], memory[49,71], memory[50,71], memory[51,71], memory[52,71], memory[53,71], memory[54,71], memory[55,71], memory[56,71], memory[57,71], memory[58,71], memory[59,71], memory[60,71], memory[61,71], memory[62,71], memory[63,71], memory[64,71], memory[65,71], memory[66,71], memory[67,71], memory[68,71], memory[69,71], memory[70,71], memory[71,71], memory[72,71], memory[73,71], memory[74,71], memory[75,71], memory[76,71], memory[77,71], memory[78,71], memory[79,71], memory[80,71], memory[81,71], memory[82,71], memory[83,71], memory[84,71], memory[85,71], memory[86,71], memory[87,71], memory[88,71], memory[89,71], memory[90,71], memory[91,71], memory[92,71], memory[93,71], memory[94,71], memory[95,71], memory[96,71], memory[97,71], memory[98,71], memory[99,71], memory[100,71], memory[101,71], memory[102,71], memory[103,71], memory[104,71], memory[105,71], memory[106,71], memory[107,71], memory[108,71], memory[109,71], memory[110,71], memory[111,71], memory[112,71], memory[113,71], memory[114,71], memory[115,71], memory[116,71], memory[117,71], memory[118,71], memory[119,71], memory[120,71], memory[1,72], memory[2,72], memory[3,72], memory[4,72], memory[5,72], memory[6,72], memory[7,72], memory[8,72], memory[9,72], memory[10,72], memory[11,72], memory[12,72], memory[13,72], memory[14,72], memory[15,72], memory[16,72], memory[17,72], memory[18,72], memory[19,72], memory[20,72], memory[21,72], memory[22,72], memory[23,72], memory[24,72], memory[25,72], memory[26,72], memory[27,72], memory[28,72], memory[29,72], memory[30,72], memory[31,72], memory[32,72], memory[33,72], memory[34,72], memory[35,72], memory[36,72], memory[37,72], memory[38,72], memory[39,72], memory[40,72], memory[41,72], memory[42,72], memory[43,72], memory[44,72], memory[45,72], memory[46,72], memory[47,72], memory[48,72], memory[49,72], memory[50,72], memory[51,72], memory[52,72], memory[53,72], memory[54,72], memory[55,72], memory[56,72], memory[57,72], memory[58,72], memory[59,72], memory[60,72], memory[61,72], memory[62,72], memory[63,72], memory[64,72], memory[65,72], memory[66,72], memory[67,72], memory[68,72], memory[69,72], memory[70,72], memory[71,72], memory[72,72], memory[73,72], memory[74,72], memory[75,72], memory[76,72], memory[77,72], memory[78,72], memory[79,72], memory[80,72], memory[81,72], memory[82,72], memory[83,72], memory[84,72], memory[85,72], memory[86,72], memory[87,72], memory[88,72], memory[89,72], memory[90,72], memory[91,72], memory[92,72], memory[93,72], memory[94,72], memory[95,72], memory[96,72], memory[97,72], memory[98,72], memory[99,72], memory[100,72], memory[101,72], memory[102,72], memory[103,72], memory[104,72], memory[105,72], memory[106,72], memory[107,72], memory[108,72], memory[109,72], memory[110,72], memory[111,72], memory[112,72], memory[113,72], memory[114,72], memory[115,72], memory[116,72], memory[117,72], memory[118,72], memory[119,72], memory[120,72], memory[1,73], memory[2,73], memory[3,73], memory[4,73], memory[5,73], memory[6,73], memory[7,73], memory[8,73], memory[9,73], memory[10,73], memory[11,73], memory[12,73], memory[13,73], memory[14,73], memory[15,73], memory[16,73], memory[17,73], memory[18,73], memory[19,73], memory[20,73], memory[21,73], memory[22,73], memory[23,73], memory[24,73], memory[25,73], memory[26,73], memory[27,73], memory[28,73], memory[29,73], memory[30,73], memory[31,73], memory[32,73], memory[33,73], memory[34,73], memory[35,73], memory[36,73], memory[37,73], memory[38,73], memory[39,73], memory[40,73], memory[41,73], memory[42,73], memory[43,73], memory[44,73], memory[45,73], memory[46,73], memory[47,73], memory[48,73], memory[49,73], memory[50,73], memory[51,73], memory[52,73], memory[53,73], memory[54,73], memory[55,73], memory[56,73], memory[57,73], memory[58,73], memory[59,73], memory[60,73], memory[61,73], memory[62,73], memory[63,73], memory[64,73], memory[65,73], memory[66,73], memory[67,73], memory[68,73], memory[69,73], memory[70,73], memory[71,73], memory[72,73], memory[73,73], memory[74,73], memory[75,73], memory[76,73], memory[77,73], memory[78,73], memory[79,73], memory[80,73], memory[81,73], memory[82,73], memory[83,73], memory[84,73], memory[85,73], memory[86,73], memory[87,73], memory[88,73], memory[89,73], memory[90,73], memory[91,73], memory[92,73], memory[93,73], memory[94,73], memory[95,73], memory[96,73], memory[97,73], memory[98,73], memory[99,73], memory[100,73], memory[101,73], memory[102,73], memory[103,73], memory[104,73], memory[105,73], memory[106,73], memory[107,73], memory[108,73], memory[109,73], memory[110,73], memory[111,73], memory[112,73], memory[113,73], memory[114,73], memory[115,73], memory[116,73], memory[117,73], memory[118,73], memory[119,73], memory[120,73], memory[1,74], memory[2,74], memory[3,74], memory[4,74], memory[5,74], memory[6,74], memory[7,74], memory[8,74], memory[9,74], memory[10,74], memory[11,74], memory[12,74], memory[13,74], memory[14,74], memory[15,74], memory[16,74], memory[17,74], memory[18,74], memory[19,74], memory[20,74], memory[21,74], memory[22,74], memory[23,74], memory[24,74], memory[25,74], memory[26,74], memory[27,74], memory[28,74], memory[29,74], memory[30,74], memory[31,74], memory[32,74], memory[33,74], memory[34,74], memory[35,74], memory[36,74], memory[37,74], memory[38,74], memory[39,74], memory[40,74], memory[41,74], memory[42,74], memory[43,74], memory[44,74], memory[45,74], memory[46,74], memory[47,74], memory[48,74], memory[49,74], memory[50,74], memory[51,74], memory[52,74], memory[53,74], memory[54,74], memory[55,74], memory[56,74], memory[57,74], memory[58,74], memory[59,74], memory[60,74], memory[61,74], memory[62,74], memory[63,74], memory[64,74], memory[65,74], memory[66,74], memory[67,74], memory[68,74], memory[69,74], memory[70,74], memory[71,74], memory[72,74], memory[73,74], memory[74,74], memory[75,74], memory[76,74], memory[77,74], memory[78,74], memory[79,74], memory[80,74], memory[81,74], memory[82,74], memory[83,74], memory[84,74], memory[85,74], memory[86,74], memory[87,74], memory[88,74], memory[89,74], memory[90,74], memory[91,74], memory[92,74], memory[93,74], memory[94,74], memory[95,74], memory[96,74], memory[97,74], memory[98,74], memory[99,74], memory[100,74], memory[101,74], memory[102,74], memory[103,74], memory[104,74], memory[105,74], memory[106,74], memory[107,74], memory[108,74], memory[109,74], memory[110,74], memory[111,74], memory[112,74], memory[113,74], memory[114,74], memory[115,74], memory[116,74], memory[117,74], memory[118,74], memory[119,74], memory[120,74], memory[1,75], memory[2,75], memory[3,75], memory[4,75], memory[5,75], memory[6,75], memory[7,75], memory[8,75], memory[9,75], memory[10,75], memory[11,75], memory[12,75], memory[13,75], memory[14,75], memory[15,75], memory[16,75], memory[17,75], memory[18,75], memory[19,75], memory[20,75], memory[21,75], memory[22,75], memory[23,75], memory[24,75], memory[25,75], memory[26,75], memory[27,75], memory[28,75], memory[29,75], memory[30,75], memory[31,75], memory[32,75], memory[33,75], memory[34,75], memory[35,75], memory[36,75], memory[37,75], memory[38,75], memory[39,75], memory[40,75], memory[41,75], memory[42,75], memory[43,75], memory[44,75], memory[45,75], memory[46,75], memory[47,75], memory[48,75], memory[49,75], memory[50,75], memory[51,75], memory[52,75], memory[53,75], memory[54,75], memory[55,75], memory[56,75], memory[57,75], memory[58,75], memory[59,75], memory[60,75], memory[61,75], memory[62,75], memory[63,75], memory[64,75], memory[65,75], memory[66,75], memory[67,75], memory[68,75], memory[69,75], memory[70,75], memory[71,75], memory[72,75], memory[73,75], memory[74,75], memory[75,75], memory[76,75], memory[77,75], memory[78,75], memory[79,75], memory[80,75], memory[81,75], memory[82,75], memory[83,75], memory[84,75], memory[85,75], memory[86,75], memory[87,75], memory[88,75], memory[89,75], memory[90,75], memory[91,75], memory[92,75], memory[93,75], memory[94,75], memory[95,75], memory[96,75], memory[97,75], memory[98,75], memory[99,75], memory[100,75], memory[101,75], memory[102,75], memory[103,75], memory[104,75], memory[105,75], memory[106,75], memory[107,75], memory[108,75], memory[109,75], memory[110,75], memory[111,75], memory[112,75], memory[113,75], memory[114,75], memory[115,75], memory[116,75], memory[117,75], memory[118,75], memory[119,75], memory[120,75], memory[1,76], memory[2,76], memory[3,76], memory[4,76], memory[5,76], memory[6,76], memory[7,76], memory[8,76], memory[9,76], memory[10,76], memory[11,76], memory[12,76], memory[13,76], memory[14,76], memory[15,76], memory[16,76], memory[17,76], memory[18,76], memory[19,76], memory[20,76], memory[21,76], memory[22,76], memory[23,76], memory[24,76], memory[25,76], memory[26,76], memory[27,76], memory[28,76], memory[29,76], memory[30,76], memory[31,76], memory[32,76], memory[33,76], memory[34,76], memory[35,76], memory[36,76], memory[37,76], memory[38,76], memory[39,76], memory[40,76], memory[41,76], memory[42,76], memory[43,76], memory[44,76], memory[45,76], memory[46,76], memory[47,76], memory[48,76], memory[49,76], memory[50,76], memory[51,76], memory[52,76], memory[53,76], memory[54,76], memory[55,76], memory[56,76], memory[57,76], memory[58,76], memory[59,76], memory[60,76], memory[61,76], memory[62,76], memory[63,76], memory[64,76], memory[65,76], memory[66,76], memory[67,76], memory[68,76], memory[69,76], memory[70,76], memory[71,76], memory[72,76], memory[73,76], memory[74,76], memory[75,76], memory[76,76], memory[77,76], memory[78,76], memory[79,76], memory[80,76], memory[81,76], memory[82,76], memory[83,76], memory[84,76], memory[85,76], memory[86,76], memory[87,76], memory[88,76], memory[89,76], memory[90,76], memory[91,76], memory[92,76], memory[93,76], memory[94,76], memory[95,76], memory[96,76], memory[97,76], memory[98,76], memory[99,76], memory[100,76], memory[101,76], memory[102,76], memory[103,76], memory[104,76], memory[105,76], memory[106,76], memory[107,76], memory[108,76], memory[109,76], memory[110,76], memory[111,76], memory[112,76], memory[113,76], memory[114,76], memory[115,76], memory[116,76], memory[117,76], memory[118,76], memory[119,76], memory[120,76], memory[1,77], memory[2,77], memory[3,77], memory[4,77], memory[5,77], memory[6,77], memory[7,77], memory[8,77], memory[9,77], memory[10,77], memory[11,77], memory[12,77], memory[13,77], memory[14,77], memory[15,77], memory[16,77], memory[17,77], memory[18,77], memory[19,77], memory[20,77], memory[21,77], memory[22,77], memory[23,77], memory[24,77], memory[25,77], memory[26,77], memory[27,77], memory[28,77], memory[29,77], memory[30,77], memory[31,77], memory[32,77], memory[33,77], memory[34,77], memory[35,77], memory[36,77], memory[37,77], memory[38,77], memory[39,77], memory[40,77], memory[41,77], memory[42,77], memory[43,77], memory[44,77], memory[45,77], memory[46,77], memory[47,77], memory[48,77], memory[49,77], memory[50,77], memory[51,77], memory[52,77], memory[53,77], memory[54,77], memory[55,77], memory[56,77], memory[57,77], memory[58,77], memory[59,77], memory[60,77], memory[61,77], memory[62,77], memory[63,77], memory[64,77], memory[65,77], memory[66,77], memory[67,77], memory[68,77], memory[69,77], memory[70,77], memory[71,77], memory[72,77], memory[73,77], memory[74,77], memory[75,77], memory[76,77], memory[77,77], memory[78,77], memory[79,77], memory[80,77], memory[81,77], memory[82,77], memory[83,77], memory[84,77], memory[85,77], memory[86,77], memory[87,77], memory[88,77], memory[89,77], memory[90,77], memory[91,77], memory[92,77], memory[93,77], memory[94,77], memory[95,77], memory[96,77], memory[97,77], memory[98,77], memory[99,77], memory[100,77], memory[101,77], memory[102,77], memory[103,77], memory[104,77], memory[105,77], memory[106,77], memory[107,77], memory[108,77], memory[109,77], memory[110,77], memory[111,77], memory[112,77], memory[113,77], memory[114,77], memory[115,77], memory[116,77], memory[117,77], memory[118,77], memory[119,77], memory[120,77], memory[1,78], memory[2,78], memory[3,78], memory[4,78], memory[5,78], memory[6,78], memory[7,78], memory[8,78], memory[9,78], memory[10,78], memory[11,78], memory[12,78], memory[13,78], memory[14,78], memory[15,78], memory[16,78], memory[17,78], memory[18,78], memory[19,78], memory[20,78], memory[21,78], memory[22,78], memory[23,78], memory[24,78], memory[25,78], memory[26,78], memory[27,78], memory[28,78], memory[29,78], memory[30,78], memory[31,78], memory[32,78], memory[33,78], memory[34,78], memory[35,78], memory[36,78], memory[37,78], memory[38,78], memory[39,78], memory[40,78], memory[41,78], memory[42,78], memory[43,78], memory[44,78], memory[45,78], memory[46,78], memory[47,78], memory[48,78], memory[49,78], memory[50,78], memory[51,78], memory[52,78], memory[53,78], memory[54,78], memory[55,78], memory[56,78], memory[57,78], memory[58,78], memory[59,78], memory[60,78], memory[61,78], memory[62,78], memory[63,78], memory[64,78], memory[65,78], memory[66,78], memory[67,78], memory[68,78], memory[69,78], memory[70,78], memory[71,78], memory[72,78], memory[73,78], memory[74,78], memory[75,78], memory[76,78], memory[77,78], memory[78,78], memory[79,78], memory[80,78], memory[81,78], memory[82,78], memory[83,78], memory[84,78], memory[85,78], memory[86,78], memory[87,78], memory[88,78], memory[89,78], memory[90,78], memory[91,78], memory[92,78], memory[93,78], memory[94,78], memory[95,78], memory[96,78], memory[97,78], memory[98,78], memory[99,78], memory[100,78], memory[101,78], memory[102,78], memory[103,78], memory[104,78], memory[105,78], memory[106,78], memory[107,78], memory[108,78], memory[109,78], memory[110,78], memory[111,78], memory[112,78], memory[113,78], memory[114,78], memory[115,78], memory[116,78], memory[117,78], memory[118,78], memory[119,78], memory[120,78], memory[1,79], memory[2,79], memory[3,79], memory[4,79], memory[5,79], memory[6,79], memory[7,79], memory[8,79], memory[9,79], memory[10,79], memory[11,79], memory[12,79], memory[13,79], memory[14,79], memory[15,79], memory[16,79], memory[17,79], memory[18,79], memory[19,79], memory[20,79], memory[21,79], memory[22,79], memory[23,79], memory[24,79], memory[25,79], memory[26,79], memory[27,79], memory[28,79], memory[29,79], memory[30,79], memory[31,79], memory[32,79], memory[33,79], memory[34,79], memory[35,79], memory[36,79], memory[37,79], memory[38,79], memory[39,79], memory[40,79], memory[41,79], memory[42,79], memory[43,79], memory[44,79], memory[45,79], memory[46,79], memory[47,79], memory[48,79], memory[49,79], memory[50,79], memory[51,79], memory[52,79], memory[53,79], memory[54,79], memory[55,79], memory[56,79], memory[57,79], memory[58,79], memory[59,79], memory[60,79], memory[61,79], memory[62,79], memory[63,79], memory[64,79], memory[65,79], memory[66,79], memory[67,79], memory[68,79], memory[69,79], memory[70,79], memory[71,79], memory[72,79], memory[73,79], memory[74,79], memory[75,79], memory[76,79], memory[77,79], memory[78,79], memory[79,79], memory[80,79], memory[81,79], memory[82,79], memory[83,79], memory[84,79], memory[85,79], memory[86,79], memory[87,79], memory[88,79], memory[89,79], memory[90,79], memory[91,79], memory[92,79], memory[93,79], memory[94,79], memory[95,79], memory[96,79], memory[97,79], memory[98,79], memory[99,79], memory[100,79], memory[101,79], memory[102,79], memory[103,79], memory[104,79], memory[105,79], memory[106,79], memory[107,79], memory[108,79], memory[109,79], memory[110,79], memory[111,79], memory[112,79], memory[113,79], memory[114,79], memory[115,79], memory[116,79], memory[117,79], memory[118,79], memory[119,79], memory[120,79], memory[1,80], memory[2,80], memory[3,80], memory[4,80], memory[5,80], memory[6,80], memory[7,80], memory[8,80], memory[9,80], memory[10,80], memory[11,80], memory[12,80], memory[13,80], memory[14,80], memory[15,80], memory[16,80], memory[17,80], memory[18,80], memory[19,80], memory[20,80], memory[21,80], memory[22,80], memory[23,80], memory[24,80], memory[25,80], memory[26,80], memory[27,80], memory[28,80], memory[29,80], memory[30,80], memory[31,80], memory[32,80], memory[33,80], memory[34,80], memory[35,80], memory[36,80], memory[37,80], memory[38,80], memory[39,80], memory[40,80], memory[41,80], memory[42,80], memory[43,80], memory[44,80], memory[45,80], memory[46,80], memory[47,80], memory[48,80], memory[49,80], memory[50,80], memory[51,80], memory[52,80], memory[53,80], memory[54,80], memory[55,80], memory[56,80], memory[57,80], memory[58,80], memory[59,80], memory[60,80], memory[61,80], memory[62,80], memory[63,80], memory[64,80], memory[65,80], memory[66,80], memory[67,80], memory[68,80], memory[69,80], memory[70,80], memory[71,80], memory[72,80], memory[73,80], memory[74,80], memory[75,80], memory[76,80], memory[77,80], memory[78,80], memory[79,80], memory[80,80], memory[81,80], memory[82,80], memory[83,80], memory[84,80], memory[85,80], memory[86,80], memory[87,80], memory[88,80], memory[89,80], memory[90,80], memory[91,80], memory[92,80], memory[93,80], memory[94,80], memory[95,80], memory[96,80], memory[97,80], memory[98,80], memory[99,80], memory[100,80], memory[101,80], memory[102,80], memory[103,80], memory[104,80], memory[105,80], memory[106,80], memory[107,80], memory[108,80], memory[109,80], memory[110,80], memory[111,80], memory[112,80], memory[113,80], memory[114,80], memory[115,80], memory[116,80], memory[117,80], memory[118,80], memory[119,80], memory[120,80], memory[1,81], memory[2,81], memory[3,81], memory[4,81], memory[5,81], memory[6,81], memory[7,81], memory[8,81], memory[9,81], memory[10,81], memory[11,81], memory[12,81], memory[13,81], memory[14,81], memory[15,81], memory[16,81], memory[17,81], memory[18,81], memory[19,81], memory[20,81], memory[21,81], memory[22,81], memory[23,81], memory[24,81], memory[25,81], memory[26,81], memory[27,81], memory[28,81], memory[29,81], memory[30,81], memory[31,81], memory[32,81], memory[33,81], memory[34,81], memory[35,81], memory[36,81], memory[37,81], memory[38,81], memory[39,81], memory[40,81], memory[41,81], memory[42,81], memory[43,81], memory[44,81], memory[45,81], memory[46,81], memory[47,81], memory[48,81], memory[49,81], memory[50,81], memory[51,81], memory[52,81], memory[53,81], memory[54,81], memory[55,81], memory[56,81], memory[57,81], memory[58,81], memory[59,81], memory[60,81], memory[61,81], memory[62,81], memory[63,81], memory[64,81], memory[65,81], memory[66,81], memory[67,81], memory[68,81], memory[69,81], memory[70,81], memory[71,81], memory[72,81], memory[73,81], memory[74,81], memory[75,81], memory[76,81], memory[77,81], memory[78,81], memory[79,81], memory[80,81], memory[81,81], memory[82,81], memory[83,81], memory[84,81], memory[85,81], memory[86,81], memory[87,81], memory[88,81], memory[89,81], memory[90,81], memory[91,81], memory[92,81], memory[93,81], memory[94,81], memory[95,81], memory[96,81], memory[97,81], memory[98,81], memory[99,81], memory[100,81], memory[101,81], memory[102,81], memory[103,81], memory[104,81], memory[105,81], memory[106,81], memory[107,81], memory[108,81], memory[109,81], memory[110,81], memory[111,81], memory[112,81], memory[113,81], memory[114,81], memory[115,81], memory[116,81], memory[117,81], memory[118,81], memory[119,81], memory[120,81], memory[1,82], memory[2,82], memory[3,82], memory[4,82], memory[5,82], memory[6,82], memory[7,82], memory[8,82], memory[9,82], memory[10,82], memory[11,82], memory[12,82], memory[13,82], memory[14,82], memory[15,82], memory[16,82], memory[17,82], memory[18,82], memory[19,82], memory[20,82], memory[21,82], memory[22,82], memory[23,82], memory[24,82], memory[25,82], memory[26,82], memory[27,82], memory[28,82], memory[29,82], memory[30,82], memory[31,82], memory[32,82], memory[33,82], memory[34,82], memory[35,82], memory[36,82], memory[37,82], memory[38,82], memory[39,82], memory[40,82], memory[41,82], memory[42,82], memory[43,82], memory[44,82], memory[45,82], memory[46,82], memory[47,82], memory[48,82], memory[49,82], memory[50,82], memory[51,82], memory[52,82], memory[53,82], memory[54,82], memory[55,82], memory[56,82], memory[57,82], memory[58,82], memory[59,82], memory[60,82], memory[61,82], memory[62,82], memory[63,82], memory[64,82], memory[65,82], memory[66,82], memory[67,82], memory[68,82], memory[69,82], memory[70,82], memory[71,82], memory[72,82], memory[73,82], memory[74,82], memory[75,82], memory[76,82], memory[77,82], memory[78,82], memory[79,82], memory[80,82], memory[81,82], memory[82,82], memory[83,82], memory[84,82], memory[85,82], memory[86,82], memory[87,82], memory[88,82], memory[89,82], memory[90,82], memory[91,82], memory[92,82], memory[93,82], memory[94,82], memory[95,82], memory[96,82], memory[97,82], memory[98,82], memory[99,82], memory[100,82], memory[101,82], memory[102,82], memory[103,82], memory[104,82], memory[105,82], memory[106,82], memory[107,82], memory[108,82], memory[109,82], memory[110,82], memory[111,82], memory[112,82], memory[113,82], memory[114,82], memory[115,82], memory[116,82], memory[117,82], memory[118,82], memory[119,82], memory[120,82], memory[1,83], memory[2,83], memory[3,83], memory[4,83], memory[5,83], memory[6,83], memory[7,83], memory[8,83], memory[9,83], memory[10,83], memory[11,83], memory[12,83], memory[13,83], memory[14,83], memory[15,83], memory[16,83], memory[17,83], memory[18,83], memory[19,83], memory[20,83], memory[21,83], memory[22,83], memory[23,83], memory[24,83], memory[25,83], memory[26,83], memory[27,83], memory[28,83], memory[29,83], memory[30,83], memory[31,83], memory[32,83], memory[33,83], memory[34,83], memory[35,83], memory[36,83], memory[37,83], memory[38,83], memory[39,83], memory[40,83], memory[41,83], memory[42,83], memory[43,83], memory[44,83], memory[45,83], memory[46,83], memory[47,83], memory[48,83], memory[49,83], memory[50,83], memory[51,83], memory[52,83], memory[53,83], memory[54,83], memory[55,83], memory[56,83], memory[57,83], memory[58,83], memory[59,83], memory[60,83], memory[61,83], memory[62,83], memory[63,83], memory[64,83], memory[65,83], memory[66,83], memory[67,83], memory[68,83], memory[69,83], memory[70,83], memory[71,83], memory[72,83], memory[73,83], memory[74,83], memory[75,83], memory[76,83], memory[77,83], memory[78,83], memory[79,83], memory[80,83], memory[81,83], memory[82,83], memory[83,83], memory[84,83], memory[85,83], memory[86,83], memory[87,83], memory[88,83], memory[89,83], memory[90,83], memory[91,83], memory[92,83], memory[93,83], memory[94,83], memory[95,83], memory[96,83], memory[97,83], memory[98,83], memory[99,83], memory[100,83], memory[101,83], memory[102,83], memory[103,83], memory[104,83], memory[105,83], memory[106,83], memory[107,83], memory[108,83], memory[109,83], memory[110,83], memory[111,83], memory[112,83], memory[113,83], memory[114,83], memory[115,83], memory[116,83], memory[117,83], memory[118,83], memory[119,83], memory[120,83], memory[1,84], memory[2,84], memory[3,84], memory[4,84], memory[5,84], memory[6,84], memory[7,84], memory[8,84], memory[9,84], memory[10,84], memory[11,84], memory[12,84], memory[13,84], memory[14,84], memory[15,84], memory[16,84], memory[17,84], memory[18,84], memory[19,84], memory[20,84], memory[21,84], memory[22,84], memory[23,84], memory[24,84], memory[25,84], memory[26,84], memory[27,84], memory[28,84], memory[29,84], memory[30,84], memory[31,84], memory[32,84], memory[33,84], memory[34,84], memory[35,84], memory[36,84], memory[37,84], memory[38,84], memory[39,84], memory[40,84], memory[41,84], memory[42,84], memory[43,84], memory[44,84], memory[45,84], memory[46,84], memory[47,84], memory[48,84], memory[49,84], memory[50,84], memory[51,84], memory[52,84], memory[53,84], memory[54,84], memory[55,84], memory[56,84], memory[57,84], memory[58,84], memory[59,84], memory[60,84], memory[61,84], memory[62,84], memory[63,84], memory[64,84], memory[65,84], memory[66,84], memory[67,84], memory[68,84], memory[69,84], memory[70,84], memory[71,84], memory[72,84], memory[73,84], memory[74,84], memory[75,84], memory[76,84], memory[77,84], memory[78,84], memory[79,84], memory[80,84], memory[81,84], memory[82,84], memory[83,84], memory[84,84], memory[85,84], memory[86,84], memory[87,84], memory[88,84], memory[89,84], memory[90,84], memory[91,84], memory[92,84], memory[93,84], memory[94,84], memory[95,84], memory[96,84], memory[97,84], memory[98,84], memory[99,84], memory[100,84], memory[101,84], memory[102,84], memory[103,84], memory[104,84], memory[105,84], memory[106,84], memory[107,84], memory[108,84], memory[109,84], memory[110,84], memory[111,84], memory[112,84], memory[113,84], memory[114,84], memory[115,84], memory[116,84], memory[117,84], memory[118,84], memory[119,84], memory[120,84], memory[1,85], memory[2,85], memory[3,85], memory[4,85], memory[5,85], memory[6,85], memory[7,85], memory[8,85], memory[9,85], memory[10,85], memory[11,85], memory[12,85], memory[13,85], memory[14,85], memory[15,85], memory[16,85], memory[17,85], memory[18,85], memory[19,85], memory[20,85], memory[21,85], memory[22,85], memory[23,85], memory[24,85], memory[25,85], memory[26,85], memory[27,85], memory[28,85], memory[29,85], memory[30,85], memory[31,85], memory[32,85], memory[33,85], memory[34,85], memory[35,85], memory[36,85], memory[37,85], memory[38,85], memory[39,85], memory[40,85], memory[41,85], memory[42,85], memory[43,85], memory[44,85], memory[45,85], memory[46,85], memory[47,85], memory[48,85], memory[49,85], memory[50,85], memory[51,85], memory[52,85], memory[53,85], memory[54,85], memory[55,85], memory[56,85], memory[57,85], memory[58,85], memory[59,85], memory[60,85], memory[61,85], memory[62,85], memory[63,85], memory[64,85], memory[65,85], memory[66,85], memory[67,85], memory[68,85], memory[69,85], memory[70,85], memory[71,85], memory[72,85], memory[73,85], memory[74,85], memory[75,85], memory[76,85], memory[77,85], memory[78,85], memory[79,85], memory[80,85], memory[81,85], memory[82,85], memory[83,85], memory[84,85], memory[85,85], memory[86,85], memory[87,85], memory[88,85], memory[89,85], memory[90,85], memory[91,85], memory[92,85], memory[93,85], memory[94,85], memory[95,85], memory[96,85], memory[97,85], memory[98,85], memory[99,85], memory[100,85], memory[101,85], memory[102,85], memory[103,85], memory[104,85], memory[105,85], memory[106,85], memory[107,85], memory[108,85], memory[109,85], memory[110,85], memory[111,85], memory[112,85], memory[113,85], memory[114,85], memory[115,85], memory[116,85], memory[117,85], memory[118,85], memory[119,85], memory[120,85], memory[1,86], memory[2,86], memory[3,86], memory[4,86], memory[5,86], memory[6,86], memory[7,86], memory[8,86], memory[9,86], memory[10,86], memory[11,86], memory[12,86], memory[13,86], memory[14,86], memory[15,86], memory[16,86], memory[17,86], memory[18,86], memory[19,86], memory[20,86], memory[21,86], memory[22,86], memory[23,86], memory[24,86], memory[25,86], memory[26,86], memory[27,86], memory[28,86], memory[29,86], memory[30,86], memory[31,86], memory[32,86], memory[33,86], memory[34,86], memory[35,86], memory[36,86], memory[37,86], memory[38,86], memory[39,86], memory[40,86], memory[41,86], memory[42,86], memory[43,86], memory[44,86], memory[45,86], memory[46,86], memory[47,86], memory[48,86], memory[49,86], memory[50,86], memory[51,86], memory[52,86], memory[53,86], memory[54,86], memory[55,86], memory[56,86], memory[57,86], memory[58,86], memory[59,86], memory[60,86], memory[61,86], memory[62,86], memory[63,86], memory[64,86], memory[65,86], memory[66,86], memory[67,86], memory[68,86], memory[69,86], memory[70,86], memory[71,86], memory[72,86], memory[73,86], memory[74,86], memory[75,86], memory[76,86], memory[77,86], memory[78,86], memory[79,86], memory[80,86], memory[81,86], memory[82,86], memory[83,86], memory[84,86], memory[85,86], memory[86,86], memory[87,86], memory[88,86], memory[89,86], memory[90,86], memory[91,86], memory[92,86], memory[93,86], memory[94,86], memory[95,86], memory[96,86], memory[97,86], memory[98,86], memory[99,86], memory[100,86], memory[101,86], memory[102,86], memory[103,86], memory[104,86], memory[105,86], memory[106,86], memory[107,86], memory[108,86], memory[109,86], memory[110,86], memory[111,86], memory[112,86], memory[113,86], memory[114,86], memory[115,86], memory[116,86], memory[117,86], memory[118,86], memory[119,86], memory[120,86], memory[1,87], memory[2,87], memory[3,87], memory[4,87], memory[5,87], memory[6,87], memory[7,87], memory[8,87], memory[9,87], memory[10,87], memory[11,87], memory[12,87], memory[13,87], memory[14,87], memory[15,87], memory[16,87], memory[17,87], memory[18,87], memory[19,87], memory[20,87], memory[21,87], memory[22,87], memory[23,87], memory[24,87], memory[25,87], memory[26,87], memory[27,87], memory[28,87], memory[29,87], memory[30,87], memory[31,87], memory[32,87], memory[33,87], memory[34,87], memory[35,87], memory[36,87], memory[37,87], memory[38,87], memory[39,87], memory[40,87], memory[41,87], memory[42,87], memory[43,87], memory[44,87], memory[45,87], memory[46,87], memory[47,87], memory[48,87], memory[49,87], memory[50,87], memory[51,87], memory[52,87], memory[53,87], memory[54,87], memory[55,87], memory[56,87], memory[57,87], memory[58,87], memory[59,87], memory[60,87], memory[61,87], memory[62,87], memory[63,87], memory[64,87], memory[65,87], memory[66,87], memory[67,87], memory[68,87], memory[69,87], memory[70,87], memory[71,87], memory[72,87], memory[73,87], memory[74,87], memory[75,87], memory[76,87], memory[77,87], memory[78,87], memory[79,87], memory[80,87], memory[81,87], memory[82,87], memory[83,87], memory[84,87], memory[85,87], memory[86,87], memory[87,87], memory[88,87], memory[89,87], memory[90,87], memory[91,87], memory[92,87], memory[93,87], memory[94,87], memory[95,87], memory[96,87], memory[97,87], memory[98,87], memory[99,87], memory[100,87], memory[101,87], memory[102,87], memory[103,87], memory[104,87], memory[105,87], memory[106,87], memory[107,87], memory[108,87], memory[109,87], memory[110,87], memory[111,87], memory[112,87], memory[113,87], memory[114,87], memory[115,87], memory[116,87], memory[117,87], memory[118,87], memory[119,87], memory[120,87], memory[1,88], memory[2,88], memory[3,88], memory[4,88], memory[5,88], memory[6,88], memory[7,88], memory[8,88], memory[9,88], memory[10,88], memory[11,88], memory[12,88], memory[13,88], memory[14,88], memory[15,88], memory[16,88], memory[17,88], memory[18,88], memory[19,88], memory[20,88], memory[21,88], memory[22,88], memory[23,88], memory[24,88], memory[25,88], memory[26,88], memory[27,88], memory[28,88], memory[29,88], memory[30,88], memory[31,88], memory[32,88], memory[33,88], memory[34,88], memory[35,88], memory[36,88], memory[37,88], memory[38,88], memory[39,88], memory[40,88], memory[41,88], memory[42,88], memory[43,88], memory[44,88], memory[45,88], memory[46,88], memory[47,88], memory[48,88], memory[49,88], memory[50,88], memory[51,88], memory[52,88], memory[53,88], memory[54,88], memory[55,88], memory[56,88], memory[57,88], memory[58,88], memory[59,88], memory[60,88], memory[61,88], memory[62,88], memory[63,88], memory[64,88], memory[65,88], memory[66,88], memory[67,88], memory[68,88], memory[69,88], memory[70,88], memory[71,88], memory[72,88], memory[73,88], memory[74,88], memory[75,88], memory[76,88], memory[77,88], memory[78,88], memory[79,88], memory[80,88], memory[81,88], memory[82,88], memory[83,88], memory[84,88], memory[85,88], memory[86,88], memory[87,88], memory[88,88], memory[89,88], memory[90,88], memory[91,88], memory[92,88], memory[93,88], memory[94,88], memory[95,88], memory[96,88], memory[97,88], memory[98,88], memory[99,88], memory[100,88], memory[101,88], memory[102,88], memory[103,88], memory[104,88], memory[105,88], memory[106,88], memory[107,88], memory[108,88], memory[109,88], memory[110,88], memory[111,88], memory[112,88], memory[113,88], memory[114,88], memory[115,88], memory[116,88], memory[117,88], memory[118,88], memory[119,88], memory[120,88], memory[1,89], memory[2,89], memory[3,89], memory[4,89], memory[5,89], memory[6,89], memory[7,89], memory[8,89], memory[9,89], memory[10,89], memory[11,89], memory[12,89], memory[13,89], memory[14,89], memory[15,89], memory[16,89], memory[17,89], memory[18,89], memory[19,89], memory[20,89], memory[21,89], memory[22,89], memory[23,89], memory[24,89], memory[25,89], memory[26,89], memory[27,89], memory[28,89], memory[29,89], memory[30,89], memory[31,89], memory[32,89], memory[33,89], memory[34,89], memory[35,89], memory[36,89], memory[37,89], memory[38,89], memory[39,89], memory[40,89], memory[41,89], memory[42,89], memory[43,89], memory[44,89], memory[45,89], memory[46,89], memory[47,89], memory[48,89], memory[49,89], memory[50,89], memory[51,89], memory[52,89], memory[53,89], memory[54,89], memory[55,89], memory[56,89], memory[57,89], memory[58,89], memory[59,89], memory[60,89], memory[61,89], memory[62,89], memory[63,89], memory[64,89], memory[65,89], memory[66,89], memory[67,89], memory[68,89], memory[69,89], memory[70,89], memory[71,89], memory[72,89], memory[73,89], memory[74,89], memory[75,89], memory[76,89], memory[77,89], memory[78,89], memory[79,89], memory[80,89], memory[81,89], memory[82,89], memory[83,89], memory[84,89], memory[85,89], memory[86,89], memory[87,89], memory[88,89], memory[89,89], memory[90,89], memory[91,89], memory[92,89], memory[93,89], memory[94,89], memory[95,89], memory[96,89], memory[97,89], memory[98,89], memory[99,89], memory[100,89], memory[101,89], memory[102,89], memory[103,89], memory[104,89], memory[105,89], memory[106,89], memory[107,89], memory[108,89], memory[109,89], memory[110,89], memory[111,89], memory[112,89], memory[113,89], memory[114,89], memory[115,89], memory[116,89], memory[117,89], memory[118,89], memory[119,89], memory[120,89], memory[1,90], memory[2,90], memory[3,90], memory[4,90], memory[5,90], memory[6,90], memory[7,90], memory[8,90], memory[9,90], memory[10,90], memory[11,90], memory[12,90], memory[13,90], memory[14,90], memory[15,90], memory[16,90], memory[17,90], memory[18,90], memory[19,90], memory[20,90], memory[21,90], memory[22,90], memory[23,90], memory[24,90], memory[25,90], memory[26,90], memory[27,90], memory[28,90], memory[29,90], memory[30,90], memory[31,90], memory[32,90], memory[33,90], memory[34,90], memory[35,90], memory[36,90], memory[37,90], memory[38,90], memory[39,90], memory[40,90], memory[41,90], memory[42,90], memory[43,90], memory[44,90], memory[45,90], memory[46,90], memory[47,90], memory[48,90], memory[49,90], memory[50,90], memory[51,90], memory[52,90], memory[53,90], memory[54,90], memory[55,90], memory[56,90], memory[57,90], memory[58,90], memory[59,90], memory[60,90], memory[61,90], memory[62,90], memory[63,90], memory[64,90], memory[65,90], memory[66,90], memory[67,90], memory[68,90], memory[69,90], memory[70,90], memory[71,90], memory[72,90], memory[73,90], memory[74,90], memory[75,90], memory[76,90], memory[77,90], memory[78,90], memory[79,90], memory[80,90], memory[81,90], memory[82,90], memory[83,90], memory[84,90], memory[85,90], memory[86,90], memory[87,90], memory[88,90], memory[89,90], memory[90,90], memory[91,90], memory[92,90], memory[93,90], memory[94,90], memory[95,90], memory[96,90], memory[97,90], memory[98,90], memory[99,90], memory[100,90], memory[101,90], memory[102,90], memory[103,90], memory[104,90], memory[105,90], memory[106,90], memory[107,90], memory[108,90], memory[109,90], memory[110,90], memory[111,90], memory[112,90], memory[113,90], memory[114,90], memory[115,90], memory[116,90], memory[117,90], memory[118,90], memory[119,90], memory[120,90], memory[1,91], memory[2,91], memory[3,91], memory[4,91], memory[5,91], memory[6,91], memory[7,91], memory[8,91], memory[9,91], memory[10,91], memory[11,91], memory[12,91], memory[13,91], memory[14,91], memory[15,91], memory[16,91], memory[17,91], memory[18,91], memory[19,91], memory[20,91], memory[21,91], memory[22,91], memory[23,91], memory[24,91], memory[25,91], memory[26,91], memory[27,91], memory[28,91], memory[29,91], memory[30,91], memory[31,91], memory[32,91], memory[33,91], memory[34,91], memory[35,91], memory[36,91], memory[37,91], memory[38,91], memory[39,91], memory[40,91], memory[41,91], memory[42,91], memory[43,91], memory[44,91], memory[45,91], memory[46,91], memory[47,91], memory[48,91], memory[49,91], memory[50,91], memory[51,91], memory[52,91], memory[53,91], memory[54,91], memory[55,91], memory[56,91], memory[57,91], memory[58,91], memory[59,91], memory[60,91], memory[61,91], memory[62,91], memory[63,91], memory[64,91], memory[65,91], memory[66,91], memory[67,91], memory[68,91], memory[69,91], memory[70,91], memory[71,91], memory[72,91], memory[73,91], memory[74,91], memory[75,91], memory[76,91], memory[77,91], memory[78,91], memory[79,91], memory[80,91], memory[81,91], memory[82,91], memory[83,91], memory[84,91], memory[85,91], memory[86,91], memory[87,91], memory[88,91], memory[89,91], memory[90,91], memory[91,91], memory[92,91], memory[93,91], memory[94,91], memory[95,91], memory[96,91], memory[97,91], memory[98,91], memory[99,91], memory[100,91], memory[101,91], memory[102,91], memory[103,91], memory[104,91], memory[105,91], memory[106,91], memory[107,91], memory[108,91], memory[109,91], memory[110,91], memory[111,91], memory[112,91], memory[113,91], memory[114,91], memory[115,91], memory[116,91], memory[117,91], memory[118,91], memory[119,91], memory[120,91], memory[1,92], memory[2,92], memory[3,92], memory[4,92], memory[5,92], memory[6,92], memory[7,92], memory[8,92], memory[9,92], memory[10,92], memory[11,92], memory[12,92], memory[13,92], memory[14,92], memory[15,92], memory[16,92], memory[17,92], memory[18,92], memory[19,92], memory[20,92], memory[21,92], memory[22,92], memory[23,92], memory[24,92], memory[25,92], memory[26,92], memory[27,92], memory[28,92], memory[29,92], memory[30,92], memory[31,92], memory[32,92], memory[33,92], memory[34,92], memory[35,92], memory[36,92], memory[37,92], memory[38,92], memory[39,92], memory[40,92], memory[41,92], memory[42,92], memory[43,92], memory[44,92], memory[45,92], memory[46,92], memory[47,92], memory[48,92], memory[49,92], memory[50,92], memory[51,92], memory[52,92], memory[53,92], memory[54,92], memory[55,92], memory[56,92], memory[57,92], memory[58,92], memory[59,92], memory[60,92], memory[61,92], memory[62,92], memory[63,92], memory[64,92], memory[65,92], memory[66,92], memory[67,92], memory[68,92], memory[69,92], memory[70,92], memory[71,92], memory[72,92], memory[73,92], memory[74,92], memory[75,92], memory[76,92], memory[77,92], memory[78,92], memory[79,92], memory[80,92], memory[81,92], memory[82,92], memory[83,92], memory[84,92], memory[85,92], memory[86,92], memory[87,92], memory[88,92], memory[89,92], memory[90,92], memory[91,92], memory[92,92], memory[93,92], memory[94,92], memory[95,92], memory[96,92], memory[97,92], memory[98,92], memory[99,92], memory[100,92], memory[101,92], memory[102,92], memory[103,92], memory[104,92], memory[105,92], memory[106,92], memory[107,92], memory[108,92], memory[109,92], memory[110,92], memory[111,92], memory[112,92], memory[113,92], memory[114,92], memory[115,92], memory[116,92], memory[117,92], memory[118,92], memory[119,92], memory[120,92], memory[1,93], memory[2,93], memory[3,93], memory[4,93], memory[5,93], memory[6,93], memory[7,93], memory[8,93], memory[9,93], memory[10,93], memory[11,93], memory[12,93], memory[13,93], memory[14,93], memory[15,93], memory[16,93], memory[17,93], memory[18,93], memory[19,93], memory[20,93], memory[21,93], memory[22,93], memory[23,93], memory[24,93], memory[25,93], memory[26,93], memory[27,93], memory[28,93], memory[29,93], memory[30,93], memory[31,93], memory[32,93], memory[33,93], memory[34,93], memory[35,93], memory[36,93], memory[37,93], memory[38,93], memory[39,93], memory[40,93], memory[41,93], memory[42,93], memory[43,93], memory[44,93], memory[45,93], memory[46,93], memory[47,93], memory[48,93], memory[49,93], memory[50,93], memory[51,93], memory[52,93], memory[53,93], memory[54,93], memory[55,93], memory[56,93], memory[57,93], memory[58,93], memory[59,93], memory[60,93], memory[61,93], memory[62,93], memory[63,93], memory[64,93], memory[65,93], memory[66,93], memory[67,93], memory[68,93], memory[69,93], memory[70,93], memory[71,93], memory[72,93], memory[73,93], memory[74,93], memory[75,93], memory[76,93], memory[77,93], memory[78,93], memory[79,93], memory[80,93], memory[81,93], memory[82,93], memory[83,93], memory[84,93], memory[85,93], memory[86,93], memory[87,93], memory[88,93], memory[89,93], memory[90,93], memory[91,93], memory[92,93], memory[93,93], memory[94,93], memory[95,93], memory[96,93], memory[97,93], memory[98,93], memory[99,93], memory[100,93], memory[101,93], memory[102,93], memory[103,93], memory[104,93], memory[105,93], memory[106,93], memory[107,93], memory[108,93], memory[109,93], memory[110,93], memory[111,93], memory[112,93], memory[113,93], memory[114,93], memory[115,93], memory[116,93], memory[117,93], memory[118,93], memory[119,93], memory[120,93], memory[1,94], memory[2,94], memory[3,94], memory[4,94], memory[5,94], memory[6,94], memory[7,94], memory[8,94], memory[9,94], memory[10,94], memory[11,94], memory[12,94], memory[13,94], memory[14,94], memory[15,94], memory[16,94], memory[17,94], memory[18,94], memory[19,94], memory[20,94], memory[21,94], memory[22,94], memory[23,94], memory[24,94], memory[25,94], memory[26,94], memory[27,94], memory[28,94], memory[29,94], memory[30,94], memory[31,94], memory[32,94], memory[33,94], memory[34,94], memory[35,94], memory[36,94], memory[37,94], memory[38,94], memory[39,94], memory[40,94], memory[41,94], memory[42,94], memory[43,94], memory[44,94], memory[45,94], memory[46,94], memory[47,94], memory[48,94], memory[49,94], memory[50,94], memory[51,94], memory[52,94], memory[53,94], memory[54,94], memory[55,94], memory[56,94], memory[57,94], memory[58,94], memory[59,94], memory[60,94], memory[61,94], memory[62,94], memory[63,94], memory[64,94], memory[65,94], memory[66,94], memory[67,94], memory[68,94], memory[69,94], memory[70,94], memory[71,94], memory[72,94], memory[73,94], memory[74,94], memory[75,94], memory[76,94], memory[77,94], memory[78,94], memory[79,94], memory[80,94], memory[81,94], memory[82,94], memory[83,94], memory[84,94], memory[85,94], memory[86,94], memory[87,94], memory[88,94], memory[89,94], memory[90,94], memory[91,94], memory[92,94], memory[93,94], memory[94,94], memory[95,94], memory[96,94], memory[97,94], memory[98,94], memory[99,94], memory[100,94], memory[101,94], memory[102,94], memory[103,94], memory[104,94], memory[105,94], memory[106,94], memory[107,94], memory[108,94], memory[109,94], memory[110,94], memory[111,94], memory[112,94], memory[113,94], memory[114,94], memory[115,94], memory[116,94], memory[117,94], memory[118,94], memory[119,94], memory[120,94], memory[1,95], memory[2,95], memory[3,95], memory[4,95], memory[5,95], memory[6,95], memory[7,95], memory[8,95], memory[9,95], memory[10,95], memory[11,95], memory[12,95], memory[13,95], memory[14,95], memory[15,95], memory[16,95], memory[17,95], memory[18,95], memory[19,95], memory[20,95], memory[21,95], memory[22,95], memory[23,95], memory[24,95], memory[25,95], memory[26,95], memory[27,95], memory[28,95], memory[29,95], memory[30,95], memory[31,95], memory[32,95], memory[33,95], memory[34,95], memory[35,95], memory[36,95], memory[37,95], memory[38,95], memory[39,95], memory[40,95], memory[41,95], memory[42,95], memory[43,95], memory[44,95], memory[45,95], memory[46,95], memory[47,95], memory[48,95], memory[49,95], memory[50,95], memory[51,95], memory[52,95], memory[53,95], memory[54,95], memory[55,95], memory[56,95], memory[57,95], memory[58,95], memory[59,95], memory[60,95], memory[61,95], memory[62,95], memory[63,95], memory[64,95], memory[65,95], memory[66,95], memory[67,95], memory[68,95], memory[69,95], memory[70,95], memory[71,95], memory[72,95], memory[73,95], memory[74,95], memory[75,95], memory[76,95], memory[77,95], memory[78,95], memory[79,95], memory[80,95], memory[81,95], memory[82,95], memory[83,95], memory[84,95], memory[85,95], memory[86,95], memory[87,95], memory[88,95], memory[89,95], memory[90,95], memory[91,95], memory[92,95], memory[93,95], memory[94,95], memory[95,95], memory[96,95], memory[97,95], memory[98,95], memory[99,95], memory[100,95], memory[101,95], memory[102,95], memory[103,95], memory[104,95], memory[105,95], memory[106,95], memory[107,95], memory[108,95], memory[109,95], memory[110,95], memory[111,95], memory[112,95], memory[113,95], memory[114,95], memory[115,95], memory[116,95], memory[117,95], memory[118,95], memory[119,95], memory[120,95], memory[1,96], memory[2,96], memory[3,96], memory[4,96], memory[5,96], memory[6,96], memory[7,96], memory[8,96], memory[9,96], memory[10,96], memory[11,96], memory[12,96], memory[13,96], memory[14,96], memory[15,96], memory[16,96], memory[17,96], memory[18,96], memory[19,96], memory[20,96], memory[21,96], memory[22,96], memory[23,96], memory[24,96], memory[25,96], memory[26,96], memory[27,96], memory[28,96], memory[29,96], memory[30,96], memory[31,96], memory[32,96], memory[33,96], memory[34,96], memory[35,96], memory[36,96], memory[37,96], memory[38,96], memory[39,96], memory[40,96], memory[41,96], memory[42,96], memory[43,96], memory[44,96], memory[45,96], memory[46,96], memory[47,96], memory[48,96], memory[49,96], memory[50,96], memory[51,96], memory[52,96], memory[53,96], memory[54,96], memory[55,96], memory[56,96], memory[57,96], memory[58,96], memory[59,96], memory[60,96], memory[61,96], memory[62,96], memory[63,96], memory[64,96], memory[65,96], memory[66,96], memory[67,96], memory[68,96], memory[69,96], memory[70,96], memory[71,96], memory[72,96], memory[73,96], memory[74,96], memory[75,96], memory[76,96], memory[77,96], memory[78,96], memory[79,96], memory[80,96], memory[81,96], memory[82,96], memory[83,96], memory[84,96], memory[85,96], memory[86,96], memory[87,96], memory[88,96], memory[89,96], memory[90,96], memory[91,96], memory[92,96], memory[93,96], memory[94,96], memory[95,96], memory[96,96], memory[97,96], memory[98,96], memory[99,96], memory[100,96], memory[101,96], memory[102,96], memory[103,96], memory[104,96], memory[105,96], memory[106,96], memory[107,96], memory[108,96], memory[109,96], memory[110,96], memory[111,96], memory[112,96], memory[113,96], memory[114,96], memory[115,96], memory[116,96], memory[117,96], memory[118,96], memory[119,96], memory[120,96], memory[1,97], memory[2,97], memory[3,97], memory[4,97], memory[5,97], memory[6,97], memory[7,97], memory[8,97], memory[9,97], memory[10,97], memory[11,97], memory[12,97], memory[13,97], memory[14,97], memory[15,97], memory[16,97], memory[17,97], memory[18,97], memory[19,97], memory[20,97], memory[21,97], memory[22,97], memory[23,97], memory[24,97], memory[25,97], memory[26,97], memory[27,97], memory[28,97], memory[29,97], memory[30,97], memory[31,97], memory[32,97], memory[33,97], memory[34,97], memory[35,97], memory[36,97], memory[37,97], memory[38,97], memory[39,97], memory[40,97], memory[41,97], memory[42,97], memory[43,97], memory[44,97], memory[45,97], memory[46,97], memory[47,97], memory[48,97], memory[49,97], memory[50,97], memory[51,97], memory[52,97], memory[53,97], memory[54,97], memory[55,97], memory[56,97], memory[57,97], memory[58,97], memory[59,97], memory[60,97], memory[61,97], memory[62,97], memory[63,97], memory[64,97], memory[65,97], memory[66,97], memory[67,97], memory[68,97], memory[69,97], memory[70,97], memory[71,97], memory[72,97], memory[73,97], memory[74,97], memory[75,97], memory[76,97], memory[77,97], memory[78,97], memory[79,97], memory[80,97], memory[81,97], memory[82,97], memory[83,97], memory[84,97], memory[85,97], memory[86,97], memory[87,97], memory[88,97], memory[89,97], memory[90,97], memory[91,97], memory[92,97], memory[93,97], memory[94,97], memory[95,97], memory[96,97], memory[97,97], memory[98,97], memory[99,97], memory[100,97], memory[101,97], memory[102,97], memory[103,97], memory[104,97], memory[105,97], memory[106,97], memory[107,97], memory[108,97], memory[109,97], memory[110,97], memory[111,97], memory[112,97], memory[113,97], memory[114,97], memory[115,97], memory[116,97], memory[117,97], memory[118,97], memory[119,97], memory[120,97], memory[1,98], memory[2,98], memory[3,98], memory[4,98], memory[5,98], memory[6,98], memory[7,98], memory[8,98], memory[9,98], memory[10,98], memory[11,98], memory[12,98], memory[13,98], memory[14,98], memory[15,98], memory[16,98], memory[17,98], memory[18,98], memory[19,98], memory[20,98], memory[21,98], memory[22,98], memory[23,98], memory[24,98], memory[25,98], memory[26,98], memory[27,98], memory[28,98], memory[29,98], memory[30,98], memory[31,98], memory[32,98], memory[33,98], memory[34,98], memory[35,98], memory[36,98], memory[37,98], memory[38,98], memory[39,98], memory[40,98], memory[41,98], memory[42,98], memory[43,98], memory[44,98], memory[45,98], memory[46,98], memory[47,98], memory[48,98], memory[49,98], memory[50,98], memory[51,98], memory[52,98], memory[53,98], memory[54,98], memory[55,98], memory[56,98], memory[57,98], memory[58,98], memory[59,98], memory[60,98], memory[61,98], memory[62,98], memory[63,98], memory[64,98], memory[65,98], memory[66,98], memory[67,98], memory[68,98], memory[69,98], memory[70,98], memory[71,98], memory[72,98], memory[73,98], memory[74,98], memory[75,98], memory[76,98], memory[77,98], memory[78,98], memory[79,98], memory[80,98], memory[81,98], memory[82,98], memory[83,98], memory[84,98], memory[85,98], memory[86,98], memory[87,98], memory[88,98], memory[89,98], memory[90,98], memory[91,98], memory[92,98], memory[93,98], memory[94,98], memory[95,98], memory[96,98], memory[97,98], memory[98,98], memory[99,98], memory[100,98], memory[101,98], memory[102,98], memory[103,98], memory[104,98], memory[105,98], memory[106,98], memory[107,98], memory[108,98], memory[109,98], memory[110,98], memory[111,98], memory[112,98], memory[113,98], memory[114,98], memory[115,98], memory[116,98], memory[117,98], memory[118,98], memory[119,98], memory[120,98], memory[1,99], memory[2,99], memory[3,99], memory[4,99], memory[5,99], memory[6,99], memory[7,99], memory[8,99], memory[9,99], memory[10,99], memory[11,99], memory[12,99], memory[13,99], memory[14,99], memory[15,99], memory[16,99], memory[17,99], memory[18,99], memory[19,99], memory[20,99], memory[21,99], memory[22,99], memory[23,99], memory[24,99], memory[25,99], memory[26,99], memory[27,99], memory[28,99], memory[29,99], memory[30,99], memory[31,99], memory[32,99], memory[33,99], memory[34,99], memory[35,99], memory[36,99], memory[37,99], memory[38,99], memory[39,99], memory[40,99], memory[41,99], memory[42,99], memory[43,99], memory[44,99], memory[45,99], memory[46,99], memory[47,99], memory[48,99], memory[49,99], memory[50,99], memory[51,99], memory[52,99], memory[53,99], memory[54,99], memory[55,99], memory[56,99], memory[57,99], memory[58,99], memory[59,99], memory[60,99], memory[61,99], memory[62,99], memory[63,99], memory[64,99], memory[65,99], memory[66,99], memory[67,99], memory[68,99], memory[69,99], memory[70,99], memory[71,99], memory[72,99], memory[73,99], memory[74,99], memory[75,99], memory[76,99], memory[77,99], memory[78,99], memory[79,99], memory[80,99], memory[81,99], memory[82,99], memory[83,99], memory[84,99], memory[85,99], memory[86,99], memory[87,99], memory[88,99], memory[89,99], memory[90,99], memory[91,99], memory[92,99], memory[93,99], memory[94,99], memory[95,99], memory[96,99], memory[97,99], memory[98,99], memory[99,99], memory[100,99], memory[101,99], memory[102,99], memory[103,99], memory[104,99], memory[105,99], memory[106,99], memory[107,99], memory[108,99], memory[109,99], memory[110,99], memory[111,99], memory[112,99], memory[113,99], memory[114,99], memory[115,99], memory[116,99], memory[117,99], memory[118,99], memory[119,99], memory[120,99], memory[1,100], memory[2,100], memory[3,100], memory[4,100], memory[5,100], memory[6,100], memory[7,100], memory[8,100], memory[9,100], memory[10,100], memory[11,100], memory[12,100], memory[13,100], memory[14,100], memory[15,100], memory[16,100], memory[17,100], memory[18,100], memory[19,100], memory[20,100], memory[21,100], memory[22,100], memory[23,100], memory[24,100], memory[25,100], memory[26,100], memory[27,100], memory[28,100], memory[29,100], memory[30,100], memory[31,100], memory[32,100], memory[33,100], memory[34,100], memory[35,100], memory[36,100], memory[37,100], memory[38,100], memory[39,100], memory[40,100], memory[41,100], memory[42,100], memory[43,100], memory[44,100], memory[45,100], memory[46,100], memory[47,100], memory[48,100], memory[49,100], memory[50,100], memory[51,100], memory[52,100], memory[53,100], memory[54,100], memory[55,100], memory[56,100], memory[57,100], memory[58,100], memory[59,100], memory[60,100], memory[61,100], memory[62,100], memory[63,100], memory[64,100], memory[65,100], memory[66,100], memory[67,100], memory[68,100], memory[69,100], memory[70,100], memory[71,100], memory[72,100], memory[73,100], memory[74,100], memory[75,100], memory[76,100], memory[77,100], memory[78,100], memory[79,100], memory[80,100], memory[81,100], memory[82,100], memory[83,100], memory[84,100], memory[85,100], memory[86,100], memory[87,100], memory[88,100], memory[89,100], memory[90,100], memory[91,100], memory[92,100], memory[93,100], memory[94,100], memory[95,100], memory[96,100], memory[97,100], memory[98,100], memory[99,100], memory[100,100], memory[101,100], memory[102,100], memory[103,100], memory[104,100], memory[105,100], memory[106,100], memory[107,100], memory[108,100], memory[109,100], memory[110,100], memory[111,100], memory[112,100], memory[113,100], memory[114,100], memory[115,100], memory[116,100], memory[117,100], memory[118,100], memory[119,100], memory[120,100], biasM_prior, biasSD_prior, betaM_prior, betaSD_prior, bias_prior, beta_prior, prior_preds0, prior_preds1, prior_preds2, posterior_preds0, posterior_preds1, posterior_preds2, posterior_predsID0[1], posterior_predsID0[2], posterior_predsID0[3], posterior_predsID0[4], posterior_predsID0[5], posterior_predsID0[6], posterior_predsID0[7], posterior_predsID0[8], posterior_predsID0[9], posterior_predsID0[10], posterior_predsID0[11], posterior_predsID0[12], posterior_predsID0[13], posterior_predsID0[14], posterior_predsID0[15], posterior_predsID0[16], posterior_predsID0[17], posterior_predsID0[18], posterior_predsID0[19], posterior_predsID0[20], posterior_predsID0[21], posterior_predsID0[22], posterior_predsID0[23], posterior_predsID0[24], posterior_predsID0[25], posterior_predsID0[26], posterior_predsID0[27], posterior_predsID0[28], posterior_predsID0[29], posterior_predsID0[30], posterior_predsID0[31], posterior_predsID0[32], posterior_predsID0[33], posterior_predsID0[34], posterior_predsID0[35], posterior_predsID0[36], posterior_predsID0[37], posterior_predsID0[38], posterior_predsID0[39], posterior_predsID0[40], posterior_predsID0[41], posterior_predsID0[42], posterior_predsID0[43], posterior_predsID0[44], posterior_predsID0[45], posterior_predsID0[46], posterior_predsID0[47], posterior_predsID0[48], posterior_predsID0[49], posterior_predsID0[50], posterior_predsID0[51], posterior_predsID0[52], posterior_predsID0[53], posterior_predsID0[54], posterior_predsID0[55], posterior_predsID0[56], posterior_predsID0[57], posterior_predsID0[58], posterior_predsID0[59], posterior_predsID0[60], posterior_predsID0[61], posterior_predsID0[62], posterior_predsID0[63], posterior_predsID0[64], posterior_predsID0[65], posterior_predsID0[66], posterior_predsID0[67], posterior_predsID0[68], posterior_predsID0[69], posterior_predsID0[70], posterior_predsID0[71], posterior_predsID0[72], posterior_predsID0[73], posterior_predsID0[74], posterior_predsID0[75], posterior_predsID0[76], posterior_predsID0[77], posterior_predsID0[78], posterior_predsID0[79], posterior_predsID0[80], posterior_predsID0[81], posterior_predsID0[82], posterior_predsID0[83], posterior_predsID0[84], posterior_predsID0[85], posterior_predsID0[86], posterior_predsID0[87], posterior_predsID0[88], posterior_predsID0[89], posterior_predsID0[90], posterior_predsID0[91], posterior_predsID0[92], posterior_predsID0[93], posterior_predsID0[94], posterior_predsID0[95], posterior_predsID0[96], posterior_predsID0[97], posterior_predsID0[98], posterior_predsID0[99], posterior_predsID0[100], posterior_predsID1[1], posterior_predsID1[2], posterior_predsID1[3], posterior_predsID1[4], posterior_predsID1[5], posterior_predsID1[6], posterior_predsID1[7], posterior_predsID1[8], posterior_predsID1[9], posterior_predsID1[10], posterior_predsID1[11], posterior_predsID1[12], posterior_predsID1[13], posterior_predsID1[14], posterior_predsID1[15], posterior_predsID1[16], posterior_predsID1[17], posterior_predsID1[18], posterior_predsID1[19], posterior_predsID1[20], posterior_predsID1[21], posterior_predsID1[22], posterior_predsID1[23], posterior_predsID1[24], posterior_predsID1[25], posterior_predsID1[26], posterior_predsID1[27], posterior_predsID1[28], posterior_predsID1[29], posterior_predsID1[30], posterior_predsID1[31], posterior_predsID1[32], posterior_predsID1[33], posterior_predsID1[34], posterior_predsID1[35], posterior_predsID1[36], posterior_predsID1[37], posterior_predsID1[38], posterior_predsID1[39], posterior_predsID1[40], posterior_predsID1[41], posterior_predsID1[42], posterior_predsID1[43], posterior_predsID1[44], posterior_predsID1[45], posterior_predsID1[46], posterior_predsID1[47], posterior_predsID1[48], posterior_predsID1[49], posterior_predsID1[50], posterior_predsID1[51], posterior_predsID1[52], posterior_predsID1[53], posterior_predsID1[54], posterior_predsID1[55], posterior_predsID1[56], posterior_predsID1[57], posterior_predsID1[58], posterior_predsID1[59], posterior_predsID1[60], posterior_predsID1[61], posterior_predsID1[62], posterior_predsID1[63], posterior_predsID1[64], posterior_predsID1[65], posterior_predsID1[66], posterior_predsID1[67], posterior_predsID1[68], posterior_predsID1[69], posterior_predsID1[70], posterior_predsID1[71], posterior_predsID1[72], posterior_predsID1[73], posterior_predsID1[74], posterior_predsID1[75], posterior_predsID1[76], posterior_predsID1[77], posterior_predsID1[78], posterior_predsID1[79], posterior_predsID1[80], posterior_predsID1[81], posterior_predsID1[82], posterior_predsID1[83], posterior_predsID1[84], posterior_predsID1[85], posterior_predsID1[86], posterior_predsID1[87], posterior_predsID1[88], posterior_predsID1[89], posterior_predsID1[90], posterior_predsID1[91], posterior_predsID1[92], posterior_predsID1[93], posterior_predsID1[94], posterior_predsID1[95], posterior_predsID1[96], posterior_predsID1[97], posterior_predsID1[98], posterior_predsID1[99], posterior_predsID1[100], posterior_predsID2[1], posterior_predsID2[2], posterior_predsID2[3], posterior_predsID2[4], posterior_predsID2[5], posterior_predsID2[6], posterior_predsID2[7], posterior_predsID2[8], posterior_predsID2[9], posterior_predsID2[10], posterior_predsID2[11], posterior_predsID2[12], posterior_predsID2[13], posterior_predsID2[14], posterior_predsID2[15], posterior_predsID2[16], posterior_predsID2[17], posterior_predsID2[18], posterior_predsID2[19], posterior_predsID2[20], posterior_predsID2[21], posterior_predsID2[22], posterior_predsID2[23], posterior_predsID2[24], posterior_predsID2[25], posterior_predsID2[26], posterior_predsID2[27], posterior_predsID2[28], posterior_predsID2[29], posterior_predsID2[30], posterior_predsID2[31], posterior_predsID2[32], posterior_predsID2[33], posterior_predsID2[34], posterior_predsID2[35], posterior_predsID2[36], posterior_predsID2[37], posterior_predsID2[38], posterior_predsID2[39], posterior_predsID2[40], posterior_predsID2[41], posterior_predsID2[42], posterior_predsID2[43], posterior_predsID2[44], posterior_predsID2[45], posterior_predsID2[46], posterior_predsID2[47], posterior_predsID2[48], posterior_predsID2[49], posterior_predsID2[50], posterior_predsID2[51], posterior_predsID2[52], posterior_predsID2[53], posterior_predsID2[54], posterior_predsID2[55], posterior_predsID2[56], posterior_predsID2[57], posterior_predsID2[58], posterior_predsID2[59], posterior_predsID2[60], posterior_predsID2[61], posterior_predsID2[62], posterior_predsID2[63], posterior_predsID2[64], posterior_predsID2[65], posterior_predsID2[66], posterior_predsID2[67], posterior_predsID2[68], posterior_predsID2[69], posterior_predsID2[70], posterior_predsID2[71], posterior_predsID2[72], posterior_predsID2[73], posterior_predsID2[74], posterior_predsID2[75], posterior_predsID2[76], posterior_predsID2[77], posterior_predsID2[78], posterior_predsID2[79], posterior_predsID2[80], posterior_predsID2[81], posterior_predsID2[82], posterior_predsID2[83], posterior_predsID2[84], posterior_predsID2[85], posterior_predsID2[86], posterior_predsID2[87], posterior_predsID2[88], posterior_predsID2[89], posterior_predsID2[90], posterior_predsID2[91], posterior_predsID2[92], posterior_predsID2[93], posterior_predsID2[94], posterior_predsID2[95], posterior_predsID2[96], posterior_predsID2[97], posterior_predsID2[98], posterior_predsID2[99], posterior_predsID2[100], .chain, .iteration, .draw # Show summary statistics for key parameters print(samples_mlvl$summary(c(&quot;biasM&quot;, &quot;betaM&quot;, &quot;biasSD&quot;, &quot;betaSD&quot;))) ## # A tibble: 4 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 biasM 0.416 0.414 0.0808 0.0800 0.285 0.552 1.01 241. 484. ## 2 betaM 1.16 1.16 0.0711 0.0727 1.04 1.27 1.00 347. 833. ## 3 biasSD 0.241 0.240 0.0611 0.0599 0.141 0.343 1.00 205. 242. ## 4 betaSD 0.381 0.379 0.0465 0.0457 0.308 0.461 1.00 1155. 1764. # Extract posterior draws for analysis draws_df &lt;- as_draws_df(samples_mlvl$draws()) # Create trace plots to check convergence p1 &lt;- mcmc_trace(draws_df, pars = c(&quot;biasM&quot;, &quot;biasSD&quot;, &quot;betaM&quot;, &quot;betaSD&quot;)) + theme_classic() + ggtitle(&quot;Trace Plots for Population Parameters&quot;) # Show trace plots p1 # Create prior-posterior update plots create_density_plot &lt;- function(param, true_value, title) { prior_name &lt;- paste0(param, &quot;_prior&quot;) ggplot(draws_df) + geom_histogram(aes(get(param)), fill = &quot;blue&quot;, alpha = 0.3) + geom_histogram(aes(get(prior_name)), fill = &quot;red&quot;, alpha = 0.3) + geom_vline(xintercept = true_value, linetype = &quot;dashed&quot;) + labs(title = title, subtitle = &quot;Blue: posterior, Red: prior, Dashed: true value&quot;, x = param, y = &quot;Density&quot;) + theme_classic() } # Create individual plots p_biasM &lt;- create_density_plot(&quot;biasM&quot;, biasM, &quot;Population Mean Bias&quot;) p_biasSD &lt;- create_density_plot(&quot;biasSD&quot;, biasSD, &quot;Population SD of Bias&quot;) p_betaM &lt;- create_density_plot(&quot;betaM&quot;, betaM, &quot;Population Mean Beta&quot;) p_betaSD &lt;- create_density_plot(&quot;betaSD&quot;, betaSD, &quot;Population SD of Beta&quot;) # Show them in a grid (p_biasM + p_biasSD) / (p_betaM + p_betaSD) # Show correlations p1 &lt;- ggplot(draws_df, aes(biasM, biasSD, group = .chain, color = .chain)) + geom_point(alpha = 0.1) + theme_classic() p2 &lt;- ggplot(draws_df, aes(betaM, betaSD, group = .chain, color = .chain)) + geom_point(alpha = 0.1) + theme_classic() p3 &lt;- ggplot(draws_df, aes(biasM, betaM, group = .chain, color = .chain)) + geom_point(alpha = 0.1) + theme_classic() p4 &lt;- ggplot(draws_df, aes(biasSD, betaSD, group = .chain, color = .chain)) + geom_point(alpha = 0.1) + theme_classic() p1 + p2 + p3 + p4 # Create posterior predictive check plots p1 &lt;- ggplot(draws_df) + geom_histogram(aes(prior_preds0), fill = &quot;red&quot;, alpha = 0.3, bins = 30) + geom_histogram(aes(posterior_preds0), fill = &quot;blue&quot;, alpha = 0.3, bins = 30) + geom_histogram(aes(posterior_preds1), fill = &quot;green&quot;, alpha = 0.3, bins = 30) + geom_histogram(aes(posterior_preds2), fill = &quot;purple&quot;, alpha = 0.3, bins = 30) + labs(title = &quot;Prior and Posterior Predictive Distributions&quot;, subtitle = &quot;Red: prior, Blue: no memory effect, Green: neutral memory, Purple: strong memory&quot;, x = &quot;Predicted Right Choices (out of 120)&quot;, y = &quot;Count&quot;) + theme_classic() # Display plots p1 # Individual-level parameter recovery # Extract individual parameters for a sample of agents sample_agents &lt;- sample(1:agents, 100) sample_data &lt;- d %&gt;% filter(agent %in% sample_agents, trial == 1) %&gt;% dplyr::select(agent, bias, beta) # Extract posterior means for individual agents bias_means &lt;- c() beta_means &lt;- c() for (i in sample_agents) { bias_means[i] &lt;- mean(draws_df[[paste0(&quot;bias[&quot;, i, &quot;]&quot;)]]) beta_means[i] &lt;- mean(draws_df[[paste0(&quot;beta[&quot;, i, &quot;]&quot;)]]) } # Create comparison data comparison_data &lt;- tibble( agent = sample_agents, true_bias = sample_data$bias, est_bias = bias_means[sample_agents], true_beta = sample_data$beta, est_beta = beta_means[sample_agents] ) # Plot comparison p1 &lt;- ggplot(comparison_data, aes(true_bias, est_bias)) + geom_point(size = 3) + geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;) + geom_smooth(method = lm) + ylim(-0.2, 1.1) + xlim(-0.2, 1.1) + labs(title = &quot;Bias Parameter Recovery&quot;, x = &quot;True Bias&quot;, y = &quot;Estimated Bias&quot;) + theme_classic() p2 &lt;- ggplot(comparison_data, aes(true_beta, est_beta)) + geom_point(size = 3) + geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;) + ylim(0.6, 2.4) + xlim(0.6, 2.4) + labs(title = &quot;Beta Parameter Recovery&quot;, x = &quot;True Beta&quot;, y = &quot;Estimated Beta&quot;) + theme_classic() # Display parameter recovery plots p1 + p2 7.12.2 Diagnosing the issue with bias and beta # First, let&#39;s examine the individual bias parameters distribution bias_params &lt;- draws_df %&gt;% dplyr::select(starts_with(&quot;bias[&quot;)) %&gt;% pivot_longer(everything(), names_to = &quot;parameter&quot;, values_to = &quot;value&quot;) # Plot distribution of all individual bias parameters ggplot(bias_params, aes(x = value)) + geom_histogram(bins = 30) + labs(title = &quot;Distribution of Individual Bias Parameters&quot;, x = &quot;Bias Value (log-odds scale)&quot;, y = &quot;Count&quot;) + theme_classic() # Check for correlation between bias and beta parameters # For a few sample agents cors &lt;- NULL for (i in sample(1:agents)) { cors[i] &lt;- (cor(draws_df[[paste0(&quot;bias[&quot;, i, &quot;]&quot;)]], draws_df[[paste0(&quot;beta[&quot;, i, &quot;]&quot;)]])) } corr_data &lt;- tibble( agent = seq_along(cors), correlation = cors ) %&gt;% filter(!is.na(correlation)) # Calculate summary statistics mean_cor &lt;- mean(corr_data$correlation) median_cor &lt;- median(corr_data$correlation) min_cor &lt;- min(corr_data$correlation) max_cor &lt;- max(corr_data$correlation) # Create correlation strength categories corr_data &lt;- corr_data %&gt;% mutate( strength = case_when( correlation &lt;= -0.7 ~ &quot;Strong negative&quot;, correlation &lt;= -0.5 ~ &quot;Moderate negative&quot;, correlation &lt;= -0.3 ~ &quot;Weak negative&quot;, correlation &lt; 0 ~ &quot;Negligible negative&quot;, correlation &gt;= 0 &amp; correlation &lt; 0.3 ~ &quot;Negligible positive&quot;, correlation &gt;= 0.3 &amp; correlation &lt; 0.5 ~ &quot;Weak positive&quot;, correlation &gt;= 0.5 &amp; correlation &lt; 0.7 ~ &quot;Moderate positive&quot;, TRUE ~ &quot;Strong positive&quot; ), strength = factor(strength, levels = c( &quot;Strong negative&quot;, &quot;Moderate negative&quot;, &quot;Weak negative&quot;, &quot;Negligible negative&quot;, &quot;Negligible positive&quot;, &quot;Weak positive&quot;, &quot;Moderate positive&quot;, &quot;Strong positive&quot; )) ) # Create the plot ggplot(corr_data, aes(x = correlation)) + # Histogram colored by correlation strength geom_histogram(aes(fill = strength), bins = 30, alpha = 0.8, color = &quot;white&quot;) + # Density curve geom_density(color = &quot;black&quot;, linewidth = 1, alpha = 0.3) + # Reference lines geom_vline(xintercept = median_cor, linewidth = 1, linetype = &quot;dotted&quot;, color = &quot;black&quot;) + geom_vline(xintercept = 0, linewidth = 0.5, color = &quot;gray50&quot;) + # Annotations annotate(&quot;label&quot;, x = median_cor, y = Inf, label = paste(&quot;Median:&quot;, round(median_cor, 2)), vjust = 3, size = 4, fill = &quot;lightblue&quot;, alpha = 0.8) + # Colors for correlation strength scale_fill_manual(values = c( &quot;Strong negative&quot; = &quot;#d73027&quot;, &quot;Moderate negative&quot; = &quot;#fc8d59&quot;, &quot;Weak negative&quot; = &quot;#fee090&quot;, &quot;Negligible negative&quot; = &quot;#ffffbf&quot;, &quot;Negligible positive&quot; = &quot;#e0f3f8&quot;, &quot;Weak positive&quot; = &quot;#91bfdb&quot;, &quot;Moderate positive&quot; = &quot;#4575b4&quot;, &quot;Strong positive&quot; = &quot;#313695&quot; )) + # Titles and labels labs( title = &quot;Distribution of Bias-Beta Parameter Correlations&quot;, subtitle = paste0( &quot;Analysis of &quot;, nrow(corr_data), &quot; agents shows consistent negative correlation\\n&quot;, &quot;Range: [&quot;, round(min_cor, 2), &quot;, &quot;, round(max_cor, 2), &quot;]&quot; ), x = &quot;Correlation Coefficient&quot;, y = &quot;Count&quot;, fill = &quot;Correlation Strength&quot; ) + # Set axis limits and theme xlim(-1, 1) + theme_minimal() 7.12.3 Multilevel memory with non centered parameterization When implementing multilevel models, we sometimes encounter sampling efficiency issues, especially when group-level variance parameters are small or data is limited. This creates a “funnel” in the posterior distribution that’s difficult for the sampler to navigate efficiently. Non-centered parameterization addresses this by reparameterizing individual parameters as standardized deviations from the group mean: Instead of: θᵢ ~ Normal(μ, σ) We use: θᵢ = μ + σ · zᵢ, where zᵢ ~ Normal(0, 1) This is conceptually similar to when we z-score variables in regression models. This approach separates the sampling of the standardized individual parameters (zᵢ) from the group-level parameters (μ and σ), improving sampling efficiency. The transformation between these parameterizations is invertible, so the models are equivalent, but the non-centered version often performs better computationally. In our code, we implement this by: Sampling standardized individual parameters (biasID_z, betaID_z) Multiplying by group SD and adding group mean to get individual parameters # Stan model for multilevel memory agent with non-centered parameterization stan_model_nc &lt;- &quot; // Multilevel Memory Agent Model (Non-Centered Parameterization) // functions{ real normal_lb_rng(real mu, real sigma, real lb) { real p = normal_cdf(lb | mu, sigma); // cdf for bounds real u = uniform_rng(p, 1); return (sigma * inv_Phi(u)) + mu; // inverse cdf for value } } // The input data for the model data { int&lt;lower = 1&gt; trials; // Number of trials per agent int&lt;lower = 1&gt; agents; // Number of agents array[trials, agents] int h; // Memory agent choices array[trials, agents] int other; // Opponent (random agent) choices } // Parameters to be estimated parameters { // Population-level parameters real biasM; // Mean of baseline bias real&lt;lower = 0&gt; biasSD; // SD of baseline bias real betaM; // Mean of memory sensitivity real&lt;lower = 0&gt; betaSD; // SD of memory sensitivity // Standardized individual parameters (non-centered parameterization) vector[agents] biasID_z; // Standardized individual bias parameters vector[agents] betaID_z; // Standardized individual beta parameters } // Transformed parameters (derived quantities) transformed parameters { // Memory state for each agent and trial array[trials, agents] real memory; // Individual parameters (constructed from non-centered parameterization) vector[agents] biasID; vector[agents] betaID; // Calculate memory states based on opponent&#39;s choices for (agent in 1:agents){ for (trial in 1:trials){ // Initial memory state (no prior information) if (trial == 1) { memory[trial, agent] = 0.5; } // Update memory based on opponent&#39;s choices if (trial &lt; trials){ // Simple averaging memory update memory[trial + 1, agent] = memory[trial, agent] + ((other[trial, agent] - memory[trial, agent]) / trial); // Handle edge cases to avoid numerical issues if (memory[trial + 1, agent] == 0){memory[trial + 1, agent] = 0.01;} if (memory[trial + 1, agent] == 1){memory[trial + 1, agent] = 0.99;} } } } // Construct individual parameters from non-centered parameterization biasID = biasM + biasID_z * biasSD; betaID = betaM + betaID_z * betaSD; } // Model definition model { // Population-level priors target += normal_lpdf(biasM | 0, 1); target += normal_lpdf(biasSD | 0, .3) - normal_lccdf(0 | 0, .3); // Half-normal target += normal_lpdf(betaM | 0, .3); target += normal_lpdf(betaSD | 0, .3) - normal_lccdf(0 | 0, .3); // Half-normal // Standardized individual parameters (non-centered parameterization) target += std_normal_lpdf(biasID_z); // Standard normal prior for z-scores target += std_normal_lpdf(betaID_z); // Standard normal prior for z-scores // Likelihood for (agent in 1:agents){ for (trial in 1:trials){ target += bernoulli_logit_lpmf(h[trial,agent] | biasID[agent] + logit(memory[trial, agent]) * betaID[agent]); } } } // Generated quantities for model checking and predictions generated quantities{ // Prior samples for checking real biasM_prior; real&lt;lower=0&gt; biasSD_prior; real betaM_prior; real&lt;lower=0&gt; betaSD_prior; real bias_prior; real beta_prior; // Predictive simulations with different memory values (individual level) array[agents] int&lt;lower=0, upper = trials&gt; prior_preds0; // No memory effect (memory=0) array[agents] int&lt;lower=0, upper = trials&gt; prior_preds1; // Neutral memory (memory=0.5) array[agents] int&lt;lower=0, upper = trials&gt; prior_preds2; // Strong memory (memory=1) array[agents] int&lt;lower=0, upper = trials&gt; posterior_preds0; array[agents] int&lt;lower=0, upper = trials&gt; posterior_preds1; array[agents] int&lt;lower=0, upper = trials&gt; posterior_preds2; // Generate prior samples biasM_prior = normal_rng(0,1); biasSD_prior = normal_lb_rng(0,0.3,0); betaM_prior = normal_rng(0,1); betaSD_prior = normal_lb_rng(0,0.3,0); bias_prior = normal_rng(biasM_prior, biasSD_prior); beta_prior = normal_rng(betaM_prior, betaSD_prior); // Generate predictions for each agent for (agent in 1:agents){ // Prior predictive checks prior_preds0[agent] = binomial_rng(trials, inv_logit(bias_prior + 0 * beta_prior)); prior_preds1[agent] = binomial_rng(trials, inv_logit(bias_prior + 1 * beta_prior)); prior_preds2[agent] = binomial_rng(trials, inv_logit(bias_prior + 2 * beta_prior)); // Posterior predictive checks posterior_preds0[agent] = binomial_rng(trials, inv_logit(biasM + biasID[agent] + 0 * (betaM + betaID[agent]))); posterior_preds1[agent] = binomial_rng(trials, inv_logit(biasM + biasID[agent] + 1 * (betaM + betaID[agent]))); posterior_preds2[agent] = binomial_rng(trials, inv_logit(biasM + biasID[agent] + 2 * (betaM + betaID[agent]))); } } &quot; # Write the Stan model to a file write_stan_file( stan_model_nc, dir = &quot;stan/&quot;, basename = &quot;W6_MultilevelMemory_nc.stan&quot; ) # File path for saved model model_file &lt;- &quot;simmodels/W6_MultilevelMemory_noncentered.RDS&quot; # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(model_file)) { # Compile the model file &lt;- file.path(&quot;stan/W6_MultilevelMemory_nc.stan&quot;) mod_nc &lt;- cmdstan_model(file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) # Sample from the posterior distribution samples_mlvl_nc &lt;- mod_nc$sample( data = data_memory, seed = 123, chains = 2, parallel_chains = 2, threads_per_chain = 1, iter_warmup = 2000, iter_sampling = 2000, refresh = 500, max_treedepth = 20, adapt_delta = 0.99 ) # Save the model results samples_mlvl_nc$save_object(file = model_file) cat(&quot;Generated new model fit and saved to&quot;, model_file, &quot;\\n&quot;) } else { # Load existing results samples_mlvl_nc &lt;- readRDS(model_file) cat(&quot;Loaded existing model fit from&quot;, model_file, &quot;\\n&quot;) } 7.12.4 Assessing multilevel memory # Check if samples_biased exists if (!exists(&quot;samples_mlvl_nc&quot;)) { cat(&quot;Loading multilevel non centered model samples...\\n&quot;) samples_mlvl_nc &lt;- readRDS(&quot;simmodels/W6_MultilevelMemory_noncentered.RDS&quot;) cat(&quot;Available parameters:&quot;, paste(colnames(as_draws_df(samples_mlvl_nc$draws())), collapse=&quot;, &quot;), &quot;\\n&quot;) } ## Loading multilevel non centered model samples... ## Available parameters: lp__, biasM, biasSD, betaM, betaSD, biasID_z[1], biasID_z[2], biasID_z[3], biasID_z[4], biasID_z[5], biasID_z[6], biasID_z[7], biasID_z[8], biasID_z[9], biasID_z[10], biasID_z[11], biasID_z[12], biasID_z[13], biasID_z[14], biasID_z[15], biasID_z[16], biasID_z[17], biasID_z[18], biasID_z[19], biasID_z[20], biasID_z[21], biasID_z[22], biasID_z[23], biasID_z[24], biasID_z[25], biasID_z[26], biasID_z[27], biasID_z[28], biasID_z[29], biasID_z[30], biasID_z[31], biasID_z[32], biasID_z[33], biasID_z[34], biasID_z[35], biasID_z[36], biasID_z[37], biasID_z[38], biasID_z[39], biasID_z[40], biasID_z[41], biasID_z[42], biasID_z[43], biasID_z[44], biasID_z[45], biasID_z[46], biasID_z[47], biasID_z[48], biasID_z[49], biasID_z[50], biasID_z[51], biasID_z[52], biasID_z[53], biasID_z[54], biasID_z[55], biasID_z[56], biasID_z[57], biasID_z[58], biasID_z[59], biasID_z[60], biasID_z[61], biasID_z[62], biasID_z[63], biasID_z[64], biasID_z[65], biasID_z[66], biasID_z[67], biasID_z[68], biasID_z[69], biasID_z[70], biasID_z[71], biasID_z[72], biasID_z[73], biasID_z[74], biasID_z[75], biasID_z[76], biasID_z[77], biasID_z[78], biasID_z[79], biasID_z[80], biasID_z[81], biasID_z[82], biasID_z[83], biasID_z[84], biasID_z[85], biasID_z[86], biasID_z[87], biasID_z[88], biasID_z[89], biasID_z[90], biasID_z[91], biasID_z[92], biasID_z[93], biasID_z[94], biasID_z[95], biasID_z[96], biasID_z[97], biasID_z[98], biasID_z[99], biasID_z[100], betaID_z[1], betaID_z[2], betaID_z[3], betaID_z[4], betaID_z[5], betaID_z[6], betaID_z[7], betaID_z[8], betaID_z[9], betaID_z[10], betaID_z[11], betaID_z[12], betaID_z[13], betaID_z[14], betaID_z[15], betaID_z[16], betaID_z[17], betaID_z[18], betaID_z[19], betaID_z[20], betaID_z[21], betaID_z[22], betaID_z[23], betaID_z[24], betaID_z[25], betaID_z[26], betaID_z[27], betaID_z[28], betaID_z[29], betaID_z[30], betaID_z[31], betaID_z[32], betaID_z[33], betaID_z[34], betaID_z[35], betaID_z[36], betaID_z[37], betaID_z[38], betaID_z[39], betaID_z[40], betaID_z[41], betaID_z[42], betaID_z[43], betaID_z[44], betaID_z[45], betaID_z[46], betaID_z[47], betaID_z[48], betaID_z[49], betaID_z[50], betaID_z[51], betaID_z[52], betaID_z[53], betaID_z[54], betaID_z[55], betaID_z[56], betaID_z[57], betaID_z[58], betaID_z[59], betaID_z[60], betaID_z[61], betaID_z[62], betaID_z[63], betaID_z[64], betaID_z[65], betaID_z[66], betaID_z[67], betaID_z[68], betaID_z[69], betaID_z[70], betaID_z[71], betaID_z[72], betaID_z[73], betaID_z[74], betaID_z[75], betaID_z[76], betaID_z[77], betaID_z[78], betaID_z[79], betaID_z[80], betaID_z[81], betaID_z[82], betaID_z[83], betaID_z[84], betaID_z[85], betaID_z[86], betaID_z[87], betaID_z[88], betaID_z[89], betaID_z[90], betaID_z[91], betaID_z[92], betaID_z[93], betaID_z[94], betaID_z[95], betaID_z[96], betaID_z[97], betaID_z[98], betaID_z[99], betaID_z[100], memory[1,1], memory[2,1], memory[3,1], memory[4,1], memory[5,1], memory[6,1], memory[7,1], memory[8,1], memory[9,1], memory[10,1], memory[11,1], memory[12,1], memory[13,1], memory[14,1], memory[15,1], memory[16,1], memory[17,1], memory[18,1], memory[19,1], memory[20,1], memory[21,1], memory[22,1], memory[23,1], memory[24,1], memory[25,1], memory[26,1], memory[27,1], memory[28,1], memory[29,1], memory[30,1], memory[31,1], memory[32,1], memory[33,1], memory[34,1], memory[35,1], memory[36,1], memory[37,1], memory[38,1], memory[39,1], memory[40,1], memory[41,1], memory[42,1], memory[43,1], memory[44,1], memory[45,1], memory[46,1], memory[47,1], memory[48,1], memory[49,1], memory[50,1], memory[51,1], memory[52,1], memory[53,1], memory[54,1], memory[55,1], memory[56,1], memory[57,1], memory[58,1], memory[59,1], memory[60,1], memory[61,1], memory[62,1], memory[63,1], memory[64,1], memory[65,1], memory[66,1], memory[67,1], memory[68,1], memory[69,1], memory[70,1], memory[71,1], memory[72,1], memory[73,1], memory[74,1], memory[75,1], memory[76,1], memory[77,1], memory[78,1], memory[79,1], memory[80,1], memory[81,1], memory[82,1], memory[83,1], memory[84,1], memory[85,1], memory[86,1], memory[87,1], memory[88,1], memory[89,1], memory[90,1], memory[91,1], memory[92,1], memory[93,1], memory[94,1], memory[95,1], memory[96,1], memory[97,1], memory[98,1], memory[99,1], memory[100,1], memory[101,1], memory[102,1], memory[103,1], memory[104,1], memory[105,1], memory[106,1], memory[107,1], memory[108,1], memory[109,1], memory[110,1], memory[111,1], memory[112,1], memory[113,1], memory[114,1], memory[115,1], memory[116,1], memory[117,1], memory[118,1], memory[119,1], memory[120,1], memory[1,2], memory[2,2], memory[3,2], memory[4,2], memory[5,2], memory[6,2], memory[7,2], memory[8,2], memory[9,2], memory[10,2], memory[11,2], memory[12,2], memory[13,2], memory[14,2], memory[15,2], memory[16,2], memory[17,2], memory[18,2], memory[19,2], memory[20,2], memory[21,2], memory[22,2], memory[23,2], memory[24,2], memory[25,2], memory[26,2], memory[27,2], memory[28,2], memory[29,2], memory[30,2], memory[31,2], memory[32,2], memory[33,2], memory[34,2], memory[35,2], memory[36,2], memory[37,2], memory[38,2], memory[39,2], memory[40,2], memory[41,2], memory[42,2], memory[43,2], memory[44,2], memory[45,2], memory[46,2], memory[47,2], memory[48,2], memory[49,2], memory[50,2], memory[51,2], memory[52,2], memory[53,2], memory[54,2], memory[55,2], memory[56,2], memory[57,2], memory[58,2], memory[59,2], memory[60,2], memory[61,2], memory[62,2], memory[63,2], memory[64,2], memory[65,2], memory[66,2], memory[67,2], memory[68,2], memory[69,2], memory[70,2], memory[71,2], memory[72,2], memory[73,2], memory[74,2], memory[75,2], memory[76,2], memory[77,2], memory[78,2], memory[79,2], memory[80,2], memory[81,2], memory[82,2], memory[83,2], memory[84,2], memory[85,2], memory[86,2], memory[87,2], memory[88,2], memory[89,2], memory[90,2], memory[91,2], memory[92,2], memory[93,2], memory[94,2], memory[95,2], memory[96,2], memory[97,2], memory[98,2], memory[99,2], memory[100,2], memory[101,2], memory[102,2], memory[103,2], memory[104,2], memory[105,2], memory[106,2], memory[107,2], memory[108,2], memory[109,2], memory[110,2], memory[111,2], memory[112,2], memory[113,2], memory[114,2], memory[115,2], memory[116,2], memory[117,2], memory[118,2], memory[119,2], memory[120,2], memory[1,3], memory[2,3], memory[3,3], memory[4,3], memory[5,3], memory[6,3], memory[7,3], memory[8,3], memory[9,3], memory[10,3], memory[11,3], memory[12,3], memory[13,3], memory[14,3], memory[15,3], memory[16,3], memory[17,3], memory[18,3], memory[19,3], memory[20,3], memory[21,3], memory[22,3], memory[23,3], memory[24,3], memory[25,3], memory[26,3], memory[27,3], memory[28,3], memory[29,3], memory[30,3], memory[31,3], memory[32,3], memory[33,3], memory[34,3], memory[35,3], memory[36,3], memory[37,3], memory[38,3], memory[39,3], memory[40,3], memory[41,3], memory[42,3], memory[43,3], memory[44,3], memory[45,3], memory[46,3], memory[47,3], memory[48,3], memory[49,3], memory[50,3], memory[51,3], memory[52,3], memory[53,3], memory[54,3], memory[55,3], memory[56,3], memory[57,3], memory[58,3], memory[59,3], memory[60,3], memory[61,3], memory[62,3], memory[63,3], memory[64,3], memory[65,3], memory[66,3], memory[67,3], memory[68,3], memory[69,3], memory[70,3], memory[71,3], memory[72,3], memory[73,3], memory[74,3], memory[75,3], memory[76,3], memory[77,3], memory[78,3], memory[79,3], memory[80,3], memory[81,3], memory[82,3], memory[83,3], memory[84,3], memory[85,3], memory[86,3], memory[87,3], memory[88,3], memory[89,3], memory[90,3], memory[91,3], memory[92,3], memory[93,3], memory[94,3], memory[95,3], memory[96,3], memory[97,3], memory[98,3], memory[99,3], memory[100,3], memory[101,3], memory[102,3], memory[103,3], memory[104,3], memory[105,3], memory[106,3], memory[107,3], memory[108,3], memory[109,3], memory[110,3], memory[111,3], memory[112,3], memory[113,3], memory[114,3], memory[115,3], memory[116,3], memory[117,3], memory[118,3], memory[119,3], memory[120,3], memory[1,4], memory[2,4], memory[3,4], memory[4,4], memory[5,4], memory[6,4], memory[7,4], memory[8,4], memory[9,4], memory[10,4], memory[11,4], memory[12,4], memory[13,4], memory[14,4], memory[15,4], memory[16,4], memory[17,4], memory[18,4], memory[19,4], memory[20,4], memory[21,4], memory[22,4], memory[23,4], memory[24,4], memory[25,4], memory[26,4], memory[27,4], memory[28,4], memory[29,4], memory[30,4], memory[31,4], memory[32,4], memory[33,4], memory[34,4], memory[35,4], memory[36,4], memory[37,4], memory[38,4], memory[39,4], memory[40,4], memory[41,4], memory[42,4], memory[43,4], memory[44,4], memory[45,4], memory[46,4], memory[47,4], memory[48,4], memory[49,4], memory[50,4], memory[51,4], memory[52,4], memory[53,4], memory[54,4], memory[55,4], memory[56,4], memory[57,4], memory[58,4], memory[59,4], memory[60,4], memory[61,4], memory[62,4], memory[63,4], memory[64,4], memory[65,4], memory[66,4], memory[67,4], memory[68,4], memory[69,4], memory[70,4], memory[71,4], memory[72,4], memory[73,4], memory[74,4], memory[75,4], memory[76,4], memory[77,4], memory[78,4], memory[79,4], memory[80,4], memory[81,4], memory[82,4], memory[83,4], memory[84,4], memory[85,4], memory[86,4], memory[87,4], memory[88,4], memory[89,4], memory[90,4], memory[91,4], memory[92,4], memory[93,4], memory[94,4], memory[95,4], memory[96,4], memory[97,4], memory[98,4], memory[99,4], memory[100,4], memory[101,4], memory[102,4], memory[103,4], memory[104,4], memory[105,4], memory[106,4], memory[107,4], memory[108,4], memory[109,4], memory[110,4], memory[111,4], memory[112,4], memory[113,4], memory[114,4], memory[115,4], memory[116,4], memory[117,4], memory[118,4], memory[119,4], memory[120,4], memory[1,5], memory[2,5], memory[3,5], memory[4,5], memory[5,5], memory[6,5], memory[7,5], memory[8,5], memory[9,5], memory[10,5], memory[11,5], memory[12,5], memory[13,5], memory[14,5], memory[15,5], memory[16,5], memory[17,5], memory[18,5], memory[19,5], memory[20,5], memory[21,5], memory[22,5], memory[23,5], memory[24,5], memory[25,5], memory[26,5], memory[27,5], memory[28,5], memory[29,5], memory[30,5], memory[31,5], memory[32,5], memory[33,5], memory[34,5], memory[35,5], memory[36,5], memory[37,5], memory[38,5], memory[39,5], memory[40,5], memory[41,5], memory[42,5], memory[43,5], memory[44,5], memory[45,5], memory[46,5], memory[47,5], memory[48,5], memory[49,5], memory[50,5], memory[51,5], memory[52,5], memory[53,5], memory[54,5], memory[55,5], memory[56,5], memory[57,5], memory[58,5], memory[59,5], memory[60,5], memory[61,5], memory[62,5], memory[63,5], memory[64,5], memory[65,5], memory[66,5], memory[67,5], memory[68,5], memory[69,5], memory[70,5], memory[71,5], memory[72,5], memory[73,5], memory[74,5], memory[75,5], memory[76,5], memory[77,5], memory[78,5], memory[79,5], memory[80,5], memory[81,5], memory[82,5], memory[83,5], memory[84,5], memory[85,5], memory[86,5], memory[87,5], memory[88,5], memory[89,5], memory[90,5], memory[91,5], memory[92,5], memory[93,5], memory[94,5], memory[95,5], memory[96,5], memory[97,5], memory[98,5], memory[99,5], memory[100,5], memory[101,5], memory[102,5], memory[103,5], memory[104,5], memory[105,5], memory[106,5], memory[107,5], memory[108,5], memory[109,5], memory[110,5], memory[111,5], memory[112,5], memory[113,5], memory[114,5], memory[115,5], memory[116,5], memory[117,5], memory[118,5], memory[119,5], memory[120,5], memory[1,6], memory[2,6], memory[3,6], memory[4,6], memory[5,6], memory[6,6], memory[7,6], memory[8,6], memory[9,6], memory[10,6], memory[11,6], memory[12,6], memory[13,6], memory[14,6], memory[15,6], memory[16,6], memory[17,6], memory[18,6], memory[19,6], memory[20,6], memory[21,6], memory[22,6], memory[23,6], memory[24,6], memory[25,6], memory[26,6], memory[27,6], memory[28,6], memory[29,6], memory[30,6], memory[31,6], memory[32,6], memory[33,6], memory[34,6], memory[35,6], memory[36,6], memory[37,6], memory[38,6], memory[39,6], memory[40,6], memory[41,6], memory[42,6], memory[43,6], memory[44,6], memory[45,6], memory[46,6], memory[47,6], memory[48,6], memory[49,6], memory[50,6], memory[51,6], memory[52,6], memory[53,6], memory[54,6], memory[55,6], memory[56,6], memory[57,6], memory[58,6], memory[59,6], memory[60,6], memory[61,6], memory[62,6], memory[63,6], memory[64,6], memory[65,6], memory[66,6], memory[67,6], memory[68,6], memory[69,6], memory[70,6], memory[71,6], memory[72,6], memory[73,6], memory[74,6], memory[75,6], memory[76,6], memory[77,6], memory[78,6], memory[79,6], memory[80,6], memory[81,6], memory[82,6], memory[83,6], memory[84,6], memory[85,6], memory[86,6], memory[87,6], memory[88,6], memory[89,6], memory[90,6], memory[91,6], memory[92,6], memory[93,6], memory[94,6], memory[95,6], memory[96,6], memory[97,6], memory[98,6], memory[99,6], memory[100,6], memory[101,6], memory[102,6], memory[103,6], memory[104,6], memory[105,6], memory[106,6], memory[107,6], memory[108,6], memory[109,6], memory[110,6], memory[111,6], memory[112,6], memory[113,6], memory[114,6], memory[115,6], memory[116,6], memory[117,6], memory[118,6], memory[119,6], memory[120,6], memory[1,7], memory[2,7], memory[3,7], memory[4,7], memory[5,7], memory[6,7], memory[7,7], memory[8,7], memory[9,7], memory[10,7], memory[11,7], memory[12,7], memory[13,7], memory[14,7], memory[15,7], memory[16,7], memory[17,7], memory[18,7], memory[19,7], memory[20,7], memory[21,7], memory[22,7], memory[23,7], memory[24,7], memory[25,7], memory[26,7], memory[27,7], memory[28,7], memory[29,7], memory[30,7], memory[31,7], memory[32,7], memory[33,7], memory[34,7], memory[35,7], memory[36,7], memory[37,7], memory[38,7], memory[39,7], memory[40,7], memory[41,7], memory[42,7], memory[43,7], memory[44,7], memory[45,7], memory[46,7], memory[47,7], memory[48,7], memory[49,7], memory[50,7], memory[51,7], memory[52,7], memory[53,7], memory[54,7], memory[55,7], memory[56,7], memory[57,7], memory[58,7], memory[59,7], memory[60,7], memory[61,7], memory[62,7], memory[63,7], memory[64,7], memory[65,7], memory[66,7], memory[67,7], memory[68,7], memory[69,7], memory[70,7], memory[71,7], memory[72,7], memory[73,7], memory[74,7], memory[75,7], memory[76,7], memory[77,7], memory[78,7], memory[79,7], memory[80,7], memory[81,7], memory[82,7], memory[83,7], memory[84,7], memory[85,7], memory[86,7], memory[87,7], memory[88,7], memory[89,7], memory[90,7], memory[91,7], memory[92,7], memory[93,7], memory[94,7], memory[95,7], memory[96,7], memory[97,7], memory[98,7], memory[99,7], memory[100,7], memory[101,7], memory[102,7], memory[103,7], memory[104,7], memory[105,7], memory[106,7], memory[107,7], memory[108,7], memory[109,7], memory[110,7], memory[111,7], memory[112,7], memory[113,7], memory[114,7], memory[115,7], memory[116,7], memory[117,7], memory[118,7], memory[119,7], memory[120,7], memory[1,8], memory[2,8], memory[3,8], memory[4,8], memory[5,8], memory[6,8], memory[7,8], memory[8,8], memory[9,8], memory[10,8], memory[11,8], memory[12,8], memory[13,8], memory[14,8], memory[15,8], memory[16,8], memory[17,8], memory[18,8], memory[19,8], memory[20,8], memory[21,8], memory[22,8], memory[23,8], memory[24,8], memory[25,8], memory[26,8], memory[27,8], memory[28,8], memory[29,8], memory[30,8], memory[31,8], memory[32,8], memory[33,8], memory[34,8], memory[35,8], memory[36,8], memory[37,8], memory[38,8], memory[39,8], memory[40,8], memory[41,8], memory[42,8], memory[43,8], memory[44,8], memory[45,8], memory[46,8], memory[47,8], memory[48,8], memory[49,8], memory[50,8], memory[51,8], memory[52,8], memory[53,8], memory[54,8], memory[55,8], memory[56,8], memory[57,8], memory[58,8], memory[59,8], memory[60,8], memory[61,8], memory[62,8], memory[63,8], memory[64,8], memory[65,8], memory[66,8], memory[67,8], memory[68,8], memory[69,8], memory[70,8], memory[71,8], memory[72,8], memory[73,8], memory[74,8], memory[75,8], memory[76,8], memory[77,8], memory[78,8], memory[79,8], memory[80,8], memory[81,8], memory[82,8], memory[83,8], memory[84,8], memory[85,8], memory[86,8], memory[87,8], memory[88,8], memory[89,8], memory[90,8], memory[91,8], memory[92,8], memory[93,8], memory[94,8], memory[95,8], memory[96,8], memory[97,8], memory[98,8], memory[99,8], memory[100,8], memory[101,8], memory[102,8], memory[103,8], memory[104,8], memory[105,8], memory[106,8], memory[107,8], memory[108,8], memory[109,8], memory[110,8], memory[111,8], memory[112,8], memory[113,8], memory[114,8], memory[115,8], memory[116,8], memory[117,8], memory[118,8], memory[119,8], memory[120,8], memory[1,9], memory[2,9], memory[3,9], memory[4,9], memory[5,9], memory[6,9], memory[7,9], memory[8,9], memory[9,9], memory[10,9], memory[11,9], memory[12,9], memory[13,9], memory[14,9], memory[15,9], memory[16,9], memory[17,9], memory[18,9], memory[19,9], memory[20,9], memory[21,9], memory[22,9], memory[23,9], memory[24,9], memory[25,9], memory[26,9], memory[27,9], memory[28,9], memory[29,9], memory[30,9], memory[31,9], memory[32,9], memory[33,9], memory[34,9], memory[35,9], memory[36,9], memory[37,9], memory[38,9], memory[39,9], memory[40,9], memory[41,9], memory[42,9], memory[43,9], memory[44,9], memory[45,9], memory[46,9], memory[47,9], memory[48,9], memory[49,9], memory[50,9], memory[51,9], memory[52,9], memory[53,9], memory[54,9], memory[55,9], memory[56,9], memory[57,9], memory[58,9], memory[59,9], memory[60,9], memory[61,9], memory[62,9], memory[63,9], memory[64,9], memory[65,9], memory[66,9], memory[67,9], memory[68,9], memory[69,9], memory[70,9], memory[71,9], memory[72,9], memory[73,9], memory[74,9], memory[75,9], memory[76,9], memory[77,9], memory[78,9], memory[79,9], memory[80,9], memory[81,9], memory[82,9], memory[83,9], memory[84,9], memory[85,9], memory[86,9], memory[87,9], memory[88,9], memory[89,9], memory[90,9], memory[91,9], memory[92,9], memory[93,9], memory[94,9], memory[95,9], memory[96,9], memory[97,9], memory[98,9], memory[99,9], memory[100,9], memory[101,9], memory[102,9], memory[103,9], memory[104,9], memory[105,9], memory[106,9], memory[107,9], memory[108,9], memory[109,9], memory[110,9], memory[111,9], memory[112,9], memory[113,9], memory[114,9], memory[115,9], memory[116,9], memory[117,9], memory[118,9], memory[119,9], memory[120,9], memory[1,10], memory[2,10], memory[3,10], memory[4,10], memory[5,10], memory[6,10], memory[7,10], memory[8,10], memory[9,10], memory[10,10], memory[11,10], memory[12,10], memory[13,10], memory[14,10], memory[15,10], memory[16,10], memory[17,10], memory[18,10], memory[19,10], memory[20,10], memory[21,10], memory[22,10], memory[23,10], memory[24,10], memory[25,10], memory[26,10], memory[27,10], memory[28,10], memory[29,10], memory[30,10], memory[31,10], memory[32,10], memory[33,10], memory[34,10], memory[35,10], memory[36,10], memory[37,10], memory[38,10], memory[39,10], memory[40,10], memory[41,10], memory[42,10], memory[43,10], memory[44,10], memory[45,10], memory[46,10], memory[47,10], memory[48,10], memory[49,10], memory[50,10], memory[51,10], memory[52,10], memory[53,10], memory[54,10], memory[55,10], memory[56,10], memory[57,10], memory[58,10], memory[59,10], memory[60,10], memory[61,10], memory[62,10], memory[63,10], memory[64,10], memory[65,10], memory[66,10], memory[67,10], memory[68,10], memory[69,10], memory[70,10], memory[71,10], memory[72,10], memory[73,10], memory[74,10], memory[75,10], memory[76,10], memory[77,10], memory[78,10], memory[79,10], memory[80,10], memory[81,10], memory[82,10], memory[83,10], memory[84,10], memory[85,10], memory[86,10], memory[87,10], memory[88,10], memory[89,10], memory[90,10], memory[91,10], memory[92,10], memory[93,10], memory[94,10], memory[95,10], memory[96,10], memory[97,10], memory[98,10], memory[99,10], memory[100,10], memory[101,10], memory[102,10], memory[103,10], memory[104,10], memory[105,10], memory[106,10], memory[107,10], memory[108,10], memory[109,10], memory[110,10], memory[111,10], memory[112,10], memory[113,10], memory[114,10], memory[115,10], memory[116,10], memory[117,10], memory[118,10], memory[119,10], memory[120,10], memory[1,11], memory[2,11], memory[3,11], memory[4,11], memory[5,11], memory[6,11], memory[7,11], memory[8,11], memory[9,11], memory[10,11], memory[11,11], memory[12,11], memory[13,11], memory[14,11], memory[15,11], memory[16,11], memory[17,11], memory[18,11], memory[19,11], memory[20,11], memory[21,11], memory[22,11], memory[23,11], memory[24,11], memory[25,11], memory[26,11], memory[27,11], memory[28,11], memory[29,11], memory[30,11], memory[31,11], memory[32,11], memory[33,11], memory[34,11], memory[35,11], memory[36,11], memory[37,11], memory[38,11], memory[39,11], memory[40,11], memory[41,11], memory[42,11], memory[43,11], memory[44,11], memory[45,11], memory[46,11], memory[47,11], memory[48,11], memory[49,11], memory[50,11], memory[51,11], memory[52,11], memory[53,11], memory[54,11], memory[55,11], memory[56,11], memory[57,11], memory[58,11], memory[59,11], memory[60,11], memory[61,11], memory[62,11], memory[63,11], memory[64,11], memory[65,11], memory[66,11], memory[67,11], memory[68,11], memory[69,11], memory[70,11], memory[71,11], memory[72,11], memory[73,11], memory[74,11], memory[75,11], memory[76,11], memory[77,11], memory[78,11], memory[79,11], memory[80,11], memory[81,11], memory[82,11], memory[83,11], memory[84,11], memory[85,11], memory[86,11], memory[87,11], memory[88,11], memory[89,11], memory[90,11], memory[91,11], memory[92,11], memory[93,11], memory[94,11], memory[95,11], memory[96,11], memory[97,11], memory[98,11], memory[99,11], memory[100,11], memory[101,11], memory[102,11], memory[103,11], memory[104,11], memory[105,11], memory[106,11], memory[107,11], memory[108,11], memory[109,11], memory[110,11], memory[111,11], memory[112,11], memory[113,11], memory[114,11], memory[115,11], memory[116,11], memory[117,11], memory[118,11], memory[119,11], memory[120,11], memory[1,12], memory[2,12], memory[3,12], memory[4,12], memory[5,12], memory[6,12], memory[7,12], memory[8,12], memory[9,12], memory[10,12], memory[11,12], memory[12,12], memory[13,12], memory[14,12], memory[15,12], memory[16,12], memory[17,12], memory[18,12], memory[19,12], memory[20,12], memory[21,12], memory[22,12], memory[23,12], memory[24,12], memory[25,12], memory[26,12], memory[27,12], memory[28,12], memory[29,12], memory[30,12], memory[31,12], memory[32,12], memory[33,12], memory[34,12], memory[35,12], memory[36,12], memory[37,12], memory[38,12], memory[39,12], memory[40,12], memory[41,12], memory[42,12], memory[43,12], memory[44,12], memory[45,12], memory[46,12], memory[47,12], memory[48,12], memory[49,12], memory[50,12], memory[51,12], memory[52,12], memory[53,12], memory[54,12], memory[55,12], memory[56,12], memory[57,12], memory[58,12], memory[59,12], memory[60,12], memory[61,12], memory[62,12], memory[63,12], memory[64,12], memory[65,12], memory[66,12], memory[67,12], memory[68,12], memory[69,12], memory[70,12], memory[71,12], memory[72,12], memory[73,12], memory[74,12], memory[75,12], memory[76,12], memory[77,12], memory[78,12], memory[79,12], memory[80,12], memory[81,12], memory[82,12], memory[83,12], memory[84,12], memory[85,12], memory[86,12], memory[87,12], memory[88,12], memory[89,12], memory[90,12], memory[91,12], memory[92,12], memory[93,12], memory[94,12], memory[95,12], memory[96,12], memory[97,12], memory[98,12], memory[99,12], memory[100,12], memory[101,12], memory[102,12], memory[103,12], memory[104,12], memory[105,12], memory[106,12], memory[107,12], memory[108,12], memory[109,12], memory[110,12], memory[111,12], memory[112,12], memory[113,12], memory[114,12], memory[115,12], memory[116,12], memory[117,12], memory[118,12], memory[119,12], memory[120,12], memory[1,13], memory[2,13], memory[3,13], memory[4,13], memory[5,13], memory[6,13], memory[7,13], memory[8,13], memory[9,13], memory[10,13], memory[11,13], memory[12,13], memory[13,13], memory[14,13], memory[15,13], memory[16,13], memory[17,13], memory[18,13], memory[19,13], memory[20,13], memory[21,13], memory[22,13], memory[23,13], memory[24,13], memory[25,13], memory[26,13], memory[27,13], memory[28,13], memory[29,13], memory[30,13], memory[31,13], memory[32,13], memory[33,13], memory[34,13], memory[35,13], memory[36,13], memory[37,13], memory[38,13], memory[39,13], memory[40,13], memory[41,13], memory[42,13], memory[43,13], memory[44,13], memory[45,13], memory[46,13], memory[47,13], memory[48,13], memory[49,13], memory[50,13], memory[51,13], memory[52,13], memory[53,13], memory[54,13], memory[55,13], memory[56,13], memory[57,13], memory[58,13], memory[59,13], memory[60,13], memory[61,13], memory[62,13], memory[63,13], memory[64,13], memory[65,13], memory[66,13], memory[67,13], memory[68,13], memory[69,13], memory[70,13], memory[71,13], memory[72,13], memory[73,13], memory[74,13], memory[75,13], memory[76,13], memory[77,13], memory[78,13], memory[79,13], memory[80,13], memory[81,13], memory[82,13], memory[83,13], memory[84,13], memory[85,13], memory[86,13], memory[87,13], memory[88,13], memory[89,13], memory[90,13], memory[91,13], memory[92,13], memory[93,13], memory[94,13], memory[95,13], memory[96,13], memory[97,13], memory[98,13], memory[99,13], memory[100,13], memory[101,13], memory[102,13], memory[103,13], memory[104,13], memory[105,13], memory[106,13], memory[107,13], memory[108,13], memory[109,13], memory[110,13], memory[111,13], memory[112,13], memory[113,13], memory[114,13], memory[115,13], memory[116,13], memory[117,13], memory[118,13], memory[119,13], memory[120,13], memory[1,14], memory[2,14], memory[3,14], memory[4,14], memory[5,14], memory[6,14], memory[7,14], memory[8,14], memory[9,14], memory[10,14], memory[11,14], memory[12,14], memory[13,14], memory[14,14], memory[15,14], memory[16,14], memory[17,14], memory[18,14], memory[19,14], memory[20,14], memory[21,14], memory[22,14], memory[23,14], memory[24,14], memory[25,14], memory[26,14], memory[27,14], memory[28,14], memory[29,14], memory[30,14], memory[31,14], memory[32,14], memory[33,14], memory[34,14], memory[35,14], memory[36,14], memory[37,14], memory[38,14], memory[39,14], memory[40,14], memory[41,14], memory[42,14], memory[43,14], memory[44,14], memory[45,14], memory[46,14], memory[47,14], memory[48,14], memory[49,14], memory[50,14], memory[51,14], memory[52,14], memory[53,14], memory[54,14], memory[55,14], memory[56,14], memory[57,14], memory[58,14], memory[59,14], memory[60,14], memory[61,14], memory[62,14], memory[63,14], memory[64,14], memory[65,14], memory[66,14], memory[67,14], memory[68,14], memory[69,14], memory[70,14], memory[71,14], memory[72,14], memory[73,14], memory[74,14], memory[75,14], memory[76,14], memory[77,14], memory[78,14], memory[79,14], memory[80,14], memory[81,14], memory[82,14], memory[83,14], memory[84,14], memory[85,14], memory[86,14], memory[87,14], memory[88,14], memory[89,14], memory[90,14], memory[91,14], memory[92,14], memory[93,14], memory[94,14], memory[95,14], memory[96,14], memory[97,14], memory[98,14], memory[99,14], memory[100,14], memory[101,14], memory[102,14], memory[103,14], memory[104,14], memory[105,14], memory[106,14], memory[107,14], memory[108,14], memory[109,14], memory[110,14], memory[111,14], memory[112,14], memory[113,14], memory[114,14], memory[115,14], memory[116,14], memory[117,14], memory[118,14], memory[119,14], memory[120,14], memory[1,15], memory[2,15], memory[3,15], memory[4,15], memory[5,15], memory[6,15], memory[7,15], memory[8,15], memory[9,15], memory[10,15], memory[11,15], memory[12,15], memory[13,15], memory[14,15], memory[15,15], memory[16,15], memory[17,15], memory[18,15], memory[19,15], memory[20,15], memory[21,15], memory[22,15], memory[23,15], memory[24,15], memory[25,15], memory[26,15], memory[27,15], memory[28,15], memory[29,15], memory[30,15], memory[31,15], memory[32,15], memory[33,15], memory[34,15], memory[35,15], memory[36,15], memory[37,15], memory[38,15], memory[39,15], memory[40,15], memory[41,15], memory[42,15], memory[43,15], memory[44,15], memory[45,15], memory[46,15], memory[47,15], memory[48,15], memory[49,15], memory[50,15], memory[51,15], memory[52,15], memory[53,15], memory[54,15], memory[55,15], memory[56,15], memory[57,15], memory[58,15], memory[59,15], memory[60,15], memory[61,15], memory[62,15], memory[63,15], memory[64,15], memory[65,15], memory[66,15], memory[67,15], memory[68,15], memory[69,15], memory[70,15], memory[71,15], memory[72,15], memory[73,15], memory[74,15], memory[75,15], memory[76,15], memory[77,15], memory[78,15], memory[79,15], memory[80,15], memory[81,15], memory[82,15], memory[83,15], memory[84,15], memory[85,15], memory[86,15], memory[87,15], memory[88,15], memory[89,15], memory[90,15], memory[91,15], memory[92,15], memory[93,15], memory[94,15], memory[95,15], memory[96,15], memory[97,15], memory[98,15], memory[99,15], memory[100,15], memory[101,15], memory[102,15], memory[103,15], memory[104,15], memory[105,15], memory[106,15], memory[107,15], memory[108,15], memory[109,15], memory[110,15], memory[111,15], memory[112,15], memory[113,15], memory[114,15], memory[115,15], memory[116,15], memory[117,15], memory[118,15], memory[119,15], memory[120,15], memory[1,16], memory[2,16], memory[3,16], memory[4,16], memory[5,16], memory[6,16], memory[7,16], memory[8,16], memory[9,16], memory[10,16], memory[11,16], memory[12,16], memory[13,16], memory[14,16], memory[15,16], memory[16,16], memory[17,16], memory[18,16], memory[19,16], memory[20,16], memory[21,16], memory[22,16], memory[23,16], memory[24,16], memory[25,16], memory[26,16], memory[27,16], memory[28,16], memory[29,16], memory[30,16], memory[31,16], memory[32,16], memory[33,16], memory[34,16], memory[35,16], memory[36,16], memory[37,16], memory[38,16], memory[39,16], memory[40,16], memory[41,16], memory[42,16], memory[43,16], memory[44,16], memory[45,16], memory[46,16], memory[47,16], memory[48,16], memory[49,16], memory[50,16], memory[51,16], memory[52,16], memory[53,16], memory[54,16], memory[55,16], memory[56,16], memory[57,16], memory[58,16], memory[59,16], memory[60,16], memory[61,16], memory[62,16], memory[63,16], memory[64,16], memory[65,16], memory[66,16], memory[67,16], memory[68,16], memory[69,16], memory[70,16], memory[71,16], memory[72,16], memory[73,16], memory[74,16], memory[75,16], memory[76,16], memory[77,16], memory[78,16], memory[79,16], memory[80,16], memory[81,16], memory[82,16], memory[83,16], memory[84,16], memory[85,16], memory[86,16], memory[87,16], memory[88,16], memory[89,16], memory[90,16], memory[91,16], memory[92,16], memory[93,16], memory[94,16], memory[95,16], memory[96,16], memory[97,16], memory[98,16], memory[99,16], memory[100,16], memory[101,16], memory[102,16], memory[103,16], memory[104,16], memory[105,16], memory[106,16], memory[107,16], memory[108,16], memory[109,16], memory[110,16], memory[111,16], memory[112,16], memory[113,16], memory[114,16], memory[115,16], memory[116,16], memory[117,16], memory[118,16], memory[119,16], memory[120,16], memory[1,17], memory[2,17], memory[3,17], memory[4,17], memory[5,17], memory[6,17], memory[7,17], memory[8,17], memory[9,17], memory[10,17], memory[11,17], memory[12,17], memory[13,17], memory[14,17], memory[15,17], memory[16,17], memory[17,17], memory[18,17], memory[19,17], memory[20,17], memory[21,17], memory[22,17], memory[23,17], memory[24,17], memory[25,17], memory[26,17], memory[27,17], memory[28,17], memory[29,17], memory[30,17], memory[31,17], memory[32,17], memory[33,17], memory[34,17], memory[35,17], memory[36,17], memory[37,17], memory[38,17], memory[39,17], memory[40,17], memory[41,17], memory[42,17], memory[43,17], memory[44,17], memory[45,17], memory[46,17], memory[47,17], memory[48,17], memory[49,17], memory[50,17], memory[51,17], memory[52,17], memory[53,17], memory[54,17], memory[55,17], memory[56,17], memory[57,17], memory[58,17], memory[59,17], memory[60,17], memory[61,17], memory[62,17], memory[63,17], memory[64,17], memory[65,17], memory[66,17], memory[67,17], memory[68,17], memory[69,17], memory[70,17], memory[71,17], memory[72,17], memory[73,17], memory[74,17], memory[75,17], memory[76,17], memory[77,17], memory[78,17], memory[79,17], memory[80,17], memory[81,17], memory[82,17], memory[83,17], memory[84,17], memory[85,17], memory[86,17], memory[87,17], memory[88,17], memory[89,17], memory[90,17], memory[91,17], memory[92,17], memory[93,17], memory[94,17], memory[95,17], memory[96,17], memory[97,17], memory[98,17], memory[99,17], memory[100,17], memory[101,17], memory[102,17], memory[103,17], memory[104,17], memory[105,17], memory[106,17], memory[107,17], memory[108,17], memory[109,17], memory[110,17], memory[111,17], memory[112,17], memory[113,17], memory[114,17], memory[115,17], memory[116,17], memory[117,17], memory[118,17], memory[119,17], memory[120,17], memory[1,18], memory[2,18], memory[3,18], memory[4,18], memory[5,18], memory[6,18], memory[7,18], memory[8,18], memory[9,18], memory[10,18], memory[11,18], memory[12,18], memory[13,18], memory[14,18], memory[15,18], memory[16,18], memory[17,18], memory[18,18], memory[19,18], memory[20,18], memory[21,18], memory[22,18], memory[23,18], memory[24,18], memory[25,18], memory[26,18], memory[27,18], memory[28,18], memory[29,18], memory[30,18], memory[31,18], memory[32,18], memory[33,18], memory[34,18], memory[35,18], memory[36,18], memory[37,18], memory[38,18], memory[39,18], memory[40,18], memory[41,18], memory[42,18], memory[43,18], memory[44,18], memory[45,18], memory[46,18], memory[47,18], memory[48,18], memory[49,18], memory[50,18], memory[51,18], memory[52,18], memory[53,18], memory[54,18], memory[55,18], memory[56,18], memory[57,18], memory[58,18], memory[59,18], memory[60,18], memory[61,18], memory[62,18], memory[63,18], memory[64,18], memory[65,18], memory[66,18], memory[67,18], memory[68,18], memory[69,18], memory[70,18], memory[71,18], memory[72,18], memory[73,18], memory[74,18], memory[75,18], memory[76,18], memory[77,18], memory[78,18], memory[79,18], memory[80,18], memory[81,18], memory[82,18], memory[83,18], memory[84,18], memory[85,18], memory[86,18], memory[87,18], memory[88,18], memory[89,18], memory[90,18], memory[91,18], memory[92,18], memory[93,18], memory[94,18], memory[95,18], memory[96,18], memory[97,18], memory[98,18], memory[99,18], memory[100,18], memory[101,18], memory[102,18], memory[103,18], memory[104,18], memory[105,18], memory[106,18], memory[107,18], memory[108,18], memory[109,18], memory[110,18], memory[111,18], memory[112,18], memory[113,18], memory[114,18], memory[115,18], memory[116,18], memory[117,18], memory[118,18], memory[119,18], memory[120,18], memory[1,19], memory[2,19], memory[3,19], memory[4,19], memory[5,19], memory[6,19], memory[7,19], memory[8,19], memory[9,19], memory[10,19], memory[11,19], memory[12,19], memory[13,19], memory[14,19], memory[15,19], memory[16,19], memory[17,19], memory[18,19], memory[19,19], memory[20,19], memory[21,19], memory[22,19], memory[23,19], memory[24,19], memory[25,19], memory[26,19], memory[27,19], memory[28,19], memory[29,19], memory[30,19], memory[31,19], memory[32,19], memory[33,19], memory[34,19], memory[35,19], memory[36,19], memory[37,19], memory[38,19], memory[39,19], memory[40,19], memory[41,19], memory[42,19], memory[43,19], memory[44,19], memory[45,19], memory[46,19], memory[47,19], memory[48,19], memory[49,19], memory[50,19], memory[51,19], memory[52,19], memory[53,19], memory[54,19], memory[55,19], memory[56,19], memory[57,19], memory[58,19], memory[59,19], memory[60,19], memory[61,19], memory[62,19], memory[63,19], memory[64,19], memory[65,19], memory[66,19], memory[67,19], memory[68,19], memory[69,19], memory[70,19], memory[71,19], memory[72,19], memory[73,19], memory[74,19], memory[75,19], memory[76,19], memory[77,19], memory[78,19], memory[79,19], memory[80,19], memory[81,19], memory[82,19], memory[83,19], memory[84,19], memory[85,19], memory[86,19], memory[87,19], memory[88,19], memory[89,19], memory[90,19], memory[91,19], memory[92,19], memory[93,19], memory[94,19], memory[95,19], memory[96,19], memory[97,19], memory[98,19], memory[99,19], memory[100,19], memory[101,19], memory[102,19], memory[103,19], memory[104,19], memory[105,19], memory[106,19], memory[107,19], memory[108,19], memory[109,19], memory[110,19], memory[111,19], memory[112,19], memory[113,19], memory[114,19], memory[115,19], memory[116,19], memory[117,19], memory[118,19], memory[119,19], memory[120,19], memory[1,20], memory[2,20], memory[3,20], memory[4,20], memory[5,20], memory[6,20], memory[7,20], memory[8,20], memory[9,20], memory[10,20], memory[11,20], memory[12,20], memory[13,20], memory[14,20], memory[15,20], memory[16,20], memory[17,20], memory[18,20], memory[19,20], memory[20,20], memory[21,20], memory[22,20], memory[23,20], memory[24,20], memory[25,20], memory[26,20], memory[27,20], memory[28,20], memory[29,20], memory[30,20], memory[31,20], memory[32,20], memory[33,20], memory[34,20], memory[35,20], memory[36,20], memory[37,20], memory[38,20], memory[39,20], memory[40,20], memory[41,20], memory[42,20], memory[43,20], memory[44,20], memory[45,20], memory[46,20], memory[47,20], memory[48,20], memory[49,20], memory[50,20], memory[51,20], memory[52,20], memory[53,20], memory[54,20], memory[55,20], memory[56,20], memory[57,20], memory[58,20], memory[59,20], memory[60,20], memory[61,20], memory[62,20], memory[63,20], memory[64,20], memory[65,20], memory[66,20], memory[67,20], memory[68,20], memory[69,20], memory[70,20], memory[71,20], memory[72,20], memory[73,20], memory[74,20], memory[75,20], memory[76,20], memory[77,20], memory[78,20], memory[79,20], memory[80,20], memory[81,20], memory[82,20], memory[83,20], memory[84,20], memory[85,20], memory[86,20], memory[87,20], memory[88,20], memory[89,20], memory[90,20], memory[91,20], memory[92,20], memory[93,20], memory[94,20], memory[95,20], memory[96,20], memory[97,20], memory[98,20], memory[99,20], memory[100,20], memory[101,20], memory[102,20], memory[103,20], memory[104,20], memory[105,20], memory[106,20], memory[107,20], memory[108,20], memory[109,20], memory[110,20], memory[111,20], memory[112,20], memory[113,20], memory[114,20], memory[115,20], memory[116,20], memory[117,20], memory[118,20], memory[119,20], memory[120,20], memory[1,21], memory[2,21], memory[3,21], memory[4,21], memory[5,21], memory[6,21], memory[7,21], memory[8,21], memory[9,21], memory[10,21], memory[11,21], memory[12,21], memory[13,21], memory[14,21], memory[15,21], memory[16,21], memory[17,21], memory[18,21], memory[19,21], memory[20,21], memory[21,21], memory[22,21], memory[23,21], memory[24,21], memory[25,21], memory[26,21], memory[27,21], memory[28,21], memory[29,21], memory[30,21], memory[31,21], memory[32,21], memory[33,21], memory[34,21], memory[35,21], memory[36,21], memory[37,21], memory[38,21], memory[39,21], memory[40,21], memory[41,21], memory[42,21], memory[43,21], memory[44,21], memory[45,21], memory[46,21], memory[47,21], memory[48,21], memory[49,21], memory[50,21], memory[51,21], memory[52,21], memory[53,21], memory[54,21], memory[55,21], memory[56,21], memory[57,21], memory[58,21], memory[59,21], memory[60,21], memory[61,21], memory[62,21], memory[63,21], memory[64,21], memory[65,21], memory[66,21], memory[67,21], memory[68,21], memory[69,21], memory[70,21], memory[71,21], memory[72,21], memory[73,21], memory[74,21], memory[75,21], memory[76,21], memory[77,21], memory[78,21], memory[79,21], memory[80,21], memory[81,21], memory[82,21], memory[83,21], memory[84,21], memory[85,21], memory[86,21], memory[87,21], memory[88,21], memory[89,21], memory[90,21], memory[91,21], memory[92,21], memory[93,21], memory[94,21], memory[95,21], memory[96,21], memory[97,21], memory[98,21], memory[99,21], memory[100,21], memory[101,21], memory[102,21], memory[103,21], memory[104,21], memory[105,21], memory[106,21], memory[107,21], memory[108,21], memory[109,21], memory[110,21], memory[111,21], memory[112,21], memory[113,21], memory[114,21], memory[115,21], memory[116,21], memory[117,21], memory[118,21], memory[119,21], memory[120,21], memory[1,22], memory[2,22], memory[3,22], memory[4,22], memory[5,22], memory[6,22], memory[7,22], memory[8,22], memory[9,22], memory[10,22], memory[11,22], memory[12,22], memory[13,22], memory[14,22], memory[15,22], memory[16,22], memory[17,22], memory[18,22], memory[19,22], memory[20,22], memory[21,22], memory[22,22], memory[23,22], memory[24,22], memory[25,22], memory[26,22], memory[27,22], memory[28,22], memory[29,22], memory[30,22], memory[31,22], memory[32,22], memory[33,22], memory[34,22], memory[35,22], memory[36,22], memory[37,22], memory[38,22], memory[39,22], memory[40,22], memory[41,22], memory[42,22], memory[43,22], memory[44,22], memory[45,22], memory[46,22], memory[47,22], memory[48,22], memory[49,22], memory[50,22], memory[51,22], memory[52,22], memory[53,22], memory[54,22], memory[55,22], memory[56,22], memory[57,22], memory[58,22], memory[59,22], memory[60,22], memory[61,22], memory[62,22], memory[63,22], memory[64,22], memory[65,22], memory[66,22], memory[67,22], memory[68,22], memory[69,22], memory[70,22], memory[71,22], memory[72,22], memory[73,22], memory[74,22], memory[75,22], memory[76,22], memory[77,22], memory[78,22], memory[79,22], memory[80,22], memory[81,22], memory[82,22], memory[83,22], memory[84,22], memory[85,22], memory[86,22], memory[87,22], memory[88,22], memory[89,22], memory[90,22], memory[91,22], memory[92,22], memory[93,22], memory[94,22], memory[95,22], memory[96,22], memory[97,22], memory[98,22], memory[99,22], memory[100,22], memory[101,22], memory[102,22], memory[103,22], memory[104,22], memory[105,22], memory[106,22], memory[107,22], memory[108,22], memory[109,22], memory[110,22], memory[111,22], memory[112,22], memory[113,22], memory[114,22], memory[115,22], memory[116,22], memory[117,22], memory[118,22], memory[119,22], memory[120,22], memory[1,23], memory[2,23], memory[3,23], memory[4,23], memory[5,23], memory[6,23], memory[7,23], memory[8,23], memory[9,23], memory[10,23], memory[11,23], memory[12,23], memory[13,23], memory[14,23], memory[15,23], memory[16,23], memory[17,23], memory[18,23], memory[19,23], memory[20,23], memory[21,23], memory[22,23], memory[23,23], memory[24,23], memory[25,23], memory[26,23], memory[27,23], memory[28,23], memory[29,23], memory[30,23], memory[31,23], memory[32,23], memory[33,23], memory[34,23], memory[35,23], memory[36,23], memory[37,23], memory[38,23], memory[39,23], memory[40,23], memory[41,23], memory[42,23], memory[43,23], memory[44,23], memory[45,23], memory[46,23], memory[47,23], memory[48,23], memory[49,23], memory[50,23], memory[51,23], memory[52,23], memory[53,23], memory[54,23], memory[55,23], memory[56,23], memory[57,23], memory[58,23], memory[59,23], memory[60,23], memory[61,23], memory[62,23], memory[63,23], memory[64,23], memory[65,23], memory[66,23], memory[67,23], memory[68,23], memory[69,23], memory[70,23], memory[71,23], memory[72,23], memory[73,23], memory[74,23], memory[75,23], memory[76,23], memory[77,23], memory[78,23], memory[79,23], memory[80,23], memory[81,23], memory[82,23], memory[83,23], memory[84,23], memory[85,23], memory[86,23], memory[87,23], memory[88,23], memory[89,23], memory[90,23], memory[91,23], memory[92,23], memory[93,23], memory[94,23], memory[95,23], memory[96,23], memory[97,23], memory[98,23], memory[99,23], memory[100,23], memory[101,23], memory[102,23], memory[103,23], memory[104,23], memory[105,23], memory[106,23], memory[107,23], memory[108,23], memory[109,23], memory[110,23], memory[111,23], memory[112,23], memory[113,23], memory[114,23], memory[115,23], memory[116,23], memory[117,23], memory[118,23], memory[119,23], memory[120,23], memory[1,24], memory[2,24], memory[3,24], memory[4,24], memory[5,24], memory[6,24], memory[7,24], memory[8,24], memory[9,24], memory[10,24], memory[11,24], memory[12,24], memory[13,24], memory[14,24], memory[15,24], memory[16,24], memory[17,24], memory[18,24], memory[19,24], memory[20,24], memory[21,24], memory[22,24], memory[23,24], memory[24,24], memory[25,24], memory[26,24], memory[27,24], memory[28,24], memory[29,24], memory[30,24], memory[31,24], memory[32,24], memory[33,24], memory[34,24], memory[35,24], memory[36,24], memory[37,24], memory[38,24], memory[39,24], memory[40,24], memory[41,24], memory[42,24], memory[43,24], memory[44,24], memory[45,24], memory[46,24], memory[47,24], memory[48,24], memory[49,24], memory[50,24], memory[51,24], memory[52,24], memory[53,24], memory[54,24], memory[55,24], memory[56,24], memory[57,24], memory[58,24], memory[59,24], memory[60,24], memory[61,24], memory[62,24], memory[63,24], memory[64,24], memory[65,24], memory[66,24], memory[67,24], memory[68,24], memory[69,24], memory[70,24], memory[71,24], memory[72,24], memory[73,24], memory[74,24], memory[75,24], memory[76,24], memory[77,24], memory[78,24], memory[79,24], memory[80,24], memory[81,24], memory[82,24], memory[83,24], memory[84,24], memory[85,24], memory[86,24], memory[87,24], memory[88,24], memory[89,24], memory[90,24], memory[91,24], memory[92,24], memory[93,24], memory[94,24], memory[95,24], memory[96,24], memory[97,24], memory[98,24], memory[99,24], memory[100,24], memory[101,24], memory[102,24], memory[103,24], memory[104,24], memory[105,24], memory[106,24], memory[107,24], memory[108,24], memory[109,24], memory[110,24], memory[111,24], memory[112,24], memory[113,24], memory[114,24], memory[115,24], memory[116,24], memory[117,24], memory[118,24], memory[119,24], memory[120,24], memory[1,25], memory[2,25], memory[3,25], memory[4,25], memory[5,25], memory[6,25], memory[7,25], memory[8,25], memory[9,25], memory[10,25], memory[11,25], memory[12,25], memory[13,25], memory[14,25], memory[15,25], memory[16,25], memory[17,25], memory[18,25], memory[19,25], memory[20,25], memory[21,25], memory[22,25], memory[23,25], memory[24,25], memory[25,25], memory[26,25], memory[27,25], memory[28,25], memory[29,25], memory[30,25], memory[31,25], memory[32,25], memory[33,25], memory[34,25], memory[35,25], memory[36,25], memory[37,25], memory[38,25], memory[39,25], memory[40,25], memory[41,25], memory[42,25], memory[43,25], memory[44,25], memory[45,25], memory[46,25], memory[47,25], memory[48,25], memory[49,25], memory[50,25], memory[51,25], memory[52,25], memory[53,25], memory[54,25], memory[55,25], memory[56,25], memory[57,25], memory[58,25], memory[59,25], memory[60,25], memory[61,25], memory[62,25], memory[63,25], memory[64,25], memory[65,25], memory[66,25], memory[67,25], memory[68,25], memory[69,25], memory[70,25], memory[71,25], memory[72,25], memory[73,25], memory[74,25], memory[75,25], memory[76,25], memory[77,25], memory[78,25], memory[79,25], memory[80,25], memory[81,25], memory[82,25], memory[83,25], memory[84,25], memory[85,25], memory[86,25], memory[87,25], memory[88,25], memory[89,25], memory[90,25], memory[91,25], memory[92,25], memory[93,25], memory[94,25], memory[95,25], memory[96,25], memory[97,25], memory[98,25], memory[99,25], memory[100,25], memory[101,25], memory[102,25], memory[103,25], memory[104,25], memory[105,25], memory[106,25], memory[107,25], memory[108,25], memory[109,25], memory[110,25], memory[111,25], memory[112,25], memory[113,25], memory[114,25], memory[115,25], memory[116,25], memory[117,25], memory[118,25], memory[119,25], memory[120,25], memory[1,26], memory[2,26], memory[3,26], memory[4,26], memory[5,26], memory[6,26], memory[7,26], memory[8,26], memory[9,26], memory[10,26], memory[11,26], memory[12,26], memory[13,26], memory[14,26], memory[15,26], memory[16,26], memory[17,26], memory[18,26], memory[19,26], memory[20,26], memory[21,26], memory[22,26], memory[23,26], memory[24,26], memory[25,26], memory[26,26], memory[27,26], memory[28,26], memory[29,26], memory[30,26], memory[31,26], memory[32,26], memory[33,26], memory[34,26], memory[35,26], memory[36,26], memory[37,26], memory[38,26], memory[39,26], memory[40,26], memory[41,26], memory[42,26], memory[43,26], memory[44,26], memory[45,26], memory[46,26], memory[47,26], memory[48,26], memory[49,26], memory[50,26], memory[51,26], memory[52,26], memory[53,26], memory[54,26], memory[55,26], memory[56,26], memory[57,26], memory[58,26], memory[59,26], memory[60,26], memory[61,26], memory[62,26], memory[63,26], memory[64,26], memory[65,26], memory[66,26], memory[67,26], memory[68,26], memory[69,26], memory[70,26], memory[71,26], memory[72,26], memory[73,26], memory[74,26], memory[75,26], memory[76,26], memory[77,26], memory[78,26], memory[79,26], memory[80,26], memory[81,26], memory[82,26], memory[83,26], memory[84,26], memory[85,26], memory[86,26], memory[87,26], memory[88,26], memory[89,26], memory[90,26], memory[91,26], memory[92,26], memory[93,26], memory[94,26], memory[95,26], memory[96,26], memory[97,26], memory[98,26], memory[99,26], memory[100,26], memory[101,26], memory[102,26], memory[103,26], memory[104,26], memory[105,26], memory[106,26], memory[107,26], memory[108,26], memory[109,26], memory[110,26], memory[111,26], memory[112,26], memory[113,26], memory[114,26], memory[115,26], memory[116,26], memory[117,26], memory[118,26], memory[119,26], memory[120,26], memory[1,27], memory[2,27], memory[3,27], memory[4,27], memory[5,27], memory[6,27], memory[7,27], memory[8,27], memory[9,27], memory[10,27], memory[11,27], memory[12,27], memory[13,27], memory[14,27], memory[15,27], memory[16,27], memory[17,27], memory[18,27], memory[19,27], memory[20,27], memory[21,27], memory[22,27], memory[23,27], memory[24,27], memory[25,27], memory[26,27], memory[27,27], memory[28,27], memory[29,27], memory[30,27], memory[31,27], memory[32,27], memory[33,27], memory[34,27], memory[35,27], memory[36,27], memory[37,27], memory[38,27], memory[39,27], memory[40,27], memory[41,27], memory[42,27], memory[43,27], memory[44,27], memory[45,27], memory[46,27], memory[47,27], memory[48,27], memory[49,27], memory[50,27], memory[51,27], memory[52,27], memory[53,27], memory[54,27], memory[55,27], memory[56,27], memory[57,27], memory[58,27], memory[59,27], memory[60,27], memory[61,27], memory[62,27], memory[63,27], memory[64,27], memory[65,27], memory[66,27], memory[67,27], memory[68,27], memory[69,27], memory[70,27], memory[71,27], memory[72,27], memory[73,27], memory[74,27], memory[75,27], memory[76,27], memory[77,27], memory[78,27], memory[79,27], memory[80,27], memory[81,27], memory[82,27], memory[83,27], memory[84,27], memory[85,27], memory[86,27], memory[87,27], memory[88,27], memory[89,27], memory[90,27], memory[91,27], memory[92,27], memory[93,27], memory[94,27], memory[95,27], memory[96,27], memory[97,27], memory[98,27], memory[99,27], memory[100,27], memory[101,27], memory[102,27], memory[103,27], memory[104,27], memory[105,27], memory[106,27], memory[107,27], memory[108,27], memory[109,27], memory[110,27], memory[111,27], memory[112,27], memory[113,27], memory[114,27], memory[115,27], memory[116,27], memory[117,27], memory[118,27], memory[119,27], memory[120,27], memory[1,28], memory[2,28], memory[3,28], memory[4,28], memory[5,28], memory[6,28], memory[7,28], memory[8,28], memory[9,28], memory[10,28], memory[11,28], memory[12,28], memory[13,28], memory[14,28], memory[15,28], memory[16,28], memory[17,28], memory[18,28], memory[19,28], memory[20,28], memory[21,28], memory[22,28], memory[23,28], memory[24,28], memory[25,28], memory[26,28], memory[27,28], memory[28,28], memory[29,28], memory[30,28], memory[31,28], memory[32,28], memory[33,28], memory[34,28], memory[35,28], memory[36,28], memory[37,28], memory[38,28], memory[39,28], memory[40,28], memory[41,28], memory[42,28], memory[43,28], memory[44,28], memory[45,28], memory[46,28], memory[47,28], memory[48,28], memory[49,28], memory[50,28], memory[51,28], memory[52,28], memory[53,28], memory[54,28], memory[55,28], memory[56,28], memory[57,28], memory[58,28], memory[59,28], memory[60,28], memory[61,28], memory[62,28], memory[63,28], memory[64,28], memory[65,28], memory[66,28], memory[67,28], memory[68,28], memory[69,28], memory[70,28], memory[71,28], memory[72,28], memory[73,28], memory[74,28], memory[75,28], memory[76,28], memory[77,28], memory[78,28], memory[79,28], memory[80,28], memory[81,28], memory[82,28], memory[83,28], memory[84,28], memory[85,28], memory[86,28], memory[87,28], memory[88,28], memory[89,28], memory[90,28], memory[91,28], memory[92,28], memory[93,28], memory[94,28], memory[95,28], memory[96,28], memory[97,28], memory[98,28], memory[99,28], memory[100,28], memory[101,28], memory[102,28], memory[103,28], memory[104,28], memory[105,28], memory[106,28], memory[107,28], memory[108,28], memory[109,28], memory[110,28], memory[111,28], memory[112,28], memory[113,28], memory[114,28], memory[115,28], memory[116,28], memory[117,28], memory[118,28], memory[119,28], memory[120,28], memory[1,29], memory[2,29], memory[3,29], memory[4,29], memory[5,29], memory[6,29], memory[7,29], memory[8,29], memory[9,29], memory[10,29], memory[11,29], memory[12,29], memory[13,29], memory[14,29], memory[15,29], memory[16,29], memory[17,29], memory[18,29], memory[19,29], memory[20,29], memory[21,29], memory[22,29], memory[23,29], memory[24,29], memory[25,29], memory[26,29], memory[27,29], memory[28,29], memory[29,29], memory[30,29], memory[31,29], memory[32,29], memory[33,29], memory[34,29], memory[35,29], memory[36,29], memory[37,29], memory[38,29], memory[39,29], memory[40,29], memory[41,29], memory[42,29], memory[43,29], memory[44,29], memory[45,29], memory[46,29], memory[47,29], memory[48,29], memory[49,29], memory[50,29], memory[51,29], memory[52,29], memory[53,29], memory[54,29], memory[55,29], memory[56,29], memory[57,29], memory[58,29], memory[59,29], memory[60,29], memory[61,29], memory[62,29], memory[63,29], memory[64,29], memory[65,29], memory[66,29], memory[67,29], memory[68,29], memory[69,29], memory[70,29], memory[71,29], memory[72,29], memory[73,29], memory[74,29], memory[75,29], memory[76,29], memory[77,29], memory[78,29], memory[79,29], memory[80,29], memory[81,29], memory[82,29], memory[83,29], memory[84,29], memory[85,29], memory[86,29], memory[87,29], memory[88,29], memory[89,29], memory[90,29], memory[91,29], memory[92,29], memory[93,29], memory[94,29], memory[95,29], memory[96,29], memory[97,29], memory[98,29], memory[99,29], memory[100,29], memory[101,29], memory[102,29], memory[103,29], memory[104,29], memory[105,29], memory[106,29], memory[107,29], memory[108,29], memory[109,29], memory[110,29], memory[111,29], memory[112,29], memory[113,29], memory[114,29], memory[115,29], memory[116,29], memory[117,29], memory[118,29], memory[119,29], memory[120,29], memory[1,30], memory[2,30], memory[3,30], memory[4,30], memory[5,30], memory[6,30], memory[7,30], memory[8,30], memory[9,30], memory[10,30], memory[11,30], memory[12,30], memory[13,30], memory[14,30], memory[15,30], memory[16,30], memory[17,30], memory[18,30], memory[19,30], memory[20,30], memory[21,30], memory[22,30], memory[23,30], memory[24,30], memory[25,30], memory[26,30], memory[27,30], memory[28,30], memory[29,30], memory[30,30], memory[31,30], memory[32,30], memory[33,30], memory[34,30], memory[35,30], memory[36,30], memory[37,30], memory[38,30], memory[39,30], memory[40,30], memory[41,30], memory[42,30], memory[43,30], memory[44,30], memory[45,30], memory[46,30], memory[47,30], memory[48,30], memory[49,30], memory[50,30], memory[51,30], memory[52,30], memory[53,30], memory[54,30], memory[55,30], memory[56,30], memory[57,30], memory[58,30], memory[59,30], memory[60,30], memory[61,30], memory[62,30], memory[63,30], memory[64,30], memory[65,30], memory[66,30], memory[67,30], memory[68,30], memory[69,30], memory[70,30], memory[71,30], memory[72,30], memory[73,30], memory[74,30], memory[75,30], memory[76,30], memory[77,30], memory[78,30], memory[79,30], memory[80,30], memory[81,30], memory[82,30], memory[83,30], memory[84,30], memory[85,30], memory[86,30], memory[87,30], memory[88,30], memory[89,30], memory[90,30], memory[91,30], memory[92,30], memory[93,30], memory[94,30], memory[95,30], memory[96,30], memory[97,30], memory[98,30], memory[99,30], memory[100,30], memory[101,30], memory[102,30], memory[103,30], memory[104,30], memory[105,30], memory[106,30], memory[107,30], memory[108,30], memory[109,30], memory[110,30], memory[111,30], memory[112,30], memory[113,30], memory[114,30], memory[115,30], memory[116,30], memory[117,30], memory[118,30], memory[119,30], memory[120,30], memory[1,31], memory[2,31], memory[3,31], memory[4,31], memory[5,31], memory[6,31], memory[7,31], memory[8,31], memory[9,31], memory[10,31], memory[11,31], memory[12,31], memory[13,31], memory[14,31], memory[15,31], memory[16,31], memory[17,31], memory[18,31], memory[19,31], memory[20,31], memory[21,31], memory[22,31], memory[23,31], memory[24,31], memory[25,31], memory[26,31], memory[27,31], memory[28,31], memory[29,31], memory[30,31], memory[31,31], memory[32,31], memory[33,31], memory[34,31], memory[35,31], memory[36,31], memory[37,31], memory[38,31], memory[39,31], memory[40,31], memory[41,31], memory[42,31], memory[43,31], memory[44,31], memory[45,31], memory[46,31], memory[47,31], memory[48,31], memory[49,31], memory[50,31], memory[51,31], memory[52,31], memory[53,31], memory[54,31], memory[55,31], memory[56,31], memory[57,31], memory[58,31], memory[59,31], memory[60,31], memory[61,31], memory[62,31], memory[63,31], memory[64,31], memory[65,31], memory[66,31], memory[67,31], memory[68,31], memory[69,31], memory[70,31], memory[71,31], memory[72,31], memory[73,31], memory[74,31], memory[75,31], memory[76,31], memory[77,31], memory[78,31], memory[79,31], memory[80,31], memory[81,31], memory[82,31], memory[83,31], memory[84,31], memory[85,31], memory[86,31], memory[87,31], memory[88,31], memory[89,31], memory[90,31], memory[91,31], memory[92,31], memory[93,31], memory[94,31], memory[95,31], memory[96,31], memory[97,31], memory[98,31], memory[99,31], memory[100,31], memory[101,31], memory[102,31], memory[103,31], memory[104,31], memory[105,31], memory[106,31], memory[107,31], memory[108,31], memory[109,31], memory[110,31], memory[111,31], memory[112,31], memory[113,31], memory[114,31], memory[115,31], memory[116,31], memory[117,31], memory[118,31], memory[119,31], memory[120,31], memory[1,32], memory[2,32], memory[3,32], memory[4,32], memory[5,32], memory[6,32], memory[7,32], memory[8,32], memory[9,32], memory[10,32], memory[11,32], memory[12,32], memory[13,32], memory[14,32], memory[15,32], memory[16,32], memory[17,32], memory[18,32], memory[19,32], memory[20,32], memory[21,32], memory[22,32], memory[23,32], memory[24,32], memory[25,32], memory[26,32], memory[27,32], memory[28,32], memory[29,32], memory[30,32], memory[31,32], memory[32,32], memory[33,32], memory[34,32], memory[35,32], memory[36,32], memory[37,32], memory[38,32], memory[39,32], memory[40,32], memory[41,32], memory[42,32], memory[43,32], memory[44,32], memory[45,32], memory[46,32], memory[47,32], memory[48,32], memory[49,32], memory[50,32], memory[51,32], memory[52,32], memory[53,32], memory[54,32], memory[55,32], memory[56,32], memory[57,32], memory[58,32], memory[59,32], memory[60,32], memory[61,32], memory[62,32], memory[63,32], memory[64,32], memory[65,32], memory[66,32], memory[67,32], memory[68,32], memory[69,32], memory[70,32], memory[71,32], memory[72,32], memory[73,32], memory[74,32], memory[75,32], memory[76,32], memory[77,32], memory[78,32], memory[79,32], memory[80,32], memory[81,32], memory[82,32], memory[83,32], memory[84,32], memory[85,32], memory[86,32], memory[87,32], memory[88,32], memory[89,32], memory[90,32], memory[91,32], memory[92,32], memory[93,32], memory[94,32], memory[95,32], memory[96,32], memory[97,32], memory[98,32], memory[99,32], memory[100,32], memory[101,32], memory[102,32], memory[103,32], memory[104,32], memory[105,32], memory[106,32], memory[107,32], memory[108,32], memory[109,32], memory[110,32], memory[111,32], memory[112,32], memory[113,32], memory[114,32], memory[115,32], memory[116,32], memory[117,32], memory[118,32], memory[119,32], memory[120,32], memory[1,33], memory[2,33], memory[3,33], memory[4,33], memory[5,33], memory[6,33], memory[7,33], memory[8,33], memory[9,33], memory[10,33], memory[11,33], memory[12,33], memory[13,33], memory[14,33], memory[15,33], memory[16,33], memory[17,33], memory[18,33], memory[19,33], memory[20,33], memory[21,33], memory[22,33], memory[23,33], memory[24,33], memory[25,33], memory[26,33], memory[27,33], memory[28,33], memory[29,33], memory[30,33], memory[31,33], memory[32,33], memory[33,33], memory[34,33], memory[35,33], memory[36,33], memory[37,33], memory[38,33], memory[39,33], memory[40,33], memory[41,33], memory[42,33], memory[43,33], memory[44,33], memory[45,33], memory[46,33], memory[47,33], memory[48,33], memory[49,33], memory[50,33], memory[51,33], memory[52,33], memory[53,33], memory[54,33], memory[55,33], memory[56,33], memory[57,33], memory[58,33], memory[59,33], memory[60,33], memory[61,33], memory[62,33], memory[63,33], memory[64,33], memory[65,33], memory[66,33], memory[67,33], memory[68,33], memory[69,33], memory[70,33], memory[71,33], memory[72,33], memory[73,33], memory[74,33], memory[75,33], memory[76,33], memory[77,33], memory[78,33], memory[79,33], memory[80,33], memory[81,33], memory[82,33], memory[83,33], memory[84,33], memory[85,33], memory[86,33], memory[87,33], memory[88,33], memory[89,33], memory[90,33], memory[91,33], memory[92,33], memory[93,33], memory[94,33], memory[95,33], memory[96,33], memory[97,33], memory[98,33], memory[99,33], memory[100,33], memory[101,33], memory[102,33], memory[103,33], memory[104,33], memory[105,33], memory[106,33], memory[107,33], memory[108,33], memory[109,33], memory[110,33], memory[111,33], memory[112,33], memory[113,33], memory[114,33], memory[115,33], memory[116,33], memory[117,33], memory[118,33], memory[119,33], memory[120,33], memory[1,34], memory[2,34], memory[3,34], memory[4,34], memory[5,34], memory[6,34], memory[7,34], memory[8,34], memory[9,34], memory[10,34], memory[11,34], memory[12,34], memory[13,34], memory[14,34], memory[15,34], memory[16,34], memory[17,34], memory[18,34], memory[19,34], memory[20,34], memory[21,34], memory[22,34], memory[23,34], memory[24,34], memory[25,34], memory[26,34], memory[27,34], memory[28,34], memory[29,34], memory[30,34], memory[31,34], memory[32,34], memory[33,34], memory[34,34], memory[35,34], memory[36,34], memory[37,34], memory[38,34], memory[39,34], memory[40,34], memory[41,34], memory[42,34], memory[43,34], memory[44,34], memory[45,34], memory[46,34], memory[47,34], memory[48,34], memory[49,34], memory[50,34], memory[51,34], memory[52,34], memory[53,34], memory[54,34], memory[55,34], memory[56,34], memory[57,34], memory[58,34], memory[59,34], memory[60,34], memory[61,34], memory[62,34], memory[63,34], memory[64,34], memory[65,34], memory[66,34], memory[67,34], memory[68,34], memory[69,34], memory[70,34], memory[71,34], memory[72,34], memory[73,34], memory[74,34], memory[75,34], memory[76,34], memory[77,34], memory[78,34], memory[79,34], memory[80,34], memory[81,34], memory[82,34], memory[83,34], memory[84,34], memory[85,34], memory[86,34], memory[87,34], memory[88,34], memory[89,34], memory[90,34], memory[91,34], memory[92,34], memory[93,34], memory[94,34], memory[95,34], memory[96,34], memory[97,34], memory[98,34], memory[99,34], memory[100,34], memory[101,34], memory[102,34], memory[103,34], memory[104,34], memory[105,34], memory[106,34], memory[107,34], memory[108,34], memory[109,34], memory[110,34], memory[111,34], memory[112,34], memory[113,34], memory[114,34], memory[115,34], memory[116,34], memory[117,34], memory[118,34], memory[119,34], memory[120,34], memory[1,35], memory[2,35], memory[3,35], memory[4,35], memory[5,35], memory[6,35], memory[7,35], memory[8,35], memory[9,35], memory[10,35], memory[11,35], memory[12,35], memory[13,35], memory[14,35], memory[15,35], memory[16,35], memory[17,35], memory[18,35], memory[19,35], memory[20,35], memory[21,35], memory[22,35], memory[23,35], memory[24,35], memory[25,35], memory[26,35], memory[27,35], memory[28,35], memory[29,35], memory[30,35], memory[31,35], memory[32,35], memory[33,35], memory[34,35], memory[35,35], memory[36,35], memory[37,35], memory[38,35], memory[39,35], memory[40,35], memory[41,35], memory[42,35], memory[43,35], memory[44,35], memory[45,35], memory[46,35], memory[47,35], memory[48,35], memory[49,35], memory[50,35], memory[51,35], memory[52,35], memory[53,35], memory[54,35], memory[55,35], memory[56,35], memory[57,35], memory[58,35], memory[59,35], memory[60,35], memory[61,35], memory[62,35], memory[63,35], memory[64,35], memory[65,35], memory[66,35], memory[67,35], memory[68,35], memory[69,35], memory[70,35], memory[71,35], memory[72,35], memory[73,35], memory[74,35], memory[75,35], memory[76,35], memory[77,35], memory[78,35], memory[79,35], memory[80,35], memory[81,35], memory[82,35], memory[83,35], memory[84,35], memory[85,35], memory[86,35], memory[87,35], memory[88,35], memory[89,35], memory[90,35], memory[91,35], memory[92,35], memory[93,35], memory[94,35], memory[95,35], memory[96,35], memory[97,35], memory[98,35], memory[99,35], memory[100,35], memory[101,35], memory[102,35], memory[103,35], memory[104,35], memory[105,35], memory[106,35], memory[107,35], memory[108,35], memory[109,35], memory[110,35], memory[111,35], memory[112,35], memory[113,35], memory[114,35], memory[115,35], memory[116,35], memory[117,35], memory[118,35], memory[119,35], memory[120,35], memory[1,36], memory[2,36], memory[3,36], memory[4,36], memory[5,36], memory[6,36], memory[7,36], memory[8,36], memory[9,36], memory[10,36], memory[11,36], memory[12,36], memory[13,36], memory[14,36], memory[15,36], memory[16,36], memory[17,36], memory[18,36], memory[19,36], memory[20,36], memory[21,36], memory[22,36], memory[23,36], memory[24,36], memory[25,36], memory[26,36], memory[27,36], memory[28,36], memory[29,36], memory[30,36], memory[31,36], memory[32,36], memory[33,36], memory[34,36], memory[35,36], memory[36,36], memory[37,36], memory[38,36], memory[39,36], memory[40,36], memory[41,36], memory[42,36], memory[43,36], memory[44,36], memory[45,36], memory[46,36], memory[47,36], memory[48,36], memory[49,36], memory[50,36], memory[51,36], memory[52,36], memory[53,36], memory[54,36], memory[55,36], memory[56,36], memory[57,36], memory[58,36], memory[59,36], memory[60,36], memory[61,36], memory[62,36], memory[63,36], memory[64,36], memory[65,36], memory[66,36], memory[67,36], memory[68,36], memory[69,36], memory[70,36], memory[71,36], memory[72,36], memory[73,36], memory[74,36], memory[75,36], memory[76,36], memory[77,36], memory[78,36], memory[79,36], memory[80,36], memory[81,36], memory[82,36], memory[83,36], memory[84,36], memory[85,36], memory[86,36], memory[87,36], memory[88,36], memory[89,36], memory[90,36], memory[91,36], memory[92,36], memory[93,36], memory[94,36], memory[95,36], memory[96,36], memory[97,36], memory[98,36], memory[99,36], memory[100,36], memory[101,36], memory[102,36], memory[103,36], memory[104,36], memory[105,36], memory[106,36], memory[107,36], memory[108,36], memory[109,36], memory[110,36], memory[111,36], memory[112,36], memory[113,36], memory[114,36], memory[115,36], memory[116,36], memory[117,36], memory[118,36], memory[119,36], memory[120,36], memory[1,37], memory[2,37], memory[3,37], memory[4,37], memory[5,37], memory[6,37], memory[7,37], memory[8,37], memory[9,37], memory[10,37], memory[11,37], memory[12,37], memory[13,37], memory[14,37], memory[15,37], memory[16,37], memory[17,37], memory[18,37], memory[19,37], memory[20,37], memory[21,37], memory[22,37], memory[23,37], memory[24,37], memory[25,37], memory[26,37], memory[27,37], memory[28,37], memory[29,37], memory[30,37], memory[31,37], memory[32,37], memory[33,37], memory[34,37], memory[35,37], memory[36,37], memory[37,37], memory[38,37], memory[39,37], memory[40,37], memory[41,37], memory[42,37], memory[43,37], memory[44,37], memory[45,37], memory[46,37], memory[47,37], memory[48,37], memory[49,37], memory[50,37], memory[51,37], memory[52,37], memory[53,37], memory[54,37], memory[55,37], memory[56,37], memory[57,37], memory[58,37], memory[59,37], memory[60,37], memory[61,37], memory[62,37], memory[63,37], memory[64,37], memory[65,37], memory[66,37], memory[67,37], memory[68,37], memory[69,37], memory[70,37], memory[71,37], memory[72,37], memory[73,37], memory[74,37], memory[75,37], memory[76,37], memory[77,37], memory[78,37], memory[79,37], memory[80,37], memory[81,37], memory[82,37], memory[83,37], memory[84,37], memory[85,37], memory[86,37], memory[87,37], memory[88,37], memory[89,37], memory[90,37], memory[91,37], memory[92,37], memory[93,37], memory[94,37], memory[95,37], memory[96,37], memory[97,37], memory[98,37], memory[99,37], memory[100,37], memory[101,37], memory[102,37], memory[103,37], memory[104,37], memory[105,37], memory[106,37], memory[107,37], memory[108,37], memory[109,37], memory[110,37], memory[111,37], memory[112,37], memory[113,37], memory[114,37], memory[115,37], memory[116,37], memory[117,37], memory[118,37], memory[119,37], memory[120,37], memory[1,38], memory[2,38], memory[3,38], memory[4,38], memory[5,38], memory[6,38], memory[7,38], memory[8,38], memory[9,38], memory[10,38], memory[11,38], memory[12,38], memory[13,38], memory[14,38], memory[15,38], memory[16,38], memory[17,38], memory[18,38], memory[19,38], memory[20,38], memory[21,38], memory[22,38], memory[23,38], memory[24,38], memory[25,38], memory[26,38], memory[27,38], memory[28,38], memory[29,38], memory[30,38], memory[31,38], memory[32,38], memory[33,38], memory[34,38], memory[35,38], memory[36,38], memory[37,38], memory[38,38], memory[39,38], memory[40,38], memory[41,38], memory[42,38], memory[43,38], memory[44,38], memory[45,38], memory[46,38], memory[47,38], memory[48,38], memory[49,38], memory[50,38], memory[51,38], memory[52,38], memory[53,38], memory[54,38], memory[55,38], memory[56,38], memory[57,38], memory[58,38], memory[59,38], memory[60,38], memory[61,38], memory[62,38], memory[63,38], memory[64,38], memory[65,38], memory[66,38], memory[67,38], memory[68,38], memory[69,38], memory[70,38], memory[71,38], memory[72,38], memory[73,38], memory[74,38], memory[75,38], memory[76,38], memory[77,38], memory[78,38], memory[79,38], memory[80,38], memory[81,38], memory[82,38], memory[83,38], memory[84,38], memory[85,38], memory[86,38], memory[87,38], memory[88,38], memory[89,38], memory[90,38], memory[91,38], memory[92,38], memory[93,38], memory[94,38], memory[95,38], memory[96,38], memory[97,38], memory[98,38], memory[99,38], memory[100,38], memory[101,38], memory[102,38], memory[103,38], memory[104,38], memory[105,38], memory[106,38], memory[107,38], memory[108,38], memory[109,38], memory[110,38], memory[111,38], memory[112,38], memory[113,38], memory[114,38], memory[115,38], memory[116,38], memory[117,38], memory[118,38], memory[119,38], memory[120,38], memory[1,39], memory[2,39], memory[3,39], memory[4,39], memory[5,39], memory[6,39], memory[7,39], memory[8,39], memory[9,39], memory[10,39], memory[11,39], memory[12,39], memory[13,39], memory[14,39], memory[15,39], memory[16,39], memory[17,39], memory[18,39], memory[19,39], memory[20,39], memory[21,39], memory[22,39], memory[23,39], memory[24,39], memory[25,39], memory[26,39], memory[27,39], memory[28,39], memory[29,39], memory[30,39], memory[31,39], memory[32,39], memory[33,39], memory[34,39], memory[35,39], memory[36,39], memory[37,39], memory[38,39], memory[39,39], memory[40,39], memory[41,39], memory[42,39], memory[43,39], memory[44,39], memory[45,39], memory[46,39], memory[47,39], memory[48,39], memory[49,39], memory[50,39], memory[51,39], memory[52,39], memory[53,39], memory[54,39], memory[55,39], memory[56,39], memory[57,39], memory[58,39], memory[59,39], memory[60,39], memory[61,39], memory[62,39], memory[63,39], memory[64,39], memory[65,39], memory[66,39], memory[67,39], memory[68,39], memory[69,39], memory[70,39], memory[71,39], memory[72,39], memory[73,39], memory[74,39], memory[75,39], memory[76,39], memory[77,39], memory[78,39], memory[79,39], memory[80,39], memory[81,39], memory[82,39], memory[83,39], memory[84,39], memory[85,39], memory[86,39], memory[87,39], memory[88,39], memory[89,39], memory[90,39], memory[91,39], memory[92,39], memory[93,39], memory[94,39], memory[95,39], memory[96,39], memory[97,39], memory[98,39], memory[99,39], memory[100,39], memory[101,39], memory[102,39], memory[103,39], memory[104,39], memory[105,39], memory[106,39], memory[107,39], memory[108,39], memory[109,39], memory[110,39], memory[111,39], memory[112,39], memory[113,39], memory[114,39], memory[115,39], memory[116,39], memory[117,39], memory[118,39], memory[119,39], memory[120,39], memory[1,40], memory[2,40], memory[3,40], memory[4,40], memory[5,40], memory[6,40], memory[7,40], memory[8,40], memory[9,40], memory[10,40], memory[11,40], memory[12,40], memory[13,40], memory[14,40], memory[15,40], memory[16,40], memory[17,40], memory[18,40], memory[19,40], memory[20,40], memory[21,40], memory[22,40], memory[23,40], memory[24,40], memory[25,40], memory[26,40], memory[27,40], memory[28,40], memory[29,40], memory[30,40], memory[31,40], memory[32,40], memory[33,40], memory[34,40], memory[35,40], memory[36,40], memory[37,40], memory[38,40], memory[39,40], memory[40,40], memory[41,40], memory[42,40], memory[43,40], memory[44,40], memory[45,40], memory[46,40], memory[47,40], memory[48,40], memory[49,40], memory[50,40], memory[51,40], memory[52,40], memory[53,40], memory[54,40], memory[55,40], memory[56,40], memory[57,40], memory[58,40], memory[59,40], memory[60,40], memory[61,40], memory[62,40], memory[63,40], memory[64,40], memory[65,40], memory[66,40], memory[67,40], memory[68,40], memory[69,40], memory[70,40], memory[71,40], memory[72,40], memory[73,40], memory[74,40], memory[75,40], memory[76,40], memory[77,40], memory[78,40], memory[79,40], memory[80,40], memory[81,40], memory[82,40], memory[83,40], memory[84,40], memory[85,40], memory[86,40], memory[87,40], memory[88,40], memory[89,40], memory[90,40], memory[91,40], memory[92,40], memory[93,40], memory[94,40], memory[95,40], memory[96,40], memory[97,40], memory[98,40], memory[99,40], memory[100,40], memory[101,40], memory[102,40], memory[103,40], memory[104,40], memory[105,40], memory[106,40], memory[107,40], memory[108,40], memory[109,40], memory[110,40], memory[111,40], memory[112,40], memory[113,40], memory[114,40], memory[115,40], memory[116,40], memory[117,40], memory[118,40], memory[119,40], memory[120,40], memory[1,41], memory[2,41], memory[3,41], memory[4,41], memory[5,41], memory[6,41], memory[7,41], memory[8,41], memory[9,41], memory[10,41], memory[11,41], memory[12,41], memory[13,41], memory[14,41], memory[15,41], memory[16,41], memory[17,41], memory[18,41], memory[19,41], memory[20,41], memory[21,41], memory[22,41], memory[23,41], memory[24,41], memory[25,41], memory[26,41], memory[27,41], memory[28,41], memory[29,41], memory[30,41], memory[31,41], memory[32,41], memory[33,41], memory[34,41], memory[35,41], memory[36,41], memory[37,41], memory[38,41], memory[39,41], memory[40,41], memory[41,41], memory[42,41], memory[43,41], memory[44,41], memory[45,41], memory[46,41], memory[47,41], memory[48,41], memory[49,41], memory[50,41], memory[51,41], memory[52,41], memory[53,41], memory[54,41], memory[55,41], memory[56,41], memory[57,41], memory[58,41], memory[59,41], memory[60,41], memory[61,41], memory[62,41], memory[63,41], memory[64,41], memory[65,41], memory[66,41], memory[67,41], memory[68,41], memory[69,41], memory[70,41], memory[71,41], memory[72,41], memory[73,41], memory[74,41], memory[75,41], memory[76,41], memory[77,41], memory[78,41], memory[79,41], memory[80,41], memory[81,41], memory[82,41], memory[83,41], memory[84,41], memory[85,41], memory[86,41], memory[87,41], memory[88,41], memory[89,41], memory[90,41], memory[91,41], memory[92,41], memory[93,41], memory[94,41], memory[95,41], memory[96,41], memory[97,41], memory[98,41], memory[99,41], memory[100,41], memory[101,41], memory[102,41], memory[103,41], memory[104,41], memory[105,41], memory[106,41], memory[107,41], memory[108,41], memory[109,41], memory[110,41], memory[111,41], memory[112,41], memory[113,41], memory[114,41], memory[115,41], memory[116,41], memory[117,41], memory[118,41], memory[119,41], memory[120,41], memory[1,42], memory[2,42], memory[3,42], memory[4,42], memory[5,42], memory[6,42], memory[7,42], memory[8,42], memory[9,42], memory[10,42], memory[11,42], memory[12,42], memory[13,42], memory[14,42], memory[15,42], memory[16,42], memory[17,42], memory[18,42], memory[19,42], memory[20,42], memory[21,42], memory[22,42], memory[23,42], memory[24,42], memory[25,42], memory[26,42], memory[27,42], memory[28,42], memory[29,42], memory[30,42], memory[31,42], memory[32,42], memory[33,42], memory[34,42], memory[35,42], memory[36,42], memory[37,42], memory[38,42], memory[39,42], memory[40,42], memory[41,42], memory[42,42], memory[43,42], memory[44,42], memory[45,42], memory[46,42], memory[47,42], memory[48,42], memory[49,42], memory[50,42], memory[51,42], memory[52,42], memory[53,42], memory[54,42], memory[55,42], memory[56,42], memory[57,42], memory[58,42], memory[59,42], memory[60,42], memory[61,42], memory[62,42], memory[63,42], memory[64,42], memory[65,42], memory[66,42], memory[67,42], memory[68,42], memory[69,42], memory[70,42], memory[71,42], memory[72,42], memory[73,42], memory[74,42], memory[75,42], memory[76,42], memory[77,42], memory[78,42], memory[79,42], memory[80,42], memory[81,42], memory[82,42], memory[83,42], memory[84,42], memory[85,42], memory[86,42], memory[87,42], memory[88,42], memory[89,42], memory[90,42], memory[91,42], memory[92,42], memory[93,42], memory[94,42], memory[95,42], memory[96,42], memory[97,42], memory[98,42], memory[99,42], memory[100,42], memory[101,42], memory[102,42], memory[103,42], memory[104,42], memory[105,42], memory[106,42], memory[107,42], memory[108,42], memory[109,42], memory[110,42], memory[111,42], memory[112,42], memory[113,42], memory[114,42], memory[115,42], memory[116,42], memory[117,42], memory[118,42], memory[119,42], memory[120,42], memory[1,43], memory[2,43], memory[3,43], memory[4,43], memory[5,43], memory[6,43], memory[7,43], memory[8,43], memory[9,43], memory[10,43], memory[11,43], memory[12,43], memory[13,43], memory[14,43], memory[15,43], memory[16,43], memory[17,43], memory[18,43], memory[19,43], memory[20,43], memory[21,43], memory[22,43], memory[23,43], memory[24,43], memory[25,43], memory[26,43], memory[27,43], memory[28,43], memory[29,43], memory[30,43], memory[31,43], memory[32,43], memory[33,43], memory[34,43], memory[35,43], memory[36,43], memory[37,43], memory[38,43], memory[39,43], memory[40,43], memory[41,43], memory[42,43], memory[43,43], memory[44,43], memory[45,43], memory[46,43], memory[47,43], memory[48,43], memory[49,43], memory[50,43], memory[51,43], memory[52,43], memory[53,43], memory[54,43], memory[55,43], memory[56,43], memory[57,43], memory[58,43], memory[59,43], memory[60,43], memory[61,43], memory[62,43], memory[63,43], memory[64,43], memory[65,43], memory[66,43], memory[67,43], memory[68,43], memory[69,43], memory[70,43], memory[71,43], memory[72,43], memory[73,43], memory[74,43], memory[75,43], memory[76,43], memory[77,43], memory[78,43], memory[79,43], memory[80,43], memory[81,43], memory[82,43], memory[83,43], memory[84,43], memory[85,43], memory[86,43], memory[87,43], memory[88,43], memory[89,43], memory[90,43], memory[91,43], memory[92,43], memory[93,43], memory[94,43], memory[95,43], memory[96,43], memory[97,43], memory[98,43], memory[99,43], memory[100,43], memory[101,43], memory[102,43], memory[103,43], memory[104,43], memory[105,43], memory[106,43], memory[107,43], memory[108,43], memory[109,43], memory[110,43], memory[111,43], memory[112,43], memory[113,43], memory[114,43], memory[115,43], memory[116,43], memory[117,43], memory[118,43], memory[119,43], memory[120,43], memory[1,44], memory[2,44], memory[3,44], memory[4,44], memory[5,44], memory[6,44], memory[7,44], memory[8,44], memory[9,44], memory[10,44], memory[11,44], memory[12,44], memory[13,44], memory[14,44], memory[15,44], memory[16,44], memory[17,44], memory[18,44], memory[19,44], memory[20,44], memory[21,44], memory[22,44], memory[23,44], memory[24,44], memory[25,44], memory[26,44], memory[27,44], memory[28,44], memory[29,44], memory[30,44], memory[31,44], memory[32,44], memory[33,44], memory[34,44], memory[35,44], memory[36,44], memory[37,44], memory[38,44], memory[39,44], memory[40,44], memory[41,44], memory[42,44], memory[43,44], memory[44,44], memory[45,44], memory[46,44], memory[47,44], memory[48,44], memory[49,44], memory[50,44], memory[51,44], memory[52,44], memory[53,44], memory[54,44], memory[55,44], memory[56,44], memory[57,44], memory[58,44], memory[59,44], memory[60,44], memory[61,44], memory[62,44], memory[63,44], memory[64,44], memory[65,44], memory[66,44], memory[67,44], memory[68,44], memory[69,44], memory[70,44], memory[71,44], memory[72,44], memory[73,44], memory[74,44], memory[75,44], memory[76,44], memory[77,44], memory[78,44], memory[79,44], memory[80,44], memory[81,44], memory[82,44], memory[83,44], memory[84,44], memory[85,44], memory[86,44], memory[87,44], memory[88,44], memory[89,44], memory[90,44], memory[91,44], memory[92,44], memory[93,44], memory[94,44], memory[95,44], memory[96,44], memory[97,44], memory[98,44], memory[99,44], memory[100,44], memory[101,44], memory[102,44], memory[103,44], memory[104,44], memory[105,44], memory[106,44], memory[107,44], memory[108,44], memory[109,44], memory[110,44], memory[111,44], memory[112,44], memory[113,44], memory[114,44], memory[115,44], memory[116,44], memory[117,44], memory[118,44], memory[119,44], memory[120,44], memory[1,45], memory[2,45], memory[3,45], memory[4,45], memory[5,45], memory[6,45], memory[7,45], memory[8,45], memory[9,45], memory[10,45], memory[11,45], memory[12,45], memory[13,45], memory[14,45], memory[15,45], memory[16,45], memory[17,45], memory[18,45], memory[19,45], memory[20,45], memory[21,45], memory[22,45], memory[23,45], memory[24,45], memory[25,45], memory[26,45], memory[27,45], memory[28,45], memory[29,45], memory[30,45], memory[31,45], memory[32,45], memory[33,45], memory[34,45], memory[35,45], memory[36,45], memory[37,45], memory[38,45], memory[39,45], memory[40,45], memory[41,45], memory[42,45], memory[43,45], memory[44,45], memory[45,45], memory[46,45], memory[47,45], memory[48,45], memory[49,45], memory[50,45], memory[51,45], memory[52,45], memory[53,45], memory[54,45], memory[55,45], memory[56,45], memory[57,45], memory[58,45], memory[59,45], memory[60,45], memory[61,45], memory[62,45], memory[63,45], memory[64,45], memory[65,45], memory[66,45], memory[67,45], memory[68,45], memory[69,45], memory[70,45], memory[71,45], memory[72,45], memory[73,45], memory[74,45], memory[75,45], memory[76,45], memory[77,45], memory[78,45], memory[79,45], memory[80,45], memory[81,45], memory[82,45], memory[83,45], memory[84,45], memory[85,45], memory[86,45], memory[87,45], memory[88,45], memory[89,45], memory[90,45], memory[91,45], memory[92,45], memory[93,45], memory[94,45], memory[95,45], memory[96,45], memory[97,45], memory[98,45], memory[99,45], memory[100,45], memory[101,45], memory[102,45], memory[103,45], memory[104,45], memory[105,45], memory[106,45], memory[107,45], memory[108,45], memory[109,45], memory[110,45], memory[111,45], memory[112,45], memory[113,45], memory[114,45], memory[115,45], memory[116,45], memory[117,45], memory[118,45], memory[119,45], memory[120,45], memory[1,46], memory[2,46], memory[3,46], memory[4,46], memory[5,46], memory[6,46], memory[7,46], memory[8,46], memory[9,46], memory[10,46], memory[11,46], memory[12,46], memory[13,46], memory[14,46], memory[15,46], memory[16,46], memory[17,46], memory[18,46], memory[19,46], memory[20,46], memory[21,46], memory[22,46], memory[23,46], memory[24,46], memory[25,46], memory[26,46], memory[27,46], memory[28,46], memory[29,46], memory[30,46], memory[31,46], memory[32,46], memory[33,46], memory[34,46], memory[35,46], memory[36,46], memory[37,46], memory[38,46], memory[39,46], memory[40,46], memory[41,46], memory[42,46], memory[43,46], memory[44,46], memory[45,46], memory[46,46], memory[47,46], memory[48,46], memory[49,46], memory[50,46], memory[51,46], memory[52,46], memory[53,46], memory[54,46], memory[55,46], memory[56,46], memory[57,46], memory[58,46], memory[59,46], memory[60,46], memory[61,46], memory[62,46], memory[63,46], memory[64,46], memory[65,46], memory[66,46], memory[67,46], memory[68,46], memory[69,46], memory[70,46], memory[71,46], memory[72,46], memory[73,46], memory[74,46], memory[75,46], memory[76,46], memory[77,46], memory[78,46], memory[79,46], memory[80,46], memory[81,46], memory[82,46], memory[83,46], memory[84,46], memory[85,46], memory[86,46], memory[87,46], memory[88,46], memory[89,46], memory[90,46], memory[91,46], memory[92,46], memory[93,46], memory[94,46], memory[95,46], memory[96,46], memory[97,46], memory[98,46], memory[99,46], memory[100,46], memory[101,46], memory[102,46], memory[103,46], memory[104,46], memory[105,46], memory[106,46], memory[107,46], memory[108,46], memory[109,46], memory[110,46], memory[111,46], memory[112,46], memory[113,46], memory[114,46], memory[115,46], memory[116,46], memory[117,46], memory[118,46], memory[119,46], memory[120,46], memory[1,47], memory[2,47], memory[3,47], memory[4,47], memory[5,47], memory[6,47], memory[7,47], memory[8,47], memory[9,47], memory[10,47], memory[11,47], memory[12,47], memory[13,47], memory[14,47], memory[15,47], memory[16,47], memory[17,47], memory[18,47], memory[19,47], memory[20,47], memory[21,47], memory[22,47], memory[23,47], memory[24,47], memory[25,47], memory[26,47], memory[27,47], memory[28,47], memory[29,47], memory[30,47], memory[31,47], memory[32,47], memory[33,47], memory[34,47], memory[35,47], memory[36,47], memory[37,47], memory[38,47], memory[39,47], memory[40,47], memory[41,47], memory[42,47], memory[43,47], memory[44,47], memory[45,47], memory[46,47], memory[47,47], memory[48,47], memory[49,47], memory[50,47], memory[51,47], memory[52,47], memory[53,47], memory[54,47], memory[55,47], memory[56,47], memory[57,47], memory[58,47], memory[59,47], memory[60,47], memory[61,47], memory[62,47], memory[63,47], memory[64,47], memory[65,47], memory[66,47], memory[67,47], memory[68,47], memory[69,47], memory[70,47], memory[71,47], memory[72,47], memory[73,47], memory[74,47], memory[75,47], memory[76,47], memory[77,47], memory[78,47], memory[79,47], memory[80,47], memory[81,47], memory[82,47], memory[83,47], memory[84,47], memory[85,47], memory[86,47], memory[87,47], memory[88,47], memory[89,47], memory[90,47], memory[91,47], memory[92,47], memory[93,47], memory[94,47], memory[95,47], memory[96,47], memory[97,47], memory[98,47], memory[99,47], memory[100,47], memory[101,47], memory[102,47], memory[103,47], memory[104,47], memory[105,47], memory[106,47], memory[107,47], memory[108,47], memory[109,47], memory[110,47], memory[111,47], memory[112,47], memory[113,47], memory[114,47], memory[115,47], memory[116,47], memory[117,47], memory[118,47], memory[119,47], memory[120,47], memory[1,48], memory[2,48], memory[3,48], memory[4,48], memory[5,48], memory[6,48], memory[7,48], memory[8,48], memory[9,48], memory[10,48], memory[11,48], memory[12,48], memory[13,48], memory[14,48], memory[15,48], memory[16,48], memory[17,48], memory[18,48], memory[19,48], memory[20,48], memory[21,48], memory[22,48], memory[23,48], memory[24,48], memory[25,48], memory[26,48], memory[27,48], memory[28,48], memory[29,48], memory[30,48], memory[31,48], memory[32,48], memory[33,48], memory[34,48], memory[35,48], memory[36,48], memory[37,48], memory[38,48], memory[39,48], memory[40,48], memory[41,48], memory[42,48], memory[43,48], memory[44,48], memory[45,48], memory[46,48], memory[47,48], memory[48,48], memory[49,48], memory[50,48], memory[51,48], memory[52,48], memory[53,48], memory[54,48], memory[55,48], memory[56,48], memory[57,48], memory[58,48], memory[59,48], memory[60,48], memory[61,48], memory[62,48], memory[63,48], memory[64,48], memory[65,48], memory[66,48], memory[67,48], memory[68,48], memory[69,48], memory[70,48], memory[71,48], memory[72,48], memory[73,48], memory[74,48], memory[75,48], memory[76,48], memory[77,48], memory[78,48], memory[79,48], memory[80,48], memory[81,48], memory[82,48], memory[83,48], memory[84,48], memory[85,48], memory[86,48], memory[87,48], memory[88,48], memory[89,48], memory[90,48], memory[91,48], memory[92,48], memory[93,48], memory[94,48], memory[95,48], memory[96,48], memory[97,48], memory[98,48], memory[99,48], memory[100,48], memory[101,48], memory[102,48], memory[103,48], memory[104,48], memory[105,48], memory[106,48], memory[107,48], memory[108,48], memory[109,48], memory[110,48], memory[111,48], memory[112,48], memory[113,48], memory[114,48], memory[115,48], memory[116,48], memory[117,48], memory[118,48], memory[119,48], memory[120,48], memory[1,49], memory[2,49], memory[3,49], memory[4,49], memory[5,49], memory[6,49], memory[7,49], memory[8,49], memory[9,49], memory[10,49], memory[11,49], memory[12,49], memory[13,49], memory[14,49], memory[15,49], memory[16,49], memory[17,49], memory[18,49], memory[19,49], memory[20,49], memory[21,49], memory[22,49], memory[23,49], memory[24,49], memory[25,49], memory[26,49], memory[27,49], memory[28,49], memory[29,49], memory[30,49], memory[31,49], memory[32,49], memory[33,49], memory[34,49], memory[35,49], memory[36,49], memory[37,49], memory[38,49], memory[39,49], memory[40,49], memory[41,49], memory[42,49], memory[43,49], memory[44,49], memory[45,49], memory[46,49], memory[47,49], memory[48,49], memory[49,49], memory[50,49], memory[51,49], memory[52,49], memory[53,49], memory[54,49], memory[55,49], memory[56,49], memory[57,49], memory[58,49], memory[59,49], memory[60,49], memory[61,49], memory[62,49], memory[63,49], memory[64,49], memory[65,49], memory[66,49], memory[67,49], memory[68,49], memory[69,49], memory[70,49], memory[71,49], memory[72,49], memory[73,49], memory[74,49], memory[75,49], memory[76,49], memory[77,49], memory[78,49], memory[79,49], memory[80,49], memory[81,49], memory[82,49], memory[83,49], memory[84,49], memory[85,49], memory[86,49], memory[87,49], memory[88,49], memory[89,49], memory[90,49], memory[91,49], memory[92,49], memory[93,49], memory[94,49], memory[95,49], memory[96,49], memory[97,49], memory[98,49], memory[99,49], memory[100,49], memory[101,49], memory[102,49], memory[103,49], memory[104,49], memory[105,49], memory[106,49], memory[107,49], memory[108,49], memory[109,49], memory[110,49], memory[111,49], memory[112,49], memory[113,49], memory[114,49], memory[115,49], memory[116,49], memory[117,49], memory[118,49], memory[119,49], memory[120,49], memory[1,50], memory[2,50], memory[3,50], memory[4,50], memory[5,50], memory[6,50], memory[7,50], memory[8,50], memory[9,50], memory[10,50], memory[11,50], memory[12,50], memory[13,50], memory[14,50], memory[15,50], memory[16,50], memory[17,50], memory[18,50], memory[19,50], memory[20,50], memory[21,50], memory[22,50], memory[23,50], memory[24,50], memory[25,50], memory[26,50], memory[27,50], memory[28,50], memory[29,50], memory[30,50], memory[31,50], memory[32,50], memory[33,50], memory[34,50], memory[35,50], memory[36,50], memory[37,50], memory[38,50], memory[39,50], memory[40,50], memory[41,50], memory[42,50], memory[43,50], memory[44,50], memory[45,50], memory[46,50], memory[47,50], memory[48,50], memory[49,50], memory[50,50], memory[51,50], memory[52,50], memory[53,50], memory[54,50], memory[55,50], memory[56,50], memory[57,50], memory[58,50], memory[59,50], memory[60,50], memory[61,50], memory[62,50], memory[63,50], memory[64,50], memory[65,50], memory[66,50], memory[67,50], memory[68,50], memory[69,50], memory[70,50], memory[71,50], memory[72,50], memory[73,50], memory[74,50], memory[75,50], memory[76,50], memory[77,50], memory[78,50], memory[79,50], memory[80,50], memory[81,50], memory[82,50], memory[83,50], memory[84,50], memory[85,50], memory[86,50], memory[87,50], memory[88,50], memory[89,50], memory[90,50], memory[91,50], memory[92,50], memory[93,50], memory[94,50], memory[95,50], memory[96,50], memory[97,50], memory[98,50], memory[99,50], memory[100,50], memory[101,50], memory[102,50], memory[103,50], memory[104,50], memory[105,50], memory[106,50], memory[107,50], memory[108,50], memory[109,50], memory[110,50], memory[111,50], memory[112,50], memory[113,50], memory[114,50], memory[115,50], memory[116,50], memory[117,50], memory[118,50], memory[119,50], memory[120,50], memory[1,51], memory[2,51], memory[3,51], memory[4,51], memory[5,51], memory[6,51], memory[7,51], memory[8,51], memory[9,51], memory[10,51], memory[11,51], memory[12,51], memory[13,51], memory[14,51], memory[15,51], memory[16,51], memory[17,51], memory[18,51], memory[19,51], memory[20,51], memory[21,51], memory[22,51], memory[23,51], memory[24,51], memory[25,51], memory[26,51], memory[27,51], memory[28,51], memory[29,51], memory[30,51], memory[31,51], memory[32,51], memory[33,51], memory[34,51], memory[35,51], memory[36,51], memory[37,51], memory[38,51], memory[39,51], memory[40,51], memory[41,51], memory[42,51], memory[43,51], memory[44,51], memory[45,51], memory[46,51], memory[47,51], memory[48,51], memory[49,51], memory[50,51], memory[51,51], memory[52,51], memory[53,51], memory[54,51], memory[55,51], memory[56,51], memory[57,51], memory[58,51], memory[59,51], memory[60,51], memory[61,51], memory[62,51], memory[63,51], memory[64,51], memory[65,51], memory[66,51], memory[67,51], memory[68,51], memory[69,51], memory[70,51], memory[71,51], memory[72,51], memory[73,51], memory[74,51], memory[75,51], memory[76,51], memory[77,51], memory[78,51], memory[79,51], memory[80,51], memory[81,51], memory[82,51], memory[83,51], memory[84,51], memory[85,51], memory[86,51], memory[87,51], memory[88,51], memory[89,51], memory[90,51], memory[91,51], memory[92,51], memory[93,51], memory[94,51], memory[95,51], memory[96,51], memory[97,51], memory[98,51], memory[99,51], memory[100,51], memory[101,51], memory[102,51], memory[103,51], memory[104,51], memory[105,51], memory[106,51], memory[107,51], memory[108,51], memory[109,51], memory[110,51], memory[111,51], memory[112,51], memory[113,51], memory[114,51], memory[115,51], memory[116,51], memory[117,51], memory[118,51], memory[119,51], memory[120,51], memory[1,52], memory[2,52], memory[3,52], memory[4,52], memory[5,52], memory[6,52], memory[7,52], memory[8,52], memory[9,52], memory[10,52], memory[11,52], memory[12,52], memory[13,52], memory[14,52], memory[15,52], memory[16,52], memory[17,52], memory[18,52], memory[19,52], memory[20,52], memory[21,52], memory[22,52], memory[23,52], memory[24,52], memory[25,52], memory[26,52], memory[27,52], memory[28,52], memory[29,52], memory[30,52], memory[31,52], memory[32,52], memory[33,52], memory[34,52], memory[35,52], memory[36,52], memory[37,52], memory[38,52], memory[39,52], memory[40,52], memory[41,52], memory[42,52], memory[43,52], memory[44,52], memory[45,52], memory[46,52], memory[47,52], memory[48,52], memory[49,52], memory[50,52], memory[51,52], memory[52,52], memory[53,52], memory[54,52], memory[55,52], memory[56,52], memory[57,52], memory[58,52], memory[59,52], memory[60,52], memory[61,52], memory[62,52], memory[63,52], memory[64,52], memory[65,52], memory[66,52], memory[67,52], memory[68,52], memory[69,52], memory[70,52], memory[71,52], memory[72,52], memory[73,52], memory[74,52], memory[75,52], memory[76,52], memory[77,52], memory[78,52], memory[79,52], memory[80,52], memory[81,52], memory[82,52], memory[83,52], memory[84,52], memory[85,52], memory[86,52], memory[87,52], memory[88,52], memory[89,52], memory[90,52], memory[91,52], memory[92,52], memory[93,52], memory[94,52], memory[95,52], memory[96,52], memory[97,52], memory[98,52], memory[99,52], memory[100,52], memory[101,52], memory[102,52], memory[103,52], memory[104,52], memory[105,52], memory[106,52], memory[107,52], memory[108,52], memory[109,52], memory[110,52], memory[111,52], memory[112,52], memory[113,52], memory[114,52], memory[115,52], memory[116,52], memory[117,52], memory[118,52], memory[119,52], memory[120,52], memory[1,53], memory[2,53], memory[3,53], memory[4,53], memory[5,53], memory[6,53], memory[7,53], memory[8,53], memory[9,53], memory[10,53], memory[11,53], memory[12,53], memory[13,53], memory[14,53], memory[15,53], memory[16,53], memory[17,53], memory[18,53], memory[19,53], memory[20,53], memory[21,53], memory[22,53], memory[23,53], memory[24,53], memory[25,53], memory[26,53], memory[27,53], memory[28,53], memory[29,53], memory[30,53], memory[31,53], memory[32,53], memory[33,53], memory[34,53], memory[35,53], memory[36,53], memory[37,53], memory[38,53], memory[39,53], memory[40,53], memory[41,53], memory[42,53], memory[43,53], memory[44,53], memory[45,53], memory[46,53], memory[47,53], memory[48,53], memory[49,53], memory[50,53], memory[51,53], memory[52,53], memory[53,53], memory[54,53], memory[55,53], memory[56,53], memory[57,53], memory[58,53], memory[59,53], memory[60,53], memory[61,53], memory[62,53], memory[63,53], memory[64,53], memory[65,53], memory[66,53], memory[67,53], memory[68,53], memory[69,53], memory[70,53], memory[71,53], memory[72,53], memory[73,53], memory[74,53], memory[75,53], memory[76,53], memory[77,53], memory[78,53], memory[79,53], memory[80,53], memory[81,53], memory[82,53], memory[83,53], memory[84,53], memory[85,53], memory[86,53], memory[87,53], memory[88,53], memory[89,53], memory[90,53], memory[91,53], memory[92,53], memory[93,53], memory[94,53], memory[95,53], memory[96,53], memory[97,53], memory[98,53], memory[99,53], memory[100,53], memory[101,53], memory[102,53], memory[103,53], memory[104,53], memory[105,53], memory[106,53], memory[107,53], memory[108,53], memory[109,53], memory[110,53], memory[111,53], memory[112,53], memory[113,53], memory[114,53], memory[115,53], memory[116,53], memory[117,53], memory[118,53], memory[119,53], memory[120,53], memory[1,54], memory[2,54], memory[3,54], memory[4,54], memory[5,54], memory[6,54], memory[7,54], memory[8,54], memory[9,54], memory[10,54], memory[11,54], memory[12,54], memory[13,54], memory[14,54], memory[15,54], memory[16,54], memory[17,54], memory[18,54], memory[19,54], memory[20,54], memory[21,54], memory[22,54], memory[23,54], memory[24,54], memory[25,54], memory[26,54], memory[27,54], memory[28,54], memory[29,54], memory[30,54], memory[31,54], memory[32,54], memory[33,54], memory[34,54], memory[35,54], memory[36,54], memory[37,54], memory[38,54], memory[39,54], memory[40,54], memory[41,54], memory[42,54], memory[43,54], memory[44,54], memory[45,54], memory[46,54], memory[47,54], memory[48,54], memory[49,54], memory[50,54], memory[51,54], memory[52,54], memory[53,54], memory[54,54], memory[55,54], memory[56,54], memory[57,54], memory[58,54], memory[59,54], memory[60,54], memory[61,54], memory[62,54], memory[63,54], memory[64,54], memory[65,54], memory[66,54], memory[67,54], memory[68,54], memory[69,54], memory[70,54], memory[71,54], memory[72,54], memory[73,54], memory[74,54], memory[75,54], memory[76,54], memory[77,54], memory[78,54], memory[79,54], memory[80,54], memory[81,54], memory[82,54], memory[83,54], memory[84,54], memory[85,54], memory[86,54], memory[87,54], memory[88,54], memory[89,54], memory[90,54], memory[91,54], memory[92,54], memory[93,54], memory[94,54], memory[95,54], memory[96,54], memory[97,54], memory[98,54], memory[99,54], memory[100,54], memory[101,54], memory[102,54], memory[103,54], memory[104,54], memory[105,54], memory[106,54], memory[107,54], memory[108,54], memory[109,54], memory[110,54], memory[111,54], memory[112,54], memory[113,54], memory[114,54], memory[115,54], memory[116,54], memory[117,54], memory[118,54], memory[119,54], memory[120,54], memory[1,55], memory[2,55], memory[3,55], memory[4,55], memory[5,55], memory[6,55], memory[7,55], memory[8,55], memory[9,55], memory[10,55], memory[11,55], memory[12,55], memory[13,55], memory[14,55], memory[15,55], memory[16,55], memory[17,55], memory[18,55], memory[19,55], memory[20,55], memory[21,55], memory[22,55], memory[23,55], memory[24,55], memory[25,55], memory[26,55], memory[27,55], memory[28,55], memory[29,55], memory[30,55], memory[31,55], memory[32,55], memory[33,55], memory[34,55], memory[35,55], memory[36,55], memory[37,55], memory[38,55], memory[39,55], memory[40,55], memory[41,55], memory[42,55], memory[43,55], memory[44,55], memory[45,55], memory[46,55], memory[47,55], memory[48,55], memory[49,55], memory[50,55], memory[51,55], memory[52,55], memory[53,55], memory[54,55], memory[55,55], memory[56,55], memory[57,55], memory[58,55], memory[59,55], memory[60,55], memory[61,55], memory[62,55], memory[63,55], memory[64,55], memory[65,55], memory[66,55], memory[67,55], memory[68,55], memory[69,55], memory[70,55], memory[71,55], memory[72,55], memory[73,55], memory[74,55], memory[75,55], memory[76,55], memory[77,55], memory[78,55], memory[79,55], memory[80,55], memory[81,55], memory[82,55], memory[83,55], memory[84,55], memory[85,55], memory[86,55], memory[87,55], memory[88,55], memory[89,55], memory[90,55], memory[91,55], memory[92,55], memory[93,55], memory[94,55], memory[95,55], memory[96,55], memory[97,55], memory[98,55], memory[99,55], memory[100,55], memory[101,55], memory[102,55], memory[103,55], memory[104,55], memory[105,55], memory[106,55], memory[107,55], memory[108,55], memory[109,55], memory[110,55], memory[111,55], memory[112,55], memory[113,55], memory[114,55], memory[115,55], memory[116,55], memory[117,55], memory[118,55], memory[119,55], memory[120,55], memory[1,56], memory[2,56], memory[3,56], memory[4,56], memory[5,56], memory[6,56], memory[7,56], memory[8,56], memory[9,56], memory[10,56], memory[11,56], memory[12,56], memory[13,56], memory[14,56], memory[15,56], memory[16,56], memory[17,56], memory[18,56], memory[19,56], memory[20,56], memory[21,56], memory[22,56], memory[23,56], memory[24,56], memory[25,56], memory[26,56], memory[27,56], memory[28,56], memory[29,56], memory[30,56], memory[31,56], memory[32,56], memory[33,56], memory[34,56], memory[35,56], memory[36,56], memory[37,56], memory[38,56], memory[39,56], memory[40,56], memory[41,56], memory[42,56], memory[43,56], memory[44,56], memory[45,56], memory[46,56], memory[47,56], memory[48,56], memory[49,56], memory[50,56], memory[51,56], memory[52,56], memory[53,56], memory[54,56], memory[55,56], memory[56,56], memory[57,56], memory[58,56], memory[59,56], memory[60,56], memory[61,56], memory[62,56], memory[63,56], memory[64,56], memory[65,56], memory[66,56], memory[67,56], memory[68,56], memory[69,56], memory[70,56], memory[71,56], memory[72,56], memory[73,56], memory[74,56], memory[75,56], memory[76,56], memory[77,56], memory[78,56], memory[79,56], memory[80,56], memory[81,56], memory[82,56], memory[83,56], memory[84,56], memory[85,56], memory[86,56], memory[87,56], memory[88,56], memory[89,56], memory[90,56], memory[91,56], memory[92,56], memory[93,56], memory[94,56], memory[95,56], memory[96,56], memory[97,56], memory[98,56], memory[99,56], memory[100,56], memory[101,56], memory[102,56], memory[103,56], memory[104,56], memory[105,56], memory[106,56], memory[107,56], memory[108,56], memory[109,56], memory[110,56], memory[111,56], memory[112,56], memory[113,56], memory[114,56], memory[115,56], memory[116,56], memory[117,56], memory[118,56], memory[119,56], memory[120,56], memory[1,57], memory[2,57], memory[3,57], memory[4,57], memory[5,57], memory[6,57], memory[7,57], memory[8,57], memory[9,57], memory[10,57], memory[11,57], memory[12,57], memory[13,57], memory[14,57], memory[15,57], memory[16,57], memory[17,57], memory[18,57], memory[19,57], memory[20,57], memory[21,57], memory[22,57], memory[23,57], memory[24,57], memory[25,57], memory[26,57], memory[27,57], memory[28,57], memory[29,57], memory[30,57], memory[31,57], memory[32,57], memory[33,57], memory[34,57], memory[35,57], memory[36,57], memory[37,57], memory[38,57], memory[39,57], memory[40,57], memory[41,57], memory[42,57], memory[43,57], memory[44,57], memory[45,57], memory[46,57], memory[47,57], memory[48,57], memory[49,57], memory[50,57], memory[51,57], memory[52,57], memory[53,57], memory[54,57], memory[55,57], memory[56,57], memory[57,57], memory[58,57], memory[59,57], memory[60,57], memory[61,57], memory[62,57], memory[63,57], memory[64,57], memory[65,57], memory[66,57], memory[67,57], memory[68,57], memory[69,57], memory[70,57], memory[71,57], memory[72,57], memory[73,57], memory[74,57], memory[75,57], memory[76,57], memory[77,57], memory[78,57], memory[79,57], memory[80,57], memory[81,57], memory[82,57], memory[83,57], memory[84,57], memory[85,57], memory[86,57], memory[87,57], memory[88,57], memory[89,57], memory[90,57], memory[91,57], memory[92,57], memory[93,57], memory[94,57], memory[95,57], memory[96,57], memory[97,57], memory[98,57], memory[99,57], memory[100,57], memory[101,57], memory[102,57], memory[103,57], memory[104,57], memory[105,57], memory[106,57], memory[107,57], memory[108,57], memory[109,57], memory[110,57], memory[111,57], memory[112,57], memory[113,57], memory[114,57], memory[115,57], memory[116,57], memory[117,57], memory[118,57], memory[119,57], memory[120,57], memory[1,58], memory[2,58], memory[3,58], memory[4,58], memory[5,58], memory[6,58], memory[7,58], memory[8,58], memory[9,58], memory[10,58], memory[11,58], memory[12,58], memory[13,58], memory[14,58], memory[15,58], memory[16,58], memory[17,58], memory[18,58], memory[19,58], memory[20,58], memory[21,58], memory[22,58], memory[23,58], memory[24,58], memory[25,58], memory[26,58], memory[27,58], memory[28,58], memory[29,58], memory[30,58], memory[31,58], memory[32,58], memory[33,58], memory[34,58], memory[35,58], memory[36,58], memory[37,58], memory[38,58], memory[39,58], memory[40,58], memory[41,58], memory[42,58], memory[43,58], memory[44,58], memory[45,58], memory[46,58], memory[47,58], memory[48,58], memory[49,58], memory[50,58], memory[51,58], memory[52,58], memory[53,58], memory[54,58], memory[55,58], memory[56,58], memory[57,58], memory[58,58], memory[59,58], memory[60,58], memory[61,58], memory[62,58], memory[63,58], memory[64,58], memory[65,58], memory[66,58], memory[67,58], memory[68,58], memory[69,58], memory[70,58], memory[71,58], memory[72,58], memory[73,58], memory[74,58], memory[75,58], memory[76,58], memory[77,58], memory[78,58], memory[79,58], memory[80,58], memory[81,58], memory[82,58], memory[83,58], memory[84,58], memory[85,58], memory[86,58], memory[87,58], memory[88,58], memory[89,58], memory[90,58], memory[91,58], memory[92,58], memory[93,58], memory[94,58], memory[95,58], memory[96,58], memory[97,58], memory[98,58], memory[99,58], memory[100,58], memory[101,58], memory[102,58], memory[103,58], memory[104,58], memory[105,58], memory[106,58], memory[107,58], memory[108,58], memory[109,58], memory[110,58], memory[111,58], memory[112,58], memory[113,58], memory[114,58], memory[115,58], memory[116,58], memory[117,58], memory[118,58], memory[119,58], memory[120,58], memory[1,59], memory[2,59], memory[3,59], memory[4,59], memory[5,59], memory[6,59], memory[7,59], memory[8,59], memory[9,59], memory[10,59], memory[11,59], memory[12,59], memory[13,59], memory[14,59], memory[15,59], memory[16,59], memory[17,59], memory[18,59], memory[19,59], memory[20,59], memory[21,59], memory[22,59], memory[23,59], memory[24,59], memory[25,59], memory[26,59], memory[27,59], memory[28,59], memory[29,59], memory[30,59], memory[31,59], memory[32,59], memory[33,59], memory[34,59], memory[35,59], memory[36,59], memory[37,59], memory[38,59], memory[39,59], memory[40,59], memory[41,59], memory[42,59], memory[43,59], memory[44,59], memory[45,59], memory[46,59], memory[47,59], memory[48,59], memory[49,59], memory[50,59], memory[51,59], memory[52,59], memory[53,59], memory[54,59], memory[55,59], memory[56,59], memory[57,59], memory[58,59], memory[59,59], memory[60,59], memory[61,59], memory[62,59], memory[63,59], memory[64,59], memory[65,59], memory[66,59], memory[67,59], memory[68,59], memory[69,59], memory[70,59], memory[71,59], memory[72,59], memory[73,59], memory[74,59], memory[75,59], memory[76,59], memory[77,59], memory[78,59], memory[79,59], memory[80,59], memory[81,59], memory[82,59], memory[83,59], memory[84,59], memory[85,59], memory[86,59], memory[87,59], memory[88,59], memory[89,59], memory[90,59], memory[91,59], memory[92,59], memory[93,59], memory[94,59], memory[95,59], memory[96,59], memory[97,59], memory[98,59], memory[99,59], memory[100,59], memory[101,59], memory[102,59], memory[103,59], memory[104,59], memory[105,59], memory[106,59], memory[107,59], memory[108,59], memory[109,59], memory[110,59], memory[111,59], memory[112,59], memory[113,59], memory[114,59], memory[115,59], memory[116,59], memory[117,59], memory[118,59], memory[119,59], memory[120,59], memory[1,60], memory[2,60], memory[3,60], memory[4,60], memory[5,60], memory[6,60], memory[7,60], memory[8,60], memory[9,60], memory[10,60], memory[11,60], memory[12,60], memory[13,60], memory[14,60], memory[15,60], memory[16,60], memory[17,60], memory[18,60], memory[19,60], memory[20,60], memory[21,60], memory[22,60], memory[23,60], memory[24,60], memory[25,60], memory[26,60], memory[27,60], memory[28,60], memory[29,60], memory[30,60], memory[31,60], memory[32,60], memory[33,60], memory[34,60], memory[35,60], memory[36,60], memory[37,60], memory[38,60], memory[39,60], memory[40,60], memory[41,60], memory[42,60], memory[43,60], memory[44,60], memory[45,60], memory[46,60], memory[47,60], memory[48,60], memory[49,60], memory[50,60], memory[51,60], memory[52,60], memory[53,60], memory[54,60], memory[55,60], memory[56,60], memory[57,60], memory[58,60], memory[59,60], memory[60,60], memory[61,60], memory[62,60], memory[63,60], memory[64,60], memory[65,60], memory[66,60], memory[67,60], memory[68,60], memory[69,60], memory[70,60], memory[71,60], memory[72,60], memory[73,60], memory[74,60], memory[75,60], memory[76,60], memory[77,60], memory[78,60], memory[79,60], memory[80,60], memory[81,60], memory[82,60], memory[83,60], memory[84,60], memory[85,60], memory[86,60], memory[87,60], memory[88,60], memory[89,60], memory[90,60], memory[91,60], memory[92,60], memory[93,60], memory[94,60], memory[95,60], memory[96,60], memory[97,60], memory[98,60], memory[99,60], memory[100,60], memory[101,60], memory[102,60], memory[103,60], memory[104,60], memory[105,60], memory[106,60], memory[107,60], memory[108,60], memory[109,60], memory[110,60], memory[111,60], memory[112,60], memory[113,60], memory[114,60], memory[115,60], memory[116,60], memory[117,60], memory[118,60], memory[119,60], memory[120,60], memory[1,61], memory[2,61], memory[3,61], memory[4,61], memory[5,61], memory[6,61], memory[7,61], memory[8,61], memory[9,61], memory[10,61], memory[11,61], memory[12,61], memory[13,61], memory[14,61], memory[15,61], memory[16,61], memory[17,61], memory[18,61], memory[19,61], memory[20,61], memory[21,61], memory[22,61], memory[23,61], memory[24,61], memory[25,61], memory[26,61], memory[27,61], memory[28,61], memory[29,61], memory[30,61], memory[31,61], memory[32,61], memory[33,61], memory[34,61], memory[35,61], memory[36,61], memory[37,61], memory[38,61], memory[39,61], memory[40,61], memory[41,61], memory[42,61], memory[43,61], memory[44,61], memory[45,61], memory[46,61], memory[47,61], memory[48,61], memory[49,61], memory[50,61], memory[51,61], memory[52,61], memory[53,61], memory[54,61], memory[55,61], memory[56,61], memory[57,61], memory[58,61], memory[59,61], memory[60,61], memory[61,61], memory[62,61], memory[63,61], memory[64,61], memory[65,61], memory[66,61], memory[67,61], memory[68,61], memory[69,61], memory[70,61], memory[71,61], memory[72,61], memory[73,61], memory[74,61], memory[75,61], memory[76,61], memory[77,61], memory[78,61], memory[79,61], memory[80,61], memory[81,61], memory[82,61], memory[83,61], memory[84,61], memory[85,61], memory[86,61], memory[87,61], memory[88,61], memory[89,61], memory[90,61], memory[91,61], memory[92,61], memory[93,61], memory[94,61], memory[95,61], memory[96,61], memory[97,61], memory[98,61], memory[99,61], memory[100,61], memory[101,61], memory[102,61], memory[103,61], memory[104,61], memory[105,61], memory[106,61], memory[107,61], memory[108,61], memory[109,61], memory[110,61], memory[111,61], memory[112,61], memory[113,61], memory[114,61], memory[115,61], memory[116,61], memory[117,61], memory[118,61], memory[119,61], memory[120,61], memory[1,62], memory[2,62], memory[3,62], memory[4,62], memory[5,62], memory[6,62], memory[7,62], memory[8,62], memory[9,62], memory[10,62], memory[11,62], memory[12,62], memory[13,62], memory[14,62], memory[15,62], memory[16,62], memory[17,62], memory[18,62], memory[19,62], memory[20,62], memory[21,62], memory[22,62], memory[23,62], memory[24,62], memory[25,62], memory[26,62], memory[27,62], memory[28,62], memory[29,62], memory[30,62], memory[31,62], memory[32,62], memory[33,62], memory[34,62], memory[35,62], memory[36,62], memory[37,62], memory[38,62], memory[39,62], memory[40,62], memory[41,62], memory[42,62], memory[43,62], memory[44,62], memory[45,62], memory[46,62], memory[47,62], memory[48,62], memory[49,62], memory[50,62], memory[51,62], memory[52,62], memory[53,62], memory[54,62], memory[55,62], memory[56,62], memory[57,62], memory[58,62], memory[59,62], memory[60,62], memory[61,62], memory[62,62], memory[63,62], memory[64,62], memory[65,62], memory[66,62], memory[67,62], memory[68,62], memory[69,62], memory[70,62], memory[71,62], memory[72,62], memory[73,62], memory[74,62], memory[75,62], memory[76,62], memory[77,62], memory[78,62], memory[79,62], memory[80,62], memory[81,62], memory[82,62], memory[83,62], memory[84,62], memory[85,62], memory[86,62], memory[87,62], memory[88,62], memory[89,62], memory[90,62], memory[91,62], memory[92,62], memory[93,62], memory[94,62], memory[95,62], memory[96,62], memory[97,62], memory[98,62], memory[99,62], memory[100,62], memory[101,62], memory[102,62], memory[103,62], memory[104,62], memory[105,62], memory[106,62], memory[107,62], memory[108,62], memory[109,62], memory[110,62], memory[111,62], memory[112,62], memory[113,62], memory[114,62], memory[115,62], memory[116,62], memory[117,62], memory[118,62], memory[119,62], memory[120,62], memory[1,63], memory[2,63], memory[3,63], memory[4,63], memory[5,63], memory[6,63], memory[7,63], memory[8,63], memory[9,63], memory[10,63], memory[11,63], memory[12,63], memory[13,63], memory[14,63], memory[15,63], memory[16,63], memory[17,63], memory[18,63], memory[19,63], memory[20,63], memory[21,63], memory[22,63], memory[23,63], memory[24,63], memory[25,63], memory[26,63], memory[27,63], memory[28,63], memory[29,63], memory[30,63], memory[31,63], memory[32,63], memory[33,63], memory[34,63], memory[35,63], memory[36,63], memory[37,63], memory[38,63], memory[39,63], memory[40,63], memory[41,63], memory[42,63], memory[43,63], memory[44,63], memory[45,63], memory[46,63], memory[47,63], memory[48,63], memory[49,63], memory[50,63], memory[51,63], memory[52,63], memory[53,63], memory[54,63], memory[55,63], memory[56,63], memory[57,63], memory[58,63], memory[59,63], memory[60,63], memory[61,63], memory[62,63], memory[63,63], memory[64,63], memory[65,63], memory[66,63], memory[67,63], memory[68,63], memory[69,63], memory[70,63], memory[71,63], memory[72,63], memory[73,63], memory[74,63], memory[75,63], memory[76,63], memory[77,63], memory[78,63], memory[79,63], memory[80,63], memory[81,63], memory[82,63], memory[83,63], memory[84,63], memory[85,63], memory[86,63], memory[87,63], memory[88,63], memory[89,63], memory[90,63], memory[91,63], memory[92,63], memory[93,63], memory[94,63], memory[95,63], memory[96,63], memory[97,63], memory[98,63], memory[99,63], memory[100,63], memory[101,63], memory[102,63], memory[103,63], memory[104,63], memory[105,63], memory[106,63], memory[107,63], memory[108,63], memory[109,63], memory[110,63], memory[111,63], memory[112,63], memory[113,63], memory[114,63], memory[115,63], memory[116,63], memory[117,63], memory[118,63], memory[119,63], memory[120,63], memory[1,64], memory[2,64], memory[3,64], memory[4,64], memory[5,64], memory[6,64], memory[7,64], memory[8,64], memory[9,64], memory[10,64], memory[11,64], memory[12,64], memory[13,64], memory[14,64], memory[15,64], memory[16,64], memory[17,64], memory[18,64], memory[19,64], memory[20,64], memory[21,64], memory[22,64], memory[23,64], memory[24,64], memory[25,64], memory[26,64], memory[27,64], memory[28,64], memory[29,64], memory[30,64], memory[31,64], memory[32,64], memory[33,64], memory[34,64], memory[35,64], memory[36,64], memory[37,64], memory[38,64], memory[39,64], memory[40,64], memory[41,64], memory[42,64], memory[43,64], memory[44,64], memory[45,64], memory[46,64], memory[47,64], memory[48,64], memory[49,64], memory[50,64], memory[51,64], memory[52,64], memory[53,64], memory[54,64], memory[55,64], memory[56,64], memory[57,64], memory[58,64], memory[59,64], memory[60,64], memory[61,64], memory[62,64], memory[63,64], memory[64,64], memory[65,64], memory[66,64], memory[67,64], memory[68,64], memory[69,64], memory[70,64], memory[71,64], memory[72,64], memory[73,64], memory[74,64], memory[75,64], memory[76,64], memory[77,64], memory[78,64], memory[79,64], memory[80,64], memory[81,64], memory[82,64], memory[83,64], memory[84,64], memory[85,64], memory[86,64], memory[87,64], memory[88,64], memory[89,64], memory[90,64], memory[91,64], memory[92,64], memory[93,64], memory[94,64], memory[95,64], memory[96,64], memory[97,64], memory[98,64], memory[99,64], memory[100,64], memory[101,64], memory[102,64], memory[103,64], memory[104,64], memory[105,64], memory[106,64], memory[107,64], memory[108,64], memory[109,64], memory[110,64], memory[111,64], memory[112,64], memory[113,64], memory[114,64], memory[115,64], memory[116,64], memory[117,64], memory[118,64], memory[119,64], memory[120,64], memory[1,65], memory[2,65], memory[3,65], memory[4,65], memory[5,65], memory[6,65], memory[7,65], memory[8,65], memory[9,65], memory[10,65], memory[11,65], memory[12,65], memory[13,65], memory[14,65], memory[15,65], memory[16,65], memory[17,65], memory[18,65], memory[19,65], memory[20,65], memory[21,65], memory[22,65], memory[23,65], memory[24,65], memory[25,65], memory[26,65], memory[27,65], memory[28,65], memory[29,65], memory[30,65], memory[31,65], memory[32,65], memory[33,65], memory[34,65], memory[35,65], memory[36,65], memory[37,65], memory[38,65], memory[39,65], memory[40,65], memory[41,65], memory[42,65], memory[43,65], memory[44,65], memory[45,65], memory[46,65], memory[47,65], memory[48,65], memory[49,65], memory[50,65], memory[51,65], memory[52,65], memory[53,65], memory[54,65], memory[55,65], memory[56,65], memory[57,65], memory[58,65], memory[59,65], memory[60,65], memory[61,65], memory[62,65], memory[63,65], memory[64,65], memory[65,65], memory[66,65], memory[67,65], memory[68,65], memory[69,65], memory[70,65], memory[71,65], memory[72,65], memory[73,65], memory[74,65], memory[75,65], memory[76,65], memory[77,65], memory[78,65], memory[79,65], memory[80,65], memory[81,65], memory[82,65], memory[83,65], memory[84,65], memory[85,65], memory[86,65], memory[87,65], memory[88,65], memory[89,65], memory[90,65], memory[91,65], memory[92,65], memory[93,65], memory[94,65], memory[95,65], memory[96,65], memory[97,65], memory[98,65], memory[99,65], memory[100,65], memory[101,65], memory[102,65], memory[103,65], memory[104,65], memory[105,65], memory[106,65], memory[107,65], memory[108,65], memory[109,65], memory[110,65], memory[111,65], memory[112,65], memory[113,65], memory[114,65], memory[115,65], memory[116,65], memory[117,65], memory[118,65], memory[119,65], memory[120,65], memory[1,66], memory[2,66], memory[3,66], memory[4,66], memory[5,66], memory[6,66], memory[7,66], memory[8,66], memory[9,66], memory[10,66], memory[11,66], memory[12,66], memory[13,66], memory[14,66], memory[15,66], memory[16,66], memory[17,66], memory[18,66], memory[19,66], memory[20,66], memory[21,66], memory[22,66], memory[23,66], memory[24,66], memory[25,66], memory[26,66], memory[27,66], memory[28,66], memory[29,66], memory[30,66], memory[31,66], memory[32,66], memory[33,66], memory[34,66], memory[35,66], memory[36,66], memory[37,66], memory[38,66], memory[39,66], memory[40,66], memory[41,66], memory[42,66], memory[43,66], memory[44,66], memory[45,66], memory[46,66], memory[47,66], memory[48,66], memory[49,66], memory[50,66], memory[51,66], memory[52,66], memory[53,66], memory[54,66], memory[55,66], memory[56,66], memory[57,66], memory[58,66], memory[59,66], memory[60,66], memory[61,66], memory[62,66], memory[63,66], memory[64,66], memory[65,66], memory[66,66], memory[67,66], memory[68,66], memory[69,66], memory[70,66], memory[71,66], memory[72,66], memory[73,66], memory[74,66], memory[75,66], memory[76,66], memory[77,66], memory[78,66], memory[79,66], memory[80,66], memory[81,66], memory[82,66], memory[83,66], memory[84,66], memory[85,66], memory[86,66], memory[87,66], memory[88,66], memory[89,66], memory[90,66], memory[91,66], memory[92,66], memory[93,66], memory[94,66], memory[95,66], memory[96,66], memory[97,66], memory[98,66], memory[99,66], memory[100,66], memory[101,66], memory[102,66], memory[103,66], memory[104,66], memory[105,66], memory[106,66], memory[107,66], memory[108,66], memory[109,66], memory[110,66], memory[111,66], memory[112,66], memory[113,66], memory[114,66], memory[115,66], memory[116,66], memory[117,66], memory[118,66], memory[119,66], memory[120,66], memory[1,67], memory[2,67], memory[3,67], memory[4,67], memory[5,67], memory[6,67], memory[7,67], memory[8,67], memory[9,67], memory[10,67], memory[11,67], memory[12,67], memory[13,67], memory[14,67], memory[15,67], memory[16,67], memory[17,67], memory[18,67], memory[19,67], memory[20,67], memory[21,67], memory[22,67], memory[23,67], memory[24,67], memory[25,67], memory[26,67], memory[27,67], memory[28,67], memory[29,67], memory[30,67], memory[31,67], memory[32,67], memory[33,67], memory[34,67], memory[35,67], memory[36,67], memory[37,67], memory[38,67], memory[39,67], memory[40,67], memory[41,67], memory[42,67], memory[43,67], memory[44,67], memory[45,67], memory[46,67], memory[47,67], memory[48,67], memory[49,67], memory[50,67], memory[51,67], memory[52,67], memory[53,67], memory[54,67], memory[55,67], memory[56,67], memory[57,67], memory[58,67], memory[59,67], memory[60,67], memory[61,67], memory[62,67], memory[63,67], memory[64,67], memory[65,67], memory[66,67], memory[67,67], memory[68,67], memory[69,67], memory[70,67], memory[71,67], memory[72,67], memory[73,67], memory[74,67], memory[75,67], memory[76,67], memory[77,67], memory[78,67], memory[79,67], memory[80,67], memory[81,67], memory[82,67], memory[83,67], memory[84,67], memory[85,67], memory[86,67], memory[87,67], memory[88,67], memory[89,67], memory[90,67], memory[91,67], memory[92,67], memory[93,67], memory[94,67], memory[95,67], memory[96,67], memory[97,67], memory[98,67], memory[99,67], memory[100,67], memory[101,67], memory[102,67], memory[103,67], memory[104,67], memory[105,67], memory[106,67], memory[107,67], memory[108,67], memory[109,67], memory[110,67], memory[111,67], memory[112,67], memory[113,67], memory[114,67], memory[115,67], memory[116,67], memory[117,67], memory[118,67], memory[119,67], memory[120,67], memory[1,68], memory[2,68], memory[3,68], memory[4,68], memory[5,68], memory[6,68], memory[7,68], memory[8,68], memory[9,68], memory[10,68], memory[11,68], memory[12,68], memory[13,68], memory[14,68], memory[15,68], memory[16,68], memory[17,68], memory[18,68], memory[19,68], memory[20,68], memory[21,68], memory[22,68], memory[23,68], memory[24,68], memory[25,68], memory[26,68], memory[27,68], memory[28,68], memory[29,68], memory[30,68], memory[31,68], memory[32,68], memory[33,68], memory[34,68], memory[35,68], memory[36,68], memory[37,68], memory[38,68], memory[39,68], memory[40,68], memory[41,68], memory[42,68], memory[43,68], memory[44,68], memory[45,68], memory[46,68], memory[47,68], memory[48,68], memory[49,68], memory[50,68], memory[51,68], memory[52,68], memory[53,68], memory[54,68], memory[55,68], memory[56,68], memory[57,68], memory[58,68], memory[59,68], memory[60,68], memory[61,68], memory[62,68], memory[63,68], memory[64,68], memory[65,68], memory[66,68], memory[67,68], memory[68,68], memory[69,68], memory[70,68], memory[71,68], memory[72,68], memory[73,68], memory[74,68], memory[75,68], memory[76,68], memory[77,68], memory[78,68], memory[79,68], memory[80,68], memory[81,68], memory[82,68], memory[83,68], memory[84,68], memory[85,68], memory[86,68], memory[87,68], memory[88,68], memory[89,68], memory[90,68], memory[91,68], memory[92,68], memory[93,68], memory[94,68], memory[95,68], memory[96,68], memory[97,68], memory[98,68], memory[99,68], memory[100,68], memory[101,68], memory[102,68], memory[103,68], memory[104,68], memory[105,68], memory[106,68], memory[107,68], memory[108,68], memory[109,68], memory[110,68], memory[111,68], memory[112,68], memory[113,68], memory[114,68], memory[115,68], memory[116,68], memory[117,68], memory[118,68], memory[119,68], memory[120,68], memory[1,69], memory[2,69], memory[3,69], memory[4,69], memory[5,69], memory[6,69], memory[7,69], memory[8,69], memory[9,69], memory[10,69], memory[11,69], memory[12,69], memory[13,69], memory[14,69], memory[15,69], memory[16,69], memory[17,69], memory[18,69], memory[19,69], memory[20,69], memory[21,69], memory[22,69], memory[23,69], memory[24,69], memory[25,69], memory[26,69], memory[27,69], memory[28,69], memory[29,69], memory[30,69], memory[31,69], memory[32,69], memory[33,69], memory[34,69], memory[35,69], memory[36,69], memory[37,69], memory[38,69], memory[39,69], memory[40,69], memory[41,69], memory[42,69], memory[43,69], memory[44,69], memory[45,69], memory[46,69], memory[47,69], memory[48,69], memory[49,69], memory[50,69], memory[51,69], memory[52,69], memory[53,69], memory[54,69], memory[55,69], memory[56,69], memory[57,69], memory[58,69], memory[59,69], memory[60,69], memory[61,69], memory[62,69], memory[63,69], memory[64,69], memory[65,69], memory[66,69], memory[67,69], memory[68,69], memory[69,69], memory[70,69], memory[71,69], memory[72,69], memory[73,69], memory[74,69], memory[75,69], memory[76,69], memory[77,69], memory[78,69], memory[79,69], memory[80,69], memory[81,69], memory[82,69], memory[83,69], memory[84,69], memory[85,69], memory[86,69], memory[87,69], memory[88,69], memory[89,69], memory[90,69], memory[91,69], memory[92,69], memory[93,69], memory[94,69], memory[95,69], memory[96,69], memory[97,69], memory[98,69], memory[99,69], memory[100,69], memory[101,69], memory[102,69], memory[103,69], memory[104,69], memory[105,69], memory[106,69], memory[107,69], memory[108,69], memory[109,69], memory[110,69], memory[111,69], memory[112,69], memory[113,69], memory[114,69], memory[115,69], memory[116,69], memory[117,69], memory[118,69], memory[119,69], memory[120,69], memory[1,70], memory[2,70], memory[3,70], memory[4,70], memory[5,70], memory[6,70], memory[7,70], memory[8,70], memory[9,70], memory[10,70], memory[11,70], memory[12,70], memory[13,70], memory[14,70], memory[15,70], memory[16,70], memory[17,70], memory[18,70], memory[19,70], memory[20,70], memory[21,70], memory[22,70], memory[23,70], memory[24,70], memory[25,70], memory[26,70], memory[27,70], memory[28,70], memory[29,70], memory[30,70], memory[31,70], memory[32,70], memory[33,70], memory[34,70], memory[35,70], memory[36,70], memory[37,70], memory[38,70], memory[39,70], memory[40,70], memory[41,70], memory[42,70], memory[43,70], memory[44,70], memory[45,70], memory[46,70], memory[47,70], memory[48,70], memory[49,70], memory[50,70], memory[51,70], memory[52,70], memory[53,70], memory[54,70], memory[55,70], memory[56,70], memory[57,70], memory[58,70], memory[59,70], memory[60,70], memory[61,70], memory[62,70], memory[63,70], memory[64,70], memory[65,70], memory[66,70], memory[67,70], memory[68,70], memory[69,70], memory[70,70], memory[71,70], memory[72,70], memory[73,70], memory[74,70], memory[75,70], memory[76,70], memory[77,70], memory[78,70], memory[79,70], memory[80,70], memory[81,70], memory[82,70], memory[83,70], memory[84,70], memory[85,70], memory[86,70], memory[87,70], memory[88,70], memory[89,70], memory[90,70], memory[91,70], memory[92,70], memory[93,70], memory[94,70], memory[95,70], memory[96,70], memory[97,70], memory[98,70], memory[99,70], memory[100,70], memory[101,70], memory[102,70], memory[103,70], memory[104,70], memory[105,70], memory[106,70], memory[107,70], memory[108,70], memory[109,70], memory[110,70], memory[111,70], memory[112,70], memory[113,70], memory[114,70], memory[115,70], memory[116,70], memory[117,70], memory[118,70], memory[119,70], memory[120,70], memory[1,71], memory[2,71], memory[3,71], memory[4,71], memory[5,71], memory[6,71], memory[7,71], memory[8,71], memory[9,71], memory[10,71], memory[11,71], memory[12,71], memory[13,71], memory[14,71], memory[15,71], memory[16,71], memory[17,71], memory[18,71], memory[19,71], memory[20,71], memory[21,71], memory[22,71], memory[23,71], memory[24,71], memory[25,71], memory[26,71], memory[27,71], memory[28,71], memory[29,71], memory[30,71], memory[31,71], memory[32,71], memory[33,71], memory[34,71], memory[35,71], memory[36,71], memory[37,71], memory[38,71], memory[39,71], memory[40,71], memory[41,71], memory[42,71], memory[43,71], memory[44,71], memory[45,71], memory[46,71], memory[47,71], memory[48,71], memory[49,71], memory[50,71], memory[51,71], memory[52,71], memory[53,71], memory[54,71], memory[55,71], memory[56,71], memory[57,71], memory[58,71], memory[59,71], memory[60,71], memory[61,71], memory[62,71], memory[63,71], memory[64,71], memory[65,71], memory[66,71], memory[67,71], memory[68,71], memory[69,71], memory[70,71], memory[71,71], memory[72,71], memory[73,71], memory[74,71], memory[75,71], memory[76,71], memory[77,71], memory[78,71], memory[79,71], memory[80,71], memory[81,71], memory[82,71], memory[83,71], memory[84,71], memory[85,71], memory[86,71], memory[87,71], memory[88,71], memory[89,71], memory[90,71], memory[91,71], memory[92,71], memory[93,71], memory[94,71], memory[95,71], memory[96,71], memory[97,71], memory[98,71], memory[99,71], memory[100,71], memory[101,71], memory[102,71], memory[103,71], memory[104,71], memory[105,71], memory[106,71], memory[107,71], memory[108,71], memory[109,71], memory[110,71], memory[111,71], memory[112,71], memory[113,71], memory[114,71], memory[115,71], memory[116,71], memory[117,71], memory[118,71], memory[119,71], memory[120,71], memory[1,72], memory[2,72], memory[3,72], memory[4,72], memory[5,72], memory[6,72], memory[7,72], memory[8,72], memory[9,72], memory[10,72], memory[11,72], memory[12,72], memory[13,72], memory[14,72], memory[15,72], memory[16,72], memory[17,72], memory[18,72], memory[19,72], memory[20,72], memory[21,72], memory[22,72], memory[23,72], memory[24,72], memory[25,72], memory[26,72], memory[27,72], memory[28,72], memory[29,72], memory[30,72], memory[31,72], memory[32,72], memory[33,72], memory[34,72], memory[35,72], memory[36,72], memory[37,72], memory[38,72], memory[39,72], memory[40,72], memory[41,72], memory[42,72], memory[43,72], memory[44,72], memory[45,72], memory[46,72], memory[47,72], memory[48,72], memory[49,72], memory[50,72], memory[51,72], memory[52,72], memory[53,72], memory[54,72], memory[55,72], memory[56,72], memory[57,72], memory[58,72], memory[59,72], memory[60,72], memory[61,72], memory[62,72], memory[63,72], memory[64,72], memory[65,72], memory[66,72], memory[67,72], memory[68,72], memory[69,72], memory[70,72], memory[71,72], memory[72,72], memory[73,72], memory[74,72], memory[75,72], memory[76,72], memory[77,72], memory[78,72], memory[79,72], memory[80,72], memory[81,72], memory[82,72], memory[83,72], memory[84,72], memory[85,72], memory[86,72], memory[87,72], memory[88,72], memory[89,72], memory[90,72], memory[91,72], memory[92,72], memory[93,72], memory[94,72], memory[95,72], memory[96,72], memory[97,72], memory[98,72], memory[99,72], memory[100,72], memory[101,72], memory[102,72], memory[103,72], memory[104,72], memory[105,72], memory[106,72], memory[107,72], memory[108,72], memory[109,72], memory[110,72], memory[111,72], memory[112,72], memory[113,72], memory[114,72], memory[115,72], memory[116,72], memory[117,72], memory[118,72], memory[119,72], memory[120,72], memory[1,73], memory[2,73], memory[3,73], memory[4,73], memory[5,73], memory[6,73], memory[7,73], memory[8,73], memory[9,73], memory[10,73], memory[11,73], memory[12,73], memory[13,73], memory[14,73], memory[15,73], memory[16,73], memory[17,73], memory[18,73], memory[19,73], memory[20,73], memory[21,73], memory[22,73], memory[23,73], memory[24,73], memory[25,73], memory[26,73], memory[27,73], memory[28,73], memory[29,73], memory[30,73], memory[31,73], memory[32,73], memory[33,73], memory[34,73], memory[35,73], memory[36,73], memory[37,73], memory[38,73], memory[39,73], memory[40,73], memory[41,73], memory[42,73], memory[43,73], memory[44,73], memory[45,73], memory[46,73], memory[47,73], memory[48,73], memory[49,73], memory[50,73], memory[51,73], memory[52,73], memory[53,73], memory[54,73], memory[55,73], memory[56,73], memory[57,73], memory[58,73], memory[59,73], memory[60,73], memory[61,73], memory[62,73], memory[63,73], memory[64,73], memory[65,73], memory[66,73], memory[67,73], memory[68,73], memory[69,73], memory[70,73], memory[71,73], memory[72,73], memory[73,73], memory[74,73], memory[75,73], memory[76,73], memory[77,73], memory[78,73], memory[79,73], memory[80,73], memory[81,73], memory[82,73], memory[83,73], memory[84,73], memory[85,73], memory[86,73], memory[87,73], memory[88,73], memory[89,73], memory[90,73], memory[91,73], memory[92,73], memory[93,73], memory[94,73], memory[95,73], memory[96,73], memory[97,73], memory[98,73], memory[99,73], memory[100,73], memory[101,73], memory[102,73], memory[103,73], memory[104,73], memory[105,73], memory[106,73], memory[107,73], memory[108,73], memory[109,73], memory[110,73], memory[111,73], memory[112,73], memory[113,73], memory[114,73], memory[115,73], memory[116,73], memory[117,73], memory[118,73], memory[119,73], memory[120,73], memory[1,74], memory[2,74], memory[3,74], memory[4,74], memory[5,74], memory[6,74], memory[7,74], memory[8,74], memory[9,74], memory[10,74], memory[11,74], memory[12,74], memory[13,74], memory[14,74], memory[15,74], memory[16,74], memory[17,74], memory[18,74], memory[19,74], memory[20,74], memory[21,74], memory[22,74], memory[23,74], memory[24,74], memory[25,74], memory[26,74], memory[27,74], memory[28,74], memory[29,74], memory[30,74], memory[31,74], memory[32,74], memory[33,74], memory[34,74], memory[35,74], memory[36,74], memory[37,74], memory[38,74], memory[39,74], memory[40,74], memory[41,74], memory[42,74], memory[43,74], memory[44,74], memory[45,74], memory[46,74], memory[47,74], memory[48,74], memory[49,74], memory[50,74], memory[51,74], memory[52,74], memory[53,74], memory[54,74], memory[55,74], memory[56,74], memory[57,74], memory[58,74], memory[59,74], memory[60,74], memory[61,74], memory[62,74], memory[63,74], memory[64,74], memory[65,74], memory[66,74], memory[67,74], memory[68,74], memory[69,74], memory[70,74], memory[71,74], memory[72,74], memory[73,74], memory[74,74], memory[75,74], memory[76,74], memory[77,74], memory[78,74], memory[79,74], memory[80,74], memory[81,74], memory[82,74], memory[83,74], memory[84,74], memory[85,74], memory[86,74], memory[87,74], memory[88,74], memory[89,74], memory[90,74], memory[91,74], memory[92,74], memory[93,74], memory[94,74], memory[95,74], memory[96,74], memory[97,74], memory[98,74], memory[99,74], memory[100,74], memory[101,74], memory[102,74], memory[103,74], memory[104,74], memory[105,74], memory[106,74], memory[107,74], memory[108,74], memory[109,74], memory[110,74], memory[111,74], memory[112,74], memory[113,74], memory[114,74], memory[115,74], memory[116,74], memory[117,74], memory[118,74], memory[119,74], memory[120,74], memory[1,75], memory[2,75], memory[3,75], memory[4,75], memory[5,75], memory[6,75], memory[7,75], memory[8,75], memory[9,75], memory[10,75], memory[11,75], memory[12,75], memory[13,75], memory[14,75], memory[15,75], memory[16,75], memory[17,75], memory[18,75], memory[19,75], memory[20,75], memory[21,75], memory[22,75], memory[23,75], memory[24,75], memory[25,75], memory[26,75], memory[27,75], memory[28,75], memory[29,75], memory[30,75], memory[31,75], memory[32,75], memory[33,75], memory[34,75], memory[35,75], memory[36,75], memory[37,75], memory[38,75], memory[39,75], memory[40,75], memory[41,75], memory[42,75], memory[43,75], memory[44,75], memory[45,75], memory[46,75], memory[47,75], memory[48,75], memory[49,75], memory[50,75], memory[51,75], memory[52,75], memory[53,75], memory[54,75], memory[55,75], memory[56,75], memory[57,75], memory[58,75], memory[59,75], memory[60,75], memory[61,75], memory[62,75], memory[63,75], memory[64,75], memory[65,75], memory[66,75], memory[67,75], memory[68,75], memory[69,75], memory[70,75], memory[71,75], memory[72,75], memory[73,75], memory[74,75], memory[75,75], memory[76,75], memory[77,75], memory[78,75], memory[79,75], memory[80,75], memory[81,75], memory[82,75], memory[83,75], memory[84,75], memory[85,75], memory[86,75], memory[87,75], memory[88,75], memory[89,75], memory[90,75], memory[91,75], memory[92,75], memory[93,75], memory[94,75], memory[95,75], memory[96,75], memory[97,75], memory[98,75], memory[99,75], memory[100,75], memory[101,75], memory[102,75], memory[103,75], memory[104,75], memory[105,75], memory[106,75], memory[107,75], memory[108,75], memory[109,75], memory[110,75], memory[111,75], memory[112,75], memory[113,75], memory[114,75], memory[115,75], memory[116,75], memory[117,75], memory[118,75], memory[119,75], memory[120,75], memory[1,76], memory[2,76], memory[3,76], memory[4,76], memory[5,76], memory[6,76], memory[7,76], memory[8,76], memory[9,76], memory[10,76], memory[11,76], memory[12,76], memory[13,76], memory[14,76], memory[15,76], memory[16,76], memory[17,76], memory[18,76], memory[19,76], memory[20,76], memory[21,76], memory[22,76], memory[23,76], memory[24,76], memory[25,76], memory[26,76], memory[27,76], memory[28,76], memory[29,76], memory[30,76], memory[31,76], memory[32,76], memory[33,76], memory[34,76], memory[35,76], memory[36,76], memory[37,76], memory[38,76], memory[39,76], memory[40,76], memory[41,76], memory[42,76], memory[43,76], memory[44,76], memory[45,76], memory[46,76], memory[47,76], memory[48,76], memory[49,76], memory[50,76], memory[51,76], memory[52,76], memory[53,76], memory[54,76], memory[55,76], memory[56,76], memory[57,76], memory[58,76], memory[59,76], memory[60,76], memory[61,76], memory[62,76], memory[63,76], memory[64,76], memory[65,76], memory[66,76], memory[67,76], memory[68,76], memory[69,76], memory[70,76], memory[71,76], memory[72,76], memory[73,76], memory[74,76], memory[75,76], memory[76,76], memory[77,76], memory[78,76], memory[79,76], memory[80,76], memory[81,76], memory[82,76], memory[83,76], memory[84,76], memory[85,76], memory[86,76], memory[87,76], memory[88,76], memory[89,76], memory[90,76], memory[91,76], memory[92,76], memory[93,76], memory[94,76], memory[95,76], memory[96,76], memory[97,76], memory[98,76], memory[99,76], memory[100,76], memory[101,76], memory[102,76], memory[103,76], memory[104,76], memory[105,76], memory[106,76], memory[107,76], memory[108,76], memory[109,76], memory[110,76], memory[111,76], memory[112,76], memory[113,76], memory[114,76], memory[115,76], memory[116,76], memory[117,76], memory[118,76], memory[119,76], memory[120,76], memory[1,77], memory[2,77], memory[3,77], memory[4,77], memory[5,77], memory[6,77], memory[7,77], memory[8,77], memory[9,77], memory[10,77], memory[11,77], memory[12,77], memory[13,77], memory[14,77], memory[15,77], memory[16,77], memory[17,77], memory[18,77], memory[19,77], memory[20,77], memory[21,77], memory[22,77], memory[23,77], memory[24,77], memory[25,77], memory[26,77], memory[27,77], memory[28,77], memory[29,77], memory[30,77], memory[31,77], memory[32,77], memory[33,77], memory[34,77], memory[35,77], memory[36,77], memory[37,77], memory[38,77], memory[39,77], memory[40,77], memory[41,77], memory[42,77], memory[43,77], memory[44,77], memory[45,77], memory[46,77], memory[47,77], memory[48,77], memory[49,77], memory[50,77], memory[51,77], memory[52,77], memory[53,77], memory[54,77], memory[55,77], memory[56,77], memory[57,77], memory[58,77], memory[59,77], memory[60,77], memory[61,77], memory[62,77], memory[63,77], memory[64,77], memory[65,77], memory[66,77], memory[67,77], memory[68,77], memory[69,77], memory[70,77], memory[71,77], memory[72,77], memory[73,77], memory[74,77], memory[75,77], memory[76,77], memory[77,77], memory[78,77], memory[79,77], memory[80,77], memory[81,77], memory[82,77], memory[83,77], memory[84,77], memory[85,77], memory[86,77], memory[87,77], memory[88,77], memory[89,77], memory[90,77], memory[91,77], memory[92,77], memory[93,77], memory[94,77], memory[95,77], memory[96,77], memory[97,77], memory[98,77], memory[99,77], memory[100,77], memory[101,77], memory[102,77], memory[103,77], memory[104,77], memory[105,77], memory[106,77], memory[107,77], memory[108,77], memory[109,77], memory[110,77], memory[111,77], memory[112,77], memory[113,77], memory[114,77], memory[115,77], memory[116,77], memory[117,77], memory[118,77], memory[119,77], memory[120,77], memory[1,78], memory[2,78], memory[3,78], memory[4,78], memory[5,78], memory[6,78], memory[7,78], memory[8,78], memory[9,78], memory[10,78], memory[11,78], memory[12,78], memory[13,78], memory[14,78], memory[15,78], memory[16,78], memory[17,78], memory[18,78], memory[19,78], memory[20,78], memory[21,78], memory[22,78], memory[23,78], memory[24,78], memory[25,78], memory[26,78], memory[27,78], memory[28,78], memory[29,78], memory[30,78], memory[31,78], memory[32,78], memory[33,78], memory[34,78], memory[35,78], memory[36,78], memory[37,78], memory[38,78], memory[39,78], memory[40,78], memory[41,78], memory[42,78], memory[43,78], memory[44,78], memory[45,78], memory[46,78], memory[47,78], memory[48,78], memory[49,78], memory[50,78], memory[51,78], memory[52,78], memory[53,78], memory[54,78], memory[55,78], memory[56,78], memory[57,78], memory[58,78], memory[59,78], memory[60,78], memory[61,78], memory[62,78], memory[63,78], memory[64,78], memory[65,78], memory[66,78], memory[67,78], memory[68,78], memory[69,78], memory[70,78], memory[71,78], memory[72,78], memory[73,78], memory[74,78], memory[75,78], memory[76,78], memory[77,78], memory[78,78], memory[79,78], memory[80,78], memory[81,78], memory[82,78], memory[83,78], memory[84,78], memory[85,78], memory[86,78], memory[87,78], memory[88,78], memory[89,78], memory[90,78], memory[91,78], memory[92,78], memory[93,78], memory[94,78], memory[95,78], memory[96,78], memory[97,78], memory[98,78], memory[99,78], memory[100,78], memory[101,78], memory[102,78], memory[103,78], memory[104,78], memory[105,78], memory[106,78], memory[107,78], memory[108,78], memory[109,78], memory[110,78], memory[111,78], memory[112,78], memory[113,78], memory[114,78], memory[115,78], memory[116,78], memory[117,78], memory[118,78], memory[119,78], memory[120,78], memory[1,79], memory[2,79], memory[3,79], memory[4,79], memory[5,79], memory[6,79], memory[7,79], memory[8,79], memory[9,79], memory[10,79], memory[11,79], memory[12,79], memory[13,79], memory[14,79], memory[15,79], memory[16,79], memory[17,79], memory[18,79], memory[19,79], memory[20,79], memory[21,79], memory[22,79], memory[23,79], memory[24,79], memory[25,79], memory[26,79], memory[27,79], memory[28,79], memory[29,79], memory[30,79], memory[31,79], memory[32,79], memory[33,79], memory[34,79], memory[35,79], memory[36,79], memory[37,79], memory[38,79], memory[39,79], memory[40,79], memory[41,79], memory[42,79], memory[43,79], memory[44,79], memory[45,79], memory[46,79], memory[47,79], memory[48,79], memory[49,79], memory[50,79], memory[51,79], memory[52,79], memory[53,79], memory[54,79], memory[55,79], memory[56,79], memory[57,79], memory[58,79], memory[59,79], memory[60,79], memory[61,79], memory[62,79], memory[63,79], memory[64,79], memory[65,79], memory[66,79], memory[67,79], memory[68,79], memory[69,79], memory[70,79], memory[71,79], memory[72,79], memory[73,79], memory[74,79], memory[75,79], memory[76,79], memory[77,79], memory[78,79], memory[79,79], memory[80,79], memory[81,79], memory[82,79], memory[83,79], memory[84,79], memory[85,79], memory[86,79], memory[87,79], memory[88,79], memory[89,79], memory[90,79], memory[91,79], memory[92,79], memory[93,79], memory[94,79], memory[95,79], memory[96,79], memory[97,79], memory[98,79], memory[99,79], memory[100,79], memory[101,79], memory[102,79], memory[103,79], memory[104,79], memory[105,79], memory[106,79], memory[107,79], memory[108,79], memory[109,79], memory[110,79], memory[111,79], memory[112,79], memory[113,79], memory[114,79], memory[115,79], memory[116,79], memory[117,79], memory[118,79], memory[119,79], memory[120,79], memory[1,80], memory[2,80], memory[3,80], memory[4,80], memory[5,80], memory[6,80], memory[7,80], memory[8,80], memory[9,80], memory[10,80], memory[11,80], memory[12,80], memory[13,80], memory[14,80], memory[15,80], memory[16,80], memory[17,80], memory[18,80], memory[19,80], memory[20,80], memory[21,80], memory[22,80], memory[23,80], memory[24,80], memory[25,80], memory[26,80], memory[27,80], memory[28,80], memory[29,80], memory[30,80], memory[31,80], memory[32,80], memory[33,80], memory[34,80], memory[35,80], memory[36,80], memory[37,80], memory[38,80], memory[39,80], memory[40,80], memory[41,80], memory[42,80], memory[43,80], memory[44,80], memory[45,80], memory[46,80], memory[47,80], memory[48,80], memory[49,80], memory[50,80], memory[51,80], memory[52,80], memory[53,80], memory[54,80], memory[55,80], memory[56,80], memory[57,80], memory[58,80], memory[59,80], memory[60,80], memory[61,80], memory[62,80], memory[63,80], memory[64,80], memory[65,80], memory[66,80], memory[67,80], memory[68,80], memory[69,80], memory[70,80], memory[71,80], memory[72,80], memory[73,80], memory[74,80], memory[75,80], memory[76,80], memory[77,80], memory[78,80], memory[79,80], memory[80,80], memory[81,80], memory[82,80], memory[83,80], memory[84,80], memory[85,80], memory[86,80], memory[87,80], memory[88,80], memory[89,80], memory[90,80], memory[91,80], memory[92,80], memory[93,80], memory[94,80], memory[95,80], memory[96,80], memory[97,80], memory[98,80], memory[99,80], memory[100,80], memory[101,80], memory[102,80], memory[103,80], memory[104,80], memory[105,80], memory[106,80], memory[107,80], memory[108,80], memory[109,80], memory[110,80], memory[111,80], memory[112,80], memory[113,80], memory[114,80], memory[115,80], memory[116,80], memory[117,80], memory[118,80], memory[119,80], memory[120,80], memory[1,81], memory[2,81], memory[3,81], memory[4,81], memory[5,81], memory[6,81], memory[7,81], memory[8,81], memory[9,81], memory[10,81], memory[11,81], memory[12,81], memory[13,81], memory[14,81], memory[15,81], memory[16,81], memory[17,81], memory[18,81], memory[19,81], memory[20,81], memory[21,81], memory[22,81], memory[23,81], memory[24,81], memory[25,81], memory[26,81], memory[27,81], memory[28,81], memory[29,81], memory[30,81], memory[31,81], memory[32,81], memory[33,81], memory[34,81], memory[35,81], memory[36,81], memory[37,81], memory[38,81], memory[39,81], memory[40,81], memory[41,81], memory[42,81], memory[43,81], memory[44,81], memory[45,81], memory[46,81], memory[47,81], memory[48,81], memory[49,81], memory[50,81], memory[51,81], memory[52,81], memory[53,81], memory[54,81], memory[55,81], memory[56,81], memory[57,81], memory[58,81], memory[59,81], memory[60,81], memory[61,81], memory[62,81], memory[63,81], memory[64,81], memory[65,81], memory[66,81], memory[67,81], memory[68,81], memory[69,81], memory[70,81], memory[71,81], memory[72,81], memory[73,81], memory[74,81], memory[75,81], memory[76,81], memory[77,81], memory[78,81], memory[79,81], memory[80,81], memory[81,81], memory[82,81], memory[83,81], memory[84,81], memory[85,81], memory[86,81], memory[87,81], memory[88,81], memory[89,81], memory[90,81], memory[91,81], memory[92,81], memory[93,81], memory[94,81], memory[95,81], memory[96,81], memory[97,81], memory[98,81], memory[99,81], memory[100,81], memory[101,81], memory[102,81], memory[103,81], memory[104,81], memory[105,81], memory[106,81], memory[107,81], memory[108,81], memory[109,81], memory[110,81], memory[111,81], memory[112,81], memory[113,81], memory[114,81], memory[115,81], memory[116,81], memory[117,81], memory[118,81], memory[119,81], memory[120,81], memory[1,82], memory[2,82], memory[3,82], memory[4,82], memory[5,82], memory[6,82], memory[7,82], memory[8,82], memory[9,82], memory[10,82], memory[11,82], memory[12,82], memory[13,82], memory[14,82], memory[15,82], memory[16,82], memory[17,82], memory[18,82], memory[19,82], memory[20,82], memory[21,82], memory[22,82], memory[23,82], memory[24,82], memory[25,82], memory[26,82], memory[27,82], memory[28,82], memory[29,82], memory[30,82], memory[31,82], memory[32,82], memory[33,82], memory[34,82], memory[35,82], memory[36,82], memory[37,82], memory[38,82], memory[39,82], memory[40,82], memory[41,82], memory[42,82], memory[43,82], memory[44,82], memory[45,82], memory[46,82], memory[47,82], memory[48,82], memory[49,82], memory[50,82], memory[51,82], memory[52,82], memory[53,82], memory[54,82], memory[55,82], memory[56,82], memory[57,82], memory[58,82], memory[59,82], memory[60,82], memory[61,82], memory[62,82], memory[63,82], memory[64,82], memory[65,82], memory[66,82], memory[67,82], memory[68,82], memory[69,82], memory[70,82], memory[71,82], memory[72,82], memory[73,82], memory[74,82], memory[75,82], memory[76,82], memory[77,82], memory[78,82], memory[79,82], memory[80,82], memory[81,82], memory[82,82], memory[83,82], memory[84,82], memory[85,82], memory[86,82], memory[87,82], memory[88,82], memory[89,82], memory[90,82], memory[91,82], memory[92,82], memory[93,82], memory[94,82], memory[95,82], memory[96,82], memory[97,82], memory[98,82], memory[99,82], memory[100,82], memory[101,82], memory[102,82], memory[103,82], memory[104,82], memory[105,82], memory[106,82], memory[107,82], memory[108,82], memory[109,82], memory[110,82], memory[111,82], memory[112,82], memory[113,82], memory[114,82], memory[115,82], memory[116,82], memory[117,82], memory[118,82], memory[119,82], memory[120,82], memory[1,83], memory[2,83], memory[3,83], memory[4,83], memory[5,83], memory[6,83], memory[7,83], memory[8,83], memory[9,83], memory[10,83], memory[11,83], memory[12,83], memory[13,83], memory[14,83], memory[15,83], memory[16,83], memory[17,83], memory[18,83], memory[19,83], memory[20,83], memory[21,83], memory[22,83], memory[23,83], memory[24,83], memory[25,83], memory[26,83], memory[27,83], memory[28,83], memory[29,83], memory[30,83], memory[31,83], memory[32,83], memory[33,83], memory[34,83], memory[35,83], memory[36,83], memory[37,83], memory[38,83], memory[39,83], memory[40,83], memory[41,83], memory[42,83], memory[43,83], memory[44,83], memory[45,83], memory[46,83], memory[47,83], memory[48,83], memory[49,83], memory[50,83], memory[51,83], memory[52,83], memory[53,83], memory[54,83], memory[55,83], memory[56,83], memory[57,83], memory[58,83], memory[59,83], memory[60,83], memory[61,83], memory[62,83], memory[63,83], memory[64,83], memory[65,83], memory[66,83], memory[67,83], memory[68,83], memory[69,83], memory[70,83], memory[71,83], memory[72,83], memory[73,83], memory[74,83], memory[75,83], memory[76,83], memory[77,83], memory[78,83], memory[79,83], memory[80,83], memory[81,83], memory[82,83], memory[83,83], memory[84,83], memory[85,83], memory[86,83], memory[87,83], memory[88,83], memory[89,83], memory[90,83], memory[91,83], memory[92,83], memory[93,83], memory[94,83], memory[95,83], memory[96,83], memory[97,83], memory[98,83], memory[99,83], memory[100,83], memory[101,83], memory[102,83], memory[103,83], memory[104,83], memory[105,83], memory[106,83], memory[107,83], memory[108,83], memory[109,83], memory[110,83], memory[111,83], memory[112,83], memory[113,83], memory[114,83], memory[115,83], memory[116,83], memory[117,83], memory[118,83], memory[119,83], memory[120,83], memory[1,84], memory[2,84], memory[3,84], memory[4,84], memory[5,84], memory[6,84], memory[7,84], memory[8,84], memory[9,84], memory[10,84], memory[11,84], memory[12,84], memory[13,84], memory[14,84], memory[15,84], memory[16,84], memory[17,84], memory[18,84], memory[19,84], memory[20,84], memory[21,84], memory[22,84], memory[23,84], memory[24,84], memory[25,84], memory[26,84], memory[27,84], memory[28,84], memory[29,84], memory[30,84], memory[31,84], memory[32,84], memory[33,84], memory[34,84], memory[35,84], memory[36,84], memory[37,84], memory[38,84], memory[39,84], memory[40,84], memory[41,84], memory[42,84], memory[43,84], memory[44,84], memory[45,84], memory[46,84], memory[47,84], memory[48,84], memory[49,84], memory[50,84], memory[51,84], memory[52,84], memory[53,84], memory[54,84], memory[55,84], memory[56,84], memory[57,84], memory[58,84], memory[59,84], memory[60,84], memory[61,84], memory[62,84], memory[63,84], memory[64,84], memory[65,84], memory[66,84], memory[67,84], memory[68,84], memory[69,84], memory[70,84], memory[71,84], memory[72,84], memory[73,84], memory[74,84], memory[75,84], memory[76,84], memory[77,84], memory[78,84], memory[79,84], memory[80,84], memory[81,84], memory[82,84], memory[83,84], memory[84,84], memory[85,84], memory[86,84], memory[87,84], memory[88,84], memory[89,84], memory[90,84], memory[91,84], memory[92,84], memory[93,84], memory[94,84], memory[95,84], memory[96,84], memory[97,84], memory[98,84], memory[99,84], memory[100,84], memory[101,84], memory[102,84], memory[103,84], memory[104,84], memory[105,84], memory[106,84], memory[107,84], memory[108,84], memory[109,84], memory[110,84], memory[111,84], memory[112,84], memory[113,84], memory[114,84], memory[115,84], memory[116,84], memory[117,84], memory[118,84], memory[119,84], memory[120,84], memory[1,85], memory[2,85], memory[3,85], memory[4,85], memory[5,85], memory[6,85], memory[7,85], memory[8,85], memory[9,85], memory[10,85], memory[11,85], memory[12,85], memory[13,85], memory[14,85], memory[15,85], memory[16,85], memory[17,85], memory[18,85], memory[19,85], memory[20,85], memory[21,85], memory[22,85], memory[23,85], memory[24,85], memory[25,85], memory[26,85], memory[27,85], memory[28,85], memory[29,85], memory[30,85], memory[31,85], memory[32,85], memory[33,85], memory[34,85], memory[35,85], memory[36,85], memory[37,85], memory[38,85], memory[39,85], memory[40,85], memory[41,85], memory[42,85], memory[43,85], memory[44,85], memory[45,85], memory[46,85], memory[47,85], memory[48,85], memory[49,85], memory[50,85], memory[51,85], memory[52,85], memory[53,85], memory[54,85], memory[55,85], memory[56,85], memory[57,85], memory[58,85], memory[59,85], memory[60,85], memory[61,85], memory[62,85], memory[63,85], memory[64,85], memory[65,85], memory[66,85], memory[67,85], memory[68,85], memory[69,85], memory[70,85], memory[71,85], memory[72,85], memory[73,85], memory[74,85], memory[75,85], memory[76,85], memory[77,85], memory[78,85], memory[79,85], memory[80,85], memory[81,85], memory[82,85], memory[83,85], memory[84,85], memory[85,85], memory[86,85], memory[87,85], memory[88,85], memory[89,85], memory[90,85], memory[91,85], memory[92,85], memory[93,85], memory[94,85], memory[95,85], memory[96,85], memory[97,85], memory[98,85], memory[99,85], memory[100,85], memory[101,85], memory[102,85], memory[103,85], memory[104,85], memory[105,85], memory[106,85], memory[107,85], memory[108,85], memory[109,85], memory[110,85], memory[111,85], memory[112,85], memory[113,85], memory[114,85], memory[115,85], memory[116,85], memory[117,85], memory[118,85], memory[119,85], memory[120,85], memory[1,86], memory[2,86], memory[3,86], memory[4,86], memory[5,86], memory[6,86], memory[7,86], memory[8,86], memory[9,86], memory[10,86], memory[11,86], memory[12,86], memory[13,86], memory[14,86], memory[15,86], memory[16,86], memory[17,86], memory[18,86], memory[19,86], memory[20,86], memory[21,86], memory[22,86], memory[23,86], memory[24,86], memory[25,86], memory[26,86], memory[27,86], memory[28,86], memory[29,86], memory[30,86], memory[31,86], memory[32,86], memory[33,86], memory[34,86], memory[35,86], memory[36,86], memory[37,86], memory[38,86], memory[39,86], memory[40,86], memory[41,86], memory[42,86], memory[43,86], memory[44,86], memory[45,86], memory[46,86], memory[47,86], memory[48,86], memory[49,86], memory[50,86], memory[51,86], memory[52,86], memory[53,86], memory[54,86], memory[55,86], memory[56,86], memory[57,86], memory[58,86], memory[59,86], memory[60,86], memory[61,86], memory[62,86], memory[63,86], memory[64,86], memory[65,86], memory[66,86], memory[67,86], memory[68,86], memory[69,86], memory[70,86], memory[71,86], memory[72,86], memory[73,86], memory[74,86], memory[75,86], memory[76,86], memory[77,86], memory[78,86], memory[79,86], memory[80,86], memory[81,86], memory[82,86], memory[83,86], memory[84,86], memory[85,86], memory[86,86], memory[87,86], memory[88,86], memory[89,86], memory[90,86], memory[91,86], memory[92,86], memory[93,86], memory[94,86], memory[95,86], memory[96,86], memory[97,86], memory[98,86], memory[99,86], memory[100,86], memory[101,86], memory[102,86], memory[103,86], memory[104,86], memory[105,86], memory[106,86], memory[107,86], memory[108,86], memory[109,86], memory[110,86], memory[111,86], memory[112,86], memory[113,86], memory[114,86], memory[115,86], memory[116,86], memory[117,86], memory[118,86], memory[119,86], memory[120,86], memory[1,87], memory[2,87], memory[3,87], memory[4,87], memory[5,87], memory[6,87], memory[7,87], memory[8,87], memory[9,87], memory[10,87], memory[11,87], memory[12,87], memory[13,87], memory[14,87], memory[15,87], memory[16,87], memory[17,87], memory[18,87], memory[19,87], memory[20,87], memory[21,87], memory[22,87], memory[23,87], memory[24,87], memory[25,87], memory[26,87], memory[27,87], memory[28,87], memory[29,87], memory[30,87], memory[31,87], memory[32,87], memory[33,87], memory[34,87], memory[35,87], memory[36,87], memory[37,87], memory[38,87], memory[39,87], memory[40,87], memory[41,87], memory[42,87], memory[43,87], memory[44,87], memory[45,87], memory[46,87], memory[47,87], memory[48,87], memory[49,87], memory[50,87], memory[51,87], memory[52,87], memory[53,87], memory[54,87], memory[55,87], memory[56,87], memory[57,87], memory[58,87], memory[59,87], memory[60,87], memory[61,87], memory[62,87], memory[63,87], memory[64,87], memory[65,87], memory[66,87], memory[67,87], memory[68,87], memory[69,87], memory[70,87], memory[71,87], memory[72,87], memory[73,87], memory[74,87], memory[75,87], memory[76,87], memory[77,87], memory[78,87], memory[79,87], memory[80,87], memory[81,87], memory[82,87], memory[83,87], memory[84,87], memory[85,87], memory[86,87], memory[87,87], memory[88,87], memory[89,87], memory[90,87], memory[91,87], memory[92,87], memory[93,87], memory[94,87], memory[95,87], memory[96,87], memory[97,87], memory[98,87], memory[99,87], memory[100,87], memory[101,87], memory[102,87], memory[103,87], memory[104,87], memory[105,87], memory[106,87], memory[107,87], memory[108,87], memory[109,87], memory[110,87], memory[111,87], memory[112,87], memory[113,87], memory[114,87], memory[115,87], memory[116,87], memory[117,87], memory[118,87], memory[119,87], memory[120,87], memory[1,88], memory[2,88], memory[3,88], memory[4,88], memory[5,88], memory[6,88], memory[7,88], memory[8,88], memory[9,88], memory[10,88], memory[11,88], memory[12,88], memory[13,88], memory[14,88], memory[15,88], memory[16,88], memory[17,88], memory[18,88], memory[19,88], memory[20,88], memory[21,88], memory[22,88], memory[23,88], memory[24,88], memory[25,88], memory[26,88], memory[27,88], memory[28,88], memory[29,88], memory[30,88], memory[31,88], memory[32,88], memory[33,88], memory[34,88], memory[35,88], memory[36,88], memory[37,88], memory[38,88], memory[39,88], memory[40,88], memory[41,88], memory[42,88], memory[43,88], memory[44,88], memory[45,88], memory[46,88], memory[47,88], memory[48,88], memory[49,88], memory[50,88], memory[51,88], memory[52,88], memory[53,88], memory[54,88], memory[55,88], memory[56,88], memory[57,88], memory[58,88], memory[59,88], memory[60,88], memory[61,88], memory[62,88], memory[63,88], memory[64,88], memory[65,88], memory[66,88], memory[67,88], memory[68,88], memory[69,88], memory[70,88], memory[71,88], memory[72,88], memory[73,88], memory[74,88], memory[75,88], memory[76,88], memory[77,88], memory[78,88], memory[79,88], memory[80,88], memory[81,88], memory[82,88], memory[83,88], memory[84,88], memory[85,88], memory[86,88], memory[87,88], memory[88,88], memory[89,88], memory[90,88], memory[91,88], memory[92,88], memory[93,88], memory[94,88], memory[95,88], memory[96,88], memory[97,88], memory[98,88], memory[99,88], memory[100,88], memory[101,88], memory[102,88], memory[103,88], memory[104,88], memory[105,88], memory[106,88], memory[107,88], memory[108,88], memory[109,88], memory[110,88], memory[111,88], memory[112,88], memory[113,88], memory[114,88], memory[115,88], memory[116,88], memory[117,88], memory[118,88], memory[119,88], memory[120,88], memory[1,89], memory[2,89], memory[3,89], memory[4,89], memory[5,89], memory[6,89], memory[7,89], memory[8,89], memory[9,89], memory[10,89], memory[11,89], memory[12,89], memory[13,89], memory[14,89], memory[15,89], memory[16,89], memory[17,89], memory[18,89], memory[19,89], memory[20,89], memory[21,89], memory[22,89], memory[23,89], memory[24,89], memory[25,89], memory[26,89], memory[27,89], memory[28,89], memory[29,89], memory[30,89], memory[31,89], memory[32,89], memory[33,89], memory[34,89], memory[35,89], memory[36,89], memory[37,89], memory[38,89], memory[39,89], memory[40,89], memory[41,89], memory[42,89], memory[43,89], memory[44,89], memory[45,89], memory[46,89], memory[47,89], memory[48,89], memory[49,89], memory[50,89], memory[51,89], memory[52,89], memory[53,89], memory[54,89], memory[55,89], memory[56,89], memory[57,89], memory[58,89], memory[59,89], memory[60,89], memory[61,89], memory[62,89], memory[63,89], memory[64,89], memory[65,89], memory[66,89], memory[67,89], memory[68,89], memory[69,89], memory[70,89], memory[71,89], memory[72,89], memory[73,89], memory[74,89], memory[75,89], memory[76,89], memory[77,89], memory[78,89], memory[79,89], memory[80,89], memory[81,89], memory[82,89], memory[83,89], memory[84,89], memory[85,89], memory[86,89], memory[87,89], memory[88,89], memory[89,89], memory[90,89], memory[91,89], memory[92,89], memory[93,89], memory[94,89], memory[95,89], memory[96,89], memory[97,89], memory[98,89], memory[99,89], memory[100,89], memory[101,89], memory[102,89], memory[103,89], memory[104,89], memory[105,89], memory[106,89], memory[107,89], memory[108,89], memory[109,89], memory[110,89], memory[111,89], memory[112,89], memory[113,89], memory[114,89], memory[115,89], memory[116,89], memory[117,89], memory[118,89], memory[119,89], memory[120,89], memory[1,90], memory[2,90], memory[3,90], memory[4,90], memory[5,90], memory[6,90], memory[7,90], memory[8,90], memory[9,90], memory[10,90], memory[11,90], memory[12,90], memory[13,90], memory[14,90], memory[15,90], memory[16,90], memory[17,90], memory[18,90], memory[19,90], memory[20,90], memory[21,90], memory[22,90], memory[23,90], memory[24,90], memory[25,90], memory[26,90], memory[27,90], memory[28,90], memory[29,90], memory[30,90], memory[31,90], memory[32,90], memory[33,90], memory[34,90], memory[35,90], memory[36,90], memory[37,90], memory[38,90], memory[39,90], memory[40,90], memory[41,90], memory[42,90], memory[43,90], memory[44,90], memory[45,90], memory[46,90], memory[47,90], memory[48,90], memory[49,90], memory[50,90], memory[51,90], memory[52,90], memory[53,90], memory[54,90], memory[55,90], memory[56,90], memory[57,90], memory[58,90], memory[59,90], memory[60,90], memory[61,90], memory[62,90], memory[63,90], memory[64,90], memory[65,90], memory[66,90], memory[67,90], memory[68,90], memory[69,90], memory[70,90], memory[71,90], memory[72,90], memory[73,90], memory[74,90], memory[75,90], memory[76,90], memory[77,90], memory[78,90], memory[79,90], memory[80,90], memory[81,90], memory[82,90], memory[83,90], memory[84,90], memory[85,90], memory[86,90], memory[87,90], memory[88,90], memory[89,90], memory[90,90], memory[91,90], memory[92,90], memory[93,90], memory[94,90], memory[95,90], memory[96,90], memory[97,90], memory[98,90], memory[99,90], memory[100,90], memory[101,90], memory[102,90], memory[103,90], memory[104,90], memory[105,90], memory[106,90], memory[107,90], memory[108,90], memory[109,90], memory[110,90], memory[111,90], memory[112,90], memory[113,90], memory[114,90], memory[115,90], memory[116,90], memory[117,90], memory[118,90], memory[119,90], memory[120,90], memory[1,91], memory[2,91], memory[3,91], memory[4,91], memory[5,91], memory[6,91], memory[7,91], memory[8,91], memory[9,91], memory[10,91], memory[11,91], memory[12,91], memory[13,91], memory[14,91], memory[15,91], memory[16,91], memory[17,91], memory[18,91], memory[19,91], memory[20,91], memory[21,91], memory[22,91], memory[23,91], memory[24,91], memory[25,91], memory[26,91], memory[27,91], memory[28,91], memory[29,91], memory[30,91], memory[31,91], memory[32,91], memory[33,91], memory[34,91], memory[35,91], memory[36,91], memory[37,91], memory[38,91], memory[39,91], memory[40,91], memory[41,91], memory[42,91], memory[43,91], memory[44,91], memory[45,91], memory[46,91], memory[47,91], memory[48,91], memory[49,91], memory[50,91], memory[51,91], memory[52,91], memory[53,91], memory[54,91], memory[55,91], memory[56,91], memory[57,91], memory[58,91], memory[59,91], memory[60,91], memory[61,91], memory[62,91], memory[63,91], memory[64,91], memory[65,91], memory[66,91], memory[67,91], memory[68,91], memory[69,91], memory[70,91], memory[71,91], memory[72,91], memory[73,91], memory[74,91], memory[75,91], memory[76,91], memory[77,91], memory[78,91], memory[79,91], memory[80,91], memory[81,91], memory[82,91], memory[83,91], memory[84,91], memory[85,91], memory[86,91], memory[87,91], memory[88,91], memory[89,91], memory[90,91], memory[91,91], memory[92,91], memory[93,91], memory[94,91], memory[95,91], memory[96,91], memory[97,91], memory[98,91], memory[99,91], memory[100,91], memory[101,91], memory[102,91], memory[103,91], memory[104,91], memory[105,91], memory[106,91], memory[107,91], memory[108,91], memory[109,91], memory[110,91], memory[111,91], memory[112,91], memory[113,91], memory[114,91], memory[115,91], memory[116,91], memory[117,91], memory[118,91], memory[119,91], memory[120,91], memory[1,92], memory[2,92], memory[3,92], memory[4,92], memory[5,92], memory[6,92], memory[7,92], memory[8,92], memory[9,92], memory[10,92], memory[11,92], memory[12,92], memory[13,92], memory[14,92], memory[15,92], memory[16,92], memory[17,92], memory[18,92], memory[19,92], memory[20,92], memory[21,92], memory[22,92], memory[23,92], memory[24,92], memory[25,92], memory[26,92], memory[27,92], memory[28,92], memory[29,92], memory[30,92], memory[31,92], memory[32,92], memory[33,92], memory[34,92], memory[35,92], memory[36,92], memory[37,92], memory[38,92], memory[39,92], memory[40,92], memory[41,92], memory[42,92], memory[43,92], memory[44,92], memory[45,92], memory[46,92], memory[47,92], memory[48,92], memory[49,92], memory[50,92], memory[51,92], memory[52,92], memory[53,92], memory[54,92], memory[55,92], memory[56,92], memory[57,92], memory[58,92], memory[59,92], memory[60,92], memory[61,92], memory[62,92], memory[63,92], memory[64,92], memory[65,92], memory[66,92], memory[67,92], memory[68,92], memory[69,92], memory[70,92], memory[71,92], memory[72,92], memory[73,92], memory[74,92], memory[75,92], memory[76,92], memory[77,92], memory[78,92], memory[79,92], memory[80,92], memory[81,92], memory[82,92], memory[83,92], memory[84,92], memory[85,92], memory[86,92], memory[87,92], memory[88,92], memory[89,92], memory[90,92], memory[91,92], memory[92,92], memory[93,92], memory[94,92], memory[95,92], memory[96,92], memory[97,92], memory[98,92], memory[99,92], memory[100,92], memory[101,92], memory[102,92], memory[103,92], memory[104,92], memory[105,92], memory[106,92], memory[107,92], memory[108,92], memory[109,92], memory[110,92], memory[111,92], memory[112,92], memory[113,92], memory[114,92], memory[115,92], memory[116,92], memory[117,92], memory[118,92], memory[119,92], memory[120,92], memory[1,93], memory[2,93], memory[3,93], memory[4,93], memory[5,93], memory[6,93], memory[7,93], memory[8,93], memory[9,93], memory[10,93], memory[11,93], memory[12,93], memory[13,93], memory[14,93], memory[15,93], memory[16,93], memory[17,93], memory[18,93], memory[19,93], memory[20,93], memory[21,93], memory[22,93], memory[23,93], memory[24,93], memory[25,93], memory[26,93], memory[27,93], memory[28,93], memory[29,93], memory[30,93], memory[31,93], memory[32,93], memory[33,93], memory[34,93], memory[35,93], memory[36,93], memory[37,93], memory[38,93], memory[39,93], memory[40,93], memory[41,93], memory[42,93], memory[43,93], memory[44,93], memory[45,93], memory[46,93], memory[47,93], memory[48,93], memory[49,93], memory[50,93], memory[51,93], memory[52,93], memory[53,93], memory[54,93], memory[55,93], memory[56,93], memory[57,93], memory[58,93], memory[59,93], memory[60,93], memory[61,93], memory[62,93], memory[63,93], memory[64,93], memory[65,93], memory[66,93], memory[67,93], memory[68,93], memory[69,93], memory[70,93], memory[71,93], memory[72,93], memory[73,93], memory[74,93], memory[75,93], memory[76,93], memory[77,93], memory[78,93], memory[79,93], memory[80,93], memory[81,93], memory[82,93], memory[83,93], memory[84,93], memory[85,93], memory[86,93], memory[87,93], memory[88,93], memory[89,93], memory[90,93], memory[91,93], memory[92,93], memory[93,93], memory[94,93], memory[95,93], memory[96,93], memory[97,93], memory[98,93], memory[99,93], memory[100,93], memory[101,93], memory[102,93], memory[103,93], memory[104,93], memory[105,93], memory[106,93], memory[107,93], memory[108,93], memory[109,93], memory[110,93], memory[111,93], memory[112,93], memory[113,93], memory[114,93], memory[115,93], memory[116,93], memory[117,93], memory[118,93], memory[119,93], memory[120,93], memory[1,94], memory[2,94], memory[3,94], memory[4,94], memory[5,94], memory[6,94], memory[7,94], memory[8,94], memory[9,94], memory[10,94], memory[11,94], memory[12,94], memory[13,94], memory[14,94], memory[15,94], memory[16,94], memory[17,94], memory[18,94], memory[19,94], memory[20,94], memory[21,94], memory[22,94], memory[23,94], memory[24,94], memory[25,94], memory[26,94], memory[27,94], memory[28,94], memory[29,94], memory[30,94], memory[31,94], memory[32,94], memory[33,94], memory[34,94], memory[35,94], memory[36,94], memory[37,94], memory[38,94], memory[39,94], memory[40,94], memory[41,94], memory[42,94], memory[43,94], memory[44,94], memory[45,94], memory[46,94], memory[47,94], memory[48,94], memory[49,94], memory[50,94], memory[51,94], memory[52,94], memory[53,94], memory[54,94], memory[55,94], memory[56,94], memory[57,94], memory[58,94], memory[59,94], memory[60,94], memory[61,94], memory[62,94], memory[63,94], memory[64,94], memory[65,94], memory[66,94], memory[67,94], memory[68,94], memory[69,94], memory[70,94], memory[71,94], memory[72,94], memory[73,94], memory[74,94], memory[75,94], memory[76,94], memory[77,94], memory[78,94], memory[79,94], memory[80,94], memory[81,94], memory[82,94], memory[83,94], memory[84,94], memory[85,94], memory[86,94], memory[87,94], memory[88,94], memory[89,94], memory[90,94], memory[91,94], memory[92,94], memory[93,94], memory[94,94], memory[95,94], memory[96,94], memory[97,94], memory[98,94], memory[99,94], memory[100,94], memory[101,94], memory[102,94], memory[103,94], memory[104,94], memory[105,94], memory[106,94], memory[107,94], memory[108,94], memory[109,94], memory[110,94], memory[111,94], memory[112,94], memory[113,94], memory[114,94], memory[115,94], memory[116,94], memory[117,94], memory[118,94], memory[119,94], memory[120,94], memory[1,95], memory[2,95], memory[3,95], memory[4,95], memory[5,95], memory[6,95], memory[7,95], memory[8,95], memory[9,95], memory[10,95], memory[11,95], memory[12,95], memory[13,95], memory[14,95], memory[15,95], memory[16,95], memory[17,95], memory[18,95], memory[19,95], memory[20,95], memory[21,95], memory[22,95], memory[23,95], memory[24,95], memory[25,95], memory[26,95], memory[27,95], memory[28,95], memory[29,95], memory[30,95], memory[31,95], memory[32,95], memory[33,95], memory[34,95], memory[35,95], memory[36,95], memory[37,95], memory[38,95], memory[39,95], memory[40,95], memory[41,95], memory[42,95], memory[43,95], memory[44,95], memory[45,95], memory[46,95], memory[47,95], memory[48,95], memory[49,95], memory[50,95], memory[51,95], memory[52,95], memory[53,95], memory[54,95], memory[55,95], memory[56,95], memory[57,95], memory[58,95], memory[59,95], memory[60,95], memory[61,95], memory[62,95], memory[63,95], memory[64,95], memory[65,95], memory[66,95], memory[67,95], memory[68,95], memory[69,95], memory[70,95], memory[71,95], memory[72,95], memory[73,95], memory[74,95], memory[75,95], memory[76,95], memory[77,95], memory[78,95], memory[79,95], memory[80,95], memory[81,95], memory[82,95], memory[83,95], memory[84,95], memory[85,95], memory[86,95], memory[87,95], memory[88,95], memory[89,95], memory[90,95], memory[91,95], memory[92,95], memory[93,95], memory[94,95], memory[95,95], memory[96,95], memory[97,95], memory[98,95], memory[99,95], memory[100,95], memory[101,95], memory[102,95], memory[103,95], memory[104,95], memory[105,95], memory[106,95], memory[107,95], memory[108,95], memory[109,95], memory[110,95], memory[111,95], memory[112,95], memory[113,95], memory[114,95], memory[115,95], memory[116,95], memory[117,95], memory[118,95], memory[119,95], memory[120,95], memory[1,96], memory[2,96], memory[3,96], memory[4,96], memory[5,96], memory[6,96], memory[7,96], memory[8,96], memory[9,96], memory[10,96], memory[11,96], memory[12,96], memory[13,96], memory[14,96], memory[15,96], memory[16,96], memory[17,96], memory[18,96], memory[19,96], memory[20,96], memory[21,96], memory[22,96], memory[23,96], memory[24,96], memory[25,96], memory[26,96], memory[27,96], memory[28,96], memory[29,96], memory[30,96], memory[31,96], memory[32,96], memory[33,96], memory[34,96], memory[35,96], memory[36,96], memory[37,96], memory[38,96], memory[39,96], memory[40,96], memory[41,96], memory[42,96], memory[43,96], memory[44,96], memory[45,96], memory[46,96], memory[47,96], memory[48,96], memory[49,96], memory[50,96], memory[51,96], memory[52,96], memory[53,96], memory[54,96], memory[55,96], memory[56,96], memory[57,96], memory[58,96], memory[59,96], memory[60,96], memory[61,96], memory[62,96], memory[63,96], memory[64,96], memory[65,96], memory[66,96], memory[67,96], memory[68,96], memory[69,96], memory[70,96], memory[71,96], memory[72,96], memory[73,96], memory[74,96], memory[75,96], memory[76,96], memory[77,96], memory[78,96], memory[79,96], memory[80,96], memory[81,96], memory[82,96], memory[83,96], memory[84,96], memory[85,96], memory[86,96], memory[87,96], memory[88,96], memory[89,96], memory[90,96], memory[91,96], memory[92,96], memory[93,96], memory[94,96], memory[95,96], memory[96,96], memory[97,96], memory[98,96], memory[99,96], memory[100,96], memory[101,96], memory[102,96], memory[103,96], memory[104,96], memory[105,96], memory[106,96], memory[107,96], memory[108,96], memory[109,96], memory[110,96], memory[111,96], memory[112,96], memory[113,96], memory[114,96], memory[115,96], memory[116,96], memory[117,96], memory[118,96], memory[119,96], memory[120,96], memory[1,97], memory[2,97], memory[3,97], memory[4,97], memory[5,97], memory[6,97], memory[7,97], memory[8,97], memory[9,97], memory[10,97], memory[11,97], memory[12,97], memory[13,97], memory[14,97], memory[15,97], memory[16,97], memory[17,97], memory[18,97], memory[19,97], memory[20,97], memory[21,97], memory[22,97], memory[23,97], memory[24,97], memory[25,97], memory[26,97], memory[27,97], memory[28,97], memory[29,97], memory[30,97], memory[31,97], memory[32,97], memory[33,97], memory[34,97], memory[35,97], memory[36,97], memory[37,97], memory[38,97], memory[39,97], memory[40,97], memory[41,97], memory[42,97], memory[43,97], memory[44,97], memory[45,97], memory[46,97], memory[47,97], memory[48,97], memory[49,97], memory[50,97], memory[51,97], memory[52,97], memory[53,97], memory[54,97], memory[55,97], memory[56,97], memory[57,97], memory[58,97], memory[59,97], memory[60,97], memory[61,97], memory[62,97], memory[63,97], memory[64,97], memory[65,97], memory[66,97], memory[67,97], memory[68,97], memory[69,97], memory[70,97], memory[71,97], memory[72,97], memory[73,97], memory[74,97], memory[75,97], memory[76,97], memory[77,97], memory[78,97], memory[79,97], memory[80,97], memory[81,97], memory[82,97], memory[83,97], memory[84,97], memory[85,97], memory[86,97], memory[87,97], memory[88,97], memory[89,97], memory[90,97], memory[91,97], memory[92,97], memory[93,97], memory[94,97], memory[95,97], memory[96,97], memory[97,97], memory[98,97], memory[99,97], memory[100,97], memory[101,97], memory[102,97], memory[103,97], memory[104,97], memory[105,97], memory[106,97], memory[107,97], memory[108,97], memory[109,97], memory[110,97], memory[111,97], memory[112,97], memory[113,97], memory[114,97], memory[115,97], memory[116,97], memory[117,97], memory[118,97], memory[119,97], memory[120,97], memory[1,98], memory[2,98], memory[3,98], memory[4,98], memory[5,98], memory[6,98], memory[7,98], memory[8,98], memory[9,98], memory[10,98], memory[11,98], memory[12,98], memory[13,98], memory[14,98], memory[15,98], memory[16,98], memory[17,98], memory[18,98], memory[19,98], memory[20,98], memory[21,98], memory[22,98], memory[23,98], memory[24,98], memory[25,98], memory[26,98], memory[27,98], memory[28,98], memory[29,98], memory[30,98], memory[31,98], memory[32,98], memory[33,98], memory[34,98], memory[35,98], memory[36,98], memory[37,98], memory[38,98], memory[39,98], memory[40,98], memory[41,98], memory[42,98], memory[43,98], memory[44,98], memory[45,98], memory[46,98], memory[47,98], memory[48,98], memory[49,98], memory[50,98], memory[51,98], memory[52,98], memory[53,98], memory[54,98], memory[55,98], memory[56,98], memory[57,98], memory[58,98], memory[59,98], memory[60,98], memory[61,98], memory[62,98], memory[63,98], memory[64,98], memory[65,98], memory[66,98], memory[67,98], memory[68,98], memory[69,98], memory[70,98], memory[71,98], memory[72,98], memory[73,98], memory[74,98], memory[75,98], memory[76,98], memory[77,98], memory[78,98], memory[79,98], memory[80,98], memory[81,98], memory[82,98], memory[83,98], memory[84,98], memory[85,98], memory[86,98], memory[87,98], memory[88,98], memory[89,98], memory[90,98], memory[91,98], memory[92,98], memory[93,98], memory[94,98], memory[95,98], memory[96,98], memory[97,98], memory[98,98], memory[99,98], memory[100,98], memory[101,98], memory[102,98], memory[103,98], memory[104,98], memory[105,98], memory[106,98], memory[107,98], memory[108,98], memory[109,98], memory[110,98], memory[111,98], memory[112,98], memory[113,98], memory[114,98], memory[115,98], memory[116,98], memory[117,98], memory[118,98], memory[119,98], memory[120,98], memory[1,99], memory[2,99], memory[3,99], memory[4,99], memory[5,99], memory[6,99], memory[7,99], memory[8,99], memory[9,99], memory[10,99], memory[11,99], memory[12,99], memory[13,99], memory[14,99], memory[15,99], memory[16,99], memory[17,99], memory[18,99], memory[19,99], memory[20,99], memory[21,99], memory[22,99], memory[23,99], memory[24,99], memory[25,99], memory[26,99], memory[27,99], memory[28,99], memory[29,99], memory[30,99], memory[31,99], memory[32,99], memory[33,99], memory[34,99], memory[35,99], memory[36,99], memory[37,99], memory[38,99], memory[39,99], memory[40,99], memory[41,99], memory[42,99], memory[43,99], memory[44,99], memory[45,99], memory[46,99], memory[47,99], memory[48,99], memory[49,99], memory[50,99], memory[51,99], memory[52,99], memory[53,99], memory[54,99], memory[55,99], memory[56,99], memory[57,99], memory[58,99], memory[59,99], memory[60,99], memory[61,99], memory[62,99], memory[63,99], memory[64,99], memory[65,99], memory[66,99], memory[67,99], memory[68,99], memory[69,99], memory[70,99], memory[71,99], memory[72,99], memory[73,99], memory[74,99], memory[75,99], memory[76,99], memory[77,99], memory[78,99], memory[79,99], memory[80,99], memory[81,99], memory[82,99], memory[83,99], memory[84,99], memory[85,99], memory[86,99], memory[87,99], memory[88,99], memory[89,99], memory[90,99], memory[91,99], memory[92,99], memory[93,99], memory[94,99], memory[95,99], memory[96,99], memory[97,99], memory[98,99], memory[99,99], memory[100,99], memory[101,99], memory[102,99], memory[103,99], memory[104,99], memory[105,99], memory[106,99], memory[107,99], memory[108,99], memory[109,99], memory[110,99], memory[111,99], memory[112,99], memory[113,99], memory[114,99], memory[115,99], memory[116,99], memory[117,99], memory[118,99], memory[119,99], memory[120,99], memory[1,100], memory[2,100], memory[3,100], memory[4,100], memory[5,100], memory[6,100], memory[7,100], memory[8,100], memory[9,100], memory[10,100], memory[11,100], memory[12,100], memory[13,100], memory[14,100], memory[15,100], memory[16,100], memory[17,100], memory[18,100], memory[19,100], memory[20,100], memory[21,100], memory[22,100], memory[23,100], memory[24,100], memory[25,100], memory[26,100], memory[27,100], memory[28,100], memory[29,100], memory[30,100], memory[31,100], memory[32,100], memory[33,100], memory[34,100], memory[35,100], memory[36,100], memory[37,100], memory[38,100], memory[39,100], memory[40,100], memory[41,100], memory[42,100], memory[43,100], memory[44,100], memory[45,100], memory[46,100], memory[47,100], memory[48,100], memory[49,100], memory[50,100], memory[51,100], memory[52,100], memory[53,100], memory[54,100], memory[55,100], memory[56,100], memory[57,100], memory[58,100], memory[59,100], memory[60,100], memory[61,100], memory[62,100], memory[63,100], memory[64,100], memory[65,100], memory[66,100], memory[67,100], memory[68,100], memory[69,100], memory[70,100], memory[71,100], memory[72,100], memory[73,100], memory[74,100], memory[75,100], memory[76,100], memory[77,100], memory[78,100], memory[79,100], memory[80,100], memory[81,100], memory[82,100], memory[83,100], memory[84,100], memory[85,100], memory[86,100], memory[87,100], memory[88,100], memory[89,100], memory[90,100], memory[91,100], memory[92,100], memory[93,100], memory[94,100], memory[95,100], memory[96,100], memory[97,100], memory[98,100], memory[99,100], memory[100,100], memory[101,100], memory[102,100], memory[103,100], memory[104,100], memory[105,100], memory[106,100], memory[107,100], memory[108,100], memory[109,100], memory[110,100], memory[111,100], memory[112,100], memory[113,100], memory[114,100], memory[115,100], memory[116,100], memory[117,100], memory[118,100], memory[119,100], memory[120,100], biasID[1], biasID[2], biasID[3], biasID[4], biasID[5], biasID[6], biasID[7], biasID[8], biasID[9], biasID[10], biasID[11], biasID[12], biasID[13], biasID[14], biasID[15], biasID[16], biasID[17], biasID[18], biasID[19], biasID[20], biasID[21], biasID[22], biasID[23], biasID[24], biasID[25], biasID[26], biasID[27], biasID[28], biasID[29], biasID[30], biasID[31], biasID[32], biasID[33], biasID[34], biasID[35], biasID[36], biasID[37], biasID[38], biasID[39], biasID[40], biasID[41], biasID[42], biasID[43], biasID[44], biasID[45], biasID[46], biasID[47], biasID[48], biasID[49], biasID[50], biasID[51], biasID[52], biasID[53], biasID[54], biasID[55], biasID[56], biasID[57], biasID[58], biasID[59], biasID[60], biasID[61], biasID[62], biasID[63], biasID[64], biasID[65], biasID[66], biasID[67], biasID[68], biasID[69], biasID[70], biasID[71], biasID[72], biasID[73], biasID[74], biasID[75], biasID[76], biasID[77], biasID[78], biasID[79], biasID[80], biasID[81], biasID[82], biasID[83], biasID[84], biasID[85], biasID[86], biasID[87], biasID[88], biasID[89], biasID[90], biasID[91], biasID[92], biasID[93], biasID[94], biasID[95], biasID[96], biasID[97], biasID[98], biasID[99], biasID[100], betaID[1], betaID[2], betaID[3], betaID[4], betaID[5], betaID[6], betaID[7], betaID[8], betaID[9], betaID[10], betaID[11], betaID[12], betaID[13], betaID[14], betaID[15], betaID[16], betaID[17], betaID[18], betaID[19], betaID[20], betaID[21], betaID[22], betaID[23], betaID[24], betaID[25], betaID[26], betaID[27], betaID[28], betaID[29], betaID[30], betaID[31], betaID[32], betaID[33], betaID[34], betaID[35], betaID[36], betaID[37], betaID[38], betaID[39], betaID[40], betaID[41], betaID[42], betaID[43], betaID[44], betaID[45], betaID[46], betaID[47], betaID[48], betaID[49], betaID[50], betaID[51], betaID[52], betaID[53], betaID[54], betaID[55], betaID[56], betaID[57], betaID[58], betaID[59], betaID[60], betaID[61], betaID[62], betaID[63], betaID[64], betaID[65], betaID[66], betaID[67], betaID[68], betaID[69], betaID[70], betaID[71], betaID[72], betaID[73], betaID[74], betaID[75], betaID[76], betaID[77], betaID[78], betaID[79], betaID[80], betaID[81], betaID[82], betaID[83], betaID[84], betaID[85], betaID[86], betaID[87], betaID[88], betaID[89], betaID[90], betaID[91], betaID[92], betaID[93], betaID[94], betaID[95], betaID[96], betaID[97], betaID[98], betaID[99], betaID[100], biasM_prior, biasSD_prior, betaM_prior, betaSD_prior, bias_prior, beta_prior, prior_preds0[1], prior_preds0[2], prior_preds0[3], prior_preds0[4], prior_preds0[5], prior_preds0[6], prior_preds0[7], prior_preds0[8], prior_preds0[9], prior_preds0[10], prior_preds0[11], prior_preds0[12], prior_preds0[13], prior_preds0[14], prior_preds0[15], prior_preds0[16], prior_preds0[17], prior_preds0[18], prior_preds0[19], prior_preds0[20], prior_preds0[21], prior_preds0[22], prior_preds0[23], prior_preds0[24], prior_preds0[25], prior_preds0[26], prior_preds0[27], prior_preds0[28], prior_preds0[29], prior_preds0[30], prior_preds0[31], prior_preds0[32], prior_preds0[33], prior_preds0[34], prior_preds0[35], prior_preds0[36], prior_preds0[37], prior_preds0[38], prior_preds0[39], prior_preds0[40], prior_preds0[41], prior_preds0[42], prior_preds0[43], prior_preds0[44], prior_preds0[45], prior_preds0[46], prior_preds0[47], prior_preds0[48], prior_preds0[49], prior_preds0[50], prior_preds0[51], prior_preds0[52], prior_preds0[53], prior_preds0[54], prior_preds0[55], prior_preds0[56], prior_preds0[57], prior_preds0[58], prior_preds0[59], prior_preds0[60], prior_preds0[61], prior_preds0[62], prior_preds0[63], prior_preds0[64], prior_preds0[65], prior_preds0[66], prior_preds0[67], prior_preds0[68], prior_preds0[69], prior_preds0[70], prior_preds0[71], prior_preds0[72], prior_preds0[73], prior_preds0[74], prior_preds0[75], prior_preds0[76], prior_preds0[77], prior_preds0[78], prior_preds0[79], prior_preds0[80], prior_preds0[81], prior_preds0[82], prior_preds0[83], prior_preds0[84], prior_preds0[85], prior_preds0[86], prior_preds0[87], prior_preds0[88], prior_preds0[89], prior_preds0[90], prior_preds0[91], prior_preds0[92], prior_preds0[93], prior_preds0[94], prior_preds0[95], prior_preds0[96], prior_preds0[97], prior_preds0[98], prior_preds0[99], prior_preds0[100], prior_preds1[1], prior_preds1[2], prior_preds1[3], prior_preds1[4], prior_preds1[5], prior_preds1[6], prior_preds1[7], prior_preds1[8], prior_preds1[9], prior_preds1[10], prior_preds1[11], prior_preds1[12], prior_preds1[13], prior_preds1[14], prior_preds1[15], prior_preds1[16], prior_preds1[17], prior_preds1[18], prior_preds1[19], prior_preds1[20], prior_preds1[21], prior_preds1[22], prior_preds1[23], prior_preds1[24], prior_preds1[25], prior_preds1[26], prior_preds1[27], prior_preds1[28], prior_preds1[29], prior_preds1[30], prior_preds1[31], prior_preds1[32], prior_preds1[33], prior_preds1[34], prior_preds1[35], prior_preds1[36], prior_preds1[37], prior_preds1[38], prior_preds1[39], prior_preds1[40], prior_preds1[41], prior_preds1[42], prior_preds1[43], prior_preds1[44], prior_preds1[45], prior_preds1[46], prior_preds1[47], prior_preds1[48], prior_preds1[49], prior_preds1[50], prior_preds1[51], prior_preds1[52], prior_preds1[53], prior_preds1[54], prior_preds1[55], prior_preds1[56], prior_preds1[57], prior_preds1[58], prior_preds1[59], prior_preds1[60], prior_preds1[61], prior_preds1[62], prior_preds1[63], prior_preds1[64], prior_preds1[65], prior_preds1[66], prior_preds1[67], prior_preds1[68], prior_preds1[69], prior_preds1[70], prior_preds1[71], prior_preds1[72], prior_preds1[73], prior_preds1[74], prior_preds1[75], prior_preds1[76], prior_preds1[77], prior_preds1[78], prior_preds1[79], prior_preds1[80], prior_preds1[81], prior_preds1[82], prior_preds1[83], prior_preds1[84], prior_preds1[85], prior_preds1[86], prior_preds1[87], prior_preds1[88], prior_preds1[89], prior_preds1[90], prior_preds1[91], prior_preds1[92], prior_preds1[93], prior_preds1[94], prior_preds1[95], prior_preds1[96], prior_preds1[97], prior_preds1[98], prior_preds1[99], prior_preds1[100], prior_preds2[1], prior_preds2[2], prior_preds2[3], prior_preds2[4], prior_preds2[5], prior_preds2[6], prior_preds2[7], prior_preds2[8], prior_preds2[9], prior_preds2[10], prior_preds2[11], prior_preds2[12], prior_preds2[13], prior_preds2[14], prior_preds2[15], prior_preds2[16], prior_preds2[17], prior_preds2[18], prior_preds2[19], prior_preds2[20], prior_preds2[21], prior_preds2[22], prior_preds2[23], prior_preds2[24], prior_preds2[25], prior_preds2[26], prior_preds2[27], prior_preds2[28], prior_preds2[29], prior_preds2[30], prior_preds2[31], prior_preds2[32], prior_preds2[33], prior_preds2[34], prior_preds2[35], prior_preds2[36], prior_preds2[37], prior_preds2[38], prior_preds2[39], prior_preds2[40], prior_preds2[41], prior_preds2[42], prior_preds2[43], prior_preds2[44], prior_preds2[45], prior_preds2[46], prior_preds2[47], prior_preds2[48], prior_preds2[49], prior_preds2[50], prior_preds2[51], prior_preds2[52], prior_preds2[53], prior_preds2[54], prior_preds2[55], prior_preds2[56], prior_preds2[57], prior_preds2[58], prior_preds2[59], prior_preds2[60], prior_preds2[61], prior_preds2[62], prior_preds2[63], prior_preds2[64], prior_preds2[65], prior_preds2[66], prior_preds2[67], prior_preds2[68], prior_preds2[69], prior_preds2[70], prior_preds2[71], prior_preds2[72], prior_preds2[73], prior_preds2[74], prior_preds2[75], prior_preds2[76], prior_preds2[77], prior_preds2[78], prior_preds2[79], prior_preds2[80], prior_preds2[81], prior_preds2[82], prior_preds2[83], prior_preds2[84], prior_preds2[85], prior_preds2[86], prior_preds2[87], prior_preds2[88], prior_preds2[89], prior_preds2[90], prior_preds2[91], prior_preds2[92], prior_preds2[93], prior_preds2[94], prior_preds2[95], prior_preds2[96], prior_preds2[97], prior_preds2[98], prior_preds2[99], prior_preds2[100], posterior_preds0[1], posterior_preds0[2], posterior_preds0[3], posterior_preds0[4], posterior_preds0[5], posterior_preds0[6], posterior_preds0[7], posterior_preds0[8], posterior_preds0[9], posterior_preds0[10], posterior_preds0[11], posterior_preds0[12], posterior_preds0[13], posterior_preds0[14], posterior_preds0[15], posterior_preds0[16], posterior_preds0[17], posterior_preds0[18], posterior_preds0[19], posterior_preds0[20], posterior_preds0[21], posterior_preds0[22], posterior_preds0[23], posterior_preds0[24], posterior_preds0[25], posterior_preds0[26], posterior_preds0[27], posterior_preds0[28], posterior_preds0[29], posterior_preds0[30], posterior_preds0[31], posterior_preds0[32], posterior_preds0[33], posterior_preds0[34], posterior_preds0[35], posterior_preds0[36], posterior_preds0[37], posterior_preds0[38], posterior_preds0[39], posterior_preds0[40], posterior_preds0[41], posterior_preds0[42], posterior_preds0[43], posterior_preds0[44], posterior_preds0[45], posterior_preds0[46], posterior_preds0[47], posterior_preds0[48], posterior_preds0[49], posterior_preds0[50], posterior_preds0[51], posterior_preds0[52], posterior_preds0[53], posterior_preds0[54], posterior_preds0[55], posterior_preds0[56], posterior_preds0[57], posterior_preds0[58], posterior_preds0[59], posterior_preds0[60], posterior_preds0[61], posterior_preds0[62], posterior_preds0[63], posterior_preds0[64], posterior_preds0[65], posterior_preds0[66], posterior_preds0[67], posterior_preds0[68], posterior_preds0[69], posterior_preds0[70], posterior_preds0[71], posterior_preds0[72], posterior_preds0[73], posterior_preds0[74], posterior_preds0[75], posterior_preds0[76], posterior_preds0[77], posterior_preds0[78], posterior_preds0[79], posterior_preds0[80], posterior_preds0[81], posterior_preds0[82], posterior_preds0[83], posterior_preds0[84], posterior_preds0[85], posterior_preds0[86], posterior_preds0[87], posterior_preds0[88], posterior_preds0[89], posterior_preds0[90], posterior_preds0[91], posterior_preds0[92], posterior_preds0[93], posterior_preds0[94], posterior_preds0[95], posterior_preds0[96], posterior_preds0[97], posterior_preds0[98], posterior_preds0[99], posterior_preds0[100], posterior_preds1[1], posterior_preds1[2], posterior_preds1[3], posterior_preds1[4], posterior_preds1[5], posterior_preds1[6], posterior_preds1[7], posterior_preds1[8], posterior_preds1[9], posterior_preds1[10], posterior_preds1[11], posterior_preds1[12], posterior_preds1[13], posterior_preds1[14], posterior_preds1[15], posterior_preds1[16], posterior_preds1[17], posterior_preds1[18], posterior_preds1[19], posterior_preds1[20], posterior_preds1[21], posterior_preds1[22], posterior_preds1[23], posterior_preds1[24], posterior_preds1[25], posterior_preds1[26], posterior_preds1[27], posterior_preds1[28], posterior_preds1[29], posterior_preds1[30], posterior_preds1[31], posterior_preds1[32], posterior_preds1[33], posterior_preds1[34], posterior_preds1[35], posterior_preds1[36], posterior_preds1[37], posterior_preds1[38], posterior_preds1[39], posterior_preds1[40], posterior_preds1[41], posterior_preds1[42], posterior_preds1[43], posterior_preds1[44], posterior_preds1[45], posterior_preds1[46], posterior_preds1[47], posterior_preds1[48], posterior_preds1[49], posterior_preds1[50], posterior_preds1[51], posterior_preds1[52], posterior_preds1[53], posterior_preds1[54], posterior_preds1[55], posterior_preds1[56], posterior_preds1[57], posterior_preds1[58], posterior_preds1[59], posterior_preds1[60], posterior_preds1[61], posterior_preds1[62], posterior_preds1[63], posterior_preds1[64], posterior_preds1[65], posterior_preds1[66], posterior_preds1[67], posterior_preds1[68], posterior_preds1[69], posterior_preds1[70], posterior_preds1[71], posterior_preds1[72], posterior_preds1[73], posterior_preds1[74], posterior_preds1[75], posterior_preds1[76], posterior_preds1[77], posterior_preds1[78], posterior_preds1[79], posterior_preds1[80], posterior_preds1[81], posterior_preds1[82], posterior_preds1[83], posterior_preds1[84], posterior_preds1[85], posterior_preds1[86], posterior_preds1[87], posterior_preds1[88], posterior_preds1[89], posterior_preds1[90], posterior_preds1[91], posterior_preds1[92], posterior_preds1[93], posterior_preds1[94], posterior_preds1[95], posterior_preds1[96], posterior_preds1[97], posterior_preds1[98], posterior_preds1[99], posterior_preds1[100], posterior_preds2[1], posterior_preds2[2], posterior_preds2[3], posterior_preds2[4], posterior_preds2[5], posterior_preds2[6], posterior_preds2[7], posterior_preds2[8], posterior_preds2[9], posterior_preds2[10], posterior_preds2[11], posterior_preds2[12], posterior_preds2[13], posterior_preds2[14], posterior_preds2[15], posterior_preds2[16], posterior_preds2[17], posterior_preds2[18], posterior_preds2[19], posterior_preds2[20], posterior_preds2[21], posterior_preds2[22], posterior_preds2[23], posterior_preds2[24], posterior_preds2[25], posterior_preds2[26], posterior_preds2[27], posterior_preds2[28], posterior_preds2[29], posterior_preds2[30], posterior_preds2[31], posterior_preds2[32], posterior_preds2[33], posterior_preds2[34], posterior_preds2[35], posterior_preds2[36], posterior_preds2[37], posterior_preds2[38], posterior_preds2[39], posterior_preds2[40], posterior_preds2[41], posterior_preds2[42], posterior_preds2[43], posterior_preds2[44], posterior_preds2[45], posterior_preds2[46], posterior_preds2[47], posterior_preds2[48], posterior_preds2[49], posterior_preds2[50], posterior_preds2[51], posterior_preds2[52], posterior_preds2[53], posterior_preds2[54], posterior_preds2[55], posterior_preds2[56], posterior_preds2[57], posterior_preds2[58], posterior_preds2[59], posterior_preds2[60], posterior_preds2[61], posterior_preds2[62], posterior_preds2[63], posterior_preds2[64], posterior_preds2[65], posterior_preds2[66], posterior_preds2[67], posterior_preds2[68], posterior_preds2[69], posterior_preds2[70], posterior_preds2[71], posterior_preds2[72], posterior_preds2[73], posterior_preds2[74], posterior_preds2[75], posterior_preds2[76], posterior_preds2[77], posterior_preds2[78], posterior_preds2[79], posterior_preds2[80], posterior_preds2[81], posterior_preds2[82], posterior_preds2[83], posterior_preds2[84], posterior_preds2[85], posterior_preds2[86], posterior_preds2[87], posterior_preds2[88], posterior_preds2[89], posterior_preds2[90], posterior_preds2[91], posterior_preds2[92], posterior_preds2[93], posterior_preds2[94], posterior_preds2[95], posterior_preds2[96], posterior_preds2[97], posterior_preds2[98], posterior_preds2[99], posterior_preds2[100], .chain, .iteration, .draw # Show summary statistics for key parameters print(samples_mlvl_nc$summary(c(&quot;biasM&quot;, &quot;betaM&quot;, &quot;biasSD&quot;, &quot;betaSD&quot;))) ## # A tibble: 4 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 biasM 0.407 0.403 0.0800 0.0802 0.280 0.542 1.00 1343. 1892. ## 2 betaM 1.16 1.17 0.0695 0.0694 1.05 1.28 1.00 1431. 2057. ## 3 biasSD 0.233 0.233 0.0660 0.0646 0.127 0.338 1.00 723. 830. ## 4 betaSD 0.379 0.378 0.0468 0.0457 0.306 0.460 1.00 1798. 2762. # Extract posterior draws for analysis draws_df &lt;- as_draws_df(samples_mlvl_nc$draws()) # Create trace plots to check convergence p1 &lt;- mcmc_trace(draws_df, pars = c(&quot;biasM&quot;, &quot;biasSD&quot;, &quot;betaM&quot;, &quot;betaSD&quot;)) + theme_classic() + ggtitle(&quot;Trace Plots for Population Parameters&quot;) # Show trace plots p1 # Create prior-posterior update plots create_density_plot &lt;- function(param, true_value, title) { prior_name &lt;- paste0(param, &quot;_prior&quot;) ggplot(draws_df) + geom_histogram(aes(get(param)), fill = &quot;blue&quot;, alpha = 0.3) + geom_histogram(aes(get(prior_name)), fill = &quot;red&quot;, alpha = 0.3) + geom_vline(xintercept = true_value, linetype = &quot;dashed&quot;) + labs(title = title, subtitle = &quot;Blue: posterior, Red: prior, Dashed: true value&quot;, x = param, y = &quot;Density&quot;) + theme_classic() } # Create individual plots p_biasM &lt;- create_density_plot(&quot;biasM&quot;, biasM, &quot;Population Mean Bias&quot;) p_biasSD &lt;- create_density_plot(&quot;biasSD&quot;, biasSD, &quot;Population SD of Bias&quot;) p_betaM &lt;- create_density_plot(&quot;betaM&quot;, betaM, &quot;Population Mean Beta&quot;) p_betaSD &lt;- create_density_plot(&quot;betaSD&quot;, betaSD, &quot;Population SD of Beta&quot;) # Show them in a grid (p_biasM + p_biasSD) / (p_betaM + p_betaSD) # Show correlations p1 &lt;- ggplot(draws_df, aes(biasM, biasSD, group = .chain, color = .chain)) + geom_point(alpha = 0.1) + theme_classic() p2 &lt;- ggplot(draws_df, aes(betaM, betaSD, group = .chain, color = .chain)) + geom_point(alpha = 0.1) + theme_classic() p3 &lt;- ggplot(draws_df, aes(biasM, betaM, group = .chain, color = .chain)) + geom_point(alpha = 0.1) + theme_classic() p4 &lt;- ggplot(draws_df, aes(biasSD, betaSD, group = .chain, color = .chain)) + geom_point(alpha = 0.1) + theme_classic() p1 + p2 + p3 + p4 # Create posterior predictive check plots p1 &lt;- ggplot(draws_df) + geom_histogram(aes(`prior_preds0[1]`), fill = &quot;red&quot;, alpha = 0.3, bins = 30) + geom_histogram(aes(`posterior_preds0[1]`), fill = &quot;blue&quot;, alpha = 0.3, bins = 30) + geom_histogram(aes(`posterior_preds1[1]`), fill = &quot;green&quot;, alpha = 0.3, bins = 30) + geom_histogram(aes(`posterior_preds2[1]`), fill = &quot;purple&quot;, alpha = 0.3, bins = 30) + labs(title = &quot;Prior and Posterior Predictive Distributions&quot;, subtitle = &quot;Red: prior, Blue: no memory effect, Green: neutral memory, Purple: strong memory&quot;, x = &quot;Predicted Right Choices (out of 120)&quot;, y = &quot;Count&quot;) + theme_classic() # Display plots p1 # Individual-level parameter recovery # Extract individual parameters for a sample of agents sample_agents &lt;- sample(1:agents, 100) sample_data &lt;- d %&gt;% filter(agent %in% sample_agents, trial == 1) %&gt;% dplyr::select(agent, bias, beta) # Extract posterior means for individual agents bias_means &lt;- c() beta_means &lt;- c() for (i in sample_agents) { bias_means[i] &lt;- mean(draws_df[[paste0(&quot;biasID[&quot;, i, &quot;]&quot;)]]) beta_means[i] &lt;- mean(draws_df[[paste0(&quot;betaID[&quot;, i, &quot;]&quot;)]]) } # Create comparison data comparison_data &lt;- tibble( agent = sample_agents, true_bias = sample_data$bias, est_bias = bias_means[sample_agents], true_beta = sample_data$beta, est_beta = beta_means[sample_agents] ) # Plot comparison p1 &lt;- ggplot(comparison_data, aes(true_bias, est_bias)) + geom_point(size = 3) + geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;) + geom_smooth(method = lm) + labs(title = &quot;Bias Parameter Recovery&quot;, x = &quot;True Bias&quot;, y = &quot;Estimated Bias&quot;) + theme_classic() p2 &lt;- ggplot(comparison_data, aes(true_beta, est_beta)) + geom_point(size = 3) + geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;) + geom_smooth(method = lm) + labs(title = &quot;Beta Parameter Recovery&quot;, x = &quot;True Beta&quot;, y = &quot;Estimated Beta&quot;) + theme_classic() # Display parameter recovery plots p1 + p2 7.12.5 Multilevel memory with correlation between parameters stan_model_nc_cor &lt;- &quot; // // This STAN model infers a random bias from a sequences of 1s and 0s (heads and tails) // functions{ real normal_lb_rng(real mu, real sigma, real lb) { real p = normal_cdf(lb | mu, sigma); // cdf for bounds real u = uniform_rng(p, 1); return (sigma * inv_Phi(u)) + mu; // inverse cdf for value } } // The input (data) for the model. data { int&lt;lower = 1&gt; trials; int&lt;lower = 1&gt; agents; array[trials, agents] int h; array[trials, agents] int other; } // The parameters accepted by the model. parameters { real biasM; real betaM; vector&lt;lower = 0&gt;[2] tau; matrix[2, agents] z_IDs; cholesky_factor_corr[2] L_u; } transformed parameters { array[trials, agents] real memory; matrix[agents,2] IDs; IDs = (diag_pre_multiply(tau, L_u) * z_IDs)&#39;; for (agent in 1:agents){ for (trial in 1:trials){ if (trial == 1) { memory[trial, agent] = 0.5; } if (trial &lt; trials){ memory[trial + 1, agent] = memory[trial, agent] + ((other[trial, agent] - memory[trial, agent]) / trial); if (memory[trial + 1, agent] == 0){memory[trial + 1, agent] = 0.01;} if (memory[trial + 1, agent] == 1){memory[trial + 1, agent] = 0.99;} } } } } // The model to be estimated. model { target += normal_lpdf(biasM | 0, 1); target += normal_lpdf(tau[1] | 0, .3) - normal_lccdf(0 | 0, .3); target += normal_lpdf(betaM | 0, .3); target += normal_lpdf(tau[2] | 0, .3) - normal_lccdf(0 | 0, .3); target += lkj_corr_cholesky_lpdf(L_u | 2); target += std_normal_lpdf(to_vector(z_IDs)); for (agent in 1:agents){ for (trial in 1:trials){ target += bernoulli_logit_lpmf(h[trial, agent] | biasM + IDs[agent, 1] + memory[trial, agent] * (betaM + IDs[agent, 2])); } } } generated quantities{ real biasM_prior; real&lt;lower=0&gt; biasSD_prior; real betaM_prior; real&lt;lower=0&gt; betaSD_prior; real bias_prior; real beta_prior; array[agents] int&lt;lower=0, upper = trials&gt; prior_preds0; array[agents] int&lt;lower=0, upper = trials&gt; prior_preds1; array[agents] int&lt;lower=0, upper = trials&gt; prior_preds2; array[agents] int&lt;lower=0, upper = trials&gt; posterior_preds0; array[agents] int&lt;lower=0, upper = trials&gt; posterior_preds1; array[agents] int&lt;lower=0, upper = trials&gt; posterior_preds2; biasM_prior = normal_rng(0,1); biasSD_prior = normal_lb_rng(0,0.3,0); betaM_prior = normal_rng(0,1); betaSD_prior = normal_lb_rng(0,0.3,0); bias_prior = normal_rng(biasM_prior, biasSD_prior); beta_prior = normal_rng(betaM_prior, betaSD_prior); for (i in 1:agents){ prior_preds0[i] = binomial_rng(trials, inv_logit(bias_prior + 0 * beta_prior)); prior_preds1[i] = binomial_rng(trials, inv_logit(bias_prior + 1 * beta_prior)); prior_preds2[i] = binomial_rng(trials, inv_logit(bias_prior + 2 * beta_prior)); posterior_preds0[i] = binomial_rng(trials, inv_logit(biasM + IDs[i,1] + 0 * (betaM + IDs[i,2]))); posterior_preds1[i] = binomial_rng(trials, inv_logit(biasM + IDs[i,1] + 1 * (betaM + IDs[i,2]))); posterior_preds2[i] = binomial_rng(trials, inv_logit(biasM + IDs[i,1] + 2 * (betaM + IDs[i,2]))); } } &quot; write_stan_file( stan_model_nc_cor, dir = &quot;stan/&quot;, basename = &quot;W6_MultilevelMemory_nc_cor.stan&quot;) model_file &lt;- &quot;simmodels/W6_MultilevelMemory_noncentered_cor.RDS&quot; # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(model_file)) { # Compile the model file &lt;- file.path(&quot;stan/W6_MultilevelMemory_nc_cor.stan&quot;) mod &lt;- cmdstan_model(file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) # Sample from the posterior distribution samples_mlvl_nc_cor &lt;- mod$sample( data = data_memory, seed = 123, chains = 2, parallel_chains = 2, threads_per_chain = 1, iter_warmup = 2000, iter_sampling = 2000, refresh = 500, max_treedepth = 20, adapt_delta = 0.99 ) # Save the model results samples_mlvl_nc_cor$save_object(file = model_file) cat(&quot;Generated new model fit and saved to&quot;, model_file, &quot;\\n&quot;) } else { # Load existing results samples_mlvl_nc_cor &lt;- readRDS(model_file) cat(&quot;Loaded existing model fit from&quot;, model_file, &quot;\\n&quot;) } 7.12.6 Assessing multilevel memory # Check if samples_biased exists if (!exists(&quot;samples_mlvl_nc_cor&quot;)) { cat(&quot;Loading multilevel non centered correlated model samples...\\n&quot;) samples_mlvl_nc_cor &lt;- readRDS(&quot;simmodels/W6_MultilevelMemory_noncentered_cor.RDS&quot;) cat(&quot;Available parameters:&quot;, paste(colnames(as_draws_df(samples_mlvl_nc_cor$draws())), collapse = &quot;, &quot;), &quot;\\n&quot;) } ## Loading multilevel non centered correlated model samples... ## Available parameters: lp__, biasM, betaM, tau[1], tau[2], z_IDs[1,1], z_IDs[2,1], z_IDs[1,2], z_IDs[2,2], z_IDs[1,3], z_IDs[2,3], z_IDs[1,4], z_IDs[2,4], z_IDs[1,5], z_IDs[2,5], z_IDs[1,6], z_IDs[2,6], z_IDs[1,7], z_IDs[2,7], z_IDs[1,8], z_IDs[2,8], z_IDs[1,9], z_IDs[2,9], z_IDs[1,10], z_IDs[2,10], z_IDs[1,11], z_IDs[2,11], z_IDs[1,12], z_IDs[2,12], z_IDs[1,13], z_IDs[2,13], z_IDs[1,14], z_IDs[2,14], z_IDs[1,15], z_IDs[2,15], z_IDs[1,16], z_IDs[2,16], z_IDs[1,17], z_IDs[2,17], z_IDs[1,18], z_IDs[2,18], z_IDs[1,19], z_IDs[2,19], z_IDs[1,20], z_IDs[2,20], z_IDs[1,21], z_IDs[2,21], z_IDs[1,22], z_IDs[2,22], z_IDs[1,23], z_IDs[2,23], z_IDs[1,24], z_IDs[2,24], z_IDs[1,25], z_IDs[2,25], z_IDs[1,26], z_IDs[2,26], z_IDs[1,27], z_IDs[2,27], z_IDs[1,28], z_IDs[2,28], z_IDs[1,29], z_IDs[2,29], z_IDs[1,30], z_IDs[2,30], z_IDs[1,31], z_IDs[2,31], z_IDs[1,32], z_IDs[2,32], z_IDs[1,33], z_IDs[2,33], z_IDs[1,34], z_IDs[2,34], z_IDs[1,35], z_IDs[2,35], z_IDs[1,36], z_IDs[2,36], z_IDs[1,37], z_IDs[2,37], z_IDs[1,38], z_IDs[2,38], z_IDs[1,39], z_IDs[2,39], z_IDs[1,40], z_IDs[2,40], z_IDs[1,41], z_IDs[2,41], z_IDs[1,42], z_IDs[2,42], z_IDs[1,43], z_IDs[2,43], z_IDs[1,44], z_IDs[2,44], z_IDs[1,45], z_IDs[2,45], z_IDs[1,46], z_IDs[2,46], z_IDs[1,47], z_IDs[2,47], z_IDs[1,48], z_IDs[2,48], z_IDs[1,49], z_IDs[2,49], z_IDs[1,50], z_IDs[2,50], z_IDs[1,51], z_IDs[2,51], z_IDs[1,52], z_IDs[2,52], z_IDs[1,53], z_IDs[2,53], z_IDs[1,54], z_IDs[2,54], z_IDs[1,55], z_IDs[2,55], z_IDs[1,56], z_IDs[2,56], z_IDs[1,57], z_IDs[2,57], z_IDs[1,58], z_IDs[2,58], z_IDs[1,59], z_IDs[2,59], z_IDs[1,60], z_IDs[2,60], z_IDs[1,61], z_IDs[2,61], z_IDs[1,62], z_IDs[2,62], z_IDs[1,63], z_IDs[2,63], z_IDs[1,64], z_IDs[2,64], z_IDs[1,65], z_IDs[2,65], z_IDs[1,66], z_IDs[2,66], z_IDs[1,67], z_IDs[2,67], z_IDs[1,68], z_IDs[2,68], z_IDs[1,69], z_IDs[2,69], z_IDs[1,70], z_IDs[2,70], z_IDs[1,71], z_IDs[2,71], z_IDs[1,72], z_IDs[2,72], z_IDs[1,73], z_IDs[2,73], z_IDs[1,74], z_IDs[2,74], z_IDs[1,75], z_IDs[2,75], z_IDs[1,76], z_IDs[2,76], z_IDs[1,77], z_IDs[2,77], z_IDs[1,78], z_IDs[2,78], z_IDs[1,79], z_IDs[2,79], z_IDs[1,80], z_IDs[2,80], z_IDs[1,81], z_IDs[2,81], z_IDs[1,82], z_IDs[2,82], z_IDs[1,83], z_IDs[2,83], z_IDs[1,84], z_IDs[2,84], z_IDs[1,85], z_IDs[2,85], z_IDs[1,86], z_IDs[2,86], z_IDs[1,87], z_IDs[2,87], z_IDs[1,88], z_IDs[2,88], z_IDs[1,89], z_IDs[2,89], z_IDs[1,90], z_IDs[2,90], z_IDs[1,91], z_IDs[2,91], z_IDs[1,92], z_IDs[2,92], z_IDs[1,93], z_IDs[2,93], z_IDs[1,94], z_IDs[2,94], z_IDs[1,95], z_IDs[2,95], z_IDs[1,96], z_IDs[2,96], z_IDs[1,97], z_IDs[2,97], z_IDs[1,98], z_IDs[2,98], z_IDs[1,99], z_IDs[2,99], z_IDs[1,100], z_IDs[2,100], L_u[1,1], L_u[2,1], L_u[1,2], L_u[2,2], memory[1,1], memory[2,1], memory[3,1], memory[4,1], memory[5,1], memory[6,1], memory[7,1], memory[8,1], memory[9,1], memory[10,1], memory[11,1], memory[12,1], memory[13,1], memory[14,1], memory[15,1], memory[16,1], memory[17,1], memory[18,1], memory[19,1], memory[20,1], memory[21,1], memory[22,1], memory[23,1], memory[24,1], memory[25,1], memory[26,1], memory[27,1], memory[28,1], memory[29,1], memory[30,1], memory[31,1], memory[32,1], memory[33,1], memory[34,1], memory[35,1], memory[36,1], memory[37,1], memory[38,1], memory[39,1], memory[40,1], memory[41,1], memory[42,1], memory[43,1], memory[44,1], memory[45,1], memory[46,1], memory[47,1], memory[48,1], memory[49,1], memory[50,1], memory[51,1], memory[52,1], memory[53,1], memory[54,1], memory[55,1], memory[56,1], memory[57,1], memory[58,1], memory[59,1], memory[60,1], memory[61,1], memory[62,1], memory[63,1], memory[64,1], memory[65,1], memory[66,1], memory[67,1], memory[68,1], memory[69,1], memory[70,1], memory[71,1], memory[72,1], memory[73,1], memory[74,1], memory[75,1], memory[76,1], memory[77,1], memory[78,1], memory[79,1], memory[80,1], memory[81,1], memory[82,1], memory[83,1], memory[84,1], memory[85,1], memory[86,1], memory[87,1], memory[88,1], memory[89,1], memory[90,1], memory[91,1], memory[92,1], memory[93,1], memory[94,1], memory[95,1], memory[96,1], memory[97,1], memory[98,1], memory[99,1], memory[100,1], memory[101,1], memory[102,1], memory[103,1], memory[104,1], memory[105,1], memory[106,1], memory[107,1], memory[108,1], memory[109,1], memory[110,1], memory[111,1], memory[112,1], memory[113,1], memory[114,1], memory[115,1], memory[116,1], memory[117,1], memory[118,1], memory[119,1], memory[120,1], memory[1,2], memory[2,2], memory[3,2], memory[4,2], memory[5,2], memory[6,2], memory[7,2], memory[8,2], memory[9,2], memory[10,2], memory[11,2], memory[12,2], memory[13,2], memory[14,2], memory[15,2], memory[16,2], memory[17,2], memory[18,2], memory[19,2], memory[20,2], memory[21,2], memory[22,2], memory[23,2], memory[24,2], memory[25,2], memory[26,2], memory[27,2], memory[28,2], memory[29,2], memory[30,2], memory[31,2], memory[32,2], memory[33,2], memory[34,2], memory[35,2], memory[36,2], memory[37,2], memory[38,2], memory[39,2], memory[40,2], memory[41,2], memory[42,2], memory[43,2], memory[44,2], memory[45,2], memory[46,2], memory[47,2], memory[48,2], memory[49,2], memory[50,2], memory[51,2], memory[52,2], memory[53,2], memory[54,2], memory[55,2], memory[56,2], memory[57,2], memory[58,2], memory[59,2], memory[60,2], memory[61,2], memory[62,2], memory[63,2], memory[64,2], memory[65,2], memory[66,2], memory[67,2], memory[68,2], memory[69,2], memory[70,2], memory[71,2], memory[72,2], memory[73,2], memory[74,2], memory[75,2], memory[76,2], memory[77,2], memory[78,2], memory[79,2], memory[80,2], memory[81,2], memory[82,2], memory[83,2], memory[84,2], memory[85,2], memory[86,2], memory[87,2], memory[88,2], memory[89,2], memory[90,2], memory[91,2], memory[92,2], memory[93,2], memory[94,2], memory[95,2], memory[96,2], memory[97,2], memory[98,2], memory[99,2], memory[100,2], memory[101,2], memory[102,2], memory[103,2], memory[104,2], memory[105,2], memory[106,2], memory[107,2], memory[108,2], memory[109,2], memory[110,2], memory[111,2], memory[112,2], memory[113,2], memory[114,2], memory[115,2], memory[116,2], memory[117,2], memory[118,2], memory[119,2], memory[120,2], memory[1,3], memory[2,3], memory[3,3], memory[4,3], memory[5,3], memory[6,3], memory[7,3], memory[8,3], memory[9,3], memory[10,3], memory[11,3], memory[12,3], memory[13,3], memory[14,3], memory[15,3], memory[16,3], memory[17,3], memory[18,3], memory[19,3], memory[20,3], memory[21,3], memory[22,3], memory[23,3], memory[24,3], memory[25,3], memory[26,3], memory[27,3], memory[28,3], memory[29,3], memory[30,3], memory[31,3], memory[32,3], memory[33,3], memory[34,3], memory[35,3], memory[36,3], memory[37,3], memory[38,3], memory[39,3], memory[40,3], memory[41,3], memory[42,3], memory[43,3], memory[44,3], memory[45,3], memory[46,3], memory[47,3], memory[48,3], memory[49,3], memory[50,3], memory[51,3], memory[52,3], memory[53,3], memory[54,3], memory[55,3], memory[56,3], memory[57,3], memory[58,3], memory[59,3], memory[60,3], memory[61,3], memory[62,3], memory[63,3], memory[64,3], memory[65,3], memory[66,3], memory[67,3], memory[68,3], memory[69,3], memory[70,3], memory[71,3], memory[72,3], memory[73,3], memory[74,3], memory[75,3], memory[76,3], memory[77,3], memory[78,3], memory[79,3], memory[80,3], memory[81,3], memory[82,3], memory[83,3], memory[84,3], memory[85,3], memory[86,3], memory[87,3], memory[88,3], memory[89,3], memory[90,3], memory[91,3], memory[92,3], memory[93,3], memory[94,3], memory[95,3], memory[96,3], memory[97,3], memory[98,3], memory[99,3], memory[100,3], memory[101,3], memory[102,3], memory[103,3], memory[104,3], memory[105,3], memory[106,3], memory[107,3], memory[108,3], memory[109,3], memory[110,3], memory[111,3], memory[112,3], memory[113,3], memory[114,3], memory[115,3], memory[116,3], memory[117,3], memory[118,3], memory[119,3], memory[120,3], memory[1,4], memory[2,4], memory[3,4], memory[4,4], memory[5,4], memory[6,4], memory[7,4], memory[8,4], memory[9,4], memory[10,4], memory[11,4], memory[12,4], memory[13,4], memory[14,4], memory[15,4], memory[16,4], memory[17,4], memory[18,4], memory[19,4], memory[20,4], memory[21,4], memory[22,4], memory[23,4], memory[24,4], memory[25,4], memory[26,4], memory[27,4], memory[28,4], memory[29,4], memory[30,4], memory[31,4], memory[32,4], memory[33,4], memory[34,4], memory[35,4], memory[36,4], memory[37,4], memory[38,4], memory[39,4], memory[40,4], memory[41,4], memory[42,4], memory[43,4], memory[44,4], memory[45,4], memory[46,4], memory[47,4], memory[48,4], memory[49,4], memory[50,4], memory[51,4], memory[52,4], memory[53,4], memory[54,4], memory[55,4], memory[56,4], memory[57,4], memory[58,4], memory[59,4], memory[60,4], memory[61,4], memory[62,4], memory[63,4], memory[64,4], memory[65,4], memory[66,4], memory[67,4], memory[68,4], memory[69,4], memory[70,4], memory[71,4], memory[72,4], memory[73,4], memory[74,4], memory[75,4], memory[76,4], memory[77,4], memory[78,4], memory[79,4], memory[80,4], memory[81,4], memory[82,4], memory[83,4], memory[84,4], memory[85,4], memory[86,4], memory[87,4], memory[88,4], memory[89,4], memory[90,4], memory[91,4], memory[92,4], memory[93,4], memory[94,4], memory[95,4], memory[96,4], memory[97,4], memory[98,4], memory[99,4], memory[100,4], memory[101,4], memory[102,4], memory[103,4], memory[104,4], memory[105,4], memory[106,4], memory[107,4], memory[108,4], memory[109,4], memory[110,4], memory[111,4], memory[112,4], memory[113,4], memory[114,4], memory[115,4], memory[116,4], memory[117,4], memory[118,4], memory[119,4], memory[120,4], memory[1,5], memory[2,5], memory[3,5], memory[4,5], memory[5,5], memory[6,5], memory[7,5], memory[8,5], memory[9,5], memory[10,5], memory[11,5], memory[12,5], memory[13,5], memory[14,5], memory[15,5], memory[16,5], memory[17,5], memory[18,5], memory[19,5], memory[20,5], memory[21,5], memory[22,5], memory[23,5], memory[24,5], memory[25,5], memory[26,5], memory[27,5], memory[28,5], memory[29,5], memory[30,5], memory[31,5], memory[32,5], memory[33,5], memory[34,5], memory[35,5], memory[36,5], memory[37,5], memory[38,5], memory[39,5], memory[40,5], memory[41,5], memory[42,5], memory[43,5], memory[44,5], memory[45,5], memory[46,5], memory[47,5], memory[48,5], memory[49,5], memory[50,5], memory[51,5], memory[52,5], memory[53,5], memory[54,5], memory[55,5], memory[56,5], memory[57,5], memory[58,5], memory[59,5], memory[60,5], memory[61,5], memory[62,5], memory[63,5], memory[64,5], memory[65,5], memory[66,5], memory[67,5], memory[68,5], memory[69,5], memory[70,5], memory[71,5], memory[72,5], memory[73,5], memory[74,5], memory[75,5], memory[76,5], memory[77,5], memory[78,5], memory[79,5], memory[80,5], memory[81,5], memory[82,5], memory[83,5], memory[84,5], memory[85,5], memory[86,5], memory[87,5], memory[88,5], memory[89,5], memory[90,5], memory[91,5], memory[92,5], memory[93,5], memory[94,5], memory[95,5], memory[96,5], memory[97,5], memory[98,5], memory[99,5], memory[100,5], memory[101,5], memory[102,5], memory[103,5], memory[104,5], memory[105,5], memory[106,5], memory[107,5], memory[108,5], memory[109,5], memory[110,5], memory[111,5], memory[112,5], memory[113,5], memory[114,5], memory[115,5], memory[116,5], memory[117,5], memory[118,5], memory[119,5], memory[120,5], memory[1,6], memory[2,6], memory[3,6], memory[4,6], memory[5,6], memory[6,6], memory[7,6], memory[8,6], memory[9,6], memory[10,6], memory[11,6], memory[12,6], memory[13,6], memory[14,6], memory[15,6], memory[16,6], memory[17,6], memory[18,6], memory[19,6], memory[20,6], memory[21,6], memory[22,6], memory[23,6], memory[24,6], memory[25,6], memory[26,6], memory[27,6], memory[28,6], memory[29,6], memory[30,6], memory[31,6], memory[32,6], memory[33,6], memory[34,6], memory[35,6], memory[36,6], memory[37,6], memory[38,6], memory[39,6], memory[40,6], memory[41,6], memory[42,6], memory[43,6], memory[44,6], memory[45,6], memory[46,6], memory[47,6], memory[48,6], memory[49,6], memory[50,6], memory[51,6], memory[52,6], memory[53,6], memory[54,6], memory[55,6], memory[56,6], memory[57,6], memory[58,6], memory[59,6], memory[60,6], memory[61,6], memory[62,6], memory[63,6], memory[64,6], memory[65,6], memory[66,6], memory[67,6], memory[68,6], memory[69,6], memory[70,6], memory[71,6], memory[72,6], memory[73,6], memory[74,6], memory[75,6], memory[76,6], memory[77,6], memory[78,6], memory[79,6], memory[80,6], memory[81,6], memory[82,6], memory[83,6], memory[84,6], memory[85,6], memory[86,6], memory[87,6], memory[88,6], memory[89,6], memory[90,6], memory[91,6], memory[92,6], memory[93,6], memory[94,6], memory[95,6], memory[96,6], memory[97,6], memory[98,6], memory[99,6], memory[100,6], memory[101,6], memory[102,6], memory[103,6], memory[104,6], memory[105,6], memory[106,6], memory[107,6], memory[108,6], memory[109,6], memory[110,6], memory[111,6], memory[112,6], memory[113,6], memory[114,6], memory[115,6], memory[116,6], memory[117,6], memory[118,6], memory[119,6], memory[120,6], memory[1,7], memory[2,7], memory[3,7], memory[4,7], memory[5,7], memory[6,7], memory[7,7], memory[8,7], memory[9,7], memory[10,7], memory[11,7], memory[12,7], memory[13,7], memory[14,7], memory[15,7], memory[16,7], memory[17,7], memory[18,7], memory[19,7], memory[20,7], memory[21,7], memory[22,7], memory[23,7], memory[24,7], memory[25,7], memory[26,7], memory[27,7], memory[28,7], memory[29,7], memory[30,7], memory[31,7], memory[32,7], memory[33,7], memory[34,7], memory[35,7], memory[36,7], memory[37,7], memory[38,7], memory[39,7], memory[40,7], memory[41,7], memory[42,7], memory[43,7], memory[44,7], memory[45,7], memory[46,7], memory[47,7], memory[48,7], memory[49,7], memory[50,7], memory[51,7], memory[52,7], memory[53,7], memory[54,7], memory[55,7], memory[56,7], memory[57,7], memory[58,7], memory[59,7], memory[60,7], memory[61,7], memory[62,7], memory[63,7], memory[64,7], memory[65,7], memory[66,7], memory[67,7], memory[68,7], memory[69,7], memory[70,7], memory[71,7], memory[72,7], memory[73,7], memory[74,7], memory[75,7], memory[76,7], memory[77,7], memory[78,7], memory[79,7], memory[80,7], memory[81,7], memory[82,7], memory[83,7], memory[84,7], memory[85,7], memory[86,7], memory[87,7], memory[88,7], memory[89,7], memory[90,7], memory[91,7], memory[92,7], memory[93,7], memory[94,7], memory[95,7], memory[96,7], memory[97,7], memory[98,7], memory[99,7], memory[100,7], memory[101,7], memory[102,7], memory[103,7], memory[104,7], memory[105,7], memory[106,7], memory[107,7], memory[108,7], memory[109,7], memory[110,7], memory[111,7], memory[112,7], memory[113,7], memory[114,7], memory[115,7], memory[116,7], memory[117,7], memory[118,7], memory[119,7], memory[120,7], memory[1,8], memory[2,8], memory[3,8], memory[4,8], memory[5,8], memory[6,8], memory[7,8], memory[8,8], memory[9,8], memory[10,8], memory[11,8], memory[12,8], memory[13,8], memory[14,8], memory[15,8], memory[16,8], memory[17,8], memory[18,8], memory[19,8], memory[20,8], memory[21,8], memory[22,8], memory[23,8], memory[24,8], memory[25,8], memory[26,8], memory[27,8], memory[28,8], memory[29,8], memory[30,8], memory[31,8], memory[32,8], memory[33,8], memory[34,8], memory[35,8], memory[36,8], memory[37,8], memory[38,8], memory[39,8], memory[40,8], memory[41,8], memory[42,8], memory[43,8], memory[44,8], memory[45,8], memory[46,8], memory[47,8], memory[48,8], memory[49,8], memory[50,8], memory[51,8], memory[52,8], memory[53,8], memory[54,8], memory[55,8], memory[56,8], memory[57,8], memory[58,8], memory[59,8], memory[60,8], memory[61,8], memory[62,8], memory[63,8], memory[64,8], memory[65,8], memory[66,8], memory[67,8], memory[68,8], memory[69,8], memory[70,8], memory[71,8], memory[72,8], memory[73,8], memory[74,8], memory[75,8], memory[76,8], memory[77,8], memory[78,8], memory[79,8], memory[80,8], memory[81,8], memory[82,8], memory[83,8], memory[84,8], memory[85,8], memory[86,8], memory[87,8], memory[88,8], memory[89,8], memory[90,8], memory[91,8], memory[92,8], memory[93,8], memory[94,8], memory[95,8], memory[96,8], memory[97,8], memory[98,8], memory[99,8], memory[100,8], memory[101,8], memory[102,8], memory[103,8], memory[104,8], memory[105,8], memory[106,8], memory[107,8], memory[108,8], memory[109,8], memory[110,8], memory[111,8], memory[112,8], memory[113,8], memory[114,8], memory[115,8], memory[116,8], memory[117,8], memory[118,8], memory[119,8], memory[120,8], memory[1,9], memory[2,9], memory[3,9], memory[4,9], memory[5,9], memory[6,9], memory[7,9], memory[8,9], memory[9,9], memory[10,9], memory[11,9], memory[12,9], memory[13,9], memory[14,9], memory[15,9], memory[16,9], memory[17,9], memory[18,9], memory[19,9], memory[20,9], memory[21,9], memory[22,9], memory[23,9], memory[24,9], memory[25,9], memory[26,9], memory[27,9], memory[28,9], memory[29,9], memory[30,9], memory[31,9], memory[32,9], memory[33,9], memory[34,9], memory[35,9], memory[36,9], memory[37,9], memory[38,9], memory[39,9], memory[40,9], memory[41,9], memory[42,9], memory[43,9], memory[44,9], memory[45,9], memory[46,9], memory[47,9], memory[48,9], memory[49,9], memory[50,9], memory[51,9], memory[52,9], memory[53,9], memory[54,9], memory[55,9], memory[56,9], memory[57,9], memory[58,9], memory[59,9], memory[60,9], memory[61,9], memory[62,9], memory[63,9], memory[64,9], memory[65,9], memory[66,9], memory[67,9], memory[68,9], memory[69,9], memory[70,9], memory[71,9], memory[72,9], memory[73,9], memory[74,9], memory[75,9], memory[76,9], memory[77,9], memory[78,9], memory[79,9], memory[80,9], memory[81,9], memory[82,9], memory[83,9], memory[84,9], memory[85,9], memory[86,9], memory[87,9], memory[88,9], memory[89,9], memory[90,9], memory[91,9], memory[92,9], memory[93,9], memory[94,9], memory[95,9], memory[96,9], memory[97,9], memory[98,9], memory[99,9], memory[100,9], memory[101,9], memory[102,9], memory[103,9], memory[104,9], memory[105,9], memory[106,9], memory[107,9], memory[108,9], memory[109,9], memory[110,9], memory[111,9], memory[112,9], memory[113,9], memory[114,9], memory[115,9], memory[116,9], memory[117,9], memory[118,9], memory[119,9], memory[120,9], memory[1,10], memory[2,10], memory[3,10], memory[4,10], memory[5,10], memory[6,10], memory[7,10], memory[8,10], memory[9,10], memory[10,10], memory[11,10], memory[12,10], memory[13,10], memory[14,10], memory[15,10], memory[16,10], memory[17,10], memory[18,10], memory[19,10], memory[20,10], memory[21,10], memory[22,10], memory[23,10], memory[24,10], memory[25,10], memory[26,10], memory[27,10], memory[28,10], memory[29,10], memory[30,10], memory[31,10], memory[32,10], memory[33,10], memory[34,10], memory[35,10], memory[36,10], memory[37,10], memory[38,10], memory[39,10], memory[40,10], memory[41,10], memory[42,10], memory[43,10], memory[44,10], memory[45,10], memory[46,10], memory[47,10], memory[48,10], memory[49,10], memory[50,10], memory[51,10], memory[52,10], memory[53,10], memory[54,10], memory[55,10], memory[56,10], memory[57,10], memory[58,10], memory[59,10], memory[60,10], memory[61,10], memory[62,10], memory[63,10], memory[64,10], memory[65,10], memory[66,10], memory[67,10], memory[68,10], memory[69,10], memory[70,10], memory[71,10], memory[72,10], memory[73,10], memory[74,10], memory[75,10], memory[76,10], memory[77,10], memory[78,10], memory[79,10], memory[80,10], memory[81,10], memory[82,10], memory[83,10], memory[84,10], memory[85,10], memory[86,10], memory[87,10], memory[88,10], memory[89,10], memory[90,10], memory[91,10], memory[92,10], memory[93,10], memory[94,10], memory[95,10], memory[96,10], memory[97,10], memory[98,10], memory[99,10], memory[100,10], memory[101,10], memory[102,10], memory[103,10], memory[104,10], memory[105,10], memory[106,10], memory[107,10], memory[108,10], memory[109,10], memory[110,10], memory[111,10], memory[112,10], memory[113,10], memory[114,10], memory[115,10], memory[116,10], memory[117,10], memory[118,10], memory[119,10], memory[120,10], memory[1,11], memory[2,11], memory[3,11], memory[4,11], memory[5,11], memory[6,11], memory[7,11], memory[8,11], memory[9,11], memory[10,11], memory[11,11], memory[12,11], memory[13,11], memory[14,11], memory[15,11], memory[16,11], memory[17,11], memory[18,11], memory[19,11], memory[20,11], memory[21,11], memory[22,11], memory[23,11], memory[24,11], memory[25,11], memory[26,11], memory[27,11], memory[28,11], memory[29,11], memory[30,11], memory[31,11], memory[32,11], memory[33,11], memory[34,11], memory[35,11], memory[36,11], memory[37,11], memory[38,11], memory[39,11], memory[40,11], memory[41,11], memory[42,11], memory[43,11], memory[44,11], memory[45,11], memory[46,11], memory[47,11], memory[48,11], memory[49,11], memory[50,11], memory[51,11], memory[52,11], memory[53,11], memory[54,11], memory[55,11], memory[56,11], memory[57,11], memory[58,11], memory[59,11], memory[60,11], memory[61,11], memory[62,11], memory[63,11], memory[64,11], memory[65,11], memory[66,11], memory[67,11], memory[68,11], memory[69,11], memory[70,11], memory[71,11], memory[72,11], memory[73,11], memory[74,11], memory[75,11], memory[76,11], memory[77,11], memory[78,11], memory[79,11], memory[80,11], memory[81,11], memory[82,11], memory[83,11], memory[84,11], memory[85,11], memory[86,11], memory[87,11], memory[88,11], memory[89,11], memory[90,11], memory[91,11], memory[92,11], memory[93,11], memory[94,11], memory[95,11], memory[96,11], memory[97,11], memory[98,11], memory[99,11], memory[100,11], memory[101,11], memory[102,11], memory[103,11], memory[104,11], memory[105,11], memory[106,11], memory[107,11], memory[108,11], memory[109,11], memory[110,11], memory[111,11], memory[112,11], memory[113,11], memory[114,11], memory[115,11], memory[116,11], memory[117,11], memory[118,11], memory[119,11], memory[120,11], memory[1,12], memory[2,12], memory[3,12], memory[4,12], memory[5,12], memory[6,12], memory[7,12], memory[8,12], memory[9,12], memory[10,12], memory[11,12], memory[12,12], memory[13,12], memory[14,12], memory[15,12], memory[16,12], memory[17,12], memory[18,12], memory[19,12], memory[20,12], memory[21,12], memory[22,12], memory[23,12], memory[24,12], memory[25,12], memory[26,12], memory[27,12], memory[28,12], memory[29,12], memory[30,12], memory[31,12], memory[32,12], memory[33,12], memory[34,12], memory[35,12], memory[36,12], memory[37,12], memory[38,12], memory[39,12], memory[40,12], memory[41,12], memory[42,12], memory[43,12], memory[44,12], memory[45,12], memory[46,12], memory[47,12], memory[48,12], memory[49,12], memory[50,12], memory[51,12], memory[52,12], memory[53,12], memory[54,12], memory[55,12], memory[56,12], memory[57,12], memory[58,12], memory[59,12], memory[60,12], memory[61,12], memory[62,12], memory[63,12], memory[64,12], memory[65,12], memory[66,12], memory[67,12], memory[68,12], memory[69,12], memory[70,12], memory[71,12], memory[72,12], memory[73,12], memory[74,12], memory[75,12], memory[76,12], memory[77,12], memory[78,12], memory[79,12], memory[80,12], memory[81,12], memory[82,12], memory[83,12], memory[84,12], memory[85,12], memory[86,12], memory[87,12], memory[88,12], memory[89,12], memory[90,12], memory[91,12], memory[92,12], memory[93,12], memory[94,12], memory[95,12], memory[96,12], memory[97,12], memory[98,12], memory[99,12], memory[100,12], memory[101,12], memory[102,12], memory[103,12], memory[104,12], memory[105,12], memory[106,12], memory[107,12], memory[108,12], memory[109,12], memory[110,12], memory[111,12], memory[112,12], memory[113,12], memory[114,12], memory[115,12], memory[116,12], memory[117,12], memory[118,12], memory[119,12], memory[120,12], memory[1,13], memory[2,13], memory[3,13], memory[4,13], memory[5,13], memory[6,13], memory[7,13], memory[8,13], memory[9,13], memory[10,13], memory[11,13], memory[12,13], memory[13,13], memory[14,13], memory[15,13], memory[16,13], memory[17,13], memory[18,13], memory[19,13], memory[20,13], memory[21,13], memory[22,13], memory[23,13], memory[24,13], memory[25,13], memory[26,13], memory[27,13], memory[28,13], memory[29,13], memory[30,13], memory[31,13], memory[32,13], memory[33,13], memory[34,13], memory[35,13], memory[36,13], memory[37,13], memory[38,13], memory[39,13], memory[40,13], memory[41,13], memory[42,13], memory[43,13], memory[44,13], memory[45,13], memory[46,13], memory[47,13], memory[48,13], memory[49,13], memory[50,13], memory[51,13], memory[52,13], memory[53,13], memory[54,13], memory[55,13], memory[56,13], memory[57,13], memory[58,13], memory[59,13], memory[60,13], memory[61,13], memory[62,13], memory[63,13], memory[64,13], memory[65,13], memory[66,13], memory[67,13], memory[68,13], memory[69,13], memory[70,13], memory[71,13], memory[72,13], memory[73,13], memory[74,13], memory[75,13], memory[76,13], memory[77,13], memory[78,13], memory[79,13], memory[80,13], memory[81,13], memory[82,13], memory[83,13], memory[84,13], memory[85,13], memory[86,13], memory[87,13], memory[88,13], memory[89,13], memory[90,13], memory[91,13], memory[92,13], memory[93,13], memory[94,13], memory[95,13], memory[96,13], memory[97,13], memory[98,13], memory[99,13], memory[100,13], memory[101,13], memory[102,13], memory[103,13], memory[104,13], memory[105,13], memory[106,13], memory[107,13], memory[108,13], memory[109,13], memory[110,13], memory[111,13], memory[112,13], memory[113,13], memory[114,13], memory[115,13], memory[116,13], memory[117,13], memory[118,13], memory[119,13], memory[120,13], memory[1,14], memory[2,14], memory[3,14], memory[4,14], memory[5,14], memory[6,14], memory[7,14], memory[8,14], memory[9,14], memory[10,14], memory[11,14], memory[12,14], memory[13,14], memory[14,14], memory[15,14], memory[16,14], memory[17,14], memory[18,14], memory[19,14], memory[20,14], memory[21,14], memory[22,14], memory[23,14], memory[24,14], memory[25,14], memory[26,14], memory[27,14], memory[28,14], memory[29,14], memory[30,14], memory[31,14], memory[32,14], memory[33,14], memory[34,14], memory[35,14], memory[36,14], memory[37,14], memory[38,14], memory[39,14], memory[40,14], memory[41,14], memory[42,14], memory[43,14], memory[44,14], memory[45,14], memory[46,14], memory[47,14], memory[48,14], memory[49,14], memory[50,14], memory[51,14], memory[52,14], memory[53,14], memory[54,14], memory[55,14], memory[56,14], memory[57,14], memory[58,14], memory[59,14], memory[60,14], memory[61,14], memory[62,14], memory[63,14], memory[64,14], memory[65,14], memory[66,14], memory[67,14], memory[68,14], memory[69,14], memory[70,14], memory[71,14], memory[72,14], memory[73,14], memory[74,14], memory[75,14], memory[76,14], memory[77,14], memory[78,14], memory[79,14], memory[80,14], memory[81,14], memory[82,14], memory[83,14], memory[84,14], memory[85,14], memory[86,14], memory[87,14], memory[88,14], memory[89,14], memory[90,14], memory[91,14], memory[92,14], memory[93,14], memory[94,14], memory[95,14], memory[96,14], memory[97,14], memory[98,14], memory[99,14], memory[100,14], memory[101,14], memory[102,14], memory[103,14], memory[104,14], memory[105,14], memory[106,14], memory[107,14], memory[108,14], memory[109,14], memory[110,14], memory[111,14], memory[112,14], memory[113,14], memory[114,14], memory[115,14], memory[116,14], memory[117,14], memory[118,14], memory[119,14], memory[120,14], memory[1,15], memory[2,15], memory[3,15], memory[4,15], memory[5,15], memory[6,15], memory[7,15], memory[8,15], memory[9,15], memory[10,15], memory[11,15], memory[12,15], memory[13,15], memory[14,15], memory[15,15], memory[16,15], memory[17,15], memory[18,15], memory[19,15], memory[20,15], memory[21,15], memory[22,15], memory[23,15], memory[24,15], memory[25,15], memory[26,15], memory[27,15], memory[28,15], memory[29,15], memory[30,15], memory[31,15], memory[32,15], memory[33,15], memory[34,15], memory[35,15], memory[36,15], memory[37,15], memory[38,15], memory[39,15], memory[40,15], memory[41,15], memory[42,15], memory[43,15], memory[44,15], memory[45,15], memory[46,15], memory[47,15], memory[48,15], memory[49,15], memory[50,15], memory[51,15], memory[52,15], memory[53,15], memory[54,15], memory[55,15], memory[56,15], memory[57,15], memory[58,15], memory[59,15], memory[60,15], memory[61,15], memory[62,15], memory[63,15], memory[64,15], memory[65,15], memory[66,15], memory[67,15], memory[68,15], memory[69,15], memory[70,15], memory[71,15], memory[72,15], memory[73,15], memory[74,15], memory[75,15], memory[76,15], memory[77,15], memory[78,15], memory[79,15], memory[80,15], memory[81,15], memory[82,15], memory[83,15], memory[84,15], memory[85,15], memory[86,15], memory[87,15], memory[88,15], memory[89,15], memory[90,15], memory[91,15], memory[92,15], memory[93,15], memory[94,15], memory[95,15], memory[96,15], memory[97,15], memory[98,15], memory[99,15], memory[100,15], memory[101,15], memory[102,15], memory[103,15], memory[104,15], memory[105,15], memory[106,15], memory[107,15], memory[108,15], memory[109,15], memory[110,15], memory[111,15], memory[112,15], memory[113,15], memory[114,15], memory[115,15], memory[116,15], memory[117,15], memory[118,15], memory[119,15], memory[120,15], memory[1,16], memory[2,16], memory[3,16], memory[4,16], memory[5,16], memory[6,16], memory[7,16], memory[8,16], memory[9,16], memory[10,16], memory[11,16], memory[12,16], memory[13,16], memory[14,16], memory[15,16], memory[16,16], memory[17,16], memory[18,16], memory[19,16], memory[20,16], memory[21,16], memory[22,16], memory[23,16], memory[24,16], memory[25,16], memory[26,16], memory[27,16], memory[28,16], memory[29,16], memory[30,16], memory[31,16], memory[32,16], memory[33,16], memory[34,16], memory[35,16], memory[36,16], memory[37,16], memory[38,16], memory[39,16], memory[40,16], memory[41,16], memory[42,16], memory[43,16], memory[44,16], memory[45,16], memory[46,16], memory[47,16], memory[48,16], memory[49,16], memory[50,16], memory[51,16], memory[52,16], memory[53,16], memory[54,16], memory[55,16], memory[56,16], memory[57,16], memory[58,16], memory[59,16], memory[60,16], memory[61,16], memory[62,16], memory[63,16], memory[64,16], memory[65,16], memory[66,16], memory[67,16], memory[68,16], memory[69,16], memory[70,16], memory[71,16], memory[72,16], memory[73,16], memory[74,16], memory[75,16], memory[76,16], memory[77,16], memory[78,16], memory[79,16], memory[80,16], memory[81,16], memory[82,16], memory[83,16], memory[84,16], memory[85,16], memory[86,16], memory[87,16], memory[88,16], memory[89,16], memory[90,16], memory[91,16], memory[92,16], memory[93,16], memory[94,16], memory[95,16], memory[96,16], memory[97,16], memory[98,16], memory[99,16], memory[100,16], memory[101,16], memory[102,16], memory[103,16], memory[104,16], memory[105,16], memory[106,16], memory[107,16], memory[108,16], memory[109,16], memory[110,16], memory[111,16], memory[112,16], memory[113,16], memory[114,16], memory[115,16], memory[116,16], memory[117,16], memory[118,16], memory[119,16], memory[120,16], memory[1,17], memory[2,17], memory[3,17], memory[4,17], memory[5,17], memory[6,17], memory[7,17], memory[8,17], memory[9,17], memory[10,17], memory[11,17], memory[12,17], memory[13,17], memory[14,17], memory[15,17], memory[16,17], memory[17,17], memory[18,17], memory[19,17], memory[20,17], memory[21,17], memory[22,17], memory[23,17], memory[24,17], memory[25,17], memory[26,17], memory[27,17], memory[28,17], memory[29,17], memory[30,17], memory[31,17], memory[32,17], memory[33,17], memory[34,17], memory[35,17], memory[36,17], memory[37,17], memory[38,17], memory[39,17], memory[40,17], memory[41,17], memory[42,17], memory[43,17], memory[44,17], memory[45,17], memory[46,17], memory[47,17], memory[48,17], memory[49,17], memory[50,17], memory[51,17], memory[52,17], memory[53,17], memory[54,17], memory[55,17], memory[56,17], memory[57,17], memory[58,17], memory[59,17], memory[60,17], memory[61,17], memory[62,17], memory[63,17], memory[64,17], memory[65,17], memory[66,17], memory[67,17], memory[68,17], memory[69,17], memory[70,17], memory[71,17], memory[72,17], memory[73,17], memory[74,17], memory[75,17], memory[76,17], memory[77,17], memory[78,17], memory[79,17], memory[80,17], memory[81,17], memory[82,17], memory[83,17], memory[84,17], memory[85,17], memory[86,17], memory[87,17], memory[88,17], memory[89,17], memory[90,17], memory[91,17], memory[92,17], memory[93,17], memory[94,17], memory[95,17], memory[96,17], memory[97,17], memory[98,17], memory[99,17], memory[100,17], memory[101,17], memory[102,17], memory[103,17], memory[104,17], memory[105,17], memory[106,17], memory[107,17], memory[108,17], memory[109,17], memory[110,17], memory[111,17], memory[112,17], memory[113,17], memory[114,17], memory[115,17], memory[116,17], memory[117,17], memory[118,17], memory[119,17], memory[120,17], memory[1,18], memory[2,18], memory[3,18], memory[4,18], memory[5,18], memory[6,18], memory[7,18], memory[8,18], memory[9,18], memory[10,18], memory[11,18], memory[12,18], memory[13,18], memory[14,18], memory[15,18], memory[16,18], memory[17,18], memory[18,18], memory[19,18], memory[20,18], memory[21,18], memory[22,18], memory[23,18], memory[24,18], memory[25,18], memory[26,18], memory[27,18], memory[28,18], memory[29,18], memory[30,18], memory[31,18], memory[32,18], memory[33,18], memory[34,18], memory[35,18], memory[36,18], memory[37,18], memory[38,18], memory[39,18], memory[40,18], memory[41,18], memory[42,18], memory[43,18], memory[44,18], memory[45,18], memory[46,18], memory[47,18], memory[48,18], memory[49,18], memory[50,18], memory[51,18], memory[52,18], memory[53,18], memory[54,18], memory[55,18], memory[56,18], memory[57,18], memory[58,18], memory[59,18], memory[60,18], memory[61,18], memory[62,18], memory[63,18], memory[64,18], memory[65,18], memory[66,18], memory[67,18], memory[68,18], memory[69,18], memory[70,18], memory[71,18], memory[72,18], memory[73,18], memory[74,18], memory[75,18], memory[76,18], memory[77,18], memory[78,18], memory[79,18], memory[80,18], memory[81,18], memory[82,18], memory[83,18], memory[84,18], memory[85,18], memory[86,18], memory[87,18], memory[88,18], memory[89,18], memory[90,18], memory[91,18], memory[92,18], memory[93,18], memory[94,18], memory[95,18], memory[96,18], memory[97,18], memory[98,18], memory[99,18], memory[100,18], memory[101,18], memory[102,18], memory[103,18], memory[104,18], memory[105,18], memory[106,18], memory[107,18], memory[108,18], memory[109,18], memory[110,18], memory[111,18], memory[112,18], memory[113,18], memory[114,18], memory[115,18], memory[116,18], memory[117,18], memory[118,18], memory[119,18], memory[120,18], memory[1,19], memory[2,19], memory[3,19], memory[4,19], memory[5,19], memory[6,19], memory[7,19], memory[8,19], memory[9,19], memory[10,19], memory[11,19], memory[12,19], memory[13,19], memory[14,19], memory[15,19], memory[16,19], memory[17,19], memory[18,19], memory[19,19], memory[20,19], memory[21,19], memory[22,19], memory[23,19], memory[24,19], memory[25,19], memory[26,19], memory[27,19], memory[28,19], memory[29,19], memory[30,19], memory[31,19], memory[32,19], memory[33,19], memory[34,19], memory[35,19], memory[36,19], memory[37,19], memory[38,19], memory[39,19], memory[40,19], memory[41,19], memory[42,19], memory[43,19], memory[44,19], memory[45,19], memory[46,19], memory[47,19], memory[48,19], memory[49,19], memory[50,19], memory[51,19], memory[52,19], memory[53,19], memory[54,19], memory[55,19], memory[56,19], memory[57,19], memory[58,19], memory[59,19], memory[60,19], memory[61,19], memory[62,19], memory[63,19], memory[64,19], memory[65,19], memory[66,19], memory[67,19], memory[68,19], memory[69,19], memory[70,19], memory[71,19], memory[72,19], memory[73,19], memory[74,19], memory[75,19], memory[76,19], memory[77,19], memory[78,19], memory[79,19], memory[80,19], memory[81,19], memory[82,19], memory[83,19], memory[84,19], memory[85,19], memory[86,19], memory[87,19], memory[88,19], memory[89,19], memory[90,19], memory[91,19], memory[92,19], memory[93,19], memory[94,19], memory[95,19], memory[96,19], memory[97,19], memory[98,19], memory[99,19], memory[100,19], memory[101,19], memory[102,19], memory[103,19], memory[104,19], memory[105,19], memory[106,19], memory[107,19], memory[108,19], memory[109,19], memory[110,19], memory[111,19], memory[112,19], memory[113,19], memory[114,19], memory[115,19], memory[116,19], memory[117,19], memory[118,19], memory[119,19], memory[120,19], memory[1,20], memory[2,20], memory[3,20], memory[4,20], memory[5,20], memory[6,20], memory[7,20], memory[8,20], memory[9,20], memory[10,20], memory[11,20], memory[12,20], memory[13,20], memory[14,20], memory[15,20], memory[16,20], memory[17,20], memory[18,20], memory[19,20], memory[20,20], memory[21,20], memory[22,20], memory[23,20], memory[24,20], memory[25,20], memory[26,20], memory[27,20], memory[28,20], memory[29,20], memory[30,20], memory[31,20], memory[32,20], memory[33,20], memory[34,20], memory[35,20], memory[36,20], memory[37,20], memory[38,20], memory[39,20], memory[40,20], memory[41,20], memory[42,20], memory[43,20], memory[44,20], memory[45,20], memory[46,20], memory[47,20], memory[48,20], memory[49,20], memory[50,20], memory[51,20], memory[52,20], memory[53,20], memory[54,20], memory[55,20], memory[56,20], memory[57,20], memory[58,20], memory[59,20], memory[60,20], memory[61,20], memory[62,20], memory[63,20], memory[64,20], memory[65,20], memory[66,20], memory[67,20], memory[68,20], memory[69,20], memory[70,20], memory[71,20], memory[72,20], memory[73,20], memory[74,20], memory[75,20], memory[76,20], memory[77,20], memory[78,20], memory[79,20], memory[80,20], memory[81,20], memory[82,20], memory[83,20], memory[84,20], memory[85,20], memory[86,20], memory[87,20], memory[88,20], memory[89,20], memory[90,20], memory[91,20], memory[92,20], memory[93,20], memory[94,20], memory[95,20], memory[96,20], memory[97,20], memory[98,20], memory[99,20], memory[100,20], memory[101,20], memory[102,20], memory[103,20], memory[104,20], memory[105,20], memory[106,20], memory[107,20], memory[108,20], memory[109,20], memory[110,20], memory[111,20], memory[112,20], memory[113,20], memory[114,20], memory[115,20], memory[116,20], memory[117,20], memory[118,20], memory[119,20], memory[120,20], memory[1,21], memory[2,21], memory[3,21], memory[4,21], memory[5,21], memory[6,21], memory[7,21], memory[8,21], memory[9,21], memory[10,21], memory[11,21], memory[12,21], memory[13,21], memory[14,21], memory[15,21], memory[16,21], memory[17,21], memory[18,21], memory[19,21], memory[20,21], memory[21,21], memory[22,21], memory[23,21], memory[24,21], memory[25,21], memory[26,21], memory[27,21], memory[28,21], memory[29,21], memory[30,21], memory[31,21], memory[32,21], memory[33,21], memory[34,21], memory[35,21], memory[36,21], memory[37,21], memory[38,21], memory[39,21], memory[40,21], memory[41,21], memory[42,21], memory[43,21], memory[44,21], memory[45,21], memory[46,21], memory[47,21], memory[48,21], memory[49,21], memory[50,21], memory[51,21], memory[52,21], memory[53,21], memory[54,21], memory[55,21], memory[56,21], memory[57,21], memory[58,21], memory[59,21], memory[60,21], memory[61,21], memory[62,21], memory[63,21], memory[64,21], memory[65,21], memory[66,21], memory[67,21], memory[68,21], memory[69,21], memory[70,21], memory[71,21], memory[72,21], memory[73,21], memory[74,21], memory[75,21], memory[76,21], memory[77,21], memory[78,21], memory[79,21], memory[80,21], memory[81,21], memory[82,21], memory[83,21], memory[84,21], memory[85,21], memory[86,21], memory[87,21], memory[88,21], memory[89,21], memory[90,21], memory[91,21], memory[92,21], memory[93,21], memory[94,21], memory[95,21], memory[96,21], memory[97,21], memory[98,21], memory[99,21], memory[100,21], memory[101,21], memory[102,21], memory[103,21], memory[104,21], memory[105,21], memory[106,21], memory[107,21], memory[108,21], memory[109,21], memory[110,21], memory[111,21], memory[112,21], memory[113,21], memory[114,21], memory[115,21], memory[116,21], memory[117,21], memory[118,21], memory[119,21], memory[120,21], memory[1,22], memory[2,22], memory[3,22], memory[4,22], memory[5,22], memory[6,22], memory[7,22], memory[8,22], memory[9,22], memory[10,22], memory[11,22], memory[12,22], memory[13,22], memory[14,22], memory[15,22], memory[16,22], memory[17,22], memory[18,22], memory[19,22], memory[20,22], memory[21,22], memory[22,22], memory[23,22], memory[24,22], memory[25,22], memory[26,22], memory[27,22], memory[28,22], memory[29,22], memory[30,22], memory[31,22], memory[32,22], memory[33,22], memory[34,22], memory[35,22], memory[36,22], memory[37,22], memory[38,22], memory[39,22], memory[40,22], memory[41,22], memory[42,22], memory[43,22], memory[44,22], memory[45,22], memory[46,22], memory[47,22], memory[48,22], memory[49,22], memory[50,22], memory[51,22], memory[52,22], memory[53,22], memory[54,22], memory[55,22], memory[56,22], memory[57,22], memory[58,22], memory[59,22], memory[60,22], memory[61,22], memory[62,22], memory[63,22], memory[64,22], memory[65,22], memory[66,22], memory[67,22], memory[68,22], memory[69,22], memory[70,22], memory[71,22], memory[72,22], memory[73,22], memory[74,22], memory[75,22], memory[76,22], memory[77,22], memory[78,22], memory[79,22], memory[80,22], memory[81,22], memory[82,22], memory[83,22], memory[84,22], memory[85,22], memory[86,22], memory[87,22], memory[88,22], memory[89,22], memory[90,22], memory[91,22], memory[92,22], memory[93,22], memory[94,22], memory[95,22], memory[96,22], memory[97,22], memory[98,22], memory[99,22], memory[100,22], memory[101,22], memory[102,22], memory[103,22], memory[104,22], memory[105,22], memory[106,22], memory[107,22], memory[108,22], memory[109,22], memory[110,22], memory[111,22], memory[112,22], memory[113,22], memory[114,22], memory[115,22], memory[116,22], memory[117,22], memory[118,22], memory[119,22], memory[120,22], memory[1,23], memory[2,23], memory[3,23], memory[4,23], memory[5,23], memory[6,23], memory[7,23], memory[8,23], memory[9,23], memory[10,23], memory[11,23], memory[12,23], memory[13,23], memory[14,23], memory[15,23], memory[16,23], memory[17,23], memory[18,23], memory[19,23], memory[20,23], memory[21,23], memory[22,23], memory[23,23], memory[24,23], memory[25,23], memory[26,23], memory[27,23], memory[28,23], memory[29,23], memory[30,23], memory[31,23], memory[32,23], memory[33,23], memory[34,23], memory[35,23], memory[36,23], memory[37,23], memory[38,23], memory[39,23], memory[40,23], memory[41,23], memory[42,23], memory[43,23], memory[44,23], memory[45,23], memory[46,23], memory[47,23], memory[48,23], memory[49,23], memory[50,23], memory[51,23], memory[52,23], memory[53,23], memory[54,23], memory[55,23], memory[56,23], memory[57,23], memory[58,23], memory[59,23], memory[60,23], memory[61,23], memory[62,23], memory[63,23], memory[64,23], memory[65,23], memory[66,23], memory[67,23], memory[68,23], memory[69,23], memory[70,23], memory[71,23], memory[72,23], memory[73,23], memory[74,23], memory[75,23], memory[76,23], memory[77,23], memory[78,23], memory[79,23], memory[80,23], memory[81,23], memory[82,23], memory[83,23], memory[84,23], memory[85,23], memory[86,23], memory[87,23], memory[88,23], memory[89,23], memory[90,23], memory[91,23], memory[92,23], memory[93,23], memory[94,23], memory[95,23], memory[96,23], memory[97,23], memory[98,23], memory[99,23], memory[100,23], memory[101,23], memory[102,23], memory[103,23], memory[104,23], memory[105,23], memory[106,23], memory[107,23], memory[108,23], memory[109,23], memory[110,23], memory[111,23], memory[112,23], memory[113,23], memory[114,23], memory[115,23], memory[116,23], memory[117,23], memory[118,23], memory[119,23], memory[120,23], memory[1,24], memory[2,24], memory[3,24], memory[4,24], memory[5,24], memory[6,24], memory[7,24], memory[8,24], memory[9,24], memory[10,24], memory[11,24], memory[12,24], memory[13,24], memory[14,24], memory[15,24], memory[16,24], memory[17,24], memory[18,24], memory[19,24], memory[20,24], memory[21,24], memory[22,24], memory[23,24], memory[24,24], memory[25,24], memory[26,24], memory[27,24], memory[28,24], memory[29,24], memory[30,24], memory[31,24], memory[32,24], memory[33,24], memory[34,24], memory[35,24], memory[36,24], memory[37,24], memory[38,24], memory[39,24], memory[40,24], memory[41,24], memory[42,24], memory[43,24], memory[44,24], memory[45,24], memory[46,24], memory[47,24], memory[48,24], memory[49,24], memory[50,24], memory[51,24], memory[52,24], memory[53,24], memory[54,24], memory[55,24], memory[56,24], memory[57,24], memory[58,24], memory[59,24], memory[60,24], memory[61,24], memory[62,24], memory[63,24], memory[64,24], memory[65,24], memory[66,24], memory[67,24], memory[68,24], memory[69,24], memory[70,24], memory[71,24], memory[72,24], memory[73,24], memory[74,24], memory[75,24], memory[76,24], memory[77,24], memory[78,24], memory[79,24], memory[80,24], memory[81,24], memory[82,24], memory[83,24], memory[84,24], memory[85,24], memory[86,24], memory[87,24], memory[88,24], memory[89,24], memory[90,24], memory[91,24], memory[92,24], memory[93,24], memory[94,24], memory[95,24], memory[96,24], memory[97,24], memory[98,24], memory[99,24], memory[100,24], memory[101,24], memory[102,24], memory[103,24], memory[104,24], memory[105,24], memory[106,24], memory[107,24], memory[108,24], memory[109,24], memory[110,24], memory[111,24], memory[112,24], memory[113,24], memory[114,24], memory[115,24], memory[116,24], memory[117,24], memory[118,24], memory[119,24], memory[120,24], memory[1,25], memory[2,25], memory[3,25], memory[4,25], memory[5,25], memory[6,25], memory[7,25], memory[8,25], memory[9,25], memory[10,25], memory[11,25], memory[12,25], memory[13,25], memory[14,25], memory[15,25], memory[16,25], memory[17,25], memory[18,25], memory[19,25], memory[20,25], memory[21,25], memory[22,25], memory[23,25], memory[24,25], memory[25,25], memory[26,25], memory[27,25], memory[28,25], memory[29,25], memory[30,25], memory[31,25], memory[32,25], memory[33,25], memory[34,25], memory[35,25], memory[36,25], memory[37,25], memory[38,25], memory[39,25], memory[40,25], memory[41,25], memory[42,25], memory[43,25], memory[44,25], memory[45,25], memory[46,25], memory[47,25], memory[48,25], memory[49,25], memory[50,25], memory[51,25], memory[52,25], memory[53,25], memory[54,25], memory[55,25], memory[56,25], memory[57,25], memory[58,25], memory[59,25], memory[60,25], memory[61,25], memory[62,25], memory[63,25], memory[64,25], memory[65,25], memory[66,25], memory[67,25], memory[68,25], memory[69,25], memory[70,25], memory[71,25], memory[72,25], memory[73,25], memory[74,25], memory[75,25], memory[76,25], memory[77,25], memory[78,25], memory[79,25], memory[80,25], memory[81,25], memory[82,25], memory[83,25], memory[84,25], memory[85,25], memory[86,25], memory[87,25], memory[88,25], memory[89,25], memory[90,25], memory[91,25], memory[92,25], memory[93,25], memory[94,25], memory[95,25], memory[96,25], memory[97,25], memory[98,25], memory[99,25], memory[100,25], memory[101,25], memory[102,25], memory[103,25], memory[104,25], memory[105,25], memory[106,25], memory[107,25], memory[108,25], memory[109,25], memory[110,25], memory[111,25], memory[112,25], memory[113,25], memory[114,25], memory[115,25], memory[116,25], memory[117,25], memory[118,25], memory[119,25], memory[120,25], memory[1,26], memory[2,26], memory[3,26], memory[4,26], memory[5,26], memory[6,26], memory[7,26], memory[8,26], memory[9,26], memory[10,26], memory[11,26], memory[12,26], memory[13,26], memory[14,26], memory[15,26], memory[16,26], memory[17,26], memory[18,26], memory[19,26], memory[20,26], memory[21,26], memory[22,26], memory[23,26], memory[24,26], memory[25,26], memory[26,26], memory[27,26], memory[28,26], memory[29,26], memory[30,26], memory[31,26], memory[32,26], memory[33,26], memory[34,26], memory[35,26], memory[36,26], memory[37,26], memory[38,26], memory[39,26], memory[40,26], memory[41,26], memory[42,26], memory[43,26], memory[44,26], memory[45,26], memory[46,26], memory[47,26], memory[48,26], memory[49,26], memory[50,26], memory[51,26], memory[52,26], memory[53,26], memory[54,26], memory[55,26], memory[56,26], memory[57,26], memory[58,26], memory[59,26], memory[60,26], memory[61,26], memory[62,26], memory[63,26], memory[64,26], memory[65,26], memory[66,26], memory[67,26], memory[68,26], memory[69,26], memory[70,26], memory[71,26], memory[72,26], memory[73,26], memory[74,26], memory[75,26], memory[76,26], memory[77,26], memory[78,26], memory[79,26], memory[80,26], memory[81,26], memory[82,26], memory[83,26], memory[84,26], memory[85,26], memory[86,26], memory[87,26], memory[88,26], memory[89,26], memory[90,26], memory[91,26], memory[92,26], memory[93,26], memory[94,26], memory[95,26], memory[96,26], memory[97,26], memory[98,26], memory[99,26], memory[100,26], memory[101,26], memory[102,26], memory[103,26], memory[104,26], memory[105,26], memory[106,26], memory[107,26], memory[108,26], memory[109,26], memory[110,26], memory[111,26], memory[112,26], memory[113,26], memory[114,26], memory[115,26], memory[116,26], memory[117,26], memory[118,26], memory[119,26], memory[120,26], memory[1,27], memory[2,27], memory[3,27], memory[4,27], memory[5,27], memory[6,27], memory[7,27], memory[8,27], memory[9,27], memory[10,27], memory[11,27], memory[12,27], memory[13,27], memory[14,27], memory[15,27], memory[16,27], memory[17,27], memory[18,27], memory[19,27], memory[20,27], memory[21,27], memory[22,27], memory[23,27], memory[24,27], memory[25,27], memory[26,27], memory[27,27], memory[28,27], memory[29,27], memory[30,27], memory[31,27], memory[32,27], memory[33,27], memory[34,27], memory[35,27], memory[36,27], memory[37,27], memory[38,27], memory[39,27], memory[40,27], memory[41,27], memory[42,27], memory[43,27], memory[44,27], memory[45,27], memory[46,27], memory[47,27], memory[48,27], memory[49,27], memory[50,27], memory[51,27], memory[52,27], memory[53,27], memory[54,27], memory[55,27], memory[56,27], memory[57,27], memory[58,27], memory[59,27], memory[60,27], memory[61,27], memory[62,27], memory[63,27], memory[64,27], memory[65,27], memory[66,27], memory[67,27], memory[68,27], memory[69,27], memory[70,27], memory[71,27], memory[72,27], memory[73,27], memory[74,27], memory[75,27], memory[76,27], memory[77,27], memory[78,27], memory[79,27], memory[80,27], memory[81,27], memory[82,27], memory[83,27], memory[84,27], memory[85,27], memory[86,27], memory[87,27], memory[88,27], memory[89,27], memory[90,27], memory[91,27], memory[92,27], memory[93,27], memory[94,27], memory[95,27], memory[96,27], memory[97,27], memory[98,27], memory[99,27], memory[100,27], memory[101,27], memory[102,27], memory[103,27], memory[104,27], memory[105,27], memory[106,27], memory[107,27], memory[108,27], memory[109,27], memory[110,27], memory[111,27], memory[112,27], memory[113,27], memory[114,27], memory[115,27], memory[116,27], memory[117,27], memory[118,27], memory[119,27], memory[120,27], memory[1,28], memory[2,28], memory[3,28], memory[4,28], memory[5,28], memory[6,28], memory[7,28], memory[8,28], memory[9,28], memory[10,28], memory[11,28], memory[12,28], memory[13,28], memory[14,28], memory[15,28], memory[16,28], memory[17,28], memory[18,28], memory[19,28], memory[20,28], memory[21,28], memory[22,28], memory[23,28], memory[24,28], memory[25,28], memory[26,28], memory[27,28], memory[28,28], memory[29,28], memory[30,28], memory[31,28], memory[32,28], memory[33,28], memory[34,28], memory[35,28], memory[36,28], memory[37,28], memory[38,28], memory[39,28], memory[40,28], memory[41,28], memory[42,28], memory[43,28], memory[44,28], memory[45,28], memory[46,28], memory[47,28], memory[48,28], memory[49,28], memory[50,28], memory[51,28], memory[52,28], memory[53,28], memory[54,28], memory[55,28], memory[56,28], memory[57,28], memory[58,28], memory[59,28], memory[60,28], memory[61,28], memory[62,28], memory[63,28], memory[64,28], memory[65,28], memory[66,28], memory[67,28], memory[68,28], memory[69,28], memory[70,28], memory[71,28], memory[72,28], memory[73,28], memory[74,28], memory[75,28], memory[76,28], memory[77,28], memory[78,28], memory[79,28], memory[80,28], memory[81,28], memory[82,28], memory[83,28], memory[84,28], memory[85,28], memory[86,28], memory[87,28], memory[88,28], memory[89,28], memory[90,28], memory[91,28], memory[92,28], memory[93,28], memory[94,28], memory[95,28], memory[96,28], memory[97,28], memory[98,28], memory[99,28], memory[100,28], memory[101,28], memory[102,28], memory[103,28], memory[104,28], memory[105,28], memory[106,28], memory[107,28], memory[108,28], memory[109,28], memory[110,28], memory[111,28], memory[112,28], memory[113,28], memory[114,28], memory[115,28], memory[116,28], memory[117,28], memory[118,28], memory[119,28], memory[120,28], memory[1,29], memory[2,29], memory[3,29], memory[4,29], memory[5,29], memory[6,29], memory[7,29], memory[8,29], memory[9,29], memory[10,29], memory[11,29], memory[12,29], memory[13,29], memory[14,29], memory[15,29], memory[16,29], memory[17,29], memory[18,29], memory[19,29], memory[20,29], memory[21,29], memory[22,29], memory[23,29], memory[24,29], memory[25,29], memory[26,29], memory[27,29], memory[28,29], memory[29,29], memory[30,29], memory[31,29], memory[32,29], memory[33,29], memory[34,29], memory[35,29], memory[36,29], memory[37,29], memory[38,29], memory[39,29], memory[40,29], memory[41,29], memory[42,29], memory[43,29], memory[44,29], memory[45,29], memory[46,29], memory[47,29], memory[48,29], memory[49,29], memory[50,29], memory[51,29], memory[52,29], memory[53,29], memory[54,29], memory[55,29], memory[56,29], memory[57,29], memory[58,29], memory[59,29], memory[60,29], memory[61,29], memory[62,29], memory[63,29], memory[64,29], memory[65,29], memory[66,29], memory[67,29], memory[68,29], memory[69,29], memory[70,29], memory[71,29], memory[72,29], memory[73,29], memory[74,29], memory[75,29], memory[76,29], memory[77,29], memory[78,29], memory[79,29], memory[80,29], memory[81,29], memory[82,29], memory[83,29], memory[84,29], memory[85,29], memory[86,29], memory[87,29], memory[88,29], memory[89,29], memory[90,29], memory[91,29], memory[92,29], memory[93,29], memory[94,29], memory[95,29], memory[96,29], memory[97,29], memory[98,29], memory[99,29], memory[100,29], memory[101,29], memory[102,29], memory[103,29], memory[104,29], memory[105,29], memory[106,29], memory[107,29], memory[108,29], memory[109,29], memory[110,29], memory[111,29], memory[112,29], memory[113,29], memory[114,29], memory[115,29], memory[116,29], memory[117,29], memory[118,29], memory[119,29], memory[120,29], memory[1,30], memory[2,30], memory[3,30], memory[4,30], memory[5,30], memory[6,30], memory[7,30], memory[8,30], memory[9,30], memory[10,30], memory[11,30], memory[12,30], memory[13,30], memory[14,30], memory[15,30], memory[16,30], memory[17,30], memory[18,30], memory[19,30], memory[20,30], memory[21,30], memory[22,30], memory[23,30], memory[24,30], memory[25,30], memory[26,30], memory[27,30], memory[28,30], memory[29,30], memory[30,30], memory[31,30], memory[32,30], memory[33,30], memory[34,30], memory[35,30], memory[36,30], memory[37,30], memory[38,30], memory[39,30], memory[40,30], memory[41,30], memory[42,30], memory[43,30], memory[44,30], memory[45,30], memory[46,30], memory[47,30], memory[48,30], memory[49,30], memory[50,30], memory[51,30], memory[52,30], memory[53,30], memory[54,30], memory[55,30], memory[56,30], memory[57,30], memory[58,30], memory[59,30], memory[60,30], memory[61,30], memory[62,30], memory[63,30], memory[64,30], memory[65,30], memory[66,30], memory[67,30], memory[68,30], memory[69,30], memory[70,30], memory[71,30], memory[72,30], memory[73,30], memory[74,30], memory[75,30], memory[76,30], memory[77,30], memory[78,30], memory[79,30], memory[80,30], memory[81,30], memory[82,30], memory[83,30], memory[84,30], memory[85,30], memory[86,30], memory[87,30], memory[88,30], memory[89,30], memory[90,30], memory[91,30], memory[92,30], memory[93,30], memory[94,30], memory[95,30], memory[96,30], memory[97,30], memory[98,30], memory[99,30], memory[100,30], memory[101,30], memory[102,30], memory[103,30], memory[104,30], memory[105,30], memory[106,30], memory[107,30], memory[108,30], memory[109,30], memory[110,30], memory[111,30], memory[112,30], memory[113,30], memory[114,30], memory[115,30], memory[116,30], memory[117,30], memory[118,30], memory[119,30], memory[120,30], memory[1,31], memory[2,31], memory[3,31], memory[4,31], memory[5,31], memory[6,31], memory[7,31], memory[8,31], memory[9,31], memory[10,31], memory[11,31], memory[12,31], memory[13,31], memory[14,31], memory[15,31], memory[16,31], memory[17,31], memory[18,31], memory[19,31], memory[20,31], memory[21,31], memory[22,31], memory[23,31], memory[24,31], memory[25,31], memory[26,31], memory[27,31], memory[28,31], memory[29,31], memory[30,31], memory[31,31], memory[32,31], memory[33,31], memory[34,31], memory[35,31], memory[36,31], memory[37,31], memory[38,31], memory[39,31], memory[40,31], memory[41,31], memory[42,31], memory[43,31], memory[44,31], memory[45,31], memory[46,31], memory[47,31], memory[48,31], memory[49,31], memory[50,31], memory[51,31], memory[52,31], memory[53,31], memory[54,31], memory[55,31], memory[56,31], memory[57,31], memory[58,31], memory[59,31], memory[60,31], memory[61,31], memory[62,31], memory[63,31], memory[64,31], memory[65,31], memory[66,31], memory[67,31], memory[68,31], memory[69,31], memory[70,31], memory[71,31], memory[72,31], memory[73,31], memory[74,31], memory[75,31], memory[76,31], memory[77,31], memory[78,31], memory[79,31], memory[80,31], memory[81,31], memory[82,31], memory[83,31], memory[84,31], memory[85,31], memory[86,31], memory[87,31], memory[88,31], memory[89,31], memory[90,31], memory[91,31], memory[92,31], memory[93,31], memory[94,31], memory[95,31], memory[96,31], memory[97,31], memory[98,31], memory[99,31], memory[100,31], memory[101,31], memory[102,31], memory[103,31], memory[104,31], memory[105,31], memory[106,31], memory[107,31], memory[108,31], memory[109,31], memory[110,31], memory[111,31], memory[112,31], memory[113,31], memory[114,31], memory[115,31], memory[116,31], memory[117,31], memory[118,31], memory[119,31], memory[120,31], memory[1,32], memory[2,32], memory[3,32], memory[4,32], memory[5,32], memory[6,32], memory[7,32], memory[8,32], memory[9,32], memory[10,32], memory[11,32], memory[12,32], memory[13,32], memory[14,32], memory[15,32], memory[16,32], memory[17,32], memory[18,32], memory[19,32], memory[20,32], memory[21,32], memory[22,32], memory[23,32], memory[24,32], memory[25,32], memory[26,32], memory[27,32], memory[28,32], memory[29,32], memory[30,32], memory[31,32], memory[32,32], memory[33,32], memory[34,32], memory[35,32], memory[36,32], memory[37,32], memory[38,32], memory[39,32], memory[40,32], memory[41,32], memory[42,32], memory[43,32], memory[44,32], memory[45,32], memory[46,32], memory[47,32], memory[48,32], memory[49,32], memory[50,32], memory[51,32], memory[52,32], memory[53,32], memory[54,32], memory[55,32], memory[56,32], memory[57,32], memory[58,32], memory[59,32], memory[60,32], memory[61,32], memory[62,32], memory[63,32], memory[64,32], memory[65,32], memory[66,32], memory[67,32], memory[68,32], memory[69,32], memory[70,32], memory[71,32], memory[72,32], memory[73,32], memory[74,32], memory[75,32], memory[76,32], memory[77,32], memory[78,32], memory[79,32], memory[80,32], memory[81,32], memory[82,32], memory[83,32], memory[84,32], memory[85,32], memory[86,32], memory[87,32], memory[88,32], memory[89,32], memory[90,32], memory[91,32], memory[92,32], memory[93,32], memory[94,32], memory[95,32], memory[96,32], memory[97,32], memory[98,32], memory[99,32], memory[100,32], memory[101,32], memory[102,32], memory[103,32], memory[104,32], memory[105,32], memory[106,32], memory[107,32], memory[108,32], memory[109,32], memory[110,32], memory[111,32], memory[112,32], memory[113,32], memory[114,32], memory[115,32], memory[116,32], memory[117,32], memory[118,32], memory[119,32], memory[120,32], memory[1,33], memory[2,33], memory[3,33], memory[4,33], memory[5,33], memory[6,33], memory[7,33], memory[8,33], memory[9,33], memory[10,33], memory[11,33], memory[12,33], memory[13,33], memory[14,33], memory[15,33], memory[16,33], memory[17,33], memory[18,33], memory[19,33], memory[20,33], memory[21,33], memory[22,33], memory[23,33], memory[24,33], memory[25,33], memory[26,33], memory[27,33], memory[28,33], memory[29,33], memory[30,33], memory[31,33], memory[32,33], memory[33,33], memory[34,33], memory[35,33], memory[36,33], memory[37,33], memory[38,33], memory[39,33], memory[40,33], memory[41,33], memory[42,33], memory[43,33], memory[44,33], memory[45,33], memory[46,33], memory[47,33], memory[48,33], memory[49,33], memory[50,33], memory[51,33], memory[52,33], memory[53,33], memory[54,33], memory[55,33], memory[56,33], memory[57,33], memory[58,33], memory[59,33], memory[60,33], memory[61,33], memory[62,33], memory[63,33], memory[64,33], memory[65,33], memory[66,33], memory[67,33], memory[68,33], memory[69,33], memory[70,33], memory[71,33], memory[72,33], memory[73,33], memory[74,33], memory[75,33], memory[76,33], memory[77,33], memory[78,33], memory[79,33], memory[80,33], memory[81,33], memory[82,33], memory[83,33], memory[84,33], memory[85,33], memory[86,33], memory[87,33], memory[88,33], memory[89,33], memory[90,33], memory[91,33], memory[92,33], memory[93,33], memory[94,33], memory[95,33], memory[96,33], memory[97,33], memory[98,33], memory[99,33], memory[100,33], memory[101,33], memory[102,33], memory[103,33], memory[104,33], memory[105,33], memory[106,33], memory[107,33], memory[108,33], memory[109,33], memory[110,33], memory[111,33], memory[112,33], memory[113,33], memory[114,33], memory[115,33], memory[116,33], memory[117,33], memory[118,33], memory[119,33], memory[120,33], memory[1,34], memory[2,34], memory[3,34], memory[4,34], memory[5,34], memory[6,34], memory[7,34], memory[8,34], memory[9,34], memory[10,34], memory[11,34], memory[12,34], memory[13,34], memory[14,34], memory[15,34], memory[16,34], memory[17,34], memory[18,34], memory[19,34], memory[20,34], memory[21,34], memory[22,34], memory[23,34], memory[24,34], memory[25,34], memory[26,34], memory[27,34], memory[28,34], memory[29,34], memory[30,34], memory[31,34], memory[32,34], memory[33,34], memory[34,34], memory[35,34], memory[36,34], memory[37,34], memory[38,34], memory[39,34], memory[40,34], memory[41,34], memory[42,34], memory[43,34], memory[44,34], memory[45,34], memory[46,34], memory[47,34], memory[48,34], memory[49,34], memory[50,34], memory[51,34], memory[52,34], memory[53,34], memory[54,34], memory[55,34], memory[56,34], memory[57,34], memory[58,34], memory[59,34], memory[60,34], memory[61,34], memory[62,34], memory[63,34], memory[64,34], memory[65,34], memory[66,34], memory[67,34], memory[68,34], memory[69,34], memory[70,34], memory[71,34], memory[72,34], memory[73,34], memory[74,34], memory[75,34], memory[76,34], memory[77,34], memory[78,34], memory[79,34], memory[80,34], memory[81,34], memory[82,34], memory[83,34], memory[84,34], memory[85,34], memory[86,34], memory[87,34], memory[88,34], memory[89,34], memory[90,34], memory[91,34], memory[92,34], memory[93,34], memory[94,34], memory[95,34], memory[96,34], memory[97,34], memory[98,34], memory[99,34], memory[100,34], memory[101,34], memory[102,34], memory[103,34], memory[104,34], memory[105,34], memory[106,34], memory[107,34], memory[108,34], memory[109,34], memory[110,34], memory[111,34], memory[112,34], memory[113,34], memory[114,34], memory[115,34], memory[116,34], memory[117,34], memory[118,34], memory[119,34], memory[120,34], memory[1,35], memory[2,35], memory[3,35], memory[4,35], memory[5,35], memory[6,35], memory[7,35], memory[8,35], memory[9,35], memory[10,35], memory[11,35], memory[12,35], memory[13,35], memory[14,35], memory[15,35], memory[16,35], memory[17,35], memory[18,35], memory[19,35], memory[20,35], memory[21,35], memory[22,35], memory[23,35], memory[24,35], memory[25,35], memory[26,35], memory[27,35], memory[28,35], memory[29,35], memory[30,35], memory[31,35], memory[32,35], memory[33,35], memory[34,35], memory[35,35], memory[36,35], memory[37,35], memory[38,35], memory[39,35], memory[40,35], memory[41,35], memory[42,35], memory[43,35], memory[44,35], memory[45,35], memory[46,35], memory[47,35], memory[48,35], memory[49,35], memory[50,35], memory[51,35], memory[52,35], memory[53,35], memory[54,35], memory[55,35], memory[56,35], memory[57,35], memory[58,35], memory[59,35], memory[60,35], memory[61,35], memory[62,35], memory[63,35], memory[64,35], memory[65,35], memory[66,35], memory[67,35], memory[68,35], memory[69,35], memory[70,35], memory[71,35], memory[72,35], memory[73,35], memory[74,35], memory[75,35], memory[76,35], memory[77,35], memory[78,35], memory[79,35], memory[80,35], memory[81,35], memory[82,35], memory[83,35], memory[84,35], memory[85,35], memory[86,35], memory[87,35], memory[88,35], memory[89,35], memory[90,35], memory[91,35], memory[92,35], memory[93,35], memory[94,35], memory[95,35], memory[96,35], memory[97,35], memory[98,35], memory[99,35], memory[100,35], memory[101,35], memory[102,35], memory[103,35], memory[104,35], memory[105,35], memory[106,35], memory[107,35], memory[108,35], memory[109,35], memory[110,35], memory[111,35], memory[112,35], memory[113,35], memory[114,35], memory[115,35], memory[116,35], memory[117,35], memory[118,35], memory[119,35], memory[120,35], memory[1,36], memory[2,36], memory[3,36], memory[4,36], memory[5,36], memory[6,36], memory[7,36], memory[8,36], memory[9,36], memory[10,36], memory[11,36], memory[12,36], memory[13,36], memory[14,36], memory[15,36], memory[16,36], memory[17,36], memory[18,36], memory[19,36], memory[20,36], memory[21,36], memory[22,36], memory[23,36], memory[24,36], memory[25,36], memory[26,36], memory[27,36], memory[28,36], memory[29,36], memory[30,36], memory[31,36], memory[32,36], memory[33,36], memory[34,36], memory[35,36], memory[36,36], memory[37,36], memory[38,36], memory[39,36], memory[40,36], memory[41,36], memory[42,36], memory[43,36], memory[44,36], memory[45,36], memory[46,36], memory[47,36], memory[48,36], memory[49,36], memory[50,36], memory[51,36], memory[52,36], memory[53,36], memory[54,36], memory[55,36], memory[56,36], memory[57,36], memory[58,36], memory[59,36], memory[60,36], memory[61,36], memory[62,36], memory[63,36], memory[64,36], memory[65,36], memory[66,36], memory[67,36], memory[68,36], memory[69,36], memory[70,36], memory[71,36], memory[72,36], memory[73,36], memory[74,36], memory[75,36], memory[76,36], memory[77,36], memory[78,36], memory[79,36], memory[80,36], memory[81,36], memory[82,36], memory[83,36], memory[84,36], memory[85,36], memory[86,36], memory[87,36], memory[88,36], memory[89,36], memory[90,36], memory[91,36], memory[92,36], memory[93,36], memory[94,36], memory[95,36], memory[96,36], memory[97,36], memory[98,36], memory[99,36], memory[100,36], memory[101,36], memory[102,36], memory[103,36], memory[104,36], memory[105,36], memory[106,36], memory[107,36], memory[108,36], memory[109,36], memory[110,36], memory[111,36], memory[112,36], memory[113,36], memory[114,36], memory[115,36], memory[116,36], memory[117,36], memory[118,36], memory[119,36], memory[120,36], memory[1,37], memory[2,37], memory[3,37], memory[4,37], memory[5,37], memory[6,37], memory[7,37], memory[8,37], memory[9,37], memory[10,37], memory[11,37], memory[12,37], memory[13,37], memory[14,37], memory[15,37], memory[16,37], memory[17,37], memory[18,37], memory[19,37], memory[20,37], memory[21,37], memory[22,37], memory[23,37], memory[24,37], memory[25,37], memory[26,37], memory[27,37], memory[28,37], memory[29,37], memory[30,37], memory[31,37], memory[32,37], memory[33,37], memory[34,37], memory[35,37], memory[36,37], memory[37,37], memory[38,37], memory[39,37], memory[40,37], memory[41,37], memory[42,37], memory[43,37], memory[44,37], memory[45,37], memory[46,37], memory[47,37], memory[48,37], memory[49,37], memory[50,37], memory[51,37], memory[52,37], memory[53,37], memory[54,37], memory[55,37], memory[56,37], memory[57,37], memory[58,37], memory[59,37], memory[60,37], memory[61,37], memory[62,37], memory[63,37], memory[64,37], memory[65,37], memory[66,37], memory[67,37], memory[68,37], memory[69,37], memory[70,37], memory[71,37], memory[72,37], memory[73,37], memory[74,37], memory[75,37], memory[76,37], memory[77,37], memory[78,37], memory[79,37], memory[80,37], memory[81,37], memory[82,37], memory[83,37], memory[84,37], memory[85,37], memory[86,37], memory[87,37], memory[88,37], memory[89,37], memory[90,37], memory[91,37], memory[92,37], memory[93,37], memory[94,37], memory[95,37], memory[96,37], memory[97,37], memory[98,37], memory[99,37], memory[100,37], memory[101,37], memory[102,37], memory[103,37], memory[104,37], memory[105,37], memory[106,37], memory[107,37], memory[108,37], memory[109,37], memory[110,37], memory[111,37], memory[112,37], memory[113,37], memory[114,37], memory[115,37], memory[116,37], memory[117,37], memory[118,37], memory[119,37], memory[120,37], memory[1,38], memory[2,38], memory[3,38], memory[4,38], memory[5,38], memory[6,38], memory[7,38], memory[8,38], memory[9,38], memory[10,38], memory[11,38], memory[12,38], memory[13,38], memory[14,38], memory[15,38], memory[16,38], memory[17,38], memory[18,38], memory[19,38], memory[20,38], memory[21,38], memory[22,38], memory[23,38], memory[24,38], memory[25,38], memory[26,38], memory[27,38], memory[28,38], memory[29,38], memory[30,38], memory[31,38], memory[32,38], memory[33,38], memory[34,38], memory[35,38], memory[36,38], memory[37,38], memory[38,38], memory[39,38], memory[40,38], memory[41,38], memory[42,38], memory[43,38], memory[44,38], memory[45,38], memory[46,38], memory[47,38], memory[48,38], memory[49,38], memory[50,38], memory[51,38], memory[52,38], memory[53,38], memory[54,38], memory[55,38], memory[56,38], memory[57,38], memory[58,38], memory[59,38], memory[60,38], memory[61,38], memory[62,38], memory[63,38], memory[64,38], memory[65,38], memory[66,38], memory[67,38], memory[68,38], memory[69,38], memory[70,38], memory[71,38], memory[72,38], memory[73,38], memory[74,38], memory[75,38], memory[76,38], memory[77,38], memory[78,38], memory[79,38], memory[80,38], memory[81,38], memory[82,38], memory[83,38], memory[84,38], memory[85,38], memory[86,38], memory[87,38], memory[88,38], memory[89,38], memory[90,38], memory[91,38], memory[92,38], memory[93,38], memory[94,38], memory[95,38], memory[96,38], memory[97,38], memory[98,38], memory[99,38], memory[100,38], memory[101,38], memory[102,38], memory[103,38], memory[104,38], memory[105,38], memory[106,38], memory[107,38], memory[108,38], memory[109,38], memory[110,38], memory[111,38], memory[112,38], memory[113,38], memory[114,38], memory[115,38], memory[116,38], memory[117,38], memory[118,38], memory[119,38], memory[120,38], memory[1,39], memory[2,39], memory[3,39], memory[4,39], memory[5,39], memory[6,39], memory[7,39], memory[8,39], memory[9,39], memory[10,39], memory[11,39], memory[12,39], memory[13,39], memory[14,39], memory[15,39], memory[16,39], memory[17,39], memory[18,39], memory[19,39], memory[20,39], memory[21,39], memory[22,39], memory[23,39], memory[24,39], memory[25,39], memory[26,39], memory[27,39], memory[28,39], memory[29,39], memory[30,39], memory[31,39], memory[32,39], memory[33,39], memory[34,39], memory[35,39], memory[36,39], memory[37,39], memory[38,39], memory[39,39], memory[40,39], memory[41,39], memory[42,39], memory[43,39], memory[44,39], memory[45,39], memory[46,39], memory[47,39], memory[48,39], memory[49,39], memory[50,39], memory[51,39], memory[52,39], memory[53,39], memory[54,39], memory[55,39], memory[56,39], memory[57,39], memory[58,39], memory[59,39], memory[60,39], memory[61,39], memory[62,39], memory[63,39], memory[64,39], memory[65,39], memory[66,39], memory[67,39], memory[68,39], memory[69,39], memory[70,39], memory[71,39], memory[72,39], memory[73,39], memory[74,39], memory[75,39], memory[76,39], memory[77,39], memory[78,39], memory[79,39], memory[80,39], memory[81,39], memory[82,39], memory[83,39], memory[84,39], memory[85,39], memory[86,39], memory[87,39], memory[88,39], memory[89,39], memory[90,39], memory[91,39], memory[92,39], memory[93,39], memory[94,39], memory[95,39], memory[96,39], memory[97,39], memory[98,39], memory[99,39], memory[100,39], memory[101,39], memory[102,39], memory[103,39], memory[104,39], memory[105,39], memory[106,39], memory[107,39], memory[108,39], memory[109,39], memory[110,39], memory[111,39], memory[112,39], memory[113,39], memory[114,39], memory[115,39], memory[116,39], memory[117,39], memory[118,39], memory[119,39], memory[120,39], memory[1,40], memory[2,40], memory[3,40], memory[4,40], memory[5,40], memory[6,40], memory[7,40], memory[8,40], memory[9,40], memory[10,40], memory[11,40], memory[12,40], memory[13,40], memory[14,40], memory[15,40], memory[16,40], memory[17,40], memory[18,40], memory[19,40], memory[20,40], memory[21,40], memory[22,40], memory[23,40], memory[24,40], memory[25,40], memory[26,40], memory[27,40], memory[28,40], memory[29,40], memory[30,40], memory[31,40], memory[32,40], memory[33,40], memory[34,40], memory[35,40], memory[36,40], memory[37,40], memory[38,40], memory[39,40], memory[40,40], memory[41,40], memory[42,40], memory[43,40], memory[44,40], memory[45,40], memory[46,40], memory[47,40], memory[48,40], memory[49,40], memory[50,40], memory[51,40], memory[52,40], memory[53,40], memory[54,40], memory[55,40], memory[56,40], memory[57,40], memory[58,40], memory[59,40], memory[60,40], memory[61,40], memory[62,40], memory[63,40], memory[64,40], memory[65,40], memory[66,40], memory[67,40], memory[68,40], memory[69,40], memory[70,40], memory[71,40], memory[72,40], memory[73,40], memory[74,40], memory[75,40], memory[76,40], memory[77,40], memory[78,40], memory[79,40], memory[80,40], memory[81,40], memory[82,40], memory[83,40], memory[84,40], memory[85,40], memory[86,40], memory[87,40], memory[88,40], memory[89,40], memory[90,40], memory[91,40], memory[92,40], memory[93,40], memory[94,40], memory[95,40], memory[96,40], memory[97,40], memory[98,40], memory[99,40], memory[100,40], memory[101,40], memory[102,40], memory[103,40], memory[104,40], memory[105,40], memory[106,40], memory[107,40], memory[108,40], memory[109,40], memory[110,40], memory[111,40], memory[112,40], memory[113,40], memory[114,40], memory[115,40], memory[116,40], memory[117,40], memory[118,40], memory[119,40], memory[120,40], memory[1,41], memory[2,41], memory[3,41], memory[4,41], memory[5,41], memory[6,41], memory[7,41], memory[8,41], memory[9,41], memory[10,41], memory[11,41], memory[12,41], memory[13,41], memory[14,41], memory[15,41], memory[16,41], memory[17,41], memory[18,41], memory[19,41], memory[20,41], memory[21,41], memory[22,41], memory[23,41], memory[24,41], memory[25,41], memory[26,41], memory[27,41], memory[28,41], memory[29,41], memory[30,41], memory[31,41], memory[32,41], memory[33,41], memory[34,41], memory[35,41], memory[36,41], memory[37,41], memory[38,41], memory[39,41], memory[40,41], memory[41,41], memory[42,41], memory[43,41], memory[44,41], memory[45,41], memory[46,41], memory[47,41], memory[48,41], memory[49,41], memory[50,41], memory[51,41], memory[52,41], memory[53,41], memory[54,41], memory[55,41], memory[56,41], memory[57,41], memory[58,41], memory[59,41], memory[60,41], memory[61,41], memory[62,41], memory[63,41], memory[64,41], memory[65,41], memory[66,41], memory[67,41], memory[68,41], memory[69,41], memory[70,41], memory[71,41], memory[72,41], memory[73,41], memory[74,41], memory[75,41], memory[76,41], memory[77,41], memory[78,41], memory[79,41], memory[80,41], memory[81,41], memory[82,41], memory[83,41], memory[84,41], memory[85,41], memory[86,41], memory[87,41], memory[88,41], memory[89,41], memory[90,41], memory[91,41], memory[92,41], memory[93,41], memory[94,41], memory[95,41], memory[96,41], memory[97,41], memory[98,41], memory[99,41], memory[100,41], memory[101,41], memory[102,41], memory[103,41], memory[104,41], memory[105,41], memory[106,41], memory[107,41], memory[108,41], memory[109,41], memory[110,41], memory[111,41], memory[112,41], memory[113,41], memory[114,41], memory[115,41], memory[116,41], memory[117,41], memory[118,41], memory[119,41], memory[120,41], memory[1,42], memory[2,42], memory[3,42], memory[4,42], memory[5,42], memory[6,42], memory[7,42], memory[8,42], memory[9,42], memory[10,42], memory[11,42], memory[12,42], memory[13,42], memory[14,42], memory[15,42], memory[16,42], memory[17,42], memory[18,42], memory[19,42], memory[20,42], memory[21,42], memory[22,42], memory[23,42], memory[24,42], memory[25,42], memory[26,42], memory[27,42], memory[28,42], memory[29,42], memory[30,42], memory[31,42], memory[32,42], memory[33,42], memory[34,42], memory[35,42], memory[36,42], memory[37,42], memory[38,42], memory[39,42], memory[40,42], memory[41,42], memory[42,42], memory[43,42], memory[44,42], memory[45,42], memory[46,42], memory[47,42], memory[48,42], memory[49,42], memory[50,42], memory[51,42], memory[52,42], memory[53,42], memory[54,42], memory[55,42], memory[56,42], memory[57,42], memory[58,42], memory[59,42], memory[60,42], memory[61,42], memory[62,42], memory[63,42], memory[64,42], memory[65,42], memory[66,42], memory[67,42], memory[68,42], memory[69,42], memory[70,42], memory[71,42], memory[72,42], memory[73,42], memory[74,42], memory[75,42], memory[76,42], memory[77,42], memory[78,42], memory[79,42], memory[80,42], memory[81,42], memory[82,42], memory[83,42], memory[84,42], memory[85,42], memory[86,42], memory[87,42], memory[88,42], memory[89,42], memory[90,42], memory[91,42], memory[92,42], memory[93,42], memory[94,42], memory[95,42], memory[96,42], memory[97,42], memory[98,42], memory[99,42], memory[100,42], memory[101,42], memory[102,42], memory[103,42], memory[104,42], memory[105,42], memory[106,42], memory[107,42], memory[108,42], memory[109,42], memory[110,42], memory[111,42], memory[112,42], memory[113,42], memory[114,42], memory[115,42], memory[116,42], memory[117,42], memory[118,42], memory[119,42], memory[120,42], memory[1,43], memory[2,43], memory[3,43], memory[4,43], memory[5,43], memory[6,43], memory[7,43], memory[8,43], memory[9,43], memory[10,43], memory[11,43], memory[12,43], memory[13,43], memory[14,43], memory[15,43], memory[16,43], memory[17,43], memory[18,43], memory[19,43], memory[20,43], memory[21,43], memory[22,43], memory[23,43], memory[24,43], memory[25,43], memory[26,43], memory[27,43], memory[28,43], memory[29,43], memory[30,43], memory[31,43], memory[32,43], memory[33,43], memory[34,43], memory[35,43], memory[36,43], memory[37,43], memory[38,43], memory[39,43], memory[40,43], memory[41,43], memory[42,43], memory[43,43], memory[44,43], memory[45,43], memory[46,43], memory[47,43], memory[48,43], memory[49,43], memory[50,43], memory[51,43], memory[52,43], memory[53,43], memory[54,43], memory[55,43], memory[56,43], memory[57,43], memory[58,43], memory[59,43], memory[60,43], memory[61,43], memory[62,43], memory[63,43], memory[64,43], memory[65,43], memory[66,43], memory[67,43], memory[68,43], memory[69,43], memory[70,43], memory[71,43], memory[72,43], memory[73,43], memory[74,43], memory[75,43], memory[76,43], memory[77,43], memory[78,43], memory[79,43], memory[80,43], memory[81,43], memory[82,43], memory[83,43], memory[84,43], memory[85,43], memory[86,43], memory[87,43], memory[88,43], memory[89,43], memory[90,43], memory[91,43], memory[92,43], memory[93,43], memory[94,43], memory[95,43], memory[96,43], memory[97,43], memory[98,43], memory[99,43], memory[100,43], memory[101,43], memory[102,43], memory[103,43], memory[104,43], memory[105,43], memory[106,43], memory[107,43], memory[108,43], memory[109,43], memory[110,43], memory[111,43], memory[112,43], memory[113,43], memory[114,43], memory[115,43], memory[116,43], memory[117,43], memory[118,43], memory[119,43], memory[120,43], memory[1,44], memory[2,44], memory[3,44], memory[4,44], memory[5,44], memory[6,44], memory[7,44], memory[8,44], memory[9,44], memory[10,44], memory[11,44], memory[12,44], memory[13,44], memory[14,44], memory[15,44], memory[16,44], memory[17,44], memory[18,44], memory[19,44], memory[20,44], memory[21,44], memory[22,44], memory[23,44], memory[24,44], memory[25,44], memory[26,44], memory[27,44], memory[28,44], memory[29,44], memory[30,44], memory[31,44], memory[32,44], memory[33,44], memory[34,44], memory[35,44], memory[36,44], memory[37,44], memory[38,44], memory[39,44], memory[40,44], memory[41,44], memory[42,44], memory[43,44], memory[44,44], memory[45,44], memory[46,44], memory[47,44], memory[48,44], memory[49,44], memory[50,44], memory[51,44], memory[52,44], memory[53,44], memory[54,44], memory[55,44], memory[56,44], memory[57,44], memory[58,44], memory[59,44], memory[60,44], memory[61,44], memory[62,44], memory[63,44], memory[64,44], memory[65,44], memory[66,44], memory[67,44], memory[68,44], memory[69,44], memory[70,44], memory[71,44], memory[72,44], memory[73,44], memory[74,44], memory[75,44], memory[76,44], memory[77,44], memory[78,44], memory[79,44], memory[80,44], memory[81,44], memory[82,44], memory[83,44], memory[84,44], memory[85,44], memory[86,44], memory[87,44], memory[88,44], memory[89,44], memory[90,44], memory[91,44], memory[92,44], memory[93,44], memory[94,44], memory[95,44], memory[96,44], memory[97,44], memory[98,44], memory[99,44], memory[100,44], memory[101,44], memory[102,44], memory[103,44], memory[104,44], memory[105,44], memory[106,44], memory[107,44], memory[108,44], memory[109,44], memory[110,44], memory[111,44], memory[112,44], memory[113,44], memory[114,44], memory[115,44], memory[116,44], memory[117,44], memory[118,44], memory[119,44], memory[120,44], memory[1,45], memory[2,45], memory[3,45], memory[4,45], memory[5,45], memory[6,45], memory[7,45], memory[8,45], memory[9,45], memory[10,45], memory[11,45], memory[12,45], memory[13,45], memory[14,45], memory[15,45], memory[16,45], memory[17,45], memory[18,45], memory[19,45], memory[20,45], memory[21,45], memory[22,45], memory[23,45], memory[24,45], memory[25,45], memory[26,45], memory[27,45], memory[28,45], memory[29,45], memory[30,45], memory[31,45], memory[32,45], memory[33,45], memory[34,45], memory[35,45], memory[36,45], memory[37,45], memory[38,45], memory[39,45], memory[40,45], memory[41,45], memory[42,45], memory[43,45], memory[44,45], memory[45,45], memory[46,45], memory[47,45], memory[48,45], memory[49,45], memory[50,45], memory[51,45], memory[52,45], memory[53,45], memory[54,45], memory[55,45], memory[56,45], memory[57,45], memory[58,45], memory[59,45], memory[60,45], memory[61,45], memory[62,45], memory[63,45], memory[64,45], memory[65,45], memory[66,45], memory[67,45], memory[68,45], memory[69,45], memory[70,45], memory[71,45], memory[72,45], memory[73,45], memory[74,45], memory[75,45], memory[76,45], memory[77,45], memory[78,45], memory[79,45], memory[80,45], memory[81,45], memory[82,45], memory[83,45], memory[84,45], memory[85,45], memory[86,45], memory[87,45], memory[88,45], memory[89,45], memory[90,45], memory[91,45], memory[92,45], memory[93,45], memory[94,45], memory[95,45], memory[96,45], memory[97,45], memory[98,45], memory[99,45], memory[100,45], memory[101,45], memory[102,45], memory[103,45], memory[104,45], memory[105,45], memory[106,45], memory[107,45], memory[108,45], memory[109,45], memory[110,45], memory[111,45], memory[112,45], memory[113,45], memory[114,45], memory[115,45], memory[116,45], memory[117,45], memory[118,45], memory[119,45], memory[120,45], memory[1,46], memory[2,46], memory[3,46], memory[4,46], memory[5,46], memory[6,46], memory[7,46], memory[8,46], memory[9,46], memory[10,46], memory[11,46], memory[12,46], memory[13,46], memory[14,46], memory[15,46], memory[16,46], memory[17,46], memory[18,46], memory[19,46], memory[20,46], memory[21,46], memory[22,46], memory[23,46], memory[24,46], memory[25,46], memory[26,46], memory[27,46], memory[28,46], memory[29,46], memory[30,46], memory[31,46], memory[32,46], memory[33,46], memory[34,46], memory[35,46], memory[36,46], memory[37,46], memory[38,46], memory[39,46], memory[40,46], memory[41,46], memory[42,46], memory[43,46], memory[44,46], memory[45,46], memory[46,46], memory[47,46], memory[48,46], memory[49,46], memory[50,46], memory[51,46], memory[52,46], memory[53,46], memory[54,46], memory[55,46], memory[56,46], memory[57,46], memory[58,46], memory[59,46], memory[60,46], memory[61,46], memory[62,46], memory[63,46], memory[64,46], memory[65,46], memory[66,46], memory[67,46], memory[68,46], memory[69,46], memory[70,46], memory[71,46], memory[72,46], memory[73,46], memory[74,46], memory[75,46], memory[76,46], memory[77,46], memory[78,46], memory[79,46], memory[80,46], memory[81,46], memory[82,46], memory[83,46], memory[84,46], memory[85,46], memory[86,46], memory[87,46], memory[88,46], memory[89,46], memory[90,46], memory[91,46], memory[92,46], memory[93,46], memory[94,46], memory[95,46], memory[96,46], memory[97,46], memory[98,46], memory[99,46], memory[100,46], memory[101,46], memory[102,46], memory[103,46], memory[104,46], memory[105,46], memory[106,46], memory[107,46], memory[108,46], memory[109,46], memory[110,46], memory[111,46], memory[112,46], memory[113,46], memory[114,46], memory[115,46], memory[116,46], memory[117,46], memory[118,46], memory[119,46], memory[120,46], memory[1,47], memory[2,47], memory[3,47], memory[4,47], memory[5,47], memory[6,47], memory[7,47], memory[8,47], memory[9,47], memory[10,47], memory[11,47], memory[12,47], memory[13,47], memory[14,47], memory[15,47], memory[16,47], memory[17,47], memory[18,47], memory[19,47], memory[20,47], memory[21,47], memory[22,47], memory[23,47], memory[24,47], memory[25,47], memory[26,47], memory[27,47], memory[28,47], memory[29,47], memory[30,47], memory[31,47], memory[32,47], memory[33,47], memory[34,47], memory[35,47], memory[36,47], memory[37,47], memory[38,47], memory[39,47], memory[40,47], memory[41,47], memory[42,47], memory[43,47], memory[44,47], memory[45,47], memory[46,47], memory[47,47], memory[48,47], memory[49,47], memory[50,47], memory[51,47], memory[52,47], memory[53,47], memory[54,47], memory[55,47], memory[56,47], memory[57,47], memory[58,47], memory[59,47], memory[60,47], memory[61,47], memory[62,47], memory[63,47], memory[64,47], memory[65,47], memory[66,47], memory[67,47], memory[68,47], memory[69,47], memory[70,47], memory[71,47], memory[72,47], memory[73,47], memory[74,47], memory[75,47], memory[76,47], memory[77,47], memory[78,47], memory[79,47], memory[80,47], memory[81,47], memory[82,47], memory[83,47], memory[84,47], memory[85,47], memory[86,47], memory[87,47], memory[88,47], memory[89,47], memory[90,47], memory[91,47], memory[92,47], memory[93,47], memory[94,47], memory[95,47], memory[96,47], memory[97,47], memory[98,47], memory[99,47], memory[100,47], memory[101,47], memory[102,47], memory[103,47], memory[104,47], memory[105,47], memory[106,47], memory[107,47], memory[108,47], memory[109,47], memory[110,47], memory[111,47], memory[112,47], memory[113,47], memory[114,47], memory[115,47], memory[116,47], memory[117,47], memory[118,47], memory[119,47], memory[120,47], memory[1,48], memory[2,48], memory[3,48], memory[4,48], memory[5,48], memory[6,48], memory[7,48], memory[8,48], memory[9,48], memory[10,48], memory[11,48], memory[12,48], memory[13,48], memory[14,48], memory[15,48], memory[16,48], memory[17,48], memory[18,48], memory[19,48], memory[20,48], memory[21,48], memory[22,48], memory[23,48], memory[24,48], memory[25,48], memory[26,48], memory[27,48], memory[28,48], memory[29,48], memory[30,48], memory[31,48], memory[32,48], memory[33,48], memory[34,48], memory[35,48], memory[36,48], memory[37,48], memory[38,48], memory[39,48], memory[40,48], memory[41,48], memory[42,48], memory[43,48], memory[44,48], memory[45,48], memory[46,48], memory[47,48], memory[48,48], memory[49,48], memory[50,48], memory[51,48], memory[52,48], memory[53,48], memory[54,48], memory[55,48], memory[56,48], memory[57,48], memory[58,48], memory[59,48], memory[60,48], memory[61,48], memory[62,48], memory[63,48], memory[64,48], memory[65,48], memory[66,48], memory[67,48], memory[68,48], memory[69,48], memory[70,48], memory[71,48], memory[72,48], memory[73,48], memory[74,48], memory[75,48], memory[76,48], memory[77,48], memory[78,48], memory[79,48], memory[80,48], memory[81,48], memory[82,48], memory[83,48], memory[84,48], memory[85,48], memory[86,48], memory[87,48], memory[88,48], memory[89,48], memory[90,48], memory[91,48], memory[92,48], memory[93,48], memory[94,48], memory[95,48], memory[96,48], memory[97,48], memory[98,48], memory[99,48], memory[100,48], memory[101,48], memory[102,48], memory[103,48], memory[104,48], memory[105,48], memory[106,48], memory[107,48], memory[108,48], memory[109,48], memory[110,48], memory[111,48], memory[112,48], memory[113,48], memory[114,48], memory[115,48], memory[116,48], memory[117,48], memory[118,48], memory[119,48], memory[120,48], memory[1,49], memory[2,49], memory[3,49], memory[4,49], memory[5,49], memory[6,49], memory[7,49], memory[8,49], memory[9,49], memory[10,49], memory[11,49], memory[12,49], memory[13,49], memory[14,49], memory[15,49], memory[16,49], memory[17,49], memory[18,49], memory[19,49], memory[20,49], memory[21,49], memory[22,49], memory[23,49], memory[24,49], memory[25,49], memory[26,49], memory[27,49], memory[28,49], memory[29,49], memory[30,49], memory[31,49], memory[32,49], memory[33,49], memory[34,49], memory[35,49], memory[36,49], memory[37,49], memory[38,49], memory[39,49], memory[40,49], memory[41,49], memory[42,49], memory[43,49], memory[44,49], memory[45,49], memory[46,49], memory[47,49], memory[48,49], memory[49,49], memory[50,49], memory[51,49], memory[52,49], memory[53,49], memory[54,49], memory[55,49], memory[56,49], memory[57,49], memory[58,49], memory[59,49], memory[60,49], memory[61,49], memory[62,49], memory[63,49], memory[64,49], memory[65,49], memory[66,49], memory[67,49], memory[68,49], memory[69,49], memory[70,49], memory[71,49], memory[72,49], memory[73,49], memory[74,49], memory[75,49], memory[76,49], memory[77,49], memory[78,49], memory[79,49], memory[80,49], memory[81,49], memory[82,49], memory[83,49], memory[84,49], memory[85,49], memory[86,49], memory[87,49], memory[88,49], memory[89,49], memory[90,49], memory[91,49], memory[92,49], memory[93,49], memory[94,49], memory[95,49], memory[96,49], memory[97,49], memory[98,49], memory[99,49], memory[100,49], memory[101,49], memory[102,49], memory[103,49], memory[104,49], memory[105,49], memory[106,49], memory[107,49], memory[108,49], memory[109,49], memory[110,49], memory[111,49], memory[112,49], memory[113,49], memory[114,49], memory[115,49], memory[116,49], memory[117,49], memory[118,49], memory[119,49], memory[120,49], memory[1,50], memory[2,50], memory[3,50], memory[4,50], memory[5,50], memory[6,50], memory[7,50], memory[8,50], memory[9,50], memory[10,50], memory[11,50], memory[12,50], memory[13,50], memory[14,50], memory[15,50], memory[16,50], memory[17,50], memory[18,50], memory[19,50], memory[20,50], memory[21,50], memory[22,50], memory[23,50], memory[24,50], memory[25,50], memory[26,50], memory[27,50], memory[28,50], memory[29,50], memory[30,50], memory[31,50], memory[32,50], memory[33,50], memory[34,50], memory[35,50], memory[36,50], memory[37,50], memory[38,50], memory[39,50], memory[40,50], memory[41,50], memory[42,50], memory[43,50], memory[44,50], memory[45,50], memory[46,50], memory[47,50], memory[48,50], memory[49,50], memory[50,50], memory[51,50], memory[52,50], memory[53,50], memory[54,50], memory[55,50], memory[56,50], memory[57,50], memory[58,50], memory[59,50], memory[60,50], memory[61,50], memory[62,50], memory[63,50], memory[64,50], memory[65,50], memory[66,50], memory[67,50], memory[68,50], memory[69,50], memory[70,50], memory[71,50], memory[72,50], memory[73,50], memory[74,50], memory[75,50], memory[76,50], memory[77,50], memory[78,50], memory[79,50], memory[80,50], memory[81,50], memory[82,50], memory[83,50], memory[84,50], memory[85,50], memory[86,50], memory[87,50], memory[88,50], memory[89,50], memory[90,50], memory[91,50], memory[92,50], memory[93,50], memory[94,50], memory[95,50], memory[96,50], memory[97,50], memory[98,50], memory[99,50], memory[100,50], memory[101,50], memory[102,50], memory[103,50], memory[104,50], memory[105,50], memory[106,50], memory[107,50], memory[108,50], memory[109,50], memory[110,50], memory[111,50], memory[112,50], memory[113,50], memory[114,50], memory[115,50], memory[116,50], memory[117,50], memory[118,50], memory[119,50], memory[120,50], memory[1,51], memory[2,51], memory[3,51], memory[4,51], memory[5,51], memory[6,51], memory[7,51], memory[8,51], memory[9,51], memory[10,51], memory[11,51], memory[12,51], memory[13,51], memory[14,51], memory[15,51], memory[16,51], memory[17,51], memory[18,51], memory[19,51], memory[20,51], memory[21,51], memory[22,51], memory[23,51], memory[24,51], memory[25,51], memory[26,51], memory[27,51], memory[28,51], memory[29,51], memory[30,51], memory[31,51], memory[32,51], memory[33,51], memory[34,51], memory[35,51], memory[36,51], memory[37,51], memory[38,51], memory[39,51], memory[40,51], memory[41,51], memory[42,51], memory[43,51], memory[44,51], memory[45,51], memory[46,51], memory[47,51], memory[48,51], memory[49,51], memory[50,51], memory[51,51], memory[52,51], memory[53,51], memory[54,51], memory[55,51], memory[56,51], memory[57,51], memory[58,51], memory[59,51], memory[60,51], memory[61,51], memory[62,51], memory[63,51], memory[64,51], memory[65,51], memory[66,51], memory[67,51], memory[68,51], memory[69,51], memory[70,51], memory[71,51], memory[72,51], memory[73,51], memory[74,51], memory[75,51], memory[76,51], memory[77,51], memory[78,51], memory[79,51], memory[80,51], memory[81,51], memory[82,51], memory[83,51], memory[84,51], memory[85,51], memory[86,51], memory[87,51], memory[88,51], memory[89,51], memory[90,51], memory[91,51], memory[92,51], memory[93,51], memory[94,51], memory[95,51], memory[96,51], memory[97,51], memory[98,51], memory[99,51], memory[100,51], memory[101,51], memory[102,51], memory[103,51], memory[104,51], memory[105,51], memory[106,51], memory[107,51], memory[108,51], memory[109,51], memory[110,51], memory[111,51], memory[112,51], memory[113,51], memory[114,51], memory[115,51], memory[116,51], memory[117,51], memory[118,51], memory[119,51], memory[120,51], memory[1,52], memory[2,52], memory[3,52], memory[4,52], memory[5,52], memory[6,52], memory[7,52], memory[8,52], memory[9,52], memory[10,52], memory[11,52], memory[12,52], memory[13,52], memory[14,52], memory[15,52], memory[16,52], memory[17,52], memory[18,52], memory[19,52], memory[20,52], memory[21,52], memory[22,52], memory[23,52], memory[24,52], memory[25,52], memory[26,52], memory[27,52], memory[28,52], memory[29,52], memory[30,52], memory[31,52], memory[32,52], memory[33,52], memory[34,52], memory[35,52], memory[36,52], memory[37,52], memory[38,52], memory[39,52], memory[40,52], memory[41,52], memory[42,52], memory[43,52], memory[44,52], memory[45,52], memory[46,52], memory[47,52], memory[48,52], memory[49,52], memory[50,52], memory[51,52], memory[52,52], memory[53,52], memory[54,52], memory[55,52], memory[56,52], memory[57,52], memory[58,52], memory[59,52], memory[60,52], memory[61,52], memory[62,52], memory[63,52], memory[64,52], memory[65,52], memory[66,52], memory[67,52], memory[68,52], memory[69,52], memory[70,52], memory[71,52], memory[72,52], memory[73,52], memory[74,52], memory[75,52], memory[76,52], memory[77,52], memory[78,52], memory[79,52], memory[80,52], memory[81,52], memory[82,52], memory[83,52], memory[84,52], memory[85,52], memory[86,52], memory[87,52], memory[88,52], memory[89,52], memory[90,52], memory[91,52], memory[92,52], memory[93,52], memory[94,52], memory[95,52], memory[96,52], memory[97,52], memory[98,52], memory[99,52], memory[100,52], memory[101,52], memory[102,52], memory[103,52], memory[104,52], memory[105,52], memory[106,52], memory[107,52], memory[108,52], memory[109,52], memory[110,52], memory[111,52], memory[112,52], memory[113,52], memory[114,52], memory[115,52], memory[116,52], memory[117,52], memory[118,52], memory[119,52], memory[120,52], memory[1,53], memory[2,53], memory[3,53], memory[4,53], memory[5,53], memory[6,53], memory[7,53], memory[8,53], memory[9,53], memory[10,53], memory[11,53], memory[12,53], memory[13,53], memory[14,53], memory[15,53], memory[16,53], memory[17,53], memory[18,53], memory[19,53], memory[20,53], memory[21,53], memory[22,53], memory[23,53], memory[24,53], memory[25,53], memory[26,53], memory[27,53], memory[28,53], memory[29,53], memory[30,53], memory[31,53], memory[32,53], memory[33,53], memory[34,53], memory[35,53], memory[36,53], memory[37,53], memory[38,53], memory[39,53], memory[40,53], memory[41,53], memory[42,53], memory[43,53], memory[44,53], memory[45,53], memory[46,53], memory[47,53], memory[48,53], memory[49,53], memory[50,53], memory[51,53], memory[52,53], memory[53,53], memory[54,53], memory[55,53], memory[56,53], memory[57,53], memory[58,53], memory[59,53], memory[60,53], memory[61,53], memory[62,53], memory[63,53], memory[64,53], memory[65,53], memory[66,53], memory[67,53], memory[68,53], memory[69,53], memory[70,53], memory[71,53], memory[72,53], memory[73,53], memory[74,53], memory[75,53], memory[76,53], memory[77,53], memory[78,53], memory[79,53], memory[80,53], memory[81,53], memory[82,53], memory[83,53], memory[84,53], memory[85,53], memory[86,53], memory[87,53], memory[88,53], memory[89,53], memory[90,53], memory[91,53], memory[92,53], memory[93,53], memory[94,53], memory[95,53], memory[96,53], memory[97,53], memory[98,53], memory[99,53], memory[100,53], memory[101,53], memory[102,53], memory[103,53], memory[104,53], memory[105,53], memory[106,53], memory[107,53], memory[108,53], memory[109,53], memory[110,53], memory[111,53], memory[112,53], memory[113,53], memory[114,53], memory[115,53], memory[116,53], memory[117,53], memory[118,53], memory[119,53], memory[120,53], memory[1,54], memory[2,54], memory[3,54], memory[4,54], memory[5,54], memory[6,54], memory[7,54], memory[8,54], memory[9,54], memory[10,54], memory[11,54], memory[12,54], memory[13,54], memory[14,54], memory[15,54], memory[16,54], memory[17,54], memory[18,54], memory[19,54], memory[20,54], memory[21,54], memory[22,54], memory[23,54], memory[24,54], memory[25,54], memory[26,54], memory[27,54], memory[28,54], memory[29,54], memory[30,54], memory[31,54], memory[32,54], memory[33,54], memory[34,54], memory[35,54], memory[36,54], memory[37,54], memory[38,54], memory[39,54], memory[40,54], memory[41,54], memory[42,54], memory[43,54], memory[44,54], memory[45,54], memory[46,54], memory[47,54], memory[48,54], memory[49,54], memory[50,54], memory[51,54], memory[52,54], memory[53,54], memory[54,54], memory[55,54], memory[56,54], memory[57,54], memory[58,54], memory[59,54], memory[60,54], memory[61,54], memory[62,54], memory[63,54], memory[64,54], memory[65,54], memory[66,54], memory[67,54], memory[68,54], memory[69,54], memory[70,54], memory[71,54], memory[72,54], memory[73,54], memory[74,54], memory[75,54], memory[76,54], memory[77,54], memory[78,54], memory[79,54], memory[80,54], memory[81,54], memory[82,54], memory[83,54], memory[84,54], memory[85,54], memory[86,54], memory[87,54], memory[88,54], memory[89,54], memory[90,54], memory[91,54], memory[92,54], memory[93,54], memory[94,54], memory[95,54], memory[96,54], memory[97,54], memory[98,54], memory[99,54], memory[100,54], memory[101,54], memory[102,54], memory[103,54], memory[104,54], memory[105,54], memory[106,54], memory[107,54], memory[108,54], memory[109,54], memory[110,54], memory[111,54], memory[112,54], memory[113,54], memory[114,54], memory[115,54], memory[116,54], memory[117,54], memory[118,54], memory[119,54], memory[120,54], memory[1,55], memory[2,55], memory[3,55], memory[4,55], memory[5,55], memory[6,55], memory[7,55], memory[8,55], memory[9,55], memory[10,55], memory[11,55], memory[12,55], memory[13,55], memory[14,55], memory[15,55], memory[16,55], memory[17,55], memory[18,55], memory[19,55], memory[20,55], memory[21,55], memory[22,55], memory[23,55], memory[24,55], memory[25,55], memory[26,55], memory[27,55], memory[28,55], memory[29,55], memory[30,55], memory[31,55], memory[32,55], memory[33,55], memory[34,55], memory[35,55], memory[36,55], memory[37,55], memory[38,55], memory[39,55], memory[40,55], memory[41,55], memory[42,55], memory[43,55], memory[44,55], memory[45,55], memory[46,55], memory[47,55], memory[48,55], memory[49,55], memory[50,55], memory[51,55], memory[52,55], memory[53,55], memory[54,55], memory[55,55], memory[56,55], memory[57,55], memory[58,55], memory[59,55], memory[60,55], memory[61,55], memory[62,55], memory[63,55], memory[64,55], memory[65,55], memory[66,55], memory[67,55], memory[68,55], memory[69,55], memory[70,55], memory[71,55], memory[72,55], memory[73,55], memory[74,55], memory[75,55], memory[76,55], memory[77,55], memory[78,55], memory[79,55], memory[80,55], memory[81,55], memory[82,55], memory[83,55], memory[84,55], memory[85,55], memory[86,55], memory[87,55], memory[88,55], memory[89,55], memory[90,55], memory[91,55], memory[92,55], memory[93,55], memory[94,55], memory[95,55], memory[96,55], memory[97,55], memory[98,55], memory[99,55], memory[100,55], memory[101,55], memory[102,55], memory[103,55], memory[104,55], memory[105,55], memory[106,55], memory[107,55], memory[108,55], memory[109,55], memory[110,55], memory[111,55], memory[112,55], memory[113,55], memory[114,55], memory[115,55], memory[116,55], memory[117,55], memory[118,55], memory[119,55], memory[120,55], memory[1,56], memory[2,56], memory[3,56], memory[4,56], memory[5,56], memory[6,56], memory[7,56], memory[8,56], memory[9,56], memory[10,56], memory[11,56], memory[12,56], memory[13,56], memory[14,56], memory[15,56], memory[16,56], memory[17,56], memory[18,56], memory[19,56], memory[20,56], memory[21,56], memory[22,56], memory[23,56], memory[24,56], memory[25,56], memory[26,56], memory[27,56], memory[28,56], memory[29,56], memory[30,56], memory[31,56], memory[32,56], memory[33,56], memory[34,56], memory[35,56], memory[36,56], memory[37,56], memory[38,56], memory[39,56], memory[40,56], memory[41,56], memory[42,56], memory[43,56], memory[44,56], memory[45,56], memory[46,56], memory[47,56], memory[48,56], memory[49,56], memory[50,56], memory[51,56], memory[52,56], memory[53,56], memory[54,56], memory[55,56], memory[56,56], memory[57,56], memory[58,56], memory[59,56], memory[60,56], memory[61,56], memory[62,56], memory[63,56], memory[64,56], memory[65,56], memory[66,56], memory[67,56], memory[68,56], memory[69,56], memory[70,56], memory[71,56], memory[72,56], memory[73,56], memory[74,56], memory[75,56], memory[76,56], memory[77,56], memory[78,56], memory[79,56], memory[80,56], memory[81,56], memory[82,56], memory[83,56], memory[84,56], memory[85,56], memory[86,56], memory[87,56], memory[88,56], memory[89,56], memory[90,56], memory[91,56], memory[92,56], memory[93,56], memory[94,56], memory[95,56], memory[96,56], memory[97,56], memory[98,56], memory[99,56], memory[100,56], memory[101,56], memory[102,56], memory[103,56], memory[104,56], memory[105,56], memory[106,56], memory[107,56], memory[108,56], memory[109,56], memory[110,56], memory[111,56], memory[112,56], memory[113,56], memory[114,56], memory[115,56], memory[116,56], memory[117,56], memory[118,56], memory[119,56], memory[120,56], memory[1,57], memory[2,57], memory[3,57], memory[4,57], memory[5,57], memory[6,57], memory[7,57], memory[8,57], memory[9,57], memory[10,57], memory[11,57], memory[12,57], memory[13,57], memory[14,57], memory[15,57], memory[16,57], memory[17,57], memory[18,57], memory[19,57], memory[20,57], memory[21,57], memory[22,57], memory[23,57], memory[24,57], memory[25,57], memory[26,57], memory[27,57], memory[28,57], memory[29,57], memory[30,57], memory[31,57], memory[32,57], memory[33,57], memory[34,57], memory[35,57], memory[36,57], memory[37,57], memory[38,57], memory[39,57], memory[40,57], memory[41,57], memory[42,57], memory[43,57], memory[44,57], memory[45,57], memory[46,57], memory[47,57], memory[48,57], memory[49,57], memory[50,57], memory[51,57], memory[52,57], memory[53,57], memory[54,57], memory[55,57], memory[56,57], memory[57,57], memory[58,57], memory[59,57], memory[60,57], memory[61,57], memory[62,57], memory[63,57], memory[64,57], memory[65,57], memory[66,57], memory[67,57], memory[68,57], memory[69,57], memory[70,57], memory[71,57], memory[72,57], memory[73,57], memory[74,57], memory[75,57], memory[76,57], memory[77,57], memory[78,57], memory[79,57], memory[80,57], memory[81,57], memory[82,57], memory[83,57], memory[84,57], memory[85,57], memory[86,57], memory[87,57], memory[88,57], memory[89,57], memory[90,57], memory[91,57], memory[92,57], memory[93,57], memory[94,57], memory[95,57], memory[96,57], memory[97,57], memory[98,57], memory[99,57], memory[100,57], memory[101,57], memory[102,57], memory[103,57], memory[104,57], memory[105,57], memory[106,57], memory[107,57], memory[108,57], memory[109,57], memory[110,57], memory[111,57], memory[112,57], memory[113,57], memory[114,57], memory[115,57], memory[116,57], memory[117,57], memory[118,57], memory[119,57], memory[120,57], memory[1,58], memory[2,58], memory[3,58], memory[4,58], memory[5,58], memory[6,58], memory[7,58], memory[8,58], memory[9,58], memory[10,58], memory[11,58], memory[12,58], memory[13,58], memory[14,58], memory[15,58], memory[16,58], memory[17,58], memory[18,58], memory[19,58], memory[20,58], memory[21,58], memory[22,58], memory[23,58], memory[24,58], memory[25,58], memory[26,58], memory[27,58], memory[28,58], memory[29,58], memory[30,58], memory[31,58], memory[32,58], memory[33,58], memory[34,58], memory[35,58], memory[36,58], memory[37,58], memory[38,58], memory[39,58], memory[40,58], memory[41,58], memory[42,58], memory[43,58], memory[44,58], memory[45,58], memory[46,58], memory[47,58], memory[48,58], memory[49,58], memory[50,58], memory[51,58], memory[52,58], memory[53,58], memory[54,58], memory[55,58], memory[56,58], memory[57,58], memory[58,58], memory[59,58], memory[60,58], memory[61,58], memory[62,58], memory[63,58], memory[64,58], memory[65,58], memory[66,58], memory[67,58], memory[68,58], memory[69,58], memory[70,58], memory[71,58], memory[72,58], memory[73,58], memory[74,58], memory[75,58], memory[76,58], memory[77,58], memory[78,58], memory[79,58], memory[80,58], memory[81,58], memory[82,58], memory[83,58], memory[84,58], memory[85,58], memory[86,58], memory[87,58], memory[88,58], memory[89,58], memory[90,58], memory[91,58], memory[92,58], memory[93,58], memory[94,58], memory[95,58], memory[96,58], memory[97,58], memory[98,58], memory[99,58], memory[100,58], memory[101,58], memory[102,58], memory[103,58], memory[104,58], memory[105,58], memory[106,58], memory[107,58], memory[108,58], memory[109,58], memory[110,58], memory[111,58], memory[112,58], memory[113,58], memory[114,58], memory[115,58], memory[116,58], memory[117,58], memory[118,58], memory[119,58], memory[120,58], memory[1,59], memory[2,59], memory[3,59], memory[4,59], memory[5,59], memory[6,59], memory[7,59], memory[8,59], memory[9,59], memory[10,59], memory[11,59], memory[12,59], memory[13,59], memory[14,59], memory[15,59], memory[16,59], memory[17,59], memory[18,59], memory[19,59], memory[20,59], memory[21,59], memory[22,59], memory[23,59], memory[24,59], memory[25,59], memory[26,59], memory[27,59], memory[28,59], memory[29,59], memory[30,59], memory[31,59], memory[32,59], memory[33,59], memory[34,59], memory[35,59], memory[36,59], memory[37,59], memory[38,59], memory[39,59], memory[40,59], memory[41,59], memory[42,59], memory[43,59], memory[44,59], memory[45,59], memory[46,59], memory[47,59], memory[48,59], memory[49,59], memory[50,59], memory[51,59], memory[52,59], memory[53,59], memory[54,59], memory[55,59], memory[56,59], memory[57,59], memory[58,59], memory[59,59], memory[60,59], memory[61,59], memory[62,59], memory[63,59], memory[64,59], memory[65,59], memory[66,59], memory[67,59], memory[68,59], memory[69,59], memory[70,59], memory[71,59], memory[72,59], memory[73,59], memory[74,59], memory[75,59], memory[76,59], memory[77,59], memory[78,59], memory[79,59], memory[80,59], memory[81,59], memory[82,59], memory[83,59], memory[84,59], memory[85,59], memory[86,59], memory[87,59], memory[88,59], memory[89,59], memory[90,59], memory[91,59], memory[92,59], memory[93,59], memory[94,59], memory[95,59], memory[96,59], memory[97,59], memory[98,59], memory[99,59], memory[100,59], memory[101,59], memory[102,59], memory[103,59], memory[104,59], memory[105,59], memory[106,59], memory[107,59], memory[108,59], memory[109,59], memory[110,59], memory[111,59], memory[112,59], memory[113,59], memory[114,59], memory[115,59], memory[116,59], memory[117,59], memory[118,59], memory[119,59], memory[120,59], memory[1,60], memory[2,60], memory[3,60], memory[4,60], memory[5,60], memory[6,60], memory[7,60], memory[8,60], memory[9,60], memory[10,60], memory[11,60], memory[12,60], memory[13,60], memory[14,60], memory[15,60], memory[16,60], memory[17,60], memory[18,60], memory[19,60], memory[20,60], memory[21,60], memory[22,60], memory[23,60], memory[24,60], memory[25,60], memory[26,60], memory[27,60], memory[28,60], memory[29,60], memory[30,60], memory[31,60], memory[32,60], memory[33,60], memory[34,60], memory[35,60], memory[36,60], memory[37,60], memory[38,60], memory[39,60], memory[40,60], memory[41,60], memory[42,60], memory[43,60], memory[44,60], memory[45,60], memory[46,60], memory[47,60], memory[48,60], memory[49,60], memory[50,60], memory[51,60], memory[52,60], memory[53,60], memory[54,60], memory[55,60], memory[56,60], memory[57,60], memory[58,60], memory[59,60], memory[60,60], memory[61,60], memory[62,60], memory[63,60], memory[64,60], memory[65,60], memory[66,60], memory[67,60], memory[68,60], memory[69,60], memory[70,60], memory[71,60], memory[72,60], memory[73,60], memory[74,60], memory[75,60], memory[76,60], memory[77,60], memory[78,60], memory[79,60], memory[80,60], memory[81,60], memory[82,60], memory[83,60], memory[84,60], memory[85,60], memory[86,60], memory[87,60], memory[88,60], memory[89,60], memory[90,60], memory[91,60], memory[92,60], memory[93,60], memory[94,60], memory[95,60], memory[96,60], memory[97,60], memory[98,60], memory[99,60], memory[100,60], memory[101,60], memory[102,60], memory[103,60], memory[104,60], memory[105,60], memory[106,60], memory[107,60], memory[108,60], memory[109,60], memory[110,60], memory[111,60], memory[112,60], memory[113,60], memory[114,60], memory[115,60], memory[116,60], memory[117,60], memory[118,60], memory[119,60], memory[120,60], memory[1,61], memory[2,61], memory[3,61], memory[4,61], memory[5,61], memory[6,61], memory[7,61], memory[8,61], memory[9,61], memory[10,61], memory[11,61], memory[12,61], memory[13,61], memory[14,61], memory[15,61], memory[16,61], memory[17,61], memory[18,61], memory[19,61], memory[20,61], memory[21,61], memory[22,61], memory[23,61], memory[24,61], memory[25,61], memory[26,61], memory[27,61], memory[28,61], memory[29,61], memory[30,61], memory[31,61], memory[32,61], memory[33,61], memory[34,61], memory[35,61], memory[36,61], memory[37,61], memory[38,61], memory[39,61], memory[40,61], memory[41,61], memory[42,61], memory[43,61], memory[44,61], memory[45,61], memory[46,61], memory[47,61], memory[48,61], memory[49,61], memory[50,61], memory[51,61], memory[52,61], memory[53,61], memory[54,61], memory[55,61], memory[56,61], memory[57,61], memory[58,61], memory[59,61], memory[60,61], memory[61,61], memory[62,61], memory[63,61], memory[64,61], memory[65,61], memory[66,61], memory[67,61], memory[68,61], memory[69,61], memory[70,61], memory[71,61], memory[72,61], memory[73,61], memory[74,61], memory[75,61], memory[76,61], memory[77,61], memory[78,61], memory[79,61], memory[80,61], memory[81,61], memory[82,61], memory[83,61], memory[84,61], memory[85,61], memory[86,61], memory[87,61], memory[88,61], memory[89,61], memory[90,61], memory[91,61], memory[92,61], memory[93,61], memory[94,61], memory[95,61], memory[96,61], memory[97,61], memory[98,61], memory[99,61], memory[100,61], memory[101,61], memory[102,61], memory[103,61], memory[104,61], memory[105,61], memory[106,61], memory[107,61], memory[108,61], memory[109,61], memory[110,61], memory[111,61], memory[112,61], memory[113,61], memory[114,61], memory[115,61], memory[116,61], memory[117,61], memory[118,61], memory[119,61], memory[120,61], memory[1,62], memory[2,62], memory[3,62], memory[4,62], memory[5,62], memory[6,62], memory[7,62], memory[8,62], memory[9,62], memory[10,62], memory[11,62], memory[12,62], memory[13,62], memory[14,62], memory[15,62], memory[16,62], memory[17,62], memory[18,62], memory[19,62], memory[20,62], memory[21,62], memory[22,62], memory[23,62], memory[24,62], memory[25,62], memory[26,62], memory[27,62], memory[28,62], memory[29,62], memory[30,62], memory[31,62], memory[32,62], memory[33,62], memory[34,62], memory[35,62], memory[36,62], memory[37,62], memory[38,62], memory[39,62], memory[40,62], memory[41,62], memory[42,62], memory[43,62], memory[44,62], memory[45,62], memory[46,62], memory[47,62], memory[48,62], memory[49,62], memory[50,62], memory[51,62], memory[52,62], memory[53,62], memory[54,62], memory[55,62], memory[56,62], memory[57,62], memory[58,62], memory[59,62], memory[60,62], memory[61,62], memory[62,62], memory[63,62], memory[64,62], memory[65,62], memory[66,62], memory[67,62], memory[68,62], memory[69,62], memory[70,62], memory[71,62], memory[72,62], memory[73,62], memory[74,62], memory[75,62], memory[76,62], memory[77,62], memory[78,62], memory[79,62], memory[80,62], memory[81,62], memory[82,62], memory[83,62], memory[84,62], memory[85,62], memory[86,62], memory[87,62], memory[88,62], memory[89,62], memory[90,62], memory[91,62], memory[92,62], memory[93,62], memory[94,62], memory[95,62], memory[96,62], memory[97,62], memory[98,62], memory[99,62], memory[100,62], memory[101,62], memory[102,62], memory[103,62], memory[104,62], memory[105,62], memory[106,62], memory[107,62], memory[108,62], memory[109,62], memory[110,62], memory[111,62], memory[112,62], memory[113,62], memory[114,62], memory[115,62], memory[116,62], memory[117,62], memory[118,62], memory[119,62], memory[120,62], memory[1,63], memory[2,63], memory[3,63], memory[4,63], memory[5,63], memory[6,63], memory[7,63], memory[8,63], memory[9,63], memory[10,63], memory[11,63], memory[12,63], memory[13,63], memory[14,63], memory[15,63], memory[16,63], memory[17,63], memory[18,63], memory[19,63], memory[20,63], memory[21,63], memory[22,63], memory[23,63], memory[24,63], memory[25,63], memory[26,63], memory[27,63], memory[28,63], memory[29,63], memory[30,63], memory[31,63], memory[32,63], memory[33,63], memory[34,63], memory[35,63], memory[36,63], memory[37,63], memory[38,63], memory[39,63], memory[40,63], memory[41,63], memory[42,63], memory[43,63], memory[44,63], memory[45,63], memory[46,63], memory[47,63], memory[48,63], memory[49,63], memory[50,63], memory[51,63], memory[52,63], memory[53,63], memory[54,63], memory[55,63], memory[56,63], memory[57,63], memory[58,63], memory[59,63], memory[60,63], memory[61,63], memory[62,63], memory[63,63], memory[64,63], memory[65,63], memory[66,63], memory[67,63], memory[68,63], memory[69,63], memory[70,63], memory[71,63], memory[72,63], memory[73,63], memory[74,63], memory[75,63], memory[76,63], memory[77,63], memory[78,63], memory[79,63], memory[80,63], memory[81,63], memory[82,63], memory[83,63], memory[84,63], memory[85,63], memory[86,63], memory[87,63], memory[88,63], memory[89,63], memory[90,63], memory[91,63], memory[92,63], memory[93,63], memory[94,63], memory[95,63], memory[96,63], memory[97,63], memory[98,63], memory[99,63], memory[100,63], memory[101,63], memory[102,63], memory[103,63], memory[104,63], memory[105,63], memory[106,63], memory[107,63], memory[108,63], memory[109,63], memory[110,63], memory[111,63], memory[112,63], memory[113,63], memory[114,63], memory[115,63], memory[116,63], memory[117,63], memory[118,63], memory[119,63], memory[120,63], memory[1,64], memory[2,64], memory[3,64], memory[4,64], memory[5,64], memory[6,64], memory[7,64], memory[8,64], memory[9,64], memory[10,64], memory[11,64], memory[12,64], memory[13,64], memory[14,64], memory[15,64], memory[16,64], memory[17,64], memory[18,64], memory[19,64], memory[20,64], memory[21,64], memory[22,64], memory[23,64], memory[24,64], memory[25,64], memory[26,64], memory[27,64], memory[28,64], memory[29,64], memory[30,64], memory[31,64], memory[32,64], memory[33,64], memory[34,64], memory[35,64], memory[36,64], memory[37,64], memory[38,64], memory[39,64], memory[40,64], memory[41,64], memory[42,64], memory[43,64], memory[44,64], memory[45,64], memory[46,64], memory[47,64], memory[48,64], memory[49,64], memory[50,64], memory[51,64], memory[52,64], memory[53,64], memory[54,64], memory[55,64], memory[56,64], memory[57,64], memory[58,64], memory[59,64], memory[60,64], memory[61,64], memory[62,64], memory[63,64], memory[64,64], memory[65,64], memory[66,64], memory[67,64], memory[68,64], memory[69,64], memory[70,64], memory[71,64], memory[72,64], memory[73,64], memory[74,64], memory[75,64], memory[76,64], memory[77,64], memory[78,64], memory[79,64], memory[80,64], memory[81,64], memory[82,64], memory[83,64], memory[84,64], memory[85,64], memory[86,64], memory[87,64], memory[88,64], memory[89,64], memory[90,64], memory[91,64], memory[92,64], memory[93,64], memory[94,64], memory[95,64], memory[96,64], memory[97,64], memory[98,64], memory[99,64], memory[100,64], memory[101,64], memory[102,64], memory[103,64], memory[104,64], memory[105,64], memory[106,64], memory[107,64], memory[108,64], memory[109,64], memory[110,64], memory[111,64], memory[112,64], memory[113,64], memory[114,64], memory[115,64], memory[116,64], memory[117,64], memory[118,64], memory[119,64], memory[120,64], memory[1,65], memory[2,65], memory[3,65], memory[4,65], memory[5,65], memory[6,65], memory[7,65], memory[8,65], memory[9,65], memory[10,65], memory[11,65], memory[12,65], memory[13,65], memory[14,65], memory[15,65], memory[16,65], memory[17,65], memory[18,65], memory[19,65], memory[20,65], memory[21,65], memory[22,65], memory[23,65], memory[24,65], memory[25,65], memory[26,65], memory[27,65], memory[28,65], memory[29,65], memory[30,65], memory[31,65], memory[32,65], memory[33,65], memory[34,65], memory[35,65], memory[36,65], memory[37,65], memory[38,65], memory[39,65], memory[40,65], memory[41,65], memory[42,65], memory[43,65], memory[44,65], memory[45,65], memory[46,65], memory[47,65], memory[48,65], memory[49,65], memory[50,65], memory[51,65], memory[52,65], memory[53,65], memory[54,65], memory[55,65], memory[56,65], memory[57,65], memory[58,65], memory[59,65], memory[60,65], memory[61,65], memory[62,65], memory[63,65], memory[64,65], memory[65,65], memory[66,65], memory[67,65], memory[68,65], memory[69,65], memory[70,65], memory[71,65], memory[72,65], memory[73,65], memory[74,65], memory[75,65], memory[76,65], memory[77,65], memory[78,65], memory[79,65], memory[80,65], memory[81,65], memory[82,65], memory[83,65], memory[84,65], memory[85,65], memory[86,65], memory[87,65], memory[88,65], memory[89,65], memory[90,65], memory[91,65], memory[92,65], memory[93,65], memory[94,65], memory[95,65], memory[96,65], memory[97,65], memory[98,65], memory[99,65], memory[100,65], memory[101,65], memory[102,65], memory[103,65], memory[104,65], memory[105,65], memory[106,65], memory[107,65], memory[108,65], memory[109,65], memory[110,65], memory[111,65], memory[112,65], memory[113,65], memory[114,65], memory[115,65], memory[116,65], memory[117,65], memory[118,65], memory[119,65], memory[120,65], memory[1,66], memory[2,66], memory[3,66], memory[4,66], memory[5,66], memory[6,66], memory[7,66], memory[8,66], memory[9,66], memory[10,66], memory[11,66], memory[12,66], memory[13,66], memory[14,66], memory[15,66], memory[16,66], memory[17,66], memory[18,66], memory[19,66], memory[20,66], memory[21,66], memory[22,66], memory[23,66], memory[24,66], memory[25,66], memory[26,66], memory[27,66], memory[28,66], memory[29,66], memory[30,66], memory[31,66], memory[32,66], memory[33,66], memory[34,66], memory[35,66], memory[36,66], memory[37,66], memory[38,66], memory[39,66], memory[40,66], memory[41,66], memory[42,66], memory[43,66], memory[44,66], memory[45,66], memory[46,66], memory[47,66], memory[48,66], memory[49,66], memory[50,66], memory[51,66], memory[52,66], memory[53,66], memory[54,66], memory[55,66], memory[56,66], memory[57,66], memory[58,66], memory[59,66], memory[60,66], memory[61,66], memory[62,66], memory[63,66], memory[64,66], memory[65,66], memory[66,66], memory[67,66], memory[68,66], memory[69,66], memory[70,66], memory[71,66], memory[72,66], memory[73,66], memory[74,66], memory[75,66], memory[76,66], memory[77,66], memory[78,66], memory[79,66], memory[80,66], memory[81,66], memory[82,66], memory[83,66], memory[84,66], memory[85,66], memory[86,66], memory[87,66], memory[88,66], memory[89,66], memory[90,66], memory[91,66], memory[92,66], memory[93,66], memory[94,66], memory[95,66], memory[96,66], memory[97,66], memory[98,66], memory[99,66], memory[100,66], memory[101,66], memory[102,66], memory[103,66], memory[104,66], memory[105,66], memory[106,66], memory[107,66], memory[108,66], memory[109,66], memory[110,66], memory[111,66], memory[112,66], memory[113,66], memory[114,66], memory[115,66], memory[116,66], memory[117,66], memory[118,66], memory[119,66], memory[120,66], memory[1,67], memory[2,67], memory[3,67], memory[4,67], memory[5,67], memory[6,67], memory[7,67], memory[8,67], memory[9,67], memory[10,67], memory[11,67], memory[12,67], memory[13,67], memory[14,67], memory[15,67], memory[16,67], memory[17,67], memory[18,67], memory[19,67], memory[20,67], memory[21,67], memory[22,67], memory[23,67], memory[24,67], memory[25,67], memory[26,67], memory[27,67], memory[28,67], memory[29,67], memory[30,67], memory[31,67], memory[32,67], memory[33,67], memory[34,67], memory[35,67], memory[36,67], memory[37,67], memory[38,67], memory[39,67], memory[40,67], memory[41,67], memory[42,67], memory[43,67], memory[44,67], memory[45,67], memory[46,67], memory[47,67], memory[48,67], memory[49,67], memory[50,67], memory[51,67], memory[52,67], memory[53,67], memory[54,67], memory[55,67], memory[56,67], memory[57,67], memory[58,67], memory[59,67], memory[60,67], memory[61,67], memory[62,67], memory[63,67], memory[64,67], memory[65,67], memory[66,67], memory[67,67], memory[68,67], memory[69,67], memory[70,67], memory[71,67], memory[72,67], memory[73,67], memory[74,67], memory[75,67], memory[76,67], memory[77,67], memory[78,67], memory[79,67], memory[80,67], memory[81,67], memory[82,67], memory[83,67], memory[84,67], memory[85,67], memory[86,67], memory[87,67], memory[88,67], memory[89,67], memory[90,67], memory[91,67], memory[92,67], memory[93,67], memory[94,67], memory[95,67], memory[96,67], memory[97,67], memory[98,67], memory[99,67], memory[100,67], memory[101,67], memory[102,67], memory[103,67], memory[104,67], memory[105,67], memory[106,67], memory[107,67], memory[108,67], memory[109,67], memory[110,67], memory[111,67], memory[112,67], memory[113,67], memory[114,67], memory[115,67], memory[116,67], memory[117,67], memory[118,67], memory[119,67], memory[120,67], memory[1,68], memory[2,68], memory[3,68], memory[4,68], memory[5,68], memory[6,68], memory[7,68], memory[8,68], memory[9,68], memory[10,68], memory[11,68], memory[12,68], memory[13,68], memory[14,68], memory[15,68], memory[16,68], memory[17,68], memory[18,68], memory[19,68], memory[20,68], memory[21,68], memory[22,68], memory[23,68], memory[24,68], memory[25,68], memory[26,68], memory[27,68], memory[28,68], memory[29,68], memory[30,68], memory[31,68], memory[32,68], memory[33,68], memory[34,68], memory[35,68], memory[36,68], memory[37,68], memory[38,68], memory[39,68], memory[40,68], memory[41,68], memory[42,68], memory[43,68], memory[44,68], memory[45,68], memory[46,68], memory[47,68], memory[48,68], memory[49,68], memory[50,68], memory[51,68], memory[52,68], memory[53,68], memory[54,68], memory[55,68], memory[56,68], memory[57,68], memory[58,68], memory[59,68], memory[60,68], memory[61,68], memory[62,68], memory[63,68], memory[64,68], memory[65,68], memory[66,68], memory[67,68], memory[68,68], memory[69,68], memory[70,68], memory[71,68], memory[72,68], memory[73,68], memory[74,68], memory[75,68], memory[76,68], memory[77,68], memory[78,68], memory[79,68], memory[80,68], memory[81,68], memory[82,68], memory[83,68], memory[84,68], memory[85,68], memory[86,68], memory[87,68], memory[88,68], memory[89,68], memory[90,68], memory[91,68], memory[92,68], memory[93,68], memory[94,68], memory[95,68], memory[96,68], memory[97,68], memory[98,68], memory[99,68], memory[100,68], memory[101,68], memory[102,68], memory[103,68], memory[104,68], memory[105,68], memory[106,68], memory[107,68], memory[108,68], memory[109,68], memory[110,68], memory[111,68], memory[112,68], memory[113,68], memory[114,68], memory[115,68], memory[116,68], memory[117,68], memory[118,68], memory[119,68], memory[120,68], memory[1,69], memory[2,69], memory[3,69], memory[4,69], memory[5,69], memory[6,69], memory[7,69], memory[8,69], memory[9,69], memory[10,69], memory[11,69], memory[12,69], memory[13,69], memory[14,69], memory[15,69], memory[16,69], memory[17,69], memory[18,69], memory[19,69], memory[20,69], memory[21,69], memory[22,69], memory[23,69], memory[24,69], memory[25,69], memory[26,69], memory[27,69], memory[28,69], memory[29,69], memory[30,69], memory[31,69], memory[32,69], memory[33,69], memory[34,69], memory[35,69], memory[36,69], memory[37,69], memory[38,69], memory[39,69], memory[40,69], memory[41,69], memory[42,69], memory[43,69], memory[44,69], memory[45,69], memory[46,69], memory[47,69], memory[48,69], memory[49,69], memory[50,69], memory[51,69], memory[52,69], memory[53,69], memory[54,69], memory[55,69], memory[56,69], memory[57,69], memory[58,69], memory[59,69], memory[60,69], memory[61,69], memory[62,69], memory[63,69], memory[64,69], memory[65,69], memory[66,69], memory[67,69], memory[68,69], memory[69,69], memory[70,69], memory[71,69], memory[72,69], memory[73,69], memory[74,69], memory[75,69], memory[76,69], memory[77,69], memory[78,69], memory[79,69], memory[80,69], memory[81,69], memory[82,69], memory[83,69], memory[84,69], memory[85,69], memory[86,69], memory[87,69], memory[88,69], memory[89,69], memory[90,69], memory[91,69], memory[92,69], memory[93,69], memory[94,69], memory[95,69], memory[96,69], memory[97,69], memory[98,69], memory[99,69], memory[100,69], memory[101,69], memory[102,69], memory[103,69], memory[104,69], memory[105,69], memory[106,69], memory[107,69], memory[108,69], memory[109,69], memory[110,69], memory[111,69], memory[112,69], memory[113,69], memory[114,69], memory[115,69], memory[116,69], memory[117,69], memory[118,69], memory[119,69], memory[120,69], memory[1,70], memory[2,70], memory[3,70], memory[4,70], memory[5,70], memory[6,70], memory[7,70], memory[8,70], memory[9,70], memory[10,70], memory[11,70], memory[12,70], memory[13,70], memory[14,70], memory[15,70], memory[16,70], memory[17,70], memory[18,70], memory[19,70], memory[20,70], memory[21,70], memory[22,70], memory[23,70], memory[24,70], memory[25,70], memory[26,70], memory[27,70], memory[28,70], memory[29,70], memory[30,70], memory[31,70], memory[32,70], memory[33,70], memory[34,70], memory[35,70], memory[36,70], memory[37,70], memory[38,70], memory[39,70], memory[40,70], memory[41,70], memory[42,70], memory[43,70], memory[44,70], memory[45,70], memory[46,70], memory[47,70], memory[48,70], memory[49,70], memory[50,70], memory[51,70], memory[52,70], memory[53,70], memory[54,70], memory[55,70], memory[56,70], memory[57,70], memory[58,70], memory[59,70], memory[60,70], memory[61,70], memory[62,70], memory[63,70], memory[64,70], memory[65,70], memory[66,70], memory[67,70], memory[68,70], memory[69,70], memory[70,70], memory[71,70], memory[72,70], memory[73,70], memory[74,70], memory[75,70], memory[76,70], memory[77,70], memory[78,70], memory[79,70], memory[80,70], memory[81,70], memory[82,70], memory[83,70], memory[84,70], memory[85,70], memory[86,70], memory[87,70], memory[88,70], memory[89,70], memory[90,70], memory[91,70], memory[92,70], memory[93,70], memory[94,70], memory[95,70], memory[96,70], memory[97,70], memory[98,70], memory[99,70], memory[100,70], memory[101,70], memory[102,70], memory[103,70], memory[104,70], memory[105,70], memory[106,70], memory[107,70], memory[108,70], memory[109,70], memory[110,70], memory[111,70], memory[112,70], memory[113,70], memory[114,70], memory[115,70], memory[116,70], memory[117,70], memory[118,70], memory[119,70], memory[120,70], memory[1,71], memory[2,71], memory[3,71], memory[4,71], memory[5,71], memory[6,71], memory[7,71], memory[8,71], memory[9,71], memory[10,71], memory[11,71], memory[12,71], memory[13,71], memory[14,71], memory[15,71], memory[16,71], memory[17,71], memory[18,71], memory[19,71], memory[20,71], memory[21,71], memory[22,71], memory[23,71], memory[24,71], memory[25,71], memory[26,71], memory[27,71], memory[28,71], memory[29,71], memory[30,71], memory[31,71], memory[32,71], memory[33,71], memory[34,71], memory[35,71], memory[36,71], memory[37,71], memory[38,71], memory[39,71], memory[40,71], memory[41,71], memory[42,71], memory[43,71], memory[44,71], memory[45,71], memory[46,71], memory[47,71], memory[48,71], memory[49,71], memory[50,71], memory[51,71], memory[52,71], memory[53,71], memory[54,71], memory[55,71], memory[56,71], memory[57,71], memory[58,71], memory[59,71], memory[60,71], memory[61,71], memory[62,71], memory[63,71], memory[64,71], memory[65,71], memory[66,71], memory[67,71], memory[68,71], memory[69,71], memory[70,71], memory[71,71], memory[72,71], memory[73,71], memory[74,71], memory[75,71], memory[76,71], memory[77,71], memory[78,71], memory[79,71], memory[80,71], memory[81,71], memory[82,71], memory[83,71], memory[84,71], memory[85,71], memory[86,71], memory[87,71], memory[88,71], memory[89,71], memory[90,71], memory[91,71], memory[92,71], memory[93,71], memory[94,71], memory[95,71], memory[96,71], memory[97,71], memory[98,71], memory[99,71], memory[100,71], memory[101,71], memory[102,71], memory[103,71], memory[104,71], memory[105,71], memory[106,71], memory[107,71], memory[108,71], memory[109,71], memory[110,71], memory[111,71], memory[112,71], memory[113,71], memory[114,71], memory[115,71], memory[116,71], memory[117,71], memory[118,71], memory[119,71], memory[120,71], memory[1,72], memory[2,72], memory[3,72], memory[4,72], memory[5,72], memory[6,72], memory[7,72], memory[8,72], memory[9,72], memory[10,72], memory[11,72], memory[12,72], memory[13,72], memory[14,72], memory[15,72], memory[16,72], memory[17,72], memory[18,72], memory[19,72], memory[20,72], memory[21,72], memory[22,72], memory[23,72], memory[24,72], memory[25,72], memory[26,72], memory[27,72], memory[28,72], memory[29,72], memory[30,72], memory[31,72], memory[32,72], memory[33,72], memory[34,72], memory[35,72], memory[36,72], memory[37,72], memory[38,72], memory[39,72], memory[40,72], memory[41,72], memory[42,72], memory[43,72], memory[44,72], memory[45,72], memory[46,72], memory[47,72], memory[48,72], memory[49,72], memory[50,72], memory[51,72], memory[52,72], memory[53,72], memory[54,72], memory[55,72], memory[56,72], memory[57,72], memory[58,72], memory[59,72], memory[60,72], memory[61,72], memory[62,72], memory[63,72], memory[64,72], memory[65,72], memory[66,72], memory[67,72], memory[68,72], memory[69,72], memory[70,72], memory[71,72], memory[72,72], memory[73,72], memory[74,72], memory[75,72], memory[76,72], memory[77,72], memory[78,72], memory[79,72], memory[80,72], memory[81,72], memory[82,72], memory[83,72], memory[84,72], memory[85,72], memory[86,72], memory[87,72], memory[88,72], memory[89,72], memory[90,72], memory[91,72], memory[92,72], memory[93,72], memory[94,72], memory[95,72], memory[96,72], memory[97,72], memory[98,72], memory[99,72], memory[100,72], memory[101,72], memory[102,72], memory[103,72], memory[104,72], memory[105,72], memory[106,72], memory[107,72], memory[108,72], memory[109,72], memory[110,72], memory[111,72], memory[112,72], memory[113,72], memory[114,72], memory[115,72], memory[116,72], memory[117,72], memory[118,72], memory[119,72], memory[120,72], memory[1,73], memory[2,73], memory[3,73], memory[4,73], memory[5,73], memory[6,73], memory[7,73], memory[8,73], memory[9,73], memory[10,73], memory[11,73], memory[12,73], memory[13,73], memory[14,73], memory[15,73], memory[16,73], memory[17,73], memory[18,73], memory[19,73], memory[20,73], memory[21,73], memory[22,73], memory[23,73], memory[24,73], memory[25,73], memory[26,73], memory[27,73], memory[28,73], memory[29,73], memory[30,73], memory[31,73], memory[32,73], memory[33,73], memory[34,73], memory[35,73], memory[36,73], memory[37,73], memory[38,73], memory[39,73], memory[40,73], memory[41,73], memory[42,73], memory[43,73], memory[44,73], memory[45,73], memory[46,73], memory[47,73], memory[48,73], memory[49,73], memory[50,73], memory[51,73], memory[52,73], memory[53,73], memory[54,73], memory[55,73], memory[56,73], memory[57,73], memory[58,73], memory[59,73], memory[60,73], memory[61,73], memory[62,73], memory[63,73], memory[64,73], memory[65,73], memory[66,73], memory[67,73], memory[68,73], memory[69,73], memory[70,73], memory[71,73], memory[72,73], memory[73,73], memory[74,73], memory[75,73], memory[76,73], memory[77,73], memory[78,73], memory[79,73], memory[80,73], memory[81,73], memory[82,73], memory[83,73], memory[84,73], memory[85,73], memory[86,73], memory[87,73], memory[88,73], memory[89,73], memory[90,73], memory[91,73], memory[92,73], memory[93,73], memory[94,73], memory[95,73], memory[96,73], memory[97,73], memory[98,73], memory[99,73], memory[100,73], memory[101,73], memory[102,73], memory[103,73], memory[104,73], memory[105,73], memory[106,73], memory[107,73], memory[108,73], memory[109,73], memory[110,73], memory[111,73], memory[112,73], memory[113,73], memory[114,73], memory[115,73], memory[116,73], memory[117,73], memory[118,73], memory[119,73], memory[120,73], memory[1,74], memory[2,74], memory[3,74], memory[4,74], memory[5,74], memory[6,74], memory[7,74], memory[8,74], memory[9,74], memory[10,74], memory[11,74], memory[12,74], memory[13,74], memory[14,74], memory[15,74], memory[16,74], memory[17,74], memory[18,74], memory[19,74], memory[20,74], memory[21,74], memory[22,74], memory[23,74], memory[24,74], memory[25,74], memory[26,74], memory[27,74], memory[28,74], memory[29,74], memory[30,74], memory[31,74], memory[32,74], memory[33,74], memory[34,74], memory[35,74], memory[36,74], memory[37,74], memory[38,74], memory[39,74], memory[40,74], memory[41,74], memory[42,74], memory[43,74], memory[44,74], memory[45,74], memory[46,74], memory[47,74], memory[48,74], memory[49,74], memory[50,74], memory[51,74], memory[52,74], memory[53,74], memory[54,74], memory[55,74], memory[56,74], memory[57,74], memory[58,74], memory[59,74], memory[60,74], memory[61,74], memory[62,74], memory[63,74], memory[64,74], memory[65,74], memory[66,74], memory[67,74], memory[68,74], memory[69,74], memory[70,74], memory[71,74], memory[72,74], memory[73,74], memory[74,74], memory[75,74], memory[76,74], memory[77,74], memory[78,74], memory[79,74], memory[80,74], memory[81,74], memory[82,74], memory[83,74], memory[84,74], memory[85,74], memory[86,74], memory[87,74], memory[88,74], memory[89,74], memory[90,74], memory[91,74], memory[92,74], memory[93,74], memory[94,74], memory[95,74], memory[96,74], memory[97,74], memory[98,74], memory[99,74], memory[100,74], memory[101,74], memory[102,74], memory[103,74], memory[104,74], memory[105,74], memory[106,74], memory[107,74], memory[108,74], memory[109,74], memory[110,74], memory[111,74], memory[112,74], memory[113,74], memory[114,74], memory[115,74], memory[116,74], memory[117,74], memory[118,74], memory[119,74], memory[120,74], memory[1,75], memory[2,75], memory[3,75], memory[4,75], memory[5,75], memory[6,75], memory[7,75], memory[8,75], memory[9,75], memory[10,75], memory[11,75], memory[12,75], memory[13,75], memory[14,75], memory[15,75], memory[16,75], memory[17,75], memory[18,75], memory[19,75], memory[20,75], memory[21,75], memory[22,75], memory[23,75], memory[24,75], memory[25,75], memory[26,75], memory[27,75], memory[28,75], memory[29,75], memory[30,75], memory[31,75], memory[32,75], memory[33,75], memory[34,75], memory[35,75], memory[36,75], memory[37,75], memory[38,75], memory[39,75], memory[40,75], memory[41,75], memory[42,75], memory[43,75], memory[44,75], memory[45,75], memory[46,75], memory[47,75], memory[48,75], memory[49,75], memory[50,75], memory[51,75], memory[52,75], memory[53,75], memory[54,75], memory[55,75], memory[56,75], memory[57,75], memory[58,75], memory[59,75], memory[60,75], memory[61,75], memory[62,75], memory[63,75], memory[64,75], memory[65,75], memory[66,75], memory[67,75], memory[68,75], memory[69,75], memory[70,75], memory[71,75], memory[72,75], memory[73,75], memory[74,75], memory[75,75], memory[76,75], memory[77,75], memory[78,75], memory[79,75], memory[80,75], memory[81,75], memory[82,75], memory[83,75], memory[84,75], memory[85,75], memory[86,75], memory[87,75], memory[88,75], memory[89,75], memory[90,75], memory[91,75], memory[92,75], memory[93,75], memory[94,75], memory[95,75], memory[96,75], memory[97,75], memory[98,75], memory[99,75], memory[100,75], memory[101,75], memory[102,75], memory[103,75], memory[104,75], memory[105,75], memory[106,75], memory[107,75], memory[108,75], memory[109,75], memory[110,75], memory[111,75], memory[112,75], memory[113,75], memory[114,75], memory[115,75], memory[116,75], memory[117,75], memory[118,75], memory[119,75], memory[120,75], memory[1,76], memory[2,76], memory[3,76], memory[4,76], memory[5,76], memory[6,76], memory[7,76], memory[8,76], memory[9,76], memory[10,76], memory[11,76], memory[12,76], memory[13,76], memory[14,76], memory[15,76], memory[16,76], memory[17,76], memory[18,76], memory[19,76], memory[20,76], memory[21,76], memory[22,76], memory[23,76], memory[24,76], memory[25,76], memory[26,76], memory[27,76], memory[28,76], memory[29,76], memory[30,76], memory[31,76], memory[32,76], memory[33,76], memory[34,76], memory[35,76], memory[36,76], memory[37,76], memory[38,76], memory[39,76], memory[40,76], memory[41,76], memory[42,76], memory[43,76], memory[44,76], memory[45,76], memory[46,76], memory[47,76], memory[48,76], memory[49,76], memory[50,76], memory[51,76], memory[52,76], memory[53,76], memory[54,76], memory[55,76], memory[56,76], memory[57,76], memory[58,76], memory[59,76], memory[60,76], memory[61,76], memory[62,76], memory[63,76], memory[64,76], memory[65,76], memory[66,76], memory[67,76], memory[68,76], memory[69,76], memory[70,76], memory[71,76], memory[72,76], memory[73,76], memory[74,76], memory[75,76], memory[76,76], memory[77,76], memory[78,76], memory[79,76], memory[80,76], memory[81,76], memory[82,76], memory[83,76], memory[84,76], memory[85,76], memory[86,76], memory[87,76], memory[88,76], memory[89,76], memory[90,76], memory[91,76], memory[92,76], memory[93,76], memory[94,76], memory[95,76], memory[96,76], memory[97,76], memory[98,76], memory[99,76], memory[100,76], memory[101,76], memory[102,76], memory[103,76], memory[104,76], memory[105,76], memory[106,76], memory[107,76], memory[108,76], memory[109,76], memory[110,76], memory[111,76], memory[112,76], memory[113,76], memory[114,76], memory[115,76], memory[116,76], memory[117,76], memory[118,76], memory[119,76], memory[120,76], memory[1,77], memory[2,77], memory[3,77], memory[4,77], memory[5,77], memory[6,77], memory[7,77], memory[8,77], memory[9,77], memory[10,77], memory[11,77], memory[12,77], memory[13,77], memory[14,77], memory[15,77], memory[16,77], memory[17,77], memory[18,77], memory[19,77], memory[20,77], memory[21,77], memory[22,77], memory[23,77], memory[24,77], memory[25,77], memory[26,77], memory[27,77], memory[28,77], memory[29,77], memory[30,77], memory[31,77], memory[32,77], memory[33,77], memory[34,77], memory[35,77], memory[36,77], memory[37,77], memory[38,77], memory[39,77], memory[40,77], memory[41,77], memory[42,77], memory[43,77], memory[44,77], memory[45,77], memory[46,77], memory[47,77], memory[48,77], memory[49,77], memory[50,77], memory[51,77], memory[52,77], memory[53,77], memory[54,77], memory[55,77], memory[56,77], memory[57,77], memory[58,77], memory[59,77], memory[60,77], memory[61,77], memory[62,77], memory[63,77], memory[64,77], memory[65,77], memory[66,77], memory[67,77], memory[68,77], memory[69,77], memory[70,77], memory[71,77], memory[72,77], memory[73,77], memory[74,77], memory[75,77], memory[76,77], memory[77,77], memory[78,77], memory[79,77], memory[80,77], memory[81,77], memory[82,77], memory[83,77], memory[84,77], memory[85,77], memory[86,77], memory[87,77], memory[88,77], memory[89,77], memory[90,77], memory[91,77], memory[92,77], memory[93,77], memory[94,77], memory[95,77], memory[96,77], memory[97,77], memory[98,77], memory[99,77], memory[100,77], memory[101,77], memory[102,77], memory[103,77], memory[104,77], memory[105,77], memory[106,77], memory[107,77], memory[108,77], memory[109,77], memory[110,77], memory[111,77], memory[112,77], memory[113,77], memory[114,77], memory[115,77], memory[116,77], memory[117,77], memory[118,77], memory[119,77], memory[120,77], memory[1,78], memory[2,78], memory[3,78], memory[4,78], memory[5,78], memory[6,78], memory[7,78], memory[8,78], memory[9,78], memory[10,78], memory[11,78], memory[12,78], memory[13,78], memory[14,78], memory[15,78], memory[16,78], memory[17,78], memory[18,78], memory[19,78], memory[20,78], memory[21,78], memory[22,78], memory[23,78], memory[24,78], memory[25,78], memory[26,78], memory[27,78], memory[28,78], memory[29,78], memory[30,78], memory[31,78], memory[32,78], memory[33,78], memory[34,78], memory[35,78], memory[36,78], memory[37,78], memory[38,78], memory[39,78], memory[40,78], memory[41,78], memory[42,78], memory[43,78], memory[44,78], memory[45,78], memory[46,78], memory[47,78], memory[48,78], memory[49,78], memory[50,78], memory[51,78], memory[52,78], memory[53,78], memory[54,78], memory[55,78], memory[56,78], memory[57,78], memory[58,78], memory[59,78], memory[60,78], memory[61,78], memory[62,78], memory[63,78], memory[64,78], memory[65,78], memory[66,78], memory[67,78], memory[68,78], memory[69,78], memory[70,78], memory[71,78], memory[72,78], memory[73,78], memory[74,78], memory[75,78], memory[76,78], memory[77,78], memory[78,78], memory[79,78], memory[80,78], memory[81,78], memory[82,78], memory[83,78], memory[84,78], memory[85,78], memory[86,78], memory[87,78], memory[88,78], memory[89,78], memory[90,78], memory[91,78], memory[92,78], memory[93,78], memory[94,78], memory[95,78], memory[96,78], memory[97,78], memory[98,78], memory[99,78], memory[100,78], memory[101,78], memory[102,78], memory[103,78], memory[104,78], memory[105,78], memory[106,78], memory[107,78], memory[108,78], memory[109,78], memory[110,78], memory[111,78], memory[112,78], memory[113,78], memory[114,78], memory[115,78], memory[116,78], memory[117,78], memory[118,78], memory[119,78], memory[120,78], memory[1,79], memory[2,79], memory[3,79], memory[4,79], memory[5,79], memory[6,79], memory[7,79], memory[8,79], memory[9,79], memory[10,79], memory[11,79], memory[12,79], memory[13,79], memory[14,79], memory[15,79], memory[16,79], memory[17,79], memory[18,79], memory[19,79], memory[20,79], memory[21,79], memory[22,79], memory[23,79], memory[24,79], memory[25,79], memory[26,79], memory[27,79], memory[28,79], memory[29,79], memory[30,79], memory[31,79], memory[32,79], memory[33,79], memory[34,79], memory[35,79], memory[36,79], memory[37,79], memory[38,79], memory[39,79], memory[40,79], memory[41,79], memory[42,79], memory[43,79], memory[44,79], memory[45,79], memory[46,79], memory[47,79], memory[48,79], memory[49,79], memory[50,79], memory[51,79], memory[52,79], memory[53,79], memory[54,79], memory[55,79], memory[56,79], memory[57,79], memory[58,79], memory[59,79], memory[60,79], memory[61,79], memory[62,79], memory[63,79], memory[64,79], memory[65,79], memory[66,79], memory[67,79], memory[68,79], memory[69,79], memory[70,79], memory[71,79], memory[72,79], memory[73,79], memory[74,79], memory[75,79], memory[76,79], memory[77,79], memory[78,79], memory[79,79], memory[80,79], memory[81,79], memory[82,79], memory[83,79], memory[84,79], memory[85,79], memory[86,79], memory[87,79], memory[88,79], memory[89,79], memory[90,79], memory[91,79], memory[92,79], memory[93,79], memory[94,79], memory[95,79], memory[96,79], memory[97,79], memory[98,79], memory[99,79], memory[100,79], memory[101,79], memory[102,79], memory[103,79], memory[104,79], memory[105,79], memory[106,79], memory[107,79], memory[108,79], memory[109,79], memory[110,79], memory[111,79], memory[112,79], memory[113,79], memory[114,79], memory[115,79], memory[116,79], memory[117,79], memory[118,79], memory[119,79], memory[120,79], memory[1,80], memory[2,80], memory[3,80], memory[4,80], memory[5,80], memory[6,80], memory[7,80], memory[8,80], memory[9,80], memory[10,80], memory[11,80], memory[12,80], memory[13,80], memory[14,80], memory[15,80], memory[16,80], memory[17,80], memory[18,80], memory[19,80], memory[20,80], memory[21,80], memory[22,80], memory[23,80], memory[24,80], memory[25,80], memory[26,80], memory[27,80], memory[28,80], memory[29,80], memory[30,80], memory[31,80], memory[32,80], memory[33,80], memory[34,80], memory[35,80], memory[36,80], memory[37,80], memory[38,80], memory[39,80], memory[40,80], memory[41,80], memory[42,80], memory[43,80], memory[44,80], memory[45,80], memory[46,80], memory[47,80], memory[48,80], memory[49,80], memory[50,80], memory[51,80], memory[52,80], memory[53,80], memory[54,80], memory[55,80], memory[56,80], memory[57,80], memory[58,80], memory[59,80], memory[60,80], memory[61,80], memory[62,80], memory[63,80], memory[64,80], memory[65,80], memory[66,80], memory[67,80], memory[68,80], memory[69,80], memory[70,80], memory[71,80], memory[72,80], memory[73,80], memory[74,80], memory[75,80], memory[76,80], memory[77,80], memory[78,80], memory[79,80], memory[80,80], memory[81,80], memory[82,80], memory[83,80], memory[84,80], memory[85,80], memory[86,80], memory[87,80], memory[88,80], memory[89,80], memory[90,80], memory[91,80], memory[92,80], memory[93,80], memory[94,80], memory[95,80], memory[96,80], memory[97,80], memory[98,80], memory[99,80], memory[100,80], memory[101,80], memory[102,80], memory[103,80], memory[104,80], memory[105,80], memory[106,80], memory[107,80], memory[108,80], memory[109,80], memory[110,80], memory[111,80], memory[112,80], memory[113,80], memory[114,80], memory[115,80], memory[116,80], memory[117,80], memory[118,80], memory[119,80], memory[120,80], memory[1,81], memory[2,81], memory[3,81], memory[4,81], memory[5,81], memory[6,81], memory[7,81], memory[8,81], memory[9,81], memory[10,81], memory[11,81], memory[12,81], memory[13,81], memory[14,81], memory[15,81], memory[16,81], memory[17,81], memory[18,81], memory[19,81], memory[20,81], memory[21,81], memory[22,81], memory[23,81], memory[24,81], memory[25,81], memory[26,81], memory[27,81], memory[28,81], memory[29,81], memory[30,81], memory[31,81], memory[32,81], memory[33,81], memory[34,81], memory[35,81], memory[36,81], memory[37,81], memory[38,81], memory[39,81], memory[40,81], memory[41,81], memory[42,81], memory[43,81], memory[44,81], memory[45,81], memory[46,81], memory[47,81], memory[48,81], memory[49,81], memory[50,81], memory[51,81], memory[52,81], memory[53,81], memory[54,81], memory[55,81], memory[56,81], memory[57,81], memory[58,81], memory[59,81], memory[60,81], memory[61,81], memory[62,81], memory[63,81], memory[64,81], memory[65,81], memory[66,81], memory[67,81], memory[68,81], memory[69,81], memory[70,81], memory[71,81], memory[72,81], memory[73,81], memory[74,81], memory[75,81], memory[76,81], memory[77,81], memory[78,81], memory[79,81], memory[80,81], memory[81,81], memory[82,81], memory[83,81], memory[84,81], memory[85,81], memory[86,81], memory[87,81], memory[88,81], memory[89,81], memory[90,81], memory[91,81], memory[92,81], memory[93,81], memory[94,81], memory[95,81], memory[96,81], memory[97,81], memory[98,81], memory[99,81], memory[100,81], memory[101,81], memory[102,81], memory[103,81], memory[104,81], memory[105,81], memory[106,81], memory[107,81], memory[108,81], memory[109,81], memory[110,81], memory[111,81], memory[112,81], memory[113,81], memory[114,81], memory[115,81], memory[116,81], memory[117,81], memory[118,81], memory[119,81], memory[120,81], memory[1,82], memory[2,82], memory[3,82], memory[4,82], memory[5,82], memory[6,82], memory[7,82], memory[8,82], memory[9,82], memory[10,82], memory[11,82], memory[12,82], memory[13,82], memory[14,82], memory[15,82], memory[16,82], memory[17,82], memory[18,82], memory[19,82], memory[20,82], memory[21,82], memory[22,82], memory[23,82], memory[24,82], memory[25,82], memory[26,82], memory[27,82], memory[28,82], memory[29,82], memory[30,82], memory[31,82], memory[32,82], memory[33,82], memory[34,82], memory[35,82], memory[36,82], memory[37,82], memory[38,82], memory[39,82], memory[40,82], memory[41,82], memory[42,82], memory[43,82], memory[44,82], memory[45,82], memory[46,82], memory[47,82], memory[48,82], memory[49,82], memory[50,82], memory[51,82], memory[52,82], memory[53,82], memory[54,82], memory[55,82], memory[56,82], memory[57,82], memory[58,82], memory[59,82], memory[60,82], memory[61,82], memory[62,82], memory[63,82], memory[64,82], memory[65,82], memory[66,82], memory[67,82], memory[68,82], memory[69,82], memory[70,82], memory[71,82], memory[72,82], memory[73,82], memory[74,82], memory[75,82], memory[76,82], memory[77,82], memory[78,82], memory[79,82], memory[80,82], memory[81,82], memory[82,82], memory[83,82], memory[84,82], memory[85,82], memory[86,82], memory[87,82], memory[88,82], memory[89,82], memory[90,82], memory[91,82], memory[92,82], memory[93,82], memory[94,82], memory[95,82], memory[96,82], memory[97,82], memory[98,82], memory[99,82], memory[100,82], memory[101,82], memory[102,82], memory[103,82], memory[104,82], memory[105,82], memory[106,82], memory[107,82], memory[108,82], memory[109,82], memory[110,82], memory[111,82], memory[112,82], memory[113,82], memory[114,82], memory[115,82], memory[116,82], memory[117,82], memory[118,82], memory[119,82], memory[120,82], memory[1,83], memory[2,83], memory[3,83], memory[4,83], memory[5,83], memory[6,83], memory[7,83], memory[8,83], memory[9,83], memory[10,83], memory[11,83], memory[12,83], memory[13,83], memory[14,83], memory[15,83], memory[16,83], memory[17,83], memory[18,83], memory[19,83], memory[20,83], memory[21,83], memory[22,83], memory[23,83], memory[24,83], memory[25,83], memory[26,83], memory[27,83], memory[28,83], memory[29,83], memory[30,83], memory[31,83], memory[32,83], memory[33,83], memory[34,83], memory[35,83], memory[36,83], memory[37,83], memory[38,83], memory[39,83], memory[40,83], memory[41,83], memory[42,83], memory[43,83], memory[44,83], memory[45,83], memory[46,83], memory[47,83], memory[48,83], memory[49,83], memory[50,83], memory[51,83], memory[52,83], memory[53,83], memory[54,83], memory[55,83], memory[56,83], memory[57,83], memory[58,83], memory[59,83], memory[60,83], memory[61,83], memory[62,83], memory[63,83], memory[64,83], memory[65,83], memory[66,83], memory[67,83], memory[68,83], memory[69,83], memory[70,83], memory[71,83], memory[72,83], memory[73,83], memory[74,83], memory[75,83], memory[76,83], memory[77,83], memory[78,83], memory[79,83], memory[80,83], memory[81,83], memory[82,83], memory[83,83], memory[84,83], memory[85,83], memory[86,83], memory[87,83], memory[88,83], memory[89,83], memory[90,83], memory[91,83], memory[92,83], memory[93,83], memory[94,83], memory[95,83], memory[96,83], memory[97,83], memory[98,83], memory[99,83], memory[100,83], memory[101,83], memory[102,83], memory[103,83], memory[104,83], memory[105,83], memory[106,83], memory[107,83], memory[108,83], memory[109,83], memory[110,83], memory[111,83], memory[112,83], memory[113,83], memory[114,83], memory[115,83], memory[116,83], memory[117,83], memory[118,83], memory[119,83], memory[120,83], memory[1,84], memory[2,84], memory[3,84], memory[4,84], memory[5,84], memory[6,84], memory[7,84], memory[8,84], memory[9,84], memory[10,84], memory[11,84], memory[12,84], memory[13,84], memory[14,84], memory[15,84], memory[16,84], memory[17,84], memory[18,84], memory[19,84], memory[20,84], memory[21,84], memory[22,84], memory[23,84], memory[24,84], memory[25,84], memory[26,84], memory[27,84], memory[28,84], memory[29,84], memory[30,84], memory[31,84], memory[32,84], memory[33,84], memory[34,84], memory[35,84], memory[36,84], memory[37,84], memory[38,84], memory[39,84], memory[40,84], memory[41,84], memory[42,84], memory[43,84], memory[44,84], memory[45,84], memory[46,84], memory[47,84], memory[48,84], memory[49,84], memory[50,84], memory[51,84], memory[52,84], memory[53,84], memory[54,84], memory[55,84], memory[56,84], memory[57,84], memory[58,84], memory[59,84], memory[60,84], memory[61,84], memory[62,84], memory[63,84], memory[64,84], memory[65,84], memory[66,84], memory[67,84], memory[68,84], memory[69,84], memory[70,84], memory[71,84], memory[72,84], memory[73,84], memory[74,84], memory[75,84], memory[76,84], memory[77,84], memory[78,84], memory[79,84], memory[80,84], memory[81,84], memory[82,84], memory[83,84], memory[84,84], memory[85,84], memory[86,84], memory[87,84], memory[88,84], memory[89,84], memory[90,84], memory[91,84], memory[92,84], memory[93,84], memory[94,84], memory[95,84], memory[96,84], memory[97,84], memory[98,84], memory[99,84], memory[100,84], memory[101,84], memory[102,84], memory[103,84], memory[104,84], memory[105,84], memory[106,84], memory[107,84], memory[108,84], memory[109,84], memory[110,84], memory[111,84], memory[112,84], memory[113,84], memory[114,84], memory[115,84], memory[116,84], memory[117,84], memory[118,84], memory[119,84], memory[120,84], memory[1,85], memory[2,85], memory[3,85], memory[4,85], memory[5,85], memory[6,85], memory[7,85], memory[8,85], memory[9,85], memory[10,85], memory[11,85], memory[12,85], memory[13,85], memory[14,85], memory[15,85], memory[16,85], memory[17,85], memory[18,85], memory[19,85], memory[20,85], memory[21,85], memory[22,85], memory[23,85], memory[24,85], memory[25,85], memory[26,85], memory[27,85], memory[28,85], memory[29,85], memory[30,85], memory[31,85], memory[32,85], memory[33,85], memory[34,85], memory[35,85], memory[36,85], memory[37,85], memory[38,85], memory[39,85], memory[40,85], memory[41,85], memory[42,85], memory[43,85], memory[44,85], memory[45,85], memory[46,85], memory[47,85], memory[48,85], memory[49,85], memory[50,85], memory[51,85], memory[52,85], memory[53,85], memory[54,85], memory[55,85], memory[56,85], memory[57,85], memory[58,85], memory[59,85], memory[60,85], memory[61,85], memory[62,85], memory[63,85], memory[64,85], memory[65,85], memory[66,85], memory[67,85], memory[68,85], memory[69,85], memory[70,85], memory[71,85], memory[72,85], memory[73,85], memory[74,85], memory[75,85], memory[76,85], memory[77,85], memory[78,85], memory[79,85], memory[80,85], memory[81,85], memory[82,85], memory[83,85], memory[84,85], memory[85,85], memory[86,85], memory[87,85], memory[88,85], memory[89,85], memory[90,85], memory[91,85], memory[92,85], memory[93,85], memory[94,85], memory[95,85], memory[96,85], memory[97,85], memory[98,85], memory[99,85], memory[100,85], memory[101,85], memory[102,85], memory[103,85], memory[104,85], memory[105,85], memory[106,85], memory[107,85], memory[108,85], memory[109,85], memory[110,85], memory[111,85], memory[112,85], memory[113,85], memory[114,85], memory[115,85], memory[116,85], memory[117,85], memory[118,85], memory[119,85], memory[120,85], memory[1,86], memory[2,86], memory[3,86], memory[4,86], memory[5,86], memory[6,86], memory[7,86], memory[8,86], memory[9,86], memory[10,86], memory[11,86], memory[12,86], memory[13,86], memory[14,86], memory[15,86], memory[16,86], memory[17,86], memory[18,86], memory[19,86], memory[20,86], memory[21,86], memory[22,86], memory[23,86], memory[24,86], memory[25,86], memory[26,86], memory[27,86], memory[28,86], memory[29,86], memory[30,86], memory[31,86], memory[32,86], memory[33,86], memory[34,86], memory[35,86], memory[36,86], memory[37,86], memory[38,86], memory[39,86], memory[40,86], memory[41,86], memory[42,86], memory[43,86], memory[44,86], memory[45,86], memory[46,86], memory[47,86], memory[48,86], memory[49,86], memory[50,86], memory[51,86], memory[52,86], memory[53,86], memory[54,86], memory[55,86], memory[56,86], memory[57,86], memory[58,86], memory[59,86], memory[60,86], memory[61,86], memory[62,86], memory[63,86], memory[64,86], memory[65,86], memory[66,86], memory[67,86], memory[68,86], memory[69,86], memory[70,86], memory[71,86], memory[72,86], memory[73,86], memory[74,86], memory[75,86], memory[76,86], memory[77,86], memory[78,86], memory[79,86], memory[80,86], memory[81,86], memory[82,86], memory[83,86], memory[84,86], memory[85,86], memory[86,86], memory[87,86], memory[88,86], memory[89,86], memory[90,86], memory[91,86], memory[92,86], memory[93,86], memory[94,86], memory[95,86], memory[96,86], memory[97,86], memory[98,86], memory[99,86], memory[100,86], memory[101,86], memory[102,86], memory[103,86], memory[104,86], memory[105,86], memory[106,86], memory[107,86], memory[108,86], memory[109,86], memory[110,86], memory[111,86], memory[112,86], memory[113,86], memory[114,86], memory[115,86], memory[116,86], memory[117,86], memory[118,86], memory[119,86], memory[120,86], memory[1,87], memory[2,87], memory[3,87], memory[4,87], memory[5,87], memory[6,87], memory[7,87], memory[8,87], memory[9,87], memory[10,87], memory[11,87], memory[12,87], memory[13,87], memory[14,87], memory[15,87], memory[16,87], memory[17,87], memory[18,87], memory[19,87], memory[20,87], memory[21,87], memory[22,87], memory[23,87], memory[24,87], memory[25,87], memory[26,87], memory[27,87], memory[28,87], memory[29,87], memory[30,87], memory[31,87], memory[32,87], memory[33,87], memory[34,87], memory[35,87], memory[36,87], memory[37,87], memory[38,87], memory[39,87], memory[40,87], memory[41,87], memory[42,87], memory[43,87], memory[44,87], memory[45,87], memory[46,87], memory[47,87], memory[48,87], memory[49,87], memory[50,87], memory[51,87], memory[52,87], memory[53,87], memory[54,87], memory[55,87], memory[56,87], memory[57,87], memory[58,87], memory[59,87], memory[60,87], memory[61,87], memory[62,87], memory[63,87], memory[64,87], memory[65,87], memory[66,87], memory[67,87], memory[68,87], memory[69,87], memory[70,87], memory[71,87], memory[72,87], memory[73,87], memory[74,87], memory[75,87], memory[76,87], memory[77,87], memory[78,87], memory[79,87], memory[80,87], memory[81,87], memory[82,87], memory[83,87], memory[84,87], memory[85,87], memory[86,87], memory[87,87], memory[88,87], memory[89,87], memory[90,87], memory[91,87], memory[92,87], memory[93,87], memory[94,87], memory[95,87], memory[96,87], memory[97,87], memory[98,87], memory[99,87], memory[100,87], memory[101,87], memory[102,87], memory[103,87], memory[104,87], memory[105,87], memory[106,87], memory[107,87], memory[108,87], memory[109,87], memory[110,87], memory[111,87], memory[112,87], memory[113,87], memory[114,87], memory[115,87], memory[116,87], memory[117,87], memory[118,87], memory[119,87], memory[120,87], memory[1,88], memory[2,88], memory[3,88], memory[4,88], memory[5,88], memory[6,88], memory[7,88], memory[8,88], memory[9,88], memory[10,88], memory[11,88], memory[12,88], memory[13,88], memory[14,88], memory[15,88], memory[16,88], memory[17,88], memory[18,88], memory[19,88], memory[20,88], memory[21,88], memory[22,88], memory[23,88], memory[24,88], memory[25,88], memory[26,88], memory[27,88], memory[28,88], memory[29,88], memory[30,88], memory[31,88], memory[32,88], memory[33,88], memory[34,88], memory[35,88], memory[36,88], memory[37,88], memory[38,88], memory[39,88], memory[40,88], memory[41,88], memory[42,88], memory[43,88], memory[44,88], memory[45,88], memory[46,88], memory[47,88], memory[48,88], memory[49,88], memory[50,88], memory[51,88], memory[52,88], memory[53,88], memory[54,88], memory[55,88], memory[56,88], memory[57,88], memory[58,88], memory[59,88], memory[60,88], memory[61,88], memory[62,88], memory[63,88], memory[64,88], memory[65,88], memory[66,88], memory[67,88], memory[68,88], memory[69,88], memory[70,88], memory[71,88], memory[72,88], memory[73,88], memory[74,88], memory[75,88], memory[76,88], memory[77,88], memory[78,88], memory[79,88], memory[80,88], memory[81,88], memory[82,88], memory[83,88], memory[84,88], memory[85,88], memory[86,88], memory[87,88], memory[88,88], memory[89,88], memory[90,88], memory[91,88], memory[92,88], memory[93,88], memory[94,88], memory[95,88], memory[96,88], memory[97,88], memory[98,88], memory[99,88], memory[100,88], memory[101,88], memory[102,88], memory[103,88], memory[104,88], memory[105,88], memory[106,88], memory[107,88], memory[108,88], memory[109,88], memory[110,88], memory[111,88], memory[112,88], memory[113,88], memory[114,88], memory[115,88], memory[116,88], memory[117,88], memory[118,88], memory[119,88], memory[120,88], memory[1,89], memory[2,89], memory[3,89], memory[4,89], memory[5,89], memory[6,89], memory[7,89], memory[8,89], memory[9,89], memory[10,89], memory[11,89], memory[12,89], memory[13,89], memory[14,89], memory[15,89], memory[16,89], memory[17,89], memory[18,89], memory[19,89], memory[20,89], memory[21,89], memory[22,89], memory[23,89], memory[24,89], memory[25,89], memory[26,89], memory[27,89], memory[28,89], memory[29,89], memory[30,89], memory[31,89], memory[32,89], memory[33,89], memory[34,89], memory[35,89], memory[36,89], memory[37,89], memory[38,89], memory[39,89], memory[40,89], memory[41,89], memory[42,89], memory[43,89], memory[44,89], memory[45,89], memory[46,89], memory[47,89], memory[48,89], memory[49,89], memory[50,89], memory[51,89], memory[52,89], memory[53,89], memory[54,89], memory[55,89], memory[56,89], memory[57,89], memory[58,89], memory[59,89], memory[60,89], memory[61,89], memory[62,89], memory[63,89], memory[64,89], memory[65,89], memory[66,89], memory[67,89], memory[68,89], memory[69,89], memory[70,89], memory[71,89], memory[72,89], memory[73,89], memory[74,89], memory[75,89], memory[76,89], memory[77,89], memory[78,89], memory[79,89], memory[80,89], memory[81,89], memory[82,89], memory[83,89], memory[84,89], memory[85,89], memory[86,89], memory[87,89], memory[88,89], memory[89,89], memory[90,89], memory[91,89], memory[92,89], memory[93,89], memory[94,89], memory[95,89], memory[96,89], memory[97,89], memory[98,89], memory[99,89], memory[100,89], memory[101,89], memory[102,89], memory[103,89], memory[104,89], memory[105,89], memory[106,89], memory[107,89], memory[108,89], memory[109,89], memory[110,89], memory[111,89], memory[112,89], memory[113,89], memory[114,89], memory[115,89], memory[116,89], memory[117,89], memory[118,89], memory[119,89], memory[120,89], memory[1,90], memory[2,90], memory[3,90], memory[4,90], memory[5,90], memory[6,90], memory[7,90], memory[8,90], memory[9,90], memory[10,90], memory[11,90], memory[12,90], memory[13,90], memory[14,90], memory[15,90], memory[16,90], memory[17,90], memory[18,90], memory[19,90], memory[20,90], memory[21,90], memory[22,90], memory[23,90], memory[24,90], memory[25,90], memory[26,90], memory[27,90], memory[28,90], memory[29,90], memory[30,90], memory[31,90], memory[32,90], memory[33,90], memory[34,90], memory[35,90], memory[36,90], memory[37,90], memory[38,90], memory[39,90], memory[40,90], memory[41,90], memory[42,90], memory[43,90], memory[44,90], memory[45,90], memory[46,90], memory[47,90], memory[48,90], memory[49,90], memory[50,90], memory[51,90], memory[52,90], memory[53,90], memory[54,90], memory[55,90], memory[56,90], memory[57,90], memory[58,90], memory[59,90], memory[60,90], memory[61,90], memory[62,90], memory[63,90], memory[64,90], memory[65,90], memory[66,90], memory[67,90], memory[68,90], memory[69,90], memory[70,90], memory[71,90], memory[72,90], memory[73,90], memory[74,90], memory[75,90], memory[76,90], memory[77,90], memory[78,90], memory[79,90], memory[80,90], memory[81,90], memory[82,90], memory[83,90], memory[84,90], memory[85,90], memory[86,90], memory[87,90], memory[88,90], memory[89,90], memory[90,90], memory[91,90], memory[92,90], memory[93,90], memory[94,90], memory[95,90], memory[96,90], memory[97,90], memory[98,90], memory[99,90], memory[100,90], memory[101,90], memory[102,90], memory[103,90], memory[104,90], memory[105,90], memory[106,90], memory[107,90], memory[108,90], memory[109,90], memory[110,90], memory[111,90], memory[112,90], memory[113,90], memory[114,90], memory[115,90], memory[116,90], memory[117,90], memory[118,90], memory[119,90], memory[120,90], memory[1,91], memory[2,91], memory[3,91], memory[4,91], memory[5,91], memory[6,91], memory[7,91], memory[8,91], memory[9,91], memory[10,91], memory[11,91], memory[12,91], memory[13,91], memory[14,91], memory[15,91], memory[16,91], memory[17,91], memory[18,91], memory[19,91], memory[20,91], memory[21,91], memory[22,91], memory[23,91], memory[24,91], memory[25,91], memory[26,91], memory[27,91], memory[28,91], memory[29,91], memory[30,91], memory[31,91], memory[32,91], memory[33,91], memory[34,91], memory[35,91], memory[36,91], memory[37,91], memory[38,91], memory[39,91], memory[40,91], memory[41,91], memory[42,91], memory[43,91], memory[44,91], memory[45,91], memory[46,91], memory[47,91], memory[48,91], memory[49,91], memory[50,91], memory[51,91], memory[52,91], memory[53,91], memory[54,91], memory[55,91], memory[56,91], memory[57,91], memory[58,91], memory[59,91], memory[60,91], memory[61,91], memory[62,91], memory[63,91], memory[64,91], memory[65,91], memory[66,91], memory[67,91], memory[68,91], memory[69,91], memory[70,91], memory[71,91], memory[72,91], memory[73,91], memory[74,91], memory[75,91], memory[76,91], memory[77,91], memory[78,91], memory[79,91], memory[80,91], memory[81,91], memory[82,91], memory[83,91], memory[84,91], memory[85,91], memory[86,91], memory[87,91], memory[88,91], memory[89,91], memory[90,91], memory[91,91], memory[92,91], memory[93,91], memory[94,91], memory[95,91], memory[96,91], memory[97,91], memory[98,91], memory[99,91], memory[100,91], memory[101,91], memory[102,91], memory[103,91], memory[104,91], memory[105,91], memory[106,91], memory[107,91], memory[108,91], memory[109,91], memory[110,91], memory[111,91], memory[112,91], memory[113,91], memory[114,91], memory[115,91], memory[116,91], memory[117,91], memory[118,91], memory[119,91], memory[120,91], memory[1,92], memory[2,92], memory[3,92], memory[4,92], memory[5,92], memory[6,92], memory[7,92], memory[8,92], memory[9,92], memory[10,92], memory[11,92], memory[12,92], memory[13,92], memory[14,92], memory[15,92], memory[16,92], memory[17,92], memory[18,92], memory[19,92], memory[20,92], memory[21,92], memory[22,92], memory[23,92], memory[24,92], memory[25,92], memory[26,92], memory[27,92], memory[28,92], memory[29,92], memory[30,92], memory[31,92], memory[32,92], memory[33,92], memory[34,92], memory[35,92], memory[36,92], memory[37,92], memory[38,92], memory[39,92], memory[40,92], memory[41,92], memory[42,92], memory[43,92], memory[44,92], memory[45,92], memory[46,92], memory[47,92], memory[48,92], memory[49,92], memory[50,92], memory[51,92], memory[52,92], memory[53,92], memory[54,92], memory[55,92], memory[56,92], memory[57,92], memory[58,92], memory[59,92], memory[60,92], memory[61,92], memory[62,92], memory[63,92], memory[64,92], memory[65,92], memory[66,92], memory[67,92], memory[68,92], memory[69,92], memory[70,92], memory[71,92], memory[72,92], memory[73,92], memory[74,92], memory[75,92], memory[76,92], memory[77,92], memory[78,92], memory[79,92], memory[80,92], memory[81,92], memory[82,92], memory[83,92], memory[84,92], memory[85,92], memory[86,92], memory[87,92], memory[88,92], memory[89,92], memory[90,92], memory[91,92], memory[92,92], memory[93,92], memory[94,92], memory[95,92], memory[96,92], memory[97,92], memory[98,92], memory[99,92], memory[100,92], memory[101,92], memory[102,92], memory[103,92], memory[104,92], memory[105,92], memory[106,92], memory[107,92], memory[108,92], memory[109,92], memory[110,92], memory[111,92], memory[112,92], memory[113,92], memory[114,92], memory[115,92], memory[116,92], memory[117,92], memory[118,92], memory[119,92], memory[120,92], memory[1,93], memory[2,93], memory[3,93], memory[4,93], memory[5,93], memory[6,93], memory[7,93], memory[8,93], memory[9,93], memory[10,93], memory[11,93], memory[12,93], memory[13,93], memory[14,93], memory[15,93], memory[16,93], memory[17,93], memory[18,93], memory[19,93], memory[20,93], memory[21,93], memory[22,93], memory[23,93], memory[24,93], memory[25,93], memory[26,93], memory[27,93], memory[28,93], memory[29,93], memory[30,93], memory[31,93], memory[32,93], memory[33,93], memory[34,93], memory[35,93], memory[36,93], memory[37,93], memory[38,93], memory[39,93], memory[40,93], memory[41,93], memory[42,93], memory[43,93], memory[44,93], memory[45,93], memory[46,93], memory[47,93], memory[48,93], memory[49,93], memory[50,93], memory[51,93], memory[52,93], memory[53,93], memory[54,93], memory[55,93], memory[56,93], memory[57,93], memory[58,93], memory[59,93], memory[60,93], memory[61,93], memory[62,93], memory[63,93], memory[64,93], memory[65,93], memory[66,93], memory[67,93], memory[68,93], memory[69,93], memory[70,93], memory[71,93], memory[72,93], memory[73,93], memory[74,93], memory[75,93], memory[76,93], memory[77,93], memory[78,93], memory[79,93], memory[80,93], memory[81,93], memory[82,93], memory[83,93], memory[84,93], memory[85,93], memory[86,93], memory[87,93], memory[88,93], memory[89,93], memory[90,93], memory[91,93], memory[92,93], memory[93,93], memory[94,93], memory[95,93], memory[96,93], memory[97,93], memory[98,93], memory[99,93], memory[100,93], memory[101,93], memory[102,93], memory[103,93], memory[104,93], memory[105,93], memory[106,93], memory[107,93], memory[108,93], memory[109,93], memory[110,93], memory[111,93], memory[112,93], memory[113,93], memory[114,93], memory[115,93], memory[116,93], memory[117,93], memory[118,93], memory[119,93], memory[120,93], memory[1,94], memory[2,94], memory[3,94], memory[4,94], memory[5,94], memory[6,94], memory[7,94], memory[8,94], memory[9,94], memory[10,94], memory[11,94], memory[12,94], memory[13,94], memory[14,94], memory[15,94], memory[16,94], memory[17,94], memory[18,94], memory[19,94], memory[20,94], memory[21,94], memory[22,94], memory[23,94], memory[24,94], memory[25,94], memory[26,94], memory[27,94], memory[28,94], memory[29,94], memory[30,94], memory[31,94], memory[32,94], memory[33,94], memory[34,94], memory[35,94], memory[36,94], memory[37,94], memory[38,94], memory[39,94], memory[40,94], memory[41,94], memory[42,94], memory[43,94], memory[44,94], memory[45,94], memory[46,94], memory[47,94], memory[48,94], memory[49,94], memory[50,94], memory[51,94], memory[52,94], memory[53,94], memory[54,94], memory[55,94], memory[56,94], memory[57,94], memory[58,94], memory[59,94], memory[60,94], memory[61,94], memory[62,94], memory[63,94], memory[64,94], memory[65,94], memory[66,94], memory[67,94], memory[68,94], memory[69,94], memory[70,94], memory[71,94], memory[72,94], memory[73,94], memory[74,94], memory[75,94], memory[76,94], memory[77,94], memory[78,94], memory[79,94], memory[80,94], memory[81,94], memory[82,94], memory[83,94], memory[84,94], memory[85,94], memory[86,94], memory[87,94], memory[88,94], memory[89,94], memory[90,94], memory[91,94], memory[92,94], memory[93,94], memory[94,94], memory[95,94], memory[96,94], memory[97,94], memory[98,94], memory[99,94], memory[100,94], memory[101,94], memory[102,94], memory[103,94], memory[104,94], memory[105,94], memory[106,94], memory[107,94], memory[108,94], memory[109,94], memory[110,94], memory[111,94], memory[112,94], memory[113,94], memory[114,94], memory[115,94], memory[116,94], memory[117,94], memory[118,94], memory[119,94], memory[120,94], memory[1,95], memory[2,95], memory[3,95], memory[4,95], memory[5,95], memory[6,95], memory[7,95], memory[8,95], memory[9,95], memory[10,95], memory[11,95], memory[12,95], memory[13,95], memory[14,95], memory[15,95], memory[16,95], memory[17,95], memory[18,95], memory[19,95], memory[20,95], memory[21,95], memory[22,95], memory[23,95], memory[24,95], memory[25,95], memory[26,95], memory[27,95], memory[28,95], memory[29,95], memory[30,95], memory[31,95], memory[32,95], memory[33,95], memory[34,95], memory[35,95], memory[36,95], memory[37,95], memory[38,95], memory[39,95], memory[40,95], memory[41,95], memory[42,95], memory[43,95], memory[44,95], memory[45,95], memory[46,95], memory[47,95], memory[48,95], memory[49,95], memory[50,95], memory[51,95], memory[52,95], memory[53,95], memory[54,95], memory[55,95], memory[56,95], memory[57,95], memory[58,95], memory[59,95], memory[60,95], memory[61,95], memory[62,95], memory[63,95], memory[64,95], memory[65,95], memory[66,95], memory[67,95], memory[68,95], memory[69,95], memory[70,95], memory[71,95], memory[72,95], memory[73,95], memory[74,95], memory[75,95], memory[76,95], memory[77,95], memory[78,95], memory[79,95], memory[80,95], memory[81,95], memory[82,95], memory[83,95], memory[84,95], memory[85,95], memory[86,95], memory[87,95], memory[88,95], memory[89,95], memory[90,95], memory[91,95], memory[92,95], memory[93,95], memory[94,95], memory[95,95], memory[96,95], memory[97,95], memory[98,95], memory[99,95], memory[100,95], memory[101,95], memory[102,95], memory[103,95], memory[104,95], memory[105,95], memory[106,95], memory[107,95], memory[108,95], memory[109,95], memory[110,95], memory[111,95], memory[112,95], memory[113,95], memory[114,95], memory[115,95], memory[116,95], memory[117,95], memory[118,95], memory[119,95], memory[120,95], memory[1,96], memory[2,96], memory[3,96], memory[4,96], memory[5,96], memory[6,96], memory[7,96], memory[8,96], memory[9,96], memory[10,96], memory[11,96], memory[12,96], memory[13,96], memory[14,96], memory[15,96], memory[16,96], memory[17,96], memory[18,96], memory[19,96], memory[20,96], memory[21,96], memory[22,96], memory[23,96], memory[24,96], memory[25,96], memory[26,96], memory[27,96], memory[28,96], memory[29,96], memory[30,96], memory[31,96], memory[32,96], memory[33,96], memory[34,96], memory[35,96], memory[36,96], memory[37,96], memory[38,96], memory[39,96], memory[40,96], memory[41,96], memory[42,96], memory[43,96], memory[44,96], memory[45,96], memory[46,96], memory[47,96], memory[48,96], memory[49,96], memory[50,96], memory[51,96], memory[52,96], memory[53,96], memory[54,96], memory[55,96], memory[56,96], memory[57,96], memory[58,96], memory[59,96], memory[60,96], memory[61,96], memory[62,96], memory[63,96], memory[64,96], memory[65,96], memory[66,96], memory[67,96], memory[68,96], memory[69,96], memory[70,96], memory[71,96], memory[72,96], memory[73,96], memory[74,96], memory[75,96], memory[76,96], memory[77,96], memory[78,96], memory[79,96], memory[80,96], memory[81,96], memory[82,96], memory[83,96], memory[84,96], memory[85,96], memory[86,96], memory[87,96], memory[88,96], memory[89,96], memory[90,96], memory[91,96], memory[92,96], memory[93,96], memory[94,96], memory[95,96], memory[96,96], memory[97,96], memory[98,96], memory[99,96], memory[100,96], memory[101,96], memory[102,96], memory[103,96], memory[104,96], memory[105,96], memory[106,96], memory[107,96], memory[108,96], memory[109,96], memory[110,96], memory[111,96], memory[112,96], memory[113,96], memory[114,96], memory[115,96], memory[116,96], memory[117,96], memory[118,96], memory[119,96], memory[120,96], memory[1,97], memory[2,97], memory[3,97], memory[4,97], memory[5,97], memory[6,97], memory[7,97], memory[8,97], memory[9,97], memory[10,97], memory[11,97], memory[12,97], memory[13,97], memory[14,97], memory[15,97], memory[16,97], memory[17,97], memory[18,97], memory[19,97], memory[20,97], memory[21,97], memory[22,97], memory[23,97], memory[24,97], memory[25,97], memory[26,97], memory[27,97], memory[28,97], memory[29,97], memory[30,97], memory[31,97], memory[32,97], memory[33,97], memory[34,97], memory[35,97], memory[36,97], memory[37,97], memory[38,97], memory[39,97], memory[40,97], memory[41,97], memory[42,97], memory[43,97], memory[44,97], memory[45,97], memory[46,97], memory[47,97], memory[48,97], memory[49,97], memory[50,97], memory[51,97], memory[52,97], memory[53,97], memory[54,97], memory[55,97], memory[56,97], memory[57,97], memory[58,97], memory[59,97], memory[60,97], memory[61,97], memory[62,97], memory[63,97], memory[64,97], memory[65,97], memory[66,97], memory[67,97], memory[68,97], memory[69,97], memory[70,97], memory[71,97], memory[72,97], memory[73,97], memory[74,97], memory[75,97], memory[76,97], memory[77,97], memory[78,97], memory[79,97], memory[80,97], memory[81,97], memory[82,97], memory[83,97], memory[84,97], memory[85,97], memory[86,97], memory[87,97], memory[88,97], memory[89,97], memory[90,97], memory[91,97], memory[92,97], memory[93,97], memory[94,97], memory[95,97], memory[96,97], memory[97,97], memory[98,97], memory[99,97], memory[100,97], memory[101,97], memory[102,97], memory[103,97], memory[104,97], memory[105,97], memory[106,97], memory[107,97], memory[108,97], memory[109,97], memory[110,97], memory[111,97], memory[112,97], memory[113,97], memory[114,97], memory[115,97], memory[116,97], memory[117,97], memory[118,97], memory[119,97], memory[120,97], memory[1,98], memory[2,98], memory[3,98], memory[4,98], memory[5,98], memory[6,98], memory[7,98], memory[8,98], memory[9,98], memory[10,98], memory[11,98], memory[12,98], memory[13,98], memory[14,98], memory[15,98], memory[16,98], memory[17,98], memory[18,98], memory[19,98], memory[20,98], memory[21,98], memory[22,98], memory[23,98], memory[24,98], memory[25,98], memory[26,98], memory[27,98], memory[28,98], memory[29,98], memory[30,98], memory[31,98], memory[32,98], memory[33,98], memory[34,98], memory[35,98], memory[36,98], memory[37,98], memory[38,98], memory[39,98], memory[40,98], memory[41,98], memory[42,98], memory[43,98], memory[44,98], memory[45,98], memory[46,98], memory[47,98], memory[48,98], memory[49,98], memory[50,98], memory[51,98], memory[52,98], memory[53,98], memory[54,98], memory[55,98], memory[56,98], memory[57,98], memory[58,98], memory[59,98], memory[60,98], memory[61,98], memory[62,98], memory[63,98], memory[64,98], memory[65,98], memory[66,98], memory[67,98], memory[68,98], memory[69,98], memory[70,98], memory[71,98], memory[72,98], memory[73,98], memory[74,98], memory[75,98], memory[76,98], memory[77,98], memory[78,98], memory[79,98], memory[80,98], memory[81,98], memory[82,98], memory[83,98], memory[84,98], memory[85,98], memory[86,98], memory[87,98], memory[88,98], memory[89,98], memory[90,98], memory[91,98], memory[92,98], memory[93,98], memory[94,98], memory[95,98], memory[96,98], memory[97,98], memory[98,98], memory[99,98], memory[100,98], memory[101,98], memory[102,98], memory[103,98], memory[104,98], memory[105,98], memory[106,98], memory[107,98], memory[108,98], memory[109,98], memory[110,98], memory[111,98], memory[112,98], memory[113,98], memory[114,98], memory[115,98], memory[116,98], memory[117,98], memory[118,98], memory[119,98], memory[120,98], memory[1,99], memory[2,99], memory[3,99], memory[4,99], memory[5,99], memory[6,99], memory[7,99], memory[8,99], memory[9,99], memory[10,99], memory[11,99], memory[12,99], memory[13,99], memory[14,99], memory[15,99], memory[16,99], memory[17,99], memory[18,99], memory[19,99], memory[20,99], memory[21,99], memory[22,99], memory[23,99], memory[24,99], memory[25,99], memory[26,99], memory[27,99], memory[28,99], memory[29,99], memory[30,99], memory[31,99], memory[32,99], memory[33,99], memory[34,99], memory[35,99], memory[36,99], memory[37,99], memory[38,99], memory[39,99], memory[40,99], memory[41,99], memory[42,99], memory[43,99], memory[44,99], memory[45,99], memory[46,99], memory[47,99], memory[48,99], memory[49,99], memory[50,99], memory[51,99], memory[52,99], memory[53,99], memory[54,99], memory[55,99], memory[56,99], memory[57,99], memory[58,99], memory[59,99], memory[60,99], memory[61,99], memory[62,99], memory[63,99], memory[64,99], memory[65,99], memory[66,99], memory[67,99], memory[68,99], memory[69,99], memory[70,99], memory[71,99], memory[72,99], memory[73,99], memory[74,99], memory[75,99], memory[76,99], memory[77,99], memory[78,99], memory[79,99], memory[80,99], memory[81,99], memory[82,99], memory[83,99], memory[84,99], memory[85,99], memory[86,99], memory[87,99], memory[88,99], memory[89,99], memory[90,99], memory[91,99], memory[92,99], memory[93,99], memory[94,99], memory[95,99], memory[96,99], memory[97,99], memory[98,99], memory[99,99], memory[100,99], memory[101,99], memory[102,99], memory[103,99], memory[104,99], memory[105,99], memory[106,99], memory[107,99], memory[108,99], memory[109,99], memory[110,99], memory[111,99], memory[112,99], memory[113,99], memory[114,99], memory[115,99], memory[116,99], memory[117,99], memory[118,99], memory[119,99], memory[120,99], memory[1,100], memory[2,100], memory[3,100], memory[4,100], memory[5,100], memory[6,100], memory[7,100], memory[8,100], memory[9,100], memory[10,100], memory[11,100], memory[12,100], memory[13,100], memory[14,100], memory[15,100], memory[16,100], memory[17,100], memory[18,100], memory[19,100], memory[20,100], memory[21,100], memory[22,100], memory[23,100], memory[24,100], memory[25,100], memory[26,100], memory[27,100], memory[28,100], memory[29,100], memory[30,100], memory[31,100], memory[32,100], memory[33,100], memory[34,100], memory[35,100], memory[36,100], memory[37,100], memory[38,100], memory[39,100], memory[40,100], memory[41,100], memory[42,100], memory[43,100], memory[44,100], memory[45,100], memory[46,100], memory[47,100], memory[48,100], memory[49,100], memory[50,100], memory[51,100], memory[52,100], memory[53,100], memory[54,100], memory[55,100], memory[56,100], memory[57,100], memory[58,100], memory[59,100], memory[60,100], memory[61,100], memory[62,100], memory[63,100], memory[64,100], memory[65,100], memory[66,100], memory[67,100], memory[68,100], memory[69,100], memory[70,100], memory[71,100], memory[72,100], memory[73,100], memory[74,100], memory[75,100], memory[76,100], memory[77,100], memory[78,100], memory[79,100], memory[80,100], memory[81,100], memory[82,100], memory[83,100], memory[84,100], memory[85,100], memory[86,100], memory[87,100], memory[88,100], memory[89,100], memory[90,100], memory[91,100], memory[92,100], memory[93,100], memory[94,100], memory[95,100], memory[96,100], memory[97,100], memory[98,100], memory[99,100], memory[100,100], memory[101,100], memory[102,100], memory[103,100], memory[104,100], memory[105,100], memory[106,100], memory[107,100], memory[108,100], memory[109,100], memory[110,100], memory[111,100], memory[112,100], memory[113,100], memory[114,100], memory[115,100], memory[116,100], memory[117,100], memory[118,100], memory[119,100], memory[120,100], IDs[1,1], IDs[2,1], IDs[3,1], IDs[4,1], IDs[5,1], IDs[6,1], IDs[7,1], IDs[8,1], IDs[9,1], IDs[10,1], IDs[11,1], IDs[12,1], IDs[13,1], IDs[14,1], IDs[15,1], IDs[16,1], IDs[17,1], IDs[18,1], IDs[19,1], IDs[20,1], IDs[21,1], IDs[22,1], IDs[23,1], IDs[24,1], IDs[25,1], IDs[26,1], IDs[27,1], IDs[28,1], IDs[29,1], IDs[30,1], IDs[31,1], IDs[32,1], IDs[33,1], IDs[34,1], IDs[35,1], IDs[36,1], IDs[37,1], IDs[38,1], IDs[39,1], IDs[40,1], IDs[41,1], IDs[42,1], IDs[43,1], IDs[44,1], IDs[45,1], IDs[46,1], IDs[47,1], IDs[48,1], IDs[49,1], IDs[50,1], IDs[51,1], IDs[52,1], IDs[53,1], IDs[54,1], IDs[55,1], IDs[56,1], IDs[57,1], IDs[58,1], IDs[59,1], IDs[60,1], IDs[61,1], IDs[62,1], IDs[63,1], IDs[64,1], IDs[65,1], IDs[66,1], IDs[67,1], IDs[68,1], IDs[69,1], IDs[70,1], IDs[71,1], IDs[72,1], IDs[73,1], IDs[74,1], IDs[75,1], IDs[76,1], IDs[77,1], IDs[78,1], IDs[79,1], IDs[80,1], IDs[81,1], IDs[82,1], IDs[83,1], IDs[84,1], IDs[85,1], IDs[86,1], IDs[87,1], IDs[88,1], IDs[89,1], IDs[90,1], IDs[91,1], IDs[92,1], IDs[93,1], IDs[94,1], IDs[95,1], IDs[96,1], IDs[97,1], IDs[98,1], IDs[99,1], IDs[100,1], IDs[1,2], IDs[2,2], IDs[3,2], IDs[4,2], IDs[5,2], IDs[6,2], IDs[7,2], IDs[8,2], IDs[9,2], IDs[10,2], IDs[11,2], IDs[12,2], IDs[13,2], IDs[14,2], IDs[15,2], IDs[16,2], IDs[17,2], IDs[18,2], IDs[19,2], IDs[20,2], IDs[21,2], IDs[22,2], IDs[23,2], IDs[24,2], IDs[25,2], IDs[26,2], IDs[27,2], IDs[28,2], IDs[29,2], IDs[30,2], IDs[31,2], IDs[32,2], IDs[33,2], IDs[34,2], IDs[35,2], IDs[36,2], IDs[37,2], IDs[38,2], IDs[39,2], IDs[40,2], IDs[41,2], IDs[42,2], IDs[43,2], IDs[44,2], IDs[45,2], IDs[46,2], IDs[47,2], IDs[48,2], IDs[49,2], IDs[50,2], IDs[51,2], IDs[52,2], IDs[53,2], IDs[54,2], IDs[55,2], IDs[56,2], IDs[57,2], IDs[58,2], IDs[59,2], IDs[60,2], IDs[61,2], IDs[62,2], IDs[63,2], IDs[64,2], IDs[65,2], IDs[66,2], IDs[67,2], IDs[68,2], IDs[69,2], IDs[70,2], IDs[71,2], IDs[72,2], IDs[73,2], IDs[74,2], IDs[75,2], IDs[76,2], IDs[77,2], IDs[78,2], IDs[79,2], IDs[80,2], IDs[81,2], IDs[82,2], IDs[83,2], IDs[84,2], IDs[85,2], IDs[86,2], IDs[87,2], IDs[88,2], IDs[89,2], IDs[90,2], IDs[91,2], IDs[92,2], IDs[93,2], IDs[94,2], IDs[95,2], IDs[96,2], IDs[97,2], IDs[98,2], IDs[99,2], IDs[100,2], biasM_prior, biasSD_prior, betaM_prior, betaSD_prior, bias_prior, beta_prior, prior_preds0[1], prior_preds0[2], prior_preds0[3], prior_preds0[4], prior_preds0[5], prior_preds0[6], prior_preds0[7], prior_preds0[8], prior_preds0[9], prior_preds0[10], prior_preds0[11], prior_preds0[12], prior_preds0[13], prior_preds0[14], prior_preds0[15], prior_preds0[16], prior_preds0[17], prior_preds0[18], prior_preds0[19], prior_preds0[20], prior_preds0[21], prior_preds0[22], prior_preds0[23], prior_preds0[24], prior_preds0[25], prior_preds0[26], prior_preds0[27], prior_preds0[28], prior_preds0[29], prior_preds0[30], prior_preds0[31], prior_preds0[32], prior_preds0[33], prior_preds0[34], prior_preds0[35], prior_preds0[36], prior_preds0[37], prior_preds0[38], prior_preds0[39], prior_preds0[40], prior_preds0[41], prior_preds0[42], prior_preds0[43], prior_preds0[44], prior_preds0[45], prior_preds0[46], prior_preds0[47], prior_preds0[48], prior_preds0[49], prior_preds0[50], prior_preds0[51], prior_preds0[52], prior_preds0[53], prior_preds0[54], prior_preds0[55], prior_preds0[56], prior_preds0[57], prior_preds0[58], prior_preds0[59], prior_preds0[60], prior_preds0[61], prior_preds0[62], prior_preds0[63], prior_preds0[64], prior_preds0[65], prior_preds0[66], prior_preds0[67], prior_preds0[68], prior_preds0[69], prior_preds0[70], prior_preds0[71], prior_preds0[72], prior_preds0[73], prior_preds0[74], prior_preds0[75], prior_preds0[76], prior_preds0[77], prior_preds0[78], prior_preds0[79], prior_preds0[80], prior_preds0[81], prior_preds0[82], prior_preds0[83], prior_preds0[84], prior_preds0[85], prior_preds0[86], prior_preds0[87], prior_preds0[88], prior_preds0[89], prior_preds0[90], prior_preds0[91], prior_preds0[92], prior_preds0[93], prior_preds0[94], prior_preds0[95], prior_preds0[96], prior_preds0[97], prior_preds0[98], prior_preds0[99], prior_preds0[100], prior_preds1[1], prior_preds1[2], prior_preds1[3], prior_preds1[4], prior_preds1[5], prior_preds1[6], prior_preds1[7], prior_preds1[8], prior_preds1[9], prior_preds1[10], prior_preds1[11], prior_preds1[12], prior_preds1[13], prior_preds1[14], prior_preds1[15], prior_preds1[16], prior_preds1[17], prior_preds1[18], prior_preds1[19], prior_preds1[20], prior_preds1[21], prior_preds1[22], prior_preds1[23], prior_preds1[24], prior_preds1[25], prior_preds1[26], prior_preds1[27], prior_preds1[28], prior_preds1[29], prior_preds1[30], prior_preds1[31], prior_preds1[32], prior_preds1[33], prior_preds1[34], prior_preds1[35], prior_preds1[36], prior_preds1[37], prior_preds1[38], prior_preds1[39], prior_preds1[40], prior_preds1[41], prior_preds1[42], prior_preds1[43], prior_preds1[44], prior_preds1[45], prior_preds1[46], prior_preds1[47], prior_preds1[48], prior_preds1[49], prior_preds1[50], prior_preds1[51], prior_preds1[52], prior_preds1[53], prior_preds1[54], prior_preds1[55], prior_preds1[56], prior_preds1[57], prior_preds1[58], prior_preds1[59], prior_preds1[60], prior_preds1[61], prior_preds1[62], prior_preds1[63], prior_preds1[64], prior_preds1[65], prior_preds1[66], prior_preds1[67], prior_preds1[68], prior_preds1[69], prior_preds1[70], prior_preds1[71], prior_preds1[72], prior_preds1[73], prior_preds1[74], prior_preds1[75], prior_preds1[76], prior_preds1[77], prior_preds1[78], prior_preds1[79], prior_preds1[80], prior_preds1[81], prior_preds1[82], prior_preds1[83], prior_preds1[84], prior_preds1[85], prior_preds1[86], prior_preds1[87], prior_preds1[88], prior_preds1[89], prior_preds1[90], prior_preds1[91], prior_preds1[92], prior_preds1[93], prior_preds1[94], prior_preds1[95], prior_preds1[96], prior_preds1[97], prior_preds1[98], prior_preds1[99], prior_preds1[100], prior_preds2[1], prior_preds2[2], prior_preds2[3], prior_preds2[4], prior_preds2[5], prior_preds2[6], prior_preds2[7], prior_preds2[8], prior_preds2[9], prior_preds2[10], prior_preds2[11], prior_preds2[12], prior_preds2[13], prior_preds2[14], prior_preds2[15], prior_preds2[16], prior_preds2[17], prior_preds2[18], prior_preds2[19], prior_preds2[20], prior_preds2[21], prior_preds2[22], prior_preds2[23], prior_preds2[24], prior_preds2[25], prior_preds2[26], prior_preds2[27], prior_preds2[28], prior_preds2[29], prior_preds2[30], prior_preds2[31], prior_preds2[32], prior_preds2[33], prior_preds2[34], prior_preds2[35], prior_preds2[36], prior_preds2[37], prior_preds2[38], prior_preds2[39], prior_preds2[40], prior_preds2[41], prior_preds2[42], prior_preds2[43], prior_preds2[44], prior_preds2[45], prior_preds2[46], prior_preds2[47], prior_preds2[48], prior_preds2[49], prior_preds2[50], prior_preds2[51], prior_preds2[52], prior_preds2[53], prior_preds2[54], prior_preds2[55], prior_preds2[56], prior_preds2[57], prior_preds2[58], prior_preds2[59], prior_preds2[60], prior_preds2[61], prior_preds2[62], prior_preds2[63], prior_preds2[64], prior_preds2[65], prior_preds2[66], prior_preds2[67], prior_preds2[68], prior_preds2[69], prior_preds2[70], prior_preds2[71], prior_preds2[72], prior_preds2[73], prior_preds2[74], prior_preds2[75], prior_preds2[76], prior_preds2[77], prior_preds2[78], prior_preds2[79], prior_preds2[80], prior_preds2[81], prior_preds2[82], prior_preds2[83], prior_preds2[84], prior_preds2[85], prior_preds2[86], prior_preds2[87], prior_preds2[88], prior_preds2[89], prior_preds2[90], prior_preds2[91], prior_preds2[92], prior_preds2[93], prior_preds2[94], prior_preds2[95], prior_preds2[96], prior_preds2[97], prior_preds2[98], prior_preds2[99], prior_preds2[100], posterior_preds0[1], posterior_preds0[2], posterior_preds0[3], posterior_preds0[4], posterior_preds0[5], posterior_preds0[6], posterior_preds0[7], posterior_preds0[8], posterior_preds0[9], posterior_preds0[10], posterior_preds0[11], posterior_preds0[12], posterior_preds0[13], posterior_preds0[14], posterior_preds0[15], posterior_preds0[16], posterior_preds0[17], posterior_preds0[18], posterior_preds0[19], posterior_preds0[20], posterior_preds0[21], posterior_preds0[22], posterior_preds0[23], posterior_preds0[24], posterior_preds0[25], posterior_preds0[26], posterior_preds0[27], posterior_preds0[28], posterior_preds0[29], posterior_preds0[30], posterior_preds0[31], posterior_preds0[32], posterior_preds0[33], posterior_preds0[34], posterior_preds0[35], posterior_preds0[36], posterior_preds0[37], posterior_preds0[38], posterior_preds0[39], posterior_preds0[40], posterior_preds0[41], posterior_preds0[42], posterior_preds0[43], posterior_preds0[44], posterior_preds0[45], posterior_preds0[46], posterior_preds0[47], posterior_preds0[48], posterior_preds0[49], posterior_preds0[50], posterior_preds0[51], posterior_preds0[52], posterior_preds0[53], posterior_preds0[54], posterior_preds0[55], posterior_preds0[56], posterior_preds0[57], posterior_preds0[58], posterior_preds0[59], posterior_preds0[60], posterior_preds0[61], posterior_preds0[62], posterior_preds0[63], posterior_preds0[64], posterior_preds0[65], posterior_preds0[66], posterior_preds0[67], posterior_preds0[68], posterior_preds0[69], posterior_preds0[70], posterior_preds0[71], posterior_preds0[72], posterior_preds0[73], posterior_preds0[74], posterior_preds0[75], posterior_preds0[76], posterior_preds0[77], posterior_preds0[78], posterior_preds0[79], posterior_preds0[80], posterior_preds0[81], posterior_preds0[82], posterior_preds0[83], posterior_preds0[84], posterior_preds0[85], posterior_preds0[86], posterior_preds0[87], posterior_preds0[88], posterior_preds0[89], posterior_preds0[90], posterior_preds0[91], posterior_preds0[92], posterior_preds0[93], posterior_preds0[94], posterior_preds0[95], posterior_preds0[96], posterior_preds0[97], posterior_preds0[98], posterior_preds0[99], posterior_preds0[100], posterior_preds1[1], posterior_preds1[2], posterior_preds1[3], posterior_preds1[4], posterior_preds1[5], posterior_preds1[6], posterior_preds1[7], posterior_preds1[8], posterior_preds1[9], posterior_preds1[10], posterior_preds1[11], posterior_preds1[12], posterior_preds1[13], posterior_preds1[14], posterior_preds1[15], posterior_preds1[16], posterior_preds1[17], posterior_preds1[18], posterior_preds1[19], posterior_preds1[20], posterior_preds1[21], posterior_preds1[22], posterior_preds1[23], posterior_preds1[24], posterior_preds1[25], posterior_preds1[26], posterior_preds1[27], posterior_preds1[28], posterior_preds1[29], posterior_preds1[30], posterior_preds1[31], posterior_preds1[32], posterior_preds1[33], posterior_preds1[34], posterior_preds1[35], posterior_preds1[36], posterior_preds1[37], posterior_preds1[38], posterior_preds1[39], posterior_preds1[40], posterior_preds1[41], posterior_preds1[42], posterior_preds1[43], posterior_preds1[44], posterior_preds1[45], posterior_preds1[46], posterior_preds1[47], posterior_preds1[48], posterior_preds1[49], posterior_preds1[50], posterior_preds1[51], posterior_preds1[52], posterior_preds1[53], posterior_preds1[54], posterior_preds1[55], posterior_preds1[56], posterior_preds1[57], posterior_preds1[58], posterior_preds1[59], posterior_preds1[60], posterior_preds1[61], posterior_preds1[62], posterior_preds1[63], posterior_preds1[64], posterior_preds1[65], posterior_preds1[66], posterior_preds1[67], posterior_preds1[68], posterior_preds1[69], posterior_preds1[70], posterior_preds1[71], posterior_preds1[72], posterior_preds1[73], posterior_preds1[74], posterior_preds1[75], posterior_preds1[76], posterior_preds1[77], posterior_preds1[78], posterior_preds1[79], posterior_preds1[80], posterior_preds1[81], posterior_preds1[82], posterior_preds1[83], posterior_preds1[84], posterior_preds1[85], posterior_preds1[86], posterior_preds1[87], posterior_preds1[88], posterior_preds1[89], posterior_preds1[90], posterior_preds1[91], posterior_preds1[92], posterior_preds1[93], posterior_preds1[94], posterior_preds1[95], posterior_preds1[96], posterior_preds1[97], posterior_preds1[98], posterior_preds1[99], posterior_preds1[100], posterior_preds2[1], posterior_preds2[2], posterior_preds2[3], posterior_preds2[4], posterior_preds2[5], posterior_preds2[6], posterior_preds2[7], posterior_preds2[8], posterior_preds2[9], posterior_preds2[10], posterior_preds2[11], posterior_preds2[12], posterior_preds2[13], posterior_preds2[14], posterior_preds2[15], posterior_preds2[16], posterior_preds2[17], posterior_preds2[18], posterior_preds2[19], posterior_preds2[20], posterior_preds2[21], posterior_preds2[22], posterior_preds2[23], posterior_preds2[24], posterior_preds2[25], posterior_preds2[26], posterior_preds2[27], posterior_preds2[28], posterior_preds2[29], posterior_preds2[30], posterior_preds2[31], posterior_preds2[32], posterior_preds2[33], posterior_preds2[34], posterior_preds2[35], posterior_preds2[36], posterior_preds2[37], posterior_preds2[38], posterior_preds2[39], posterior_preds2[40], posterior_preds2[41], posterior_preds2[42], posterior_preds2[43], posterior_preds2[44], posterior_preds2[45], posterior_preds2[46], posterior_preds2[47], posterior_preds2[48], posterior_preds2[49], posterior_preds2[50], posterior_preds2[51], posterior_preds2[52], posterior_preds2[53], posterior_preds2[54], posterior_preds2[55], posterior_preds2[56], posterior_preds2[57], posterior_preds2[58], posterior_preds2[59], posterior_preds2[60], posterior_preds2[61], posterior_preds2[62], posterior_preds2[63], posterior_preds2[64], posterior_preds2[65], posterior_preds2[66], posterior_preds2[67], posterior_preds2[68], posterior_preds2[69], posterior_preds2[70], posterior_preds2[71], posterior_preds2[72], posterior_preds2[73], posterior_preds2[74], posterior_preds2[75], posterior_preds2[76], posterior_preds2[77], posterior_preds2[78], posterior_preds2[79], posterior_preds2[80], posterior_preds2[81], posterior_preds2[82], posterior_preds2[83], posterior_preds2[84], posterior_preds2[85], posterior_preds2[86], posterior_preds2[87], posterior_preds2[88], posterior_preds2[89], posterior_preds2[90], posterior_preds2[91], posterior_preds2[92], posterior_preds2[93], posterior_preds2[94], posterior_preds2[95], posterior_preds2[96], posterior_preds2[97], posterior_preds2[98], posterior_preds2[99], posterior_preds2[100], .chain, .iteration, .draw # Show summary statistics for key parameters print(samples_mlvl_nc_cor$summary(c(&quot;biasM&quot;, &quot;betaM&quot;, &quot;tau[1]&quot;, &quot;tau[2]&quot;, &quot;L_u[2,2]&quot;))) ## # A tibble: 5 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 biasM -0.536 -0.538 0.217 0.224 -0.885 -0.175 1.00 1812. 2564. ## 2 betaM 3.11 3.11 0.265 0.268 2.66 3.54 1.00 1522. 2250. ## 3 tau[1] 0.850 0.852 0.191 0.194 0.536 1.17 1.01 408. 950. ## 4 tau[2] 1.17 1.22 0.361 0.342 0.503 1.70 1.03 81.0 173. ## 5 L_u[2,2] 0.709 0.693 0.155 0.167 0.476 0.987 1.02 164. 511. # Extract posterior draws for analysis draws_df &lt;- as_draws_df(samples_mlvl_nc_cor$draws()) # Create trace plots to check convergence p1 &lt;- mcmc_trace(draws_df, pars = c(&quot;biasM&quot;, &quot;betaM&quot;, &quot;tau[1]&quot;, &quot;tau[2]&quot;, &quot;L_u[2,2]&quot;)) + theme_classic() + ggtitle(&quot;Trace Plots for Population Parameters&quot;) # Show trace plots p1 # Create prior-posterior update plots create_density_plot &lt;- function(param, true_value, title) { prior_name &lt;- paste0(param, &quot;_prior&quot;) param &lt;- case_when( param == &quot;biasSD&quot; ~ &quot;tau[1]&quot;, param == &quot;betaSD&quot; ~ &quot;tau[2]&quot;, TRUE ~ param ) ggplot(draws_df) + geom_histogram(aes(get(param)), fill = &quot;blue&quot;, alpha = 0.3) + geom_histogram(aes(get(prior_name)), fill = &quot;red&quot;, alpha = 0.3) + geom_vline(xintercept = true_value, linetype = &quot;dashed&quot;) + labs(title = title, subtitle = &quot;Blue: posterior, Red: prior, Dashed: true value&quot;, x = param, y = &quot;Density&quot;) + theme_classic() } # Create individual plots p_biasM &lt;- create_density_plot(&quot;biasM&quot;, biasM, &quot;Population Mean Bias&quot;) p_biasSD &lt;- create_density_plot(&quot;biasSD&quot;, biasSD, &quot;Population SD of Bias&quot;) p_betaM &lt;- create_density_plot(&quot;betaM&quot;, betaM, &quot;Population Mean Beta&quot;) p_betaSD &lt;- create_density_plot(&quot;betaSD&quot;, betaSD, &quot;Population SD of Beta&quot;) # Show them in a grid (p_biasM + p_biasSD) / (p_betaM + p_betaSD) # Show correlations between pop level parameters p1 &lt;- ggplot(draws_df, aes(biasM, biasSD, group = .chain, color = .chain)) + geom_point(alpha = 0.1) + theme_classic() p2 &lt;- ggplot(draws_df, aes(betaM, betaSD, group = .chain, color = .chain)) + geom_point(alpha = 0.1) + theme_classic() p3 &lt;- ggplot(draws_df, aes(biasM, betaM, group = .chain, color = .chain)) + geom_point(alpha = 0.1) + theme_classic() p4 &lt;- ggplot(draws_df, aes(biasSD, betaSD, group = .chain, color = .chain)) + geom_point(alpha = 0.1) + theme_classic() p1 + p2 + p3 + p4 # Show correlation between individual level parameters # Function to convert Cholesky factor to correlation matrix chol_to_corr &lt;- function(L) { # L is lower triangular cholesky factor # For 2x2 matrix, correlation is L[2,1] # We assume the input is a 2x2 cholesky factor where L[1,1] and L[2,2] are ignored L_full &lt;- matrix(0, 2, 2) L_full[1,1] &lt;- 1 L_full[2,1] &lt;- L[1] L_full[2,2] &lt;- sqrt(1 - L[1]^2) # Correlation = L * L^T corr &lt;- L_full %*% t(L_full) return(corr[1,2]) # Return correlation between dimension 1 and 2 } # Extract the Cholesky factor from posterior samples posterior_L &lt;- draws_df %&gt;% dplyr::select(starts_with(&quot;L_u[2,1]&quot;)) # This is the cholesky factor element for correlation # Convert to correlation values posterior_corr &lt;- posterior_L %&gt;% mutate(correlation = `L_u[2,1]`) # For 2×2 case, directly using the parameter works # Generate prior samples from LKJ distribution (approximated via a beta) n_prior_samples &lt;- nrow(posterior_corr) prior_corr &lt;- tibble( correlation = 2 * rbeta(n_prior_samples, 2, 2) - 1 # Scale beta to [-1,1] ) # Combine for plotting plot_data &lt;- bind_rows( mutate(posterior_corr, type = &quot;Posterior&quot;), mutate(prior_corr, type = &quot;Prior&quot;) ) # Create the visualization ggplot(plot_data, aes(x = correlation, fill = type)) + geom_density(alpha = 0.5) + geom_vline(xintercept = 0, linetype = &quot;dashed&quot;, color = &quot;gray50&quot;) + scale_fill_manual(values = c(&quot;Prior&quot; = &quot;red&quot;, &quot;Posterior&quot; = &quot;blue&quot;)) + labs( title = &quot;Prior vs Posterior: Correlation Between Bias and Beta Parameters&quot;, subtitle = &quot;LKJ(2) prior vs posterior correlation distribution&quot;, x = &quot;Correlation Coefficient&quot;, y = &quot;Density&quot;, fill = &quot;Distribution&quot; ) + coord_cartesian(xlim = c(-1, 1)) + theme_minimal() + annotate(&quot;text&quot;, x = 0.2, y = Inf, label = &quot;Negative correlation suggests\\ntradeoff between bias and\\nmemory sensitivity parameters&quot;, vjust = 2, hjust = 0, size = 3.5) # Create posterior predictive check plots p1 &lt;- ggplot(draws_df) + geom_histogram(aes(`prior_preds0[1]`), fill = &quot;red&quot;, alpha = 0.3, bins = 30) + geom_histogram(aes(`posterior_preds0[1]`), fill = &quot;blue&quot;, alpha = 0.3, bins = 30) + geom_histogram(aes(`posterior_preds1[1]`), fill = &quot;green&quot;, alpha = 0.3, bins = 30) + geom_histogram(aes(`posterior_preds2[1]`), fill = &quot;purple&quot;, alpha = 0.3, bins = 30) + labs(title = &quot;Prior and Posterior Predictive Distributions&quot;, subtitle = &quot;Red: prior, Blue: no memory effect, Green: neutral memory, Purple: strong memory&quot;, x = &quot;Predicted Right Choices (out of 120)&quot;, y = &quot;Count&quot;) + theme_classic() # Display plots p1 # Individual-level parameter recovery # Extract individual parameters for a sample of agents sample_agents &lt;- sample(1:agents, 100) sample_data &lt;- d %&gt;% filter(agent %in% sample_agents, trial == 1) %&gt;% dplyr::select(agent, bias, beta) # Extract posterior means for individual agents bias_means &lt;- c() beta_means &lt;- c() for (i in sample_agents) { bias_means[i] &lt;- mean(draws_df[[paste0(&quot;z_IDs[1,&quot;, i, &quot;]&quot;)]]) beta_means[i] &lt;- mean(draws_df[[paste0(&quot;z_IDs[2,&quot;, i, &quot;]&quot;)]]) } # Create comparison data comparison_data &lt;- tibble( agent = sample_agents, true_bias = scale(sample_data$bias), est_bias = scale(bias_means[sample_agents]), true_beta = scale(sample_data$beta), est_beta = scale(beta_means[sample_agents]) ) # Plot comparison p1 &lt;- ggplot(comparison_data, aes(true_bias, est_bias)) + geom_point(size = 3) + geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;) + geom_smooth(method = lm) + labs(title = &quot;Bias Parameter Recovery&quot;, x = &quot;Standardized True Bias&quot;, y = &quot;Standardized Estimated Bias&quot;) + theme_classic() p2 &lt;- ggplot(comparison_data, aes(true_beta, est_beta)) + geom_point(size = 3) + geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;) + geom_smooth(method = lm) + labs(title = &quot;Beta Parameter Recovery&quot;, x = &quot;Standardized True Beta&quot;, y = &quot;Standardized Estimated Beta&quot;) + theme_classic() # Display parameter recovery plots p1 + p2 7.12.7 # Load both models centered_model &lt;- readRDS(&quot;simmodels/W6_MultilevelMemory_centered.RDS&quot;) noncentered_model &lt;- readRDS(&quot;simmodels/W6_MultilevelMemory_noncentered.RDS&quot;) # Function to extract divergences and tree depths extract_diagnostics &lt;- function(model_fit) { draws &lt;- as_draws_df(model_fit$draws()) # Extract diagnostic information diagnostics &lt;- tibble( model = model_fit$metadata()$id, divergent = sum(draws$.divergent), max_treedepth = sum(draws$.treedepth &gt;= 10), n_draws = nrow(draws) ) return(diagnostics) } # Get diagnostics for both models centered_diag &lt;- extract_diagnostics(centered_model) noncentered_diag &lt;- extract_diagnostics(noncentered_model) # Combine diagnostics diagnostics &lt;- bind_rows(centered_diag, noncentered_diag) # Create diagnostic summary table diagnostics_table &lt;- diagnostics %&gt;% mutate( divergent_pct = round(divergent / n_draws * 100, 2), max_treedepth_pct = round(max_treedepth / n_draws * 100, 2) ) %&gt;% dplyr::select(model, divergent, divergent_pct, max_treedepth, max_treedepth_pct) # Display diagnostics table knitr::kable(diagnostics_table, caption = &quot;Sampling Diagnostics Comparison: Centered vs. Non-Centered&quot;, col.names = c(&quot;Model&quot;, &quot;Divergent Transitions&quot;, &quot;% Divergent&quot;, &quot;Max Tree Depth&quot;, &quot;% Max Tree&quot;)) (#tab:compare_parameterizations)Sampling Diagnostics Comparison: Centered vs. Non-Centered Model Divergent Transitions % Divergent Max Tree Depth % Max Tree 1 0 0 0 0 2 0 0 0 0 1 0 0 0 0 2 0 0 0 0 # Extract summary statistics for key parameters from both models centered_summary &lt;- centered_model$summary(c(&quot;biasM&quot;, &quot;biasSD&quot;, &quot;betaM&quot;, &quot;betaSD&quot;)) noncentered_summary &lt;- noncentered_model$summary(c(&quot;biasM&quot;, &quot;biasSD&quot;, &quot;betaM&quot;, &quot;betaSD&quot;)) # Combine and format for comparison parameter_comparison &lt;- bind_rows( mutate(centered_summary, model = &quot;Centered&quot;), mutate(noncentered_summary, model = &quot;Non-Centered&quot;) ) # Display parameter comparison knitr::kable(parameter_comparison %&gt;% dplyr::select(model, variable, mean, q5, q95), caption = &quot;Parameter Estimates: Centered vs. Non-Centered&quot;, col.names = c(&quot;Model&quot;, &quot;Parameter&quot;, &quot;Mean&quot;, &quot;5% Quantile&quot;, &quot;95% Quantile&quot;)) (#tab:compare_parameterizations)Parameter Estimates: Centered vs. Non-Centered Model Parameter Mean 5% Quantile 95% Quantile Centered biasM 0.4157257 0.2854175 0.5518449 Centered biasSD 0.2409784 0.1410021 0.3429320 Centered betaM 1.1583858 1.0421215 1.2717900 Centered betaSD 0.3811980 0.3079295 0.4605237 Non-Centered biasM 0.4071065 0.2802553 0.5417424 Non-Centered biasSD 0.2331416 0.1272396 0.3384460 Non-Centered betaM 1.1646613 1.0494895 1.2778800 Non-Centered betaSD 0.3793339 0.3056419 0.4599512 # Visual comparison of posterior distributions # Extract draws from both models centered_draws &lt;- as_draws_df(centered_model$draws()) %&gt;% dplyr::select(biasM, biasSD, betaM, betaSD) %&gt;% mutate(model = &quot;Centered&quot;) noncentered_draws &lt;- as_draws_df(noncentered_model$draws()) %&gt;% dplyr::select(biasM, biasSD, betaM, betaSD) %&gt;% mutate(model = &quot;Non-Centered&quot;) # Combine draws combined_draws &lt;- bind_rows(centered_draws, noncentered_draws) # Create comparison plots compare_density &lt;- function(param, true_value) { ggplot(combined_draws, aes(x = .data[[param]], fill = model)) + geom_histogram(alpha = 0.5) + geom_vline(xintercept = true_value, linetype = &quot;dashed&quot;) + labs( title = paste(&quot;Posterior Distribution Comparison:&quot;, param), subtitle = &quot;Centered vs. Non-Centered Parameterization&quot;, x = param, y = &quot;Density&quot; ) + theme_classic() } # Create comparison plots for each parameter p1 &lt;- compare_density(&quot;biasM&quot;, biasM) p2 &lt;- compare_density(&quot;biasSD&quot;, biasSD) p3 &lt;- compare_density(&quot;betaM&quot;, betaM) p4 &lt;- compare_density(&quot;betaSD&quot;, betaSD) # Display comparison plots p1 + p2 p3 + p4 7.13 Comparing Pooling Approaches To better understand the trade-offs between different modeling approaches, let’s implement and compare three ways of handling individual differences: No Pooling: Separate models for each agent with no sharing of information Complete Pooling: A single model with identical parameters for all agents Partial Pooling: Our multilevel approach that balances individual and group information Each approach has advantages and disadvantages: Approach Advantages Disadvantages No Pooling Captures all individual differences Unstable for agents with little data; Can’t generalize Complete Pooling Stable estimates; Simple Ignores individual differences Partial Pooling Balances individual vs. group data; Better for small samples More complex; Requires careful implementation Let’s compare how these approaches perform with our memory agent data # First we&#39;ll implement the no-pooling model stan_model_nopooling &lt;- &quot; // Memory Agent Model - No Pooling Approach // (Separate parameters for each agent, no sharing of information) data { int&lt;lower = 1&gt; trials; // Number of trials per agent int&lt;lower = 1&gt; agents; // Number of agents array[trials, agents] int h; // Memory agent choices array[trials, agents] int other; // Opponent (random agent) choices } parameters { // Individual parameters for each agent (no population structure) array[agents] real bias; // Individual bias parameters array[agents] real beta; // Individual beta parameters } transformed parameters { // Memory state for each agent and trial array[trials, agents] real memory; // Calculate memory states for (agent in 1:agents){ for (trial in 1:trials){ if (trial == 1) { memory[trial, agent] = 0.5; } if (trial &lt; trials){ memory[trial + 1, agent] = memory[trial, agent] + ((other[trial, agent] - memory[trial, agent]) / trial); if (memory[trial + 1, agent] == 0){memory[trial + 1, agent] = 0.01;} if (memory[trial + 1, agent] == 1){memory[trial + 1, agent] = 0.99;} } } } } model { // Separate priors for each agent (no pooling) for (agent in 1:agents) { target += normal_lpdf(bias[agent] | 0, 1); target += normal_lpdf(beta[agent] | 0, 1); } // Likelihood for (agent in 1:agents){ for (trial in 1:trials){ target += bernoulli_logit_lpmf(h[trial, agent] | bias[agent] + memory[trial, agent] * beta[agent]); } } } generated quantities{ // Predictions with different memory values array[agents] int&lt;lower=0, upper = trials&gt; posterior_preds0; array[agents] int&lt;lower=0, upper = trials&gt; posterior_preds1; array[agents] int&lt;lower=0, upper = trials&gt; posterior_preds2; // Generate predictions for (agent in 1:agents){ posterior_preds0[agent] = binomial_rng(trials, inv_logit(bias[agent] + 0 * beta[agent])); posterior_preds1[agent] = binomial_rng(trials, inv_logit(bias[agent] + 1 * beta[agent])); posterior_preds2[agent] = binomial_rng(trials, inv_logit(bias[agent] + 2 * beta[agent])); } } &quot; # Now implement the complete pooling model stan_model_fullpooling &lt;- &quot; // Memory Agent Model - Complete Pooling Approach // (Single set of parameters shared by all agents) data { int&lt;lower = 1&gt; trials; // Number of trials per agent int&lt;lower = 1&gt; agents; // Number of agents array[trials, agents] int h; // Memory agent choices array[trials, agents] int other; // Opponent (random agent) choices } parameters { // Single set of parameters shared by all agents real bias; // Shared bias parameter real beta; // Shared beta parameter } transformed parameters { // Memory state for each agent and trial array[trials, agents] real memory; // Calculate memory states for (agent in 1:agents){ for (trial in 1:trials){ if (trial == 1) { memory[trial, agent] = 0.5; } if (trial &lt; trials){ memory[trial + 1, agent] = memory[trial, agent] + ((other[trial, agent] - memory[trial, agent]) / trial); if (memory[trial + 1, agent] == 0){memory[trial + 1, agent] = 0.01;} if (memory[trial + 1, agent] == 1){memory[trial + 1, agent] = 0.99;} } } } } model { // Priors for shared parameters target += normal_lpdf(bias | 0, 1); target += normal_lpdf(beta | 0, 1); // Likelihood (same parameters for all agents) for (agent in 1:agents){ for (trial in 1:trials){ target += bernoulli_logit_lpmf(h[trial, agent] | bias + memory[trial, agent] * beta); } } } generated quantities{ // Single set of predictions for all agents int&lt;lower=0, upper = trials&gt; posterior_preds0; int&lt;lower=0, upper = trials&gt; posterior_preds1; int&lt;lower=0, upper = trials&gt; posterior_preds2; // Generate predictions posterior_preds0 = binomial_rng(trials, inv_logit(bias + 0 * beta)); posterior_preds1 = binomial_rng(trials, inv_logit(bias + 1 * beta)); posterior_preds2 = binomial_rng(trials, inv_logit(bias + 2 * beta)); } &quot; # Write the models to files write_stan_file(stan_model_nopooling, dir = &quot;stan/&quot;, basename = &quot;W5_MultilevelMemory_nopooling.stan&quot;) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W5_MultilevelMemory_nopooling.stan&quot; write_stan_file(stan_model_fullpooling, dir = &quot;stan/&quot;, basename = &quot;W5_MultilevelMemory_fullpooling.stan&quot;) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W5_MultilevelMemory_fullpooling.stan&quot; # Define file paths for saved models file_nopooling_results &lt;- &quot;simmodels/W5_MultilevelMemory_nopooling.RDS&quot; file_fullpooling_results &lt;- &quot;simmodels/W5_MultilevelMemory_fullpooling.RDS&quot; # Fit no pooling model if needed if (regenerate_simulations || !file.exists(file_nopooling_results)) { # Compile the models file_nopooling &lt;- file.path(&quot;stan/W5_MultilevelMemory_nopooling.stan&quot;) mod_nopooling &lt;- cmdstan_model(file_nopooling, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) samples_nopooling &lt;- mod_nopooling$sample( data = data_memory, seed = 123, chains = 2, parallel_chains = 2, threads_per_chain = 2, iter_warmup = 2000, iter_sampling = 2000, refresh = 500, max_treedepth = 20, adapt_delta = 0.99 ) samples_nopooling$save_object(file = file_nopooling_results) cat(&quot;Generated new no-pooling model fit\\n&quot;) } else { cat(&quot;Loading existing no-pooling model fit\\n&quot;) } ## Loading existing no-pooling model fit # Fit full pooling model if needed if (regenerate_simulations || !file.exists(file_fullpooling_results)) { file_fullpooling &lt;- file.path(&quot;stan/W5_MultilevelMemory_fullpooling.stan&quot;) mod_fullpooling &lt;- cmdstan_model(file_fullpooling, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) samples_fullpooling &lt;- mod_fullpooling$sample( data = data_memory, seed = 123, chains = 2, parallel_chains = 2, threads_per_chain = 2, iter_warmup = 2000, iter_sampling = 2000, refresh = 500, max_treedepth = 20, adapt_delta = 0.99 ) samples_fullpooling$save_object(file = file_fullpooling_results) cat(&quot;Generated new full-pooling model fit\\n&quot;) } else { cat(&quot;Loading existing full-pooling model fit\\n&quot;) } ## Loading existing full-pooling model fit 7.14 Comparing Pooling Approaches # Load required packages # Function to simulate and visualize shrinkage from different pooling approaches visualize_pooling_approaches &lt;- function() { # First, simulate some data for demonstration set.seed(42) n_groups &lt;- 20 n_per_group &lt;- c(5, 10, 20, 50) # Different group sizes # True group means (population distribution) true_pop_mean &lt;- 0 true_pop_sd &lt;- 1 true_group_means &lt;- rnorm(n_groups, true_pop_mean, true_pop_sd) # Function to simulate data and estimates for one scenario simulate_one_scenario &lt;- function(n_per_group) { # Create data frame results &lt;- tibble( group_id = factor(1:n_groups), true_mean = true_group_means, n_obs = n_per_group ) # Simulate observed data observed_data &lt;- map2_dfr(1:n_groups, n_per_group, function(group, n) { tibble( group_id = factor(group), value = rnorm(n, true_group_means[group], 1) # Within-group SD = 1 ) }) # Calculate no-pooling estimates (just the group means) no_pooling &lt;- observed_data %&gt;% group_by(group_id) %&gt;% summarize(estimate = mean(value)) %&gt;% pull(estimate) # Calculate full-pooling estimate (grand mean) full_pooling &lt;- mean(observed_data$value) # Calculate partial-pooling estimates (empirical Bayes approach) # This is a simplified version of what happens in a multilevel model grand_mean &lt;- mean(observed_data$value) group_means &lt;- observed_data %&gt;% group_by(group_id) %&gt;% summarize(mean = mean(value), n = n()) # Calculate group variances and total variance components group_var &lt;- var(group_means$mean) within_var &lt;- mean((observed_data %&gt;% group_by(group_id) %&gt;% summarize(var = var(value)) %&gt;% pull(var))) # Calculate shrinkage factor for each group partial_pooling &lt;- map_dbl(1:n_groups, function(i) { group_mean &lt;- group_means$mean[i] group_size &lt;- group_means$n[i] # Optimal shrinkage factor lambda &lt;- within_var / (within_var + group_var * group_size) # Shrunk estimate lambda * grand_mean + (1 - lambda) * group_mean }) # Add estimates to results results &lt;- results %&gt;% mutate( no_pooling = no_pooling, full_pooling = full_pooling, partial_pooling = partial_pooling, # Calculate absolute errors no_pooling_error = abs(no_pooling - true_mean), full_pooling_error = abs(full_pooling - true_mean), partial_pooling_error = abs(partial_pooling - true_mean), scenario = paste(n_per_group, &quot;observations per group&quot;) ) return(results) } # Simulate all scenarios all_results &lt;- map_dfr(n_per_group, simulate_one_scenario) # Convert to long format for plotting results_long &lt;- all_results %&gt;% pivot_longer( cols = c(no_pooling, full_pooling, partial_pooling), names_to = &quot;method&quot;, values_to = &quot;estimate&quot; ) %&gt;% mutate( method = factor(method, levels = c(&quot;no_pooling&quot;, &quot;partial_pooling&quot;, &quot;full_pooling&quot;), labels = c(&quot;No Pooling&quot;, &quot;Partial Pooling&quot;, &quot;Full Pooling&quot;)) ) # Plot 1: Shrinkage visualization p1 &lt;- ggplot(results_long, aes(x = true_mean, y = estimate, color = method)) + geom_point(alpha = 0.7) + geom_abline(slope = 1, intercept = 0, linetype = &quot;dashed&quot;) + geom_hline(yintercept = true_pop_mean, linetype = &quot;dotted&quot;) + facet_wrap(~scenario) + scale_color_manual(values = c(&quot;No Pooling&quot; = &quot;red&quot;, &quot;Partial Pooling&quot; = &quot;green&quot;, &quot;Full Pooling&quot; = &quot;blue&quot;)) + labs( title = &quot;Shrinkage Effects in Different Pooling Approaches&quot;, subtitle = &quot;Dashed line: perfect recovery; Dotted line: population mean&quot;, x = &quot;True Group Mean&quot;, y = &quot;Estimated Mean&quot;, color = &quot;Method&quot; ) + theme_minimal() # Calculate error metrics for each scenario and method error_summary &lt;- all_results %&gt;% group_by(scenario) %&gt;% summarize( No_Pooling_MSE = mean(no_pooling_error^2), Full_Pooling_MSE = mean(full_pooling_error^2), Partial_Pooling_MSE = mean(partial_pooling_error^2) ) %&gt;% pivot_longer( cols = contains(&quot;_MSE&quot;), names_to = &quot;method&quot;, values_to = &quot;mse&quot; ) %&gt;% mutate( method = gsub(&quot;_MSE&quot;, &quot;&quot;, method), method = gsub(&quot;_&quot;, &quot; &quot;, method) ) # Plot 2: Error comparison p2 &lt;- ggplot(error_summary, aes(x = scenario, y = mse, fill = method)) + geom_col(position = &quot;dodge&quot;) + scale_fill_manual(values = c(&quot;No Pooling&quot; = &quot;red&quot;, &quot;Partial Pooling&quot; = &quot;green&quot;, &quot;Full Pooling&quot; = &quot;blue&quot;)) + labs( title = &quot;Mean Squared Error by Pooling Approach&quot;, subtitle = &quot;Lower values indicate better parameter recovery&quot;, x = &quot;Scenario&quot;, y = &quot;Mean Squared Error&quot;, fill = &quot;Method&quot; ) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Plot 3: Shrinkage as a function of group size and distance from mean # Calculate shrinkage ratio shrinkage_data &lt;- all_results %&gt;% mutate( dist_from_mean = true_mean - true_pop_mean, # Shrinkage ratio: how much of the distance from population mean is preserved # 1 = no shrinkage, 0 = complete shrinkage to mean no_pool_shrinkage = (no_pooling - true_pop_mean) / dist_from_mean, full_pool_shrinkage = (full_pooling - true_pop_mean) / dist_from_mean, partial_pool_shrinkage = (partial_pooling - true_pop_mean) / dist_from_mean ) %&gt;% # Filter out cases where dist_from_mean is too close to zero filter(abs(dist_from_mean) &gt; 0.1) # Convert to long format shrinkage_long &lt;- shrinkage_data %&gt;% dplyr::select(group_id, scenario, n_obs, dist_from_mean, contains(&quot;_shrinkage&quot;)) %&gt;% pivot_longer( cols = contains(&quot;_shrinkage&quot;), names_to = &quot;method&quot;, values_to = &quot;shrinkage_ratio&quot; ) %&gt;% mutate( method = gsub(&quot;_shrinkage&quot;, &quot;&quot;, method), method = gsub(&quot;_&quot;, &quot; &quot;, method), method = factor(method, levels = c(&quot;no pool&quot;, &quot;partial pool&quot;, &quot;full pool&quot;), labels = c(&quot;No Pooling&quot;, &quot;Partial Pooling&quot;, &quot;Full Pooling&quot;)), # Clip extreme values for visualization shrinkage_ratio = pmin(pmax(shrinkage_ratio, -0.5), 1.5) ) # Plot shrinkage ratio p3 &lt;- ggplot(shrinkage_long, aes(x = abs(dist_from_mean), y = shrinkage_ratio, color = method)) + geom_point(alpha = 0.7) + geom_smooth(method = &quot;loess&quot;, se = FALSE) + geom_hline(yintercept = 1, linetype = &quot;dashed&quot;) + geom_hline(yintercept = 0, linetype = &quot;dotted&quot;) + facet_wrap(~scenario) + scale_color_manual(values = c(&quot;No Pooling&quot; = &quot;red&quot;, &quot;Partial Pooling&quot; = &quot;green&quot;, &quot;Full Pooling&quot; = &quot;blue&quot;)) + labs( title = &quot;Shrinkage Ratio by Distance from Population Mean&quot;, subtitle = &quot;1.0 = No shrinkage; 0.0 = Complete shrinkage to population mean&quot;, x = &quot;Distance from Population Mean&quot;, y = &quot;Shrinkage Ratio&quot;, color = &quot;Method&quot; ) + theme_minimal() # Return all plots together return(list( main_plot = p1, error_plot = p2, shrinkage_plot = p3 )) } # Generate the visualizations plots &lt;- visualize_pooling_approaches() # Display the plots plots$main_plot plots$error_plot plots$shrinkage_plot # Create combined visualization for conceptual understanding set.seed(123) # Generate data for a single example n_groups &lt;- 8 true_pop_mean &lt;- 0 true_pop_sd &lt;- 1 true_means &lt;- rnorm(n_groups, true_pop_mean, true_pop_sd) group_sizes &lt;- sample(c(3, 5, 10, 20, 30), n_groups, replace = TRUE) # Generate observations generate_group_data &lt;- function(group_id, true_mean, n_obs) { tibble( group = factor(group_id), true_mean = true_mean, n_obs = n_obs, value = rnorm(n_obs, true_mean, 1) ) } sim_data &lt;- map_dfr(1:n_groups, ~generate_group_data( ., true_means[.], group_sizes[.] )) # Calculate estimates estimates &lt;- sim_data %&gt;% group_by(group) %&gt;% summarize( n = n(), true_mean = first(true_mean), no_pooling = mean(value), full_pooling = mean(sim_data$value) ) %&gt;% mutate( # Simplified partial pooling calculation reliability = n / (n + 10), # 10 is arbitrary scaling factor for demonstration partial_pooling = reliability * no_pooling + (1 - reliability) * full_pooling ) # Convert to long format for visualization est_long &lt;- estimates %&gt;% pivot_longer( cols = c(no_pooling, partial_pooling, full_pooling), names_to = &quot;method&quot;, values_to = &quot;estimate&quot; ) %&gt;% mutate( method = factor(method, levels = c(&quot;no_pooling&quot;, &quot;partial_pooling&quot;, &quot;full_pooling&quot;), labels = c(&quot;No Pooling&quot;, &quot;Partial Pooling&quot;, &quot;Full Pooling&quot;)) ) # Create conceptual visualization conceptual_plot &lt;- ggplot(est_long, aes(x = reorder(group, true_mean), y = estimate, color = method)) + # Draw vertical lines showing shrinkage #geom_segment(data = est_long %&gt;% filter(method == &quot;No Pooling&quot;), # aes(xend = group, y = estimate, yend = full_pooling), # color = &quot;gray&quot;, linetype = &quot;dotted&quot;) + # Draw points for estimates geom_point(aes(size = n), alpha = 0.8) + # Draw true means geom_point(aes(y = true_mean), shape = 4, size = 3, color = &quot;black&quot;) + # Draw horizontal line for population mean geom_hline(yintercept = mean(sim_data$value), linetype = &quot;dashed&quot;, color = &quot;gray&quot;) + # Formatting scale_color_manual(values = c(&quot;No Pooling&quot; = &quot;red&quot;, &quot;Partial Pooling&quot; = &quot;green&quot;, &quot;Full Pooling&quot; = &quot;blue&quot;)) + labs( title = &quot;Conceptual Visualization of Shrinkage in Multilevel Modeling&quot;, subtitle = &quot;X = True group mean; Dotted lines show shrinkage; Point size = group sample size&quot;, x = &quot;Group&quot;, y = &quot;Estimate&quot;, color = &quot;Pooling Method&quot;, size = &quot;Group Size&quot; ) + theme_minimal() # Display conceptual plot conceptual_plot [MISSING: PARAMETER RECOVERY IN A MULTILEVEL FRAMEWORK (IND VS POP)] 7.15 Multilevel Modeling Cheatsheet Multilevel Model Visualization 7.15.1 When to Use Each Pooling Approach: Approach When to Use Advantages Disadvantages No Pooling Many observations per group; groups truly independent • Simple to implement• Captures all individual differences • Unstable with small sample sizes• Can’t predict for new individuals Full Pooling Few observations per group; minimal individual differences • Stable estimates• Simple model • Ignores individual differences• Can lead to poor predictions for outliers Partial Pooling Moderate observations per group; meaningful individual differences • Balances individual and group data• More accurate for small groups• Can predict for new individuals • More complex model• Requires careful implementation 7.15.2 Parameter Recovery Rules of Thumb: Group-level means (e.g., biasM) typically require fewer observations for good recovery than group-level variances (e.g., biasSD) Individual parameters undergo more shrinkage when: Group-level variance is small Individual data is limited Individual estimates are far from the group mean 7.16 Conclusion: The Power and Challenges of Multilevel Modeling In this chapter, we’ve explored how multilevel modeling provides a principled approach to analyzing data with hierarchical structure. By implementing models for both biased agents and memory agents, we’ve seen how to: Represent population-level distributions of parameters Allow for individual variations while maintaining population constraints Implement different parameterizations to improve sampling efficiency Assess model quality through various diagnostic techniques Multilevel modeling offers several key advantages for cognitive modeling (only some of which have been exemplified here): - Improved parameter estimation for individuals with limited data - Detection of population-level patterns while respecting individual differences - More efficient use of data through partial pooling of information - Capacity to model correlations between different cognitive parameters The practical implementation challenges we’ve encountered—such as sampling difficulties with correlated parameters and the need for non-centered parameterization—are common in cognitive modeling applications. Developing familiarity with these techniques prepares you for implementing more complex models in your own research. 7.17 Exercises (just some ideas) Parameter Recovery Analysis Simulate data with different levels of individual variability (try biasSD values of 0.05, 0.3, and 0.8). Fit the multilevel model to each dataset and assess how well individual and population parameters are recovered. How does the amount of individual variability affect the benefits of partial pooling? Model Comparison Challenge For the memory agent model, compare the predictive performance of: Full pooling (single parameters for all agents) No pooling (separate parameters for each agent) Partial pooling (hierarchical model) Use different amounts of data (60, 120 and 500 trials) to determine when each approach works best. Create a plot showing the relative advantage of each approach as data quantity changes. Extend the Model Modify the memory agent model to include a “forgetting rate” parameter that weights recent observations more heavily. Implement this as a multilevel parameter (varying across individuals). Does this additional parameter improve model fit? How does it correlate with the other parameters? Applied Modeling [MISSING] The file cognitive_data.csv contains real data from a sequential decision-making experiment. Apply the multilevel memory model to this dataset. Interpret the population-level parameters and identify any interesting individual differences. Create visualizations that communicate your findings effectively. Debugging Challenge [MISSING] The file problematic_model.stan contains a multilevel model with several implementation issues. Identify and fix the problems to get the model running efficiently. Common issues include poor parameterization, inefficient computation, or misspecified priors. "],["model-comparison-in-cognitive-science.html", "Chapter 8 Model Comparison in Cognitive Science 8.1 Learning Objectives 8.2 Why Compare Models? 8.3 Cross-Validation: The Foundation of Model Comparison 8.4 PSIS-LOO: An Efficient Approximation to LOO-CV 8.5 Simulation-Based Model Comparison 8.6 Implementing Models for Comparison 8.7 Fitting Models and Calculating Expected Log Predictive Density 8.8 Fitting the models to the data 8.9 Cross-Validation for Model Comparison 8.10 Formal Model Comparison 8.11 Limitations of Model Comparison Approaches 8.12 Exercises", " Chapter 8 Model Comparison in Cognitive Science Cognitive science aims to understand the processes that give rise to human thought and behavior. To do this effectively, we often create formal models that represent our hypotheses about these underlying processes. However, human cognition is complex, and multiple theoretical accounts might plausibly explain the same observed behaviors. This is where model comparison becomes essential. Model comparison is the principled evaluation of competing models to determine which best explains observed data. Model comparison techniques as described here balance a model’s ability to fit existing data against its ability to generalize to new observations, helping us avoid the trap of overfitting. But remember, model comparison is not a fail-safe procedure to determine which model embodies the truth, as always we need to be careful, tentative and open about the probabilistic and fallible nature of our inference. 8.1 Learning Objectives After completing this chapter, you will be able to: Implement cross-validation techniques for comparing cognitive models using Stan Calculate and interpret expected log predictive density (ELPD) scores Assess model predictions through posterior and prior predictive checks Understand the strengths and limitations of different model comparison approaches Apply these techniques to compare competing cognitive models using real data 8.2 Why Compare Models? Model comparison serves multiple purposes in cognitive science: Theory Testing: Different models often represent competing theoretical accounts of cognitive processes. Comparing their fit to data helps evaluate these theories. Parsimony: When multiple models can explain the data, more complex models should only be preferred if they are justified by better predictive performance. Generalization: By assessing how well different models predict new data, we can evaluate their ability to capture general patterns rather than just fitting to specific samples. Individual Differences: Model comparison can reveal whether different individuals or groups are better described by different cognitive strategies. This chapter demonstrates these principles using our matching pennies models as concrete examples. We’ll compare simple random choice models against more sophisticated memory-based approaches, showing how to rigorously evaluate which better explains observed behavior. 8.2.1 The Challenge of Model Selection Imagine having several models of what might be going on and wanting to know which is the best explanation of the data. For example: Are people more likely to use a memory strategy or a win-stay-lose-shift strategy? Are we justified in assuming that people react differently to losses than to wins? Would we be justified in assuming that capuchin monkeys and cognitive science students use the same model? Model comparison defines a broad range of practices aimed at identifying the best model for a given dataset. What “best” means is, however, a non-trivial question. Ideally, “best” would mean the model describing the mechanism that actually generated the data. However, knowing the truth is a tricky proposition and we need to use proxies. There are many of such proxies in the literature, for instance Bayes Factors (see Nicenboim et al 2023, https://vasishth.github.io/bayescogsci/book/ch-comparison.html). In this course, we rely on predictive performance - this helps combat overfitting, but has limitations we’ll discuss at the end. In other words, this chapter will assess models in terms of their (estimated) ability to predict new (test) data. Remember that predictive performance is a very useful tool, but not a magical solution. It allows us to combat overfitting to the training sample (your model snuggling to your data so much that it fits both signal and noise), but it has key limitations, which we will discuss at the end of the chapter. To learn how to make model comparison, in this chapter, we rely on our usual simulation based approach to ensure that the method is doing what we want. We simulate the behavior of biased agents playing against the memory agents. This provides us with data generated according to two different mechanisms: biased agents and memory agents. We can fit both models separately on each of the two sets of agents, so we can compare the relative performance of the two models: can we identify the true model generating the data (in a setup where truth is known)? This is what is usually called “model recovery” and complements nicely “parameter recovery”. In model recovery we assess whether we can identify the correct model, in parameter recovery we assess whether - once we know the correct model - we can identify the correct parameter values. Let’s get going. # Flag to control whether to regenerate simulations # Set this to TRUE when you need to rerun the models regenerate_simulations &lt;- FALSE # Load required packages pacman::p_load( tidyverse, # For data manipulation and visualization here, # For file path management posterior, # For working with posterior samples cmdstanr, # For interfacing with Stan brms, # For Bayesian regression models tidybayes, # For working with Bayesian samples loo, # For leave-one-out cross-validation patchwork, # For combining plots bayesplot, # For Bayesian visualization MASS ) # Set seed for reproducibility set.seed(123) # Generate demonstration data trials &lt;- 40 # Model 1: Random biased agent (80% right choice) biased_agent &lt;- rbinom(trials, 1, 0.8) cumulative_biased &lt;- cumsum(biased_agent) / seq_along(biased_agent) # Model 2: Memory-based agent (adjusts based on past patterns) memory_agent &lt;- rep(NA, trials) memory_agent[1] &lt;- rbinom(1, 1, 0.5) # First choice is random for (i in 2:trials) { # Agent uses memory of past choices with some randomness memory_agent[i] &lt;- rbinom(1, 1, 0.3 + 0.4 * mean(biased_agent[1:(i - 1)])) } cumulative_memory &lt;- cumsum(memory_agent) / seq_along(memory_agent) # Create plotting data model_data &lt;- tibble( trial = rep(1:trials, 2), choice = c(biased_agent, memory_agent), cumulative = c(cumulative_biased, cumulative_memory), model = rep(c(&quot;Biased Agent&quot;, &quot;Memory Agent&quot;), each = trials) ) # Generate simulated data that could come from either model observed_data &lt;- if (runif(1) &gt; 0.5) biased_agent else memory_agent observed_cumulative &lt;- cumsum(observed_data) / seq_along(observed_data) observed_df &lt;- tibble( trial = 1:trials, choice = observed_data, cumulative = observed_cumulative ) # Create the plot p1 &lt;- ggplot() + geom_jitter(data = observed_df %&gt;% filter(trial &gt; 2), aes(x = trial, y = choice), height = 0.05, alpha = 0.7, color = &quot;black&quot;) + geom_line(data = model_data %&gt;% filter(trial &gt; 2), aes(x = trial, y = cumulative, color = model, linetype = model)) + scale_color_brewer(palette = &quot;Set1&quot;) + labs( title = &quot;Two Competing Models of Decision Making&quot;, subtitle = &quot;The biased agent makes choices with fixed 80% bias toward right option, regardless of context \\nThe memory agent adjusts choices based on memory of opponent&#39;s previous moves \\nJust looking at the data (sampled at random from one of the two models) it&#39;s hard to identify the model generating it&quot;, x = &quot;Trial Number&quot;, y = &quot;Choice (0 = Left, 1 = Right)&quot;, color = &quot;Model&quot; ) + theme_minimal() + theme(legend.position = &quot;top&quot;) p1 In this example, we have: Model 1 (Biased Agent): Makes choices with a consistent 80% bias toward the right option Model 2 (Memory Agent): Adjusts choices based on memory of previous patterns The critical insight is that both models can produce similar-looking data, making it difficult to determine which cognitive process generated the observed behavior by simple visual inspection. Formal model comparison techniques give us principled ways to evaluate which model better explains the data while accounting for model complexity and generalization ability. 8.3 Cross-Validation: The Foundation of Model Comparison Cross-validation is a fundamental technique for comparing models based on their predictive performance. The core idea is simple: a good model should not only fit the observed data but also generalize well to new, unseen data. 8.3.1 The Problem of Overfitting When we fit a model to data, there’s always a risk of overfitting - capturing noise or idiosyncrasies in the particular sample rather than the underlying pattern we care about. [MISSING: A QUICK ILLUSTRATION OF AN EXAMPLE] Cross-validation helps us find the optimal balance between fitting the training data and generalizing to new data. When the datasets are small, as it is often the case in cognitive science, keeping a substantial portion of the data out - substantial enough to be representative of a more general population - is problematic as it risks starving the model of data: there might not be enough data for reliable estimation of the parameter values. This is where the notion of cross-validation comes in: we can split the dataset in k folds, let’s say k = 10. Then each fold is in turn kept aside as validation set, the model is fitted on the other folds, and its predictive performance tested on the validation set. Repeat this operation of each of the folds. This operation ensures that all the data can be used for training as well as for validation, and is in its own terms quite genial. However, this does not mean it is free of shortcomings. First, small validation folds might not be representative of the diversity of true out-of-sample populations - and there is a tendency to set k equal to the number of datapoints (leave-one-out cross validation). Second, there are many ways in which information could leak or contaminate across folds if the pipeline is not very careful (e.g. via data preprocessing scaling the full dataset, or hyper-parameter estimation). Third, and crucial for our case here, cross validation implies refitting the model k times, which for Bayesian models might be very cumbersome (I once had a model that took 6 weeks to run). 8.3.2 How Cross-Validation Works The basic idea of cross-validation is to: Split your data into training and test sets Fit your model to the training data Evaluate the model’s performance on the test data (which it hasn’t seen during training) Repeat with different training/test splits and average the results There are several variations of cross-validation: 8.3.2.1 K-Fold Cross-Validation In k-fold cross-validation, we: 1. Divide the data into k equally sized subsets (folds) 2. Use k-1 folds for training and the remaining fold for testing 3. Repeat k times, each time using a different fold as the test set 4. Average the k test performance metrics This visualization shows how 5-fold cross-validation works: # Visualization of k-fold cross-validation set.seed(123) n_data &lt;- 20 # K-Fold CV (k=5) cv_data &lt;- tibble( index = 1:n_data, value = rnorm(n_data) ) set.seed(456) cv_data$fold &lt;- sample(rep(1:5, length.out = n_data)) # Create visualization data for k-fold CV cv_viz_data &lt;- tibble( iteration = rep(1:5, each = n_data), data_point = rep(1:n_data, 5), role = ifelse(cv_data$fold[data_point] == iteration, &quot;test&quot;, &quot;train&quot;), approach = &quot;5-Fold CV&quot; ) # Define a consistent color scheme role_colors &lt;- c(&quot;train&quot; = &quot;steelblue&quot;, &quot;test&quot; = &quot;firebrick&quot;) # Create the plot ggplot(cv_viz_data, aes(x = data_point, y = iteration, fill = role)) + geom_tile(color = &quot;white&quot;, size = 0.5) + scale_fill_manual(values = role_colors, name = &quot;Data Role&quot;) + labs( title = &quot;K-Fold Cross-Validation (k=5)&quot;, subtitle = &quot;Each row shows one iteration, columns represent data points&quot;, x = &quot;Data Point&quot;, y = &quot;Iteration&quot;, fill = &quot;Data Usage&quot; ) + theme_minimal() + theme( panel.grid = element_blank(), legend.position = &quot;bottom&quot; ) 8.3.2.2 Leave-One-Out Cross-Validation (LOO-CV) Leave-one-out is a special case of k-fold cross-validation where k equals the number of data points. In each iteration, we: 1. Hold out a single observation for testing 2. Train on all other observations 3. Repeat for every observation 4. Average the performance metrics This approach can be very computationally intensive for large datasets or complex models. # Create visualization data for LOO CV loo_viz_data &lt;- tibble( iteration = rep(1:n_data, each = n_data), data_point = rep(1:n_data, n_data), role = ifelse(iteration == data_point, &quot;test&quot;, &quot;train&quot;), approach = &quot;LOO CV&quot; ) # Create the plot ggplot(loo_viz_data, aes(x = data_point, y = iteration, fill = role)) + geom_tile(color = &quot;white&quot;, size = 0.5) + scale_fill_manual(values = role_colors, name = &quot;Data Role&quot;) + labs( title = &quot;Leave-One-Out Cross-Validation&quot;, subtitle = &quot;Each row shows one iteration, columns represent data points&quot;, x = &quot;Data Point&quot;, y = &quot;Iteration&quot;, fill = &quot;Data Usage&quot; ) + theme_minimal() + theme( panel.grid = element_blank(), legend.position = &quot;bottom&quot; ) 8.3.3 Cross-Validation in Bayesian Models Cross-validation is especially important in Bayesian modeling for several reasons: Bayesian models can be highly parameterized and prone to overfitting Prior choices can significantly influence model performance The complexity of hierarchical structures needs careful validation We often need to compare competing theoretical accounts However, cross-validation for Bayesian models presents two key challenges: Computational cost: Bayesian models fitted with MCMC can take hours or days to run, making it impractical to refit them k times for cross-validation Proper scoring: We need appropriate metrics for evaluating predictive performance in a Bayesian framework Next, we’ll see how these challenges are addressed. 8.3.4 Expected Log Predictive Density (ELPD) When comparing Bayesian models, we use the expected log predictive density (ELPD) as our metric. This measures how well the model predicts new data points on the log scale. For a single observation, the log predictive density is: \\[\\log p(y_i | y_{-i})\\] where \\(y_i\\) is the observation we’re trying to predict, and \\(y_{-i}\\) represents all other observations that were used for training. For the entire dataset, we sum across all observations: \\[\\text{ELPD} = \\sum_{i=1}^{n} \\log p(y_i | y_{-i})\\] The ELPD has several desirable properties: It accounts for the full predictive distribution, not just point estimates It automatically penalizes overconfident models Higher values indicate better predictive performance Computing the ELPD exactly requires fitting the model n times (for n data points), which brings us back to the computational challenge. 8.4 PSIS-LOO: An Efficient Approximation to LOO-CV For complex Bayesian models, true leave-one-out cross-validation (LOO-CV) is often computationally infeasible. Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO) provides an elegant solution by approximating LOO-CV using just a single model fit. 8.4.1 How Importance Sampling Works When we fit a Bayesian model, we obtain samples from the posterior distribution \\(p(\\theta|y_1,...,y_n)\\) - the distribution of model parameters given all observations. For LOO-CV, we need \\(p(\\theta|y_1,...,y_{i-1},y_{i+1},...,y_n)\\) - the distribution without the i-th observation. Importance sampling bridges this gap by reweighting the full posterior samples to approximate the LOO posterior. The importance weights for the i-th observation are: \\[w_i(\\theta) = \\frac{p(\\theta|y_1,...,y_{i-1},y_{i+1},...,y_n)}{p(\\theta|y_1,...,y_n)} \\propto \\frac{1}{p(y_i|\\theta)}\\] By Bayes’ theorem, this simplifies to: w_i(θ) ∝ 1 / p(yᵢ|θ) These weights effectively “undo” the influence of the i-th observation on the posterior. However, standard importance sampling can be unstable when: * The full posterior and LOO posterior differ substantially * Some importance weights become extremely large * The variance of the weights is high 8.4.2 Pareto Smoothing for Stability Pareto smoothing improves the reliability of importance sampling: Fit a generalized Pareto distribution to the largest importance weights Replace the largest weights with values from this smoothed distribution Use the modified weights for more stable LOO estimation The diagnostic parameter k from the Pareto fit helps assess reliability: * k &lt; 0.5: Reliable estimation * 0.5 &lt; k &lt; 0.7: Somewhat reliable, proceed with caution * k &gt; 0.7: Unreliable, consider using other methods for this observation These diagnostics help identify problematic observations that might require more attention or alternative methods. 8.4.3 The Complete PSIS-LOO Process The full PSIS-LOO method follows these steps: Fit the Bayesian model once to all available data For each observation i: Calculate raw importance weights using the log-likelihood: w_i(θ) ∝ 1/p(yᵢ|θ) Apply Pareto smoothing to stabilize the largest weights Normalize the smoothed weights Use the weights to compute the expected log predictive density (ELPD) Sum the individual ELPD contributions to get the overall PSIS-LOO estimate [I NEED TO FIND A BETTER WAY TO EXPLAIN AND VISUALIZE!] # Create improved visualizations of PSIS-LOO set.seed(789) # Generate some sample data n &lt;- 50 x &lt;- seq(-3, 3, length.out = n) y_true &lt;- 2 + 1.5 * x + 0.5 * x^2 y &lt;- y_true + rnorm(n, 0, 2) data &lt;- data.frame(x = x, y = y) # Fit a simple model to all data (this will be our &quot;Bayesian model&quot;) full_model &lt;- lm(y ~ x + I(x^2)) full_predictions &lt;- predict(full_model) # Generate posterior samples (simplified for illustration) n_samples &lt;- 1000 beta_samples &lt;- mvrnorm(n_samples, coef(full_model), vcov(full_model)) sigma_samples &lt;- rep(sigma(full_model), n_samples) # Function to compute log-likelihood for each observation given parameters log_lik &lt;- function(beta, sigma, x, y) { mu &lt;- beta[1] + beta[2] * x + beta[3] * x^2 dnorm(y, mean = mu, sd = sigma, log = TRUE) } # Compute log-likelihood matrix (n_samples × n_observations) log_lik_matrix &lt;- matrix(NA, nrow = n_samples, ncol = n) for (i in 1:n_samples) { for (j in 1:n) { log_lik_matrix[i, j] &lt;- log_lik(beta_samples[i,], sigma_samples[i], x[j], y[j]) } } # Select a point for demonstration loo_idx &lt;- 25 # This will be our &quot;left out&quot; point # Calculate raw importance weights for the selected point log_weights &lt;- -log_lik_matrix[, loo_idx] # Negative log-likelihood weights &lt;- exp(log_weights - max(log_weights)) # Stabilize by subtracting max # Function to simulate Pareto smoothing (simplified) pareto_smooth &lt;- function(weights) { # Sort weights sorted_weights &lt;- sort(weights, decreasing = TRUE) n_tail &lt;- min(500, length(weights) / 5) # Top 20% or 500, whichever is smaller # Fit generalized Pareto to the tail (simplified) # In practice, actual fitting would be used k &lt;- 0.4 # Simulated Pareto shape parameter sigma &lt;- mean(sorted_weights[1:n_tail]) * 0.8 # Smooth the tail weights for (i in 1:n_tail) { q &lt;- (i - 0.5) / n_tail sorted_weights[i] &lt;- sigma/k * ((1 - q)^(-k) - 1) } # Rearrange to original order smoothed_weights &lt;- weights smoothed_weights[order(weights, decreasing = TRUE)[1:n_tail]] &lt;- sorted_weights[1:n_tail] return(smoothed_weights) } # Apply simulated Pareto smoothing smoothed_weights &lt;- pareto_smooth(weights) # Normalize weights raw_norm_weights &lt;- weights / sum(weights) smoothed_norm_weights &lt;- smoothed_weights / sum(smoothed_weights) # Calculate true LOO prediction for comparison loo_model &lt;- lm(y[-loo_idx] ~ x[-loo_idx] + I(x[-loo_idx]^2)) loo_prediction &lt;- predict(loo_model, newdata = data.frame(x = x[loo_idx])) # Calculate PSIS-LOO prediction for the point psis_prediction &lt;- 0 for (i in 1:n_samples) { beta &lt;- beta_samples[i,] psis_prediction &lt;- psis_prediction + smoothed_norm_weights[i] * (beta[1] + beta[2] * x[loo_idx] + beta[3] * x[loo_idx]^2) } # Create visualization data viz_data &lt;- data.frame( x = x, y = y, full_fit = full_predictions, highlighted = ifelse(seq_along(x) == loo_idx, &quot;Point Left Out&quot;, &quot;Other Points&quot;) ) # Sample subset for posterior draws sample_indices &lt;- sample(1:n_samples, 50) posterior_lines &lt;- data.frame( x = rep(x, length(sample_indices)), sample = rep(1:length(sample_indices), each = n), y_pred = NA ) for (i in 1:length(sample_indices)) { s &lt;- sample_indices[i] beta &lt;- beta_samples[s,] posterior_lines$y_pred[posterior_lines$sample == i] &lt;- beta[1] + beta[2] * x + beta[3] * x^2 } # Create data for weight visualization weight_data &lt;- data.frame( weight_idx = 1:100, # Show only first 100 weights for clarity raw_weight = raw_norm_weights[1:100], smoothed_weight = smoothed_norm_weights[1:100] ) # PLOT 1: Data and model fit p1 &lt;- ggplot(viz_data, aes(x = x, y = y)) + # Add posterior draws geom_line(data = posterior_lines, aes(y = y_pred, group = sample), alpha = 0.1, color = &quot;blue&quot;) + # Add full model fit geom_line(aes(y = full_fit), color = &quot;blue&quot;, size = 1) + # Add data points geom_point(aes(color = highlighted, size = highlighted), alpha = 0.7) + # Add predictions for the left-out point geom_segment( aes(x = x[loo_idx], y = y[loo_idx], xend = x[loo_idx], yend = loo_prediction), arrow = arrow(length = unit(0.3, &quot;cm&quot;)), color = &quot;red&quot;, linetype = &quot;dashed&quot; ) + # Add the true LOO prediction point geom_point( aes(x = x[loo_idx], y = loo_prediction), color = &quot;red&quot;, size = 4, shape = 18 ) + geom_segment( aes(x = x[loo_idx] + 0.1, y = y[loo_idx], xend = x[loo_idx] + 0.1, yend = psis_prediction), arrow = arrow(length = unit(0.3, &quot;cm&quot;)), color = &quot;purple&quot;, linetype = &quot;dashed&quot; ) + # Add the PSIS-LOO prediction point geom_point( aes(x = x[loo_idx] + 0.1, y = psis_prediction), color = &quot;purple&quot;, size = 4, shape = 18 ) + # Add legend annotations annotate(&quot;text&quot;, x = x[loo_idx] - 0.3, y = (y[loo_idx] + loo_prediction)/2, label = &quot;True LOO&quot;, color = &quot;red&quot;, hjust = 1) + annotate(&quot;text&quot;, x = x[loo_idx] + 0.4, y = (y[loo_idx] + psis_prediction)/2, label = &quot;PSIS-LOO&quot;, color = &quot;purple&quot;, hjust = 0) + # Styling scale_color_manual(values = c(&quot;Other Points&quot; = &quot;black&quot;, &quot;Point Left Out&quot; = &quot;red&quot;)) + scale_size_manual(values = c(&quot;Other Points&quot; = 2, &quot;Point Left Out&quot; = 3)) + labs( title = &quot;PSIS-LOO Approximation of Leave-One-Out Cross-Validation&quot;, subtitle = &quot;Comparing true LOO (red) vs. PSIS-LOO approximation (purple)&quot;, x = &quot;x&quot;, y = &quot;y&quot;, color = NULL, size = NULL ) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) # PLOT 2: Importance weights before and after Pareto smoothing p2 &lt;- ggplot(weight_data, aes(x = weight_idx)) + # Raw weights geom_segment(aes(xend = weight_idx, y = 0, yend = raw_weight), color = &quot;gray50&quot;, alpha = 0.7) + geom_point(aes(y = raw_weight), color = &quot;blue&quot;, size = 2, alpha = 0.7) + # Smoothed weights geom_segment(aes(xend = weight_idx, y = 0, yend = smoothed_weight), color = &quot;gray50&quot;, alpha = 0.7) + geom_point(aes(y = smoothed_weight), color = &quot;purple&quot;, size = 2, alpha = 0.7) + # Connect raw and smoothed weights for visual clarity geom_segment(aes(xend = weight_idx, y = raw_weight, yend = smoothed_weight), arrow = arrow(length = unit(0.1, &quot;cm&quot;)), color = &quot;purple&quot;, alpha = 0.3) + # Styling labs( title = &quot;Importance Sampling Weights for PSIS-LOO&quot;, subtitle = &quot;Pareto smoothing stabilizes extreme weights (arrows show the adjustment)&quot;, x = &quot;Sample Index (ordered by weight)&quot;, y = &quot;Normalized Weight&quot; ) + scale_x_continuous(breaks = seq(0, 100, by = 20)) + theme_minimal() + annotate(&quot;text&quot;, x = 20, y = max(weight_data$raw_weight) * 0.9, label = &quot;Before smoothing&quot;, color = &quot;blue&quot;, hjust = 0) + annotate(&quot;text&quot;, x = 60, y = max(weight_data$smoothed_weight) * 0.9, label = &quot;After Pareto smoothing&quot;, color = &quot;purple&quot;, hjust = 0) # PLOT 3: Conceptual diagram of PSIS-LOO process p3 &lt;- ggplot() + # Add process steps annotate(&quot;rect&quot;, xmin = 0.5, xmax = 3.5, ymin = 4, ymax = 5, fill = &quot;lightblue&quot;, alpha = 0.7, color = &quot;black&quot;) + annotate(&quot;text&quot;, x = 2, y = 4.5, label = &quot;1. Fit Model to All Data\\nObtain posterior samples p(θ|y₁,...,yₙ)&quot;) + annotate(&quot;rect&quot;, xmin = 0.5, xmax = 3.5, ymin = 2.5, ymax = 3.5, fill = &quot;lightgreen&quot;, alpha = 0.7, color = &quot;black&quot;) + annotate(&quot;text&quot;, x = 2, y = 3, label = &quot;2. Calculate Log-Likelihood for Each Observation\\nℓᵢ(θ) = log p(yᵢ|θ)&quot;) + annotate(&quot;rect&quot;, xmin = 0.5, xmax = 3.5, ymin = 1, ymax = 2, fill = &quot;lightyellow&quot;, alpha = 0.7, color = &quot;black&quot;) + annotate(&quot;text&quot;, x = 2, y = 1.5, label = &quot;3. Compute Importance Weights\\nwᵢ(θ) ∝ 1/p(yᵢ|θ)&quot;) + annotate(&quot;rect&quot;, xmin = 0.5, xmax = 3.5, ymin = -0.5, ymax = 0.5, fill = &quot;mistyrose&quot;, alpha = 0.7, color = &quot;black&quot;) + annotate(&quot;text&quot;, x = 2, y = 0, label = &quot;4. Apply Pareto Smoothing\\nStabilize extreme weights&quot;) + annotate(&quot;rect&quot;, xmin = 0.5, xmax = 3.5, ymin = -2, ymax = -1, fill = &quot;lavender&quot;, alpha = 0.7, color = &quot;black&quot;) + annotate(&quot;text&quot;, x = 2, y = -1.5, label = &quot;5. Compute ELPD Using Smoothed Weights\\nApproximate leave-one-out prediction&quot;) + # Add arrows annotate(&quot;segment&quot;, x = 2, y = 4, xend = 2, yend = 3.5, arrow = arrow(length = unit(0.2, &quot;cm&quot;))) + annotate(&quot;segment&quot;, x = 2, y = 2.5, xend = 2, yend = 2, arrow = arrow(length = unit(0.2, &quot;cm&quot;))) + annotate(&quot;segment&quot;, x = 2, y = 1, xend = 2, yend = 0.5, arrow = arrow(length = unit(0.2, &quot;cm&quot;))) + annotate(&quot;segment&quot;, x = 2, y = -0.5, xend = 2, yend = -1, arrow = arrow(length = unit(0.2, &quot;cm&quot;))) + # Explain the advantage annotate(&quot;text&quot;, x = 2, y = -2.7, label = &quot;PSIS-LOO provides accurate LOO approximation\\nfrom a single model fit&quot;, fontface = &quot;italic&quot;) + # Add Pareto k value diagnostic information annotate(&quot;rect&quot;, xmin = 3.7, xmax = 5.7, ymin = -0.2, ymax = 1.5, fill = &quot;white&quot;, alpha = 0.8, color = &quot;black&quot;, linetype = &quot;dashed&quot;) + annotate(&quot;text&quot;, x = 4.7, y = 1.2, label = &quot;Pareto k Diagnostics&quot;, fontface = &quot;bold&quot;) + annotate(&quot;text&quot;, x = 4.7, y = 0.8, label = &quot;k &lt; 0.5: Reliable&quot;, color = &quot;darkgreen&quot;) + annotate(&quot;text&quot;, x = 4.7, y = 0.4, label = &quot;0.5 &lt; k &lt; 0.7: Use caution&quot;, color = &quot;darkorange&quot;) + annotate(&quot;text&quot;, x = 4.7, y = 0, label = &quot;k &gt; 0.7: Unreliable&quot;, color = &quot;darkred&quot;) + # Styling theme_void() + labs(title = &quot;PSIS-LOO Process Flow&quot;) # Arrange the plots in a grid p1 / p2 p3 The red circle represents our “left-out” data point, while the blue line shows the model fit using all data points (including that red circle). The red diamond shows the prediction we get when we actually refit the model without the red circle. When we fit a model (the blue line), each data point “pulls” the model fit toward itself to some degree. The red circle data point influenced the original model to bend slightly closer to it. This is why the red circle appears relatively close to the blue line—it helped shape that line! When we perform true leave-one-out cross-validation, we remove that red circle point completely and refit the model using only the remaining data. Without the “pull” from that point, the model (which we don’t directly show) follows a slightly different path determined solely by the other points. The prediction from this new model (the red diamond) naturally lands in a different position. This difference between the original model prediction and the leave-one-out prediction is exactly what makes cross-validation valuable: It reveals how much individual data points influence your model It gives a more honest assessment of how your model will perform on truly unseen data Large differences can help identify influential or outlier points The purple diamond (PSIS-LOO prediction) attempts to approximate where that red diamond would be without actually refitting the model, by mathematically down-weighting the influence of the left-out point—which is why it’s positioned close to the red diamond if the approximation is working well. 8.5 Simulation-Based Model Comparison Now that we understand the principles, let’s apply these techniques to compare cognitive models using a simulation-based approach. This approach has two key advantages: We know the “ground truth” (which model generated each dataset) We can systematically test if our comparison methods work We’ll simulate data from two different model types: Biased agents: Make choices based on a fixed bias parameter Memory agents: Make choices influenced by the history of their opponent’s actions By fitting both models to data generated from each type of agent, we can evaluate whether our model comparison techniques correctly identify the true generating model. 8.5.1 Define Simulation Parameters # Shared parameters agents &lt;- 100 # Number of agents to simulate trials &lt;- 120 # Number of trials per agent noise &lt;- 0 # Base noise level (probability of random choice) # Biased agents parameters (on log-odds scale) rateM &lt;- 1.386 # Population mean of bias (~0.8 in probability space) rateSD &lt;- 0.65 # Population SD of bias (~0.1 in probability space) # Memory agents parameters biasM &lt;- 0 # Population mean of baseline bias biasSD &lt;- 0.1 # Population SD of baseline bias betaM &lt;- 1.5 # Population mean of memory sensitivity betaSD &lt;- 0.3 # Population SD of memory sensitivity # Print the parameters in probability space for easier interpretation cat(&quot;Biased agents have mean choice probability of:&quot;, round(plogis(rateM), 2), &quot;\\n&quot;) ## Biased agents have mean choice probability of: 0.8 cat(&quot;Memory agents have mean baseline bias of:&quot;, round(plogis(biasM), 2), &quot;\\n&quot;) ## Memory agents have mean baseline bias of: 0.5 cat(&quot;Memory agents have mean sensitivity to opponent&#39;s history of:&quot;, betaM, &quot;\\n&quot;) ## Memory agents have mean sensitivity to opponent&#39;s history of: 1.5 8.5.2 Define Agent Functions # Random agent function: makes choices based on bias parameter RandomAgentNoise_f &lt;- function(rate, noise) { # Generate choice based on agent&#39;s bias parameter (on log-odds scale) choice &lt;- rbinom(1, 1, plogis(rate)) # With probability &#39;noise&#39;, override choice with random 50/50 selection if (rbinom(1, 1, noise) == 1) { choice = rbinom(1, 1, 0.5) } return(choice) } # Memory agent function: makes choices based on opponent&#39;s historical choices MemoryAgentNoise_f &lt;- function(bias, beta, otherRate, noise) { # Calculate log-odds of choosing option 1, influenced by opponent&#39;s historical choice rate log_odds &lt;- bias + beta * qlogis(otherRate) # Convert to probability and generate choice choice &lt;- rbinom(1, 1, plogis(log_odds)) # With probability &#39;noise&#39;, override choice with random 50/50 selection if (rbinom(1, 1, noise) == 1) { choice = rbinom(1, 1, 0.5) } return(choice) } 8.5.3 Generate Simulation Data # Function to simulate one run of agents playing against each other simulate_agents &lt;- function() { # Create empty dataframe to store results d &lt;- NULL # Loop through all agents for (agent in 1:agents) { # Sample individual agent parameters from population distributions rate &lt;- rnorm(1, rateM, rateSD) # Individual bias for random agent bias &lt;- rnorm(1, biasM, biasSD) # Individual baseline bias for memory agent beta &lt;- rnorm(1, betaM, betaSD) # Individual memory sensitivity # Initialize choice vectors randomChoice &lt;- rep(NA, trials) memoryChoice &lt;- rep(NA, trials) memoryRate &lt;- rep(NA, trials) # Generate choices for each trial for (trial in 1:trials) { # Random (biased) agent makes choice randomChoice[trial] &lt;- RandomAgentNoise_f(rate, noise) # Memory agent responds (with no history for first trial) if (trial == 1) { memoryChoice[trial] &lt;- rbinom(1, 1, 0.5) # First choice is random } else { # Use mean of random agent&#39;s previous choices as &quot;memory&quot; memoryChoice[trial] &lt;- MemoryAgentNoise_f( bias, beta, mean(randomChoice[1:(trial - 1)], na.rm = TRUE), noise ) } } # Create data frame for this agent temp &lt;- tibble( agent = agent, trial = seq(trials), randomChoice = randomChoice, randomRate = rate, memoryChoice = memoryChoice, noise = noise, rateM = rateM, rateSD = rateSD, bias = bias, beta = beta, biasM = biasM, biasSD = biasSD, betaM = betaM, betaSD = betaSD ) # Append to main dataframe if (agent &gt; 1) { d &lt;- rbind(d, temp) } else { d &lt;- temp } } # Calculate cumulative choice rates d &lt;- d %&gt;% group_by(agent) %&gt;% mutate( randomRate_cumulative = cumsum(randomChoice) / seq_along(randomChoice), memoryRate_cumulative = cumsum(memoryChoice) / seq_along(memoryChoice) ) return(d) } # Generate the data d &lt;- simulate_agents() # Show a quick summary of the generated data summary_stats &lt;- d %&gt;% filter(trial == trials) %&gt;% # Get final trial stats summarize( mean_random_rate = mean(randomRate_cumulative), mean_memory_rate = mean(memoryRate_cumulative) ) print(summary_stats) ## # A tibble: 100 × 3 ## agent mean_random_rate mean_memory_rate ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.892 0.833 ## 2 2 0.775 0.883 ## 3 3 0.917 0.958 ## 4 4 0.883 0.992 ## 5 5 0.783 0.692 ## 6 6 0.825 0.867 ## 7 7 0.875 0.983 ## 8 8 0.842 0.925 ## 9 9 0.608 0.608 ## 10 10 0.875 0.975 ## # ℹ 90 more rows 8.5.4 Visualize the Simulation Data # Select a random sample of agents to visualize sample_agents &lt;- sample(unique(d$agent), 6) # Filter data for these agents sample_data &lt;- d %&gt;% filter(agent %in% sample_agents) # Create plot showing choice patterns p1 &lt;- ggplot(sample_data, aes(x = trial)) + geom_line(aes(y = randomRate_cumulative, color = &quot;Biased Agent&quot;), size = 1) + geom_line(aes(y = memoryRate_cumulative, color = &quot;Memory Agent&quot;), size = 1) + geom_hline(yintercept = 0.5, linetype = &quot;dashed&quot;, color = &quot;gray50&quot;) + facet_wrap(~agent, ncol = 3) + scale_color_brewer(palette = &quot;Set1&quot;) + labs( title = &quot;Choice Patterns of Biased vs. Memory Agents&quot;, subtitle = &quot;Lines show cumulative proportion of &#39;right&#39; choices&quot;, x = &quot;Trial&quot;, y = &quot;Cumulative Proportion Right Choices&quot;, color = &quot;Agent Type&quot; ) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) # Create density plot of final choice rates p2 &lt;- d %&gt;% filter(trial == trials) %&gt;% dplyr::select(agent, randomRate_cumulative, memoryRate_cumulative) %&gt;% pivot_longer(cols = c(randomRate_cumulative, memoryRate_cumulative), names_to = &quot;agent_type&quot;, values_to = &quot;rate&quot;) %&gt;% mutate(agent_type = ifelse(agent_type == &quot;randomRate_cumulative&quot;, &quot;Biased Agent&quot;, &quot;Memory Agent&quot;)) %&gt;% ggplot(aes(x = rate, fill = agent_type)) + geom_density(alpha = 0.5) + scale_fill_brewer(palette = &quot;Set1&quot;) + labs( title = &quot;Distribution of Final Choice Rates&quot;, subtitle = &quot;After 120 trials&quot;, x = &quot;Proportion of Right Choices&quot;, y = &quot;Density&quot;, fill = &quot;Agent Type&quot; ) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) # Combine the plots p1 / p2 + plot_layout(heights = c(2, 1)) 8.5.5 Prepare Data for Stan Models # Prepare the data for the biased agent model d_random &lt;- d %&gt;% dplyr::select(agent, randomChoice) %&gt;% mutate(row = row_number()) %&gt;% pivot_wider(names_from = agent, values_from = randomChoice) d_memory &lt;- d %&gt;% dplyr::select(agent, memoryChoice) %&gt;% mutate(row = row_number()) %&gt;% pivot_wider(names_from = agent, values_from = memoryChoice) # Create data lists for Stan data_biased &lt;- list( trials = trials, agents = agents, h = as.matrix(d_random[, 2:(agents + 1)]), # Matrix where columns are agents, rows are trials other = as.matrix(d_memory[, 2:(agents + 1)]) # Other agent&#39;s choices (for memory model) ) data_memory &lt;- list( trials = trials, agents = agents, h = as.matrix(d_memory[, 2:(agents + 1)]), # For memory agent model, we&#39;ll predict memory choices other = as.matrix(d_random[, 2:(agents + 1)]) # And use random agent choices as input ) # Show dimensions of the data matrices cat(&quot;Dimensions of data matrices for biased agent model:\\n&quot;) ## Dimensions of data matrices for biased agent model: cat(&quot;h matrix:&quot;, dim(data_biased$h), &quot;\\n&quot;) ## h matrix: 120 100 cat(&quot;other matrix:&quot;, dim(data_biased$other), &quot;\\n\\n&quot;) ## other matrix: 120 100 cat(&quot;Dimensions of data matrices for memory agent model:\\n&quot;) ## Dimensions of data matrices for memory agent model: cat(&quot;h matrix:&quot;, dim(data_memory$h), &quot;\\n&quot;) ## h matrix: 120 100 cat(&quot;other matrix:&quot;, dim(data_memory$other), &quot;\\n&quot;) ## other matrix: 120 100 8.6 Implementing Models for Comparison Now we need to implement our two competing models in Stan. Both will be multilevel (hierarchical) to account for individual differences among agents. The key feature for model comparison is that we’ll include a log_lik calculation in the generated quantities block of each model. 8.6.1 Understanding Log-Likelihood in Model Comparison When comparing models, we need a way to quantify how well each model explains the observed data. The log-likelihood represents the logarithm of the probability that a model would generate the observed data given specific parameter values. Given certain values for our parameters (let’s say a bias of 0 and beta for memory of 1) and for our variables (let’s say the vector of memory values estimated by the agent on a trial by trial basis), the model will predict a certain distribution of outcomes, that is, a certain distribution of choices (n times right, m times left hand). Comparing this to the actual data, we can identify how likely the model is to produce it. In other words, the probability that the model will actually generate the data we observed out of all its possible outcomes. Remember that we are doing Bayesian statistics, so this probability needs to be combined with the probability of the parameter values given the priors on those parameters. This would give us a posterior likelihood of the model’s parameter values given the data. The last step is that we need to work on a log scale. Working on a log scale is very useful because it avoids low probabilities (close to 0) being rounded down to exactly 0. By log-transforming the posterior likelihood, we now have the log-posterior likelihood. Now, remember that our agent’s memory varies on a trial by trial level. In other words, for each data point, for each agent we can calculate separate values of log-posterior likelihood for each of the possible values of the parameters. That is, we can have a distribution of log-posterior likelihood for each data point. Telling Stan to calculate these distributions is straightforward: we add to the generated quantities block the same log probability statements used in the model block, but save them to variables instead of adding them to the target. N.B. Some of you might be wandering: if Stan is already using the log-posterior probability in the sampling process, why do we need to tell it to calculate and save it? Fair enough point. But Stan does not save by default (to avoid clogging your computer with endless data) and we need the log posterior likelihood saved as “log_lik” in order to be able to use more automated functions later on. 8.6.2 Multilevel Biased Agent Model Here’s the Stan model for the biased agent (remember that we will add the log_lik part in the generated quantities block!). # Stan model for multilevel biased agent stan_biased_model &lt;- &quot; // Multilevel Biased Agent Model // // This model assumes each agent has a fixed bias (theta) that determines // their probability of choosing option 1 (&#39;right&#39;) functions{ // Helper function for generating truncated normal random numbers real normal_lb_rng(real mu, real sigma, real lb) { real p = normal_cdf(lb | mu, sigma); // CDF for bounds real u = uniform_rng(p, 1); return (sigma * inv_Phi(u)) + mu; // Inverse CDF for value } } // Input data data { int&lt;lower = 1&gt; trials; // Number of trials per agent int&lt;lower = 1&gt; agents; // Number of agents array[trials, agents] int h; // Choice data (0/1 for each trial and agent) } // Parameters to be estimated parameters { real thetaM; // Population mean of bias (log-odds scale) real&lt;lower = 0&gt; thetaSD; // Population SD of bias array[agents] real theta; // Individual agent biases (log-odds scale) } // Model definition model { // Population-level priors target += normal_lpdf(thetaM | 0, 1); // Prior for SD with lower bound at zero (half-normal) target += normal_lpdf(thetaSD | 0, .3) - normal_lccdf(0 | 0, .3); // Individual-level parameters drawn from population distribution target += normal_lpdf(theta | thetaM, thetaSD); // Likelihood: predict each agent&#39;s choices for (i in 1:agents) target += bernoulli_logit_lpmf(h[,i] | theta[i]); } // Additional quantities to calculate generated quantities{ // Prior predictive samples real thetaM_prior; real&lt;lower=0&gt; thetaSD_prior; real&lt;lower=0, upper=1&gt; theta_prior; real&lt;lower=0, upper=1&gt; theta_posterior; // Posterior predictive samples int&lt;lower=0, upper = trials&gt; prior_preds; int&lt;lower=0, upper = trials&gt; posterior_preds; // Log-likelihood for each observation (crucial for model comparison) array[trials, agents] real log_lik; // Generate prior samples thetaM_prior = normal_rng(0, 1); thetaSD_prior = normal_lb_rng(0, 0.3, 0); theta_prior = inv_logit(normal_rng(thetaM_prior, thetaSD_prior)); theta_posterior = inv_logit(normal_rng(thetaM, thetaSD)); // Generate predictions from prior and posterior prior_preds = binomial_rng(trials, inv_logit(thetaM_prior)); posterior_preds = binomial_rng(trials, inv_logit(thetaM)); // Calculate log-likelihood for each observation for (i in 1:agents){ for (t in 1:trials){ log_lik[t, i] = bernoulli_logit_lpmf(h[t, i] | theta[i]); } } } &quot; # Write the Stan model to a file write_stan_file( stan_biased_model, dir = &quot;stan/&quot;, basename = &quot;W7_MultilevelBias.stan&quot; ) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W7_MultilevelBias.stan&quot; # Compile the model file &lt;- file.path(&quot;stan/W7_MultilevelBias.stan&quot;) mod_biased &lt;- cmdstan_model( file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;) ) Let’s break down this Stan model: Data Block: Defines the input data - number of trials, number of agents, and the choice data matrix. Parameters Block: Specifies the parameters we want to estimate: thetaM: The population mean bias thetaSD: The population standard deviation of bias theta: Individual bias parameters for each agent Model Block: Defines the prior distributions and likelihood function: Priors for population parameters Individual parameters drawn from the population distribution Likelihood of observing the choice data given the parameters Generated Quantities Block: Calculates additional quantities of interest: Prior and posterior predictive samples Log-likelihood for each observation - this is crucial for model comparison The most important part for model comparison is the log_lik calculation in the generated quantities block. This computes the log probability of each observation given the model and its parameters, which we’ll use for comparing models. 8.6.3 Multilevel Memory Agent Model Now let’s implement our second model - the memory agent model: # Stan model for multilevel memory agent stan_memory_model &lt;- &quot; // Multilevel Memory Agent Model // // This model assumes agents make choices based on their memory of // the opponent&#39;s previous choices. functions{ // Helper function for generating truncated normal random numbers real normal_lb_rng(real mu, real sigma, real lb) { real p = normal_cdf(lb | mu, sigma); // CDF for bounds real u = uniform_rng(p, 1); return (sigma * inv_Phi(u)) + mu; // Inverse CDF for value } } // Input data data { int&lt;lower = 1&gt; trials; // Number of trials per agent int&lt;lower = 1&gt; agents; // Number of agents array[trials, agents] int h; // Choice data (0/1 for each trial and agent) array[trials, agents] int other;// Opponent&#39;s choices (input to memory) } // Parameters to be estimated parameters { real biasM; // Population mean baseline bias real betaM; // Population mean memory sensitivity vector&lt;lower = 0&gt;[2] tau; // Population SDs for bias and beta matrix[2, agents] z_IDs; // Standardized individual parameters (non-centered) cholesky_factor_corr[2] L_u; // Cholesky factor of correlation matrix } // Transformed parameters (derived quantities) transformed parameters { // Memory state for each agent and trial array[trials, agents] real memory; // Individual parameters (bias and beta for each agent) matrix[agents, 2] IDs; // Transform standardized parameters to actual parameters (non-centered parameterization) IDs = (diag_pre_multiply(tau, L_u) * z_IDs)&#39;; // Calculate memory states based on opponent&#39;s choices for (agent in 1:agents){ for (trial in 1:trials){ // Initialize first trial with neutral memory if (trial == 1) { memory[trial, agent] = 0.5; } // Update memory based on opponent&#39;s choices if (trial &lt; trials){ // Simple averaging memory update memory[trial + 1, agent] = memory[trial, agent] + ((other[trial, agent] - memory[trial, agent]) / trial); // Handle edge cases to avoid numerical issues if (memory[trial + 1, agent] == 0){memory[trial + 1, agent] = 0.01;} if (memory[trial + 1, agent] == 1){memory[trial + 1, agent] = 0.99;} } } } } // Model definition model { // Population-level priors target += normal_lpdf(biasM | 0, 1); target += normal_lpdf(tau[1] | 0, .3) - normal_lccdf(0 | 0, .3); // Half-normal for SD target += normal_lpdf(betaM | 0, .3); target += normal_lpdf(tau[2] | 0, .3) - normal_lccdf(0 | 0, .3); // Half-normal for SD // Prior for correlation matrix target += lkj_corr_cholesky_lpdf(L_u | 2); // Standardized individual parameters have standard normal prior target += std_normal_lpdf(to_vector(z_IDs)); // Likelihood: predict each agent&#39;s choices for (agent in 1:agents){ for (trial in 1:trials){ // choice ~ bias + memory_effect*beta target += bernoulli_logit_lpmf(h[trial, agent] | biasM + IDs[agent, 1] + memory[trial, agent] * (betaM + IDs[agent, 2])); } } } // Additional quantities to calculate generated quantities{ // Prior predictive samples real biasM_prior; real&lt;lower=0&gt; biasSD_prior; real betaM_prior; real&lt;lower=0&gt; betaSD_prior; real bias_prior; real beta_prior; // Posterior predictive samples for different memory conditions array[agents] int&lt;lower=0, upper = trials&gt; prior_preds0; array[agents] int&lt;lower=0, upper = trials&gt; prior_preds1; array[agents] int&lt;lower=0, upper = trials&gt; prior_preds2; array[agents] int&lt;lower=0, upper = trials&gt; posterior_preds0; array[agents] int&lt;lower=0, upper = trials&gt; posterior_preds1; array[agents] int&lt;lower=0, upper = trials&gt; posterior_preds2; // Log-likelihood for each observation (crucial for model comparison) array[trials, agents] real log_lik; // Generate prior samples biasM_prior = normal_rng(0,1); biasSD_prior = normal_lb_rng(0,0.3,0); betaM_prior = normal_rng(0,1); betaSD_prior = normal_lb_rng(0,0.3,0); bias_prior = normal_rng(biasM_prior, biasSD_prior); beta_prior = normal_rng(betaM_prior, betaSD_prior); // Generate predictions for different memory conditions for (i in 1:agents){ // Prior predictions for low, medium, high memory prior_preds0[i] = binomial_rng(trials, inv_logit(bias_prior + 0 * beta_prior)); prior_preds1[i] = binomial_rng(trials, inv_logit(bias_prior + 1 * beta_prior)); prior_preds2[i] = binomial_rng(trials, inv_logit(bias_prior + 2 * beta_prior)); // Posterior predictions for low, medium, high memory posterior_preds0[i] = binomial_rng(trials, inv_logit(biasM + IDs[i,1] + 0 * (betaM + IDs[i,2]))); posterior_preds1[i] = binomial_rng(trials, inv_logit(biasM + IDs[i,1] + 1 * (betaM + IDs[i,2]))); posterior_preds2[i] = binomial_rng(trials, inv_logit(biasM + IDs[i,1] + 2 * (betaM + IDs[i,2]))); // Calculate log-likelihood for each observation for (t in 1:trials){ log_lik[t,i] = bernoulli_logit_lpmf(h[t, i] | biasM + IDs[i, 1] + memory[t, i] * (betaM + IDs[i, 2])); } } } &quot; # Write the Stan model to a file write_stan_file( stan_memory_model, dir = &quot;stan/&quot;, basename = &quot;W7_MultilevelMemory.stan&quot; ) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W7_MultilevelMemory.stan&quot; # Compile the model file &lt;- file.path(&quot;stan/W7_MultilevelMemory.stan&quot;) mod_memory &lt;- cmdstan_model( file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;) ) The memory agent model is more complex, but follows a similar structure: Data Block: Includes the same data as the biased model, plus the opponent’s choices. Parameters Block: Includes parameters for the memory model: biasM: Population mean baseline bias betaM: Population mean memory sensitivity tau: Population standard deviations z_IDs: Standardized individual parameters L_u: Cholesky factor of correlation matrix Transformed Parameters Block: Calculates derived quantities: memory: The memory state for each agent and trial IDs: Individual parameters for each agent Model Block: Defines priors and likelihood: Priors for population parameters Memory-based choice likelihood Generated Quantities Block: Calculates additional quantities: Prior and posterior predictive samples Log-likelihood for each observation The use of a non-centered parameterization for the individual parameters (through z_IDs and transformation) is a technique to improve sampling efficiency in hierarchical models. This is important when estimating multilevel models with potentially correlated parameters. 8.7 Fitting Models and Calculating Expected Log Predictive Density Now that we’ve defined our models, we’ll fit them to our simulated data and perform model comparison. We’ll fit both models to both types of data: Biased agent model fitted to biased agent data Biased agent model fitted to memory agent data Memory agent model fitted to biased agent data Memory agent model fitted to memory agent data 8.8 Fitting the models to the data # File path for saved model model_file &lt;- &quot;simmodels/W7_fit_biased2biased.RDS&quot; # Check if we need to rerun the simulation or we can load a pre-run one (for computational efficiency) if (regenerate_simulations || !file.exists(model_file)) { # Fitting biased agent model to biased agent data fit_biased2biased &lt;- mod_biased$sample( data = data_biased, seed = 123, chains = 1, parallel_chains = 1, threads_per_chain = 1, iter_warmup = 2000, iter_sampling = 2000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99 ) fit_biased2biased$save_object(file = model_file) cat(&quot;Generated new model fit and saved to&quot;, model_file, &quot;\\n&quot;) } else { # Load existing results fit_biased2biased &lt;- readRDS(model_file) cat(&quot;Loaded existing model fit from&quot;, model_file, &quot;\\n&quot;) } ## Loaded existing model fit from simmodels/W7_fit_biased2biased.RDS # File path for saved model model_file &lt;- &quot;simmodels/W7_fit_biased2memory.RDS&quot; # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(model_file)) { # Fitting biased agent model to memory agent data fit_biased2memory &lt;- mod_biased$sample( data = data_memory, seed = 123, chains = 1, parallel_chains = 1, threads_per_chain = 1, iter_warmup = 2000, iter_sampling = 2000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99 ) fit_biased2memory$save_object(file = model_file) cat(&quot;Generated new model fit and saved to&quot;, model_file, &quot;\\n&quot;) } else { # Load existing results fit_biased2memory &lt;- readRDS(model_file) cat(&quot;Loaded existing model fit from&quot;, model_file, &quot;\\n&quot;) } ## Loaded existing model fit from simmodels/W7_fit_biased2memory.RDS # File path for saved model model_file &lt;- &quot;simmodels/W7_fit_memory2biased.RDS&quot; # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(model_file)) { # Fitting memory agent model to biased agent data fit_memory2biased &lt;- mod_memory$sample( data = data_biased, seed = 123, chains = 1, parallel_chains = 1, threads_per_chain = 1, iter_warmup = 2000, iter_sampling = 2000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99 ) fit_memory2biased$save_object(file = model_file) cat(&quot;Generated new model fit and saved to&quot;, model_file, &quot;\\n&quot;) } else { # Load existing results fit_memory2biased &lt;- readRDS(model_file) cat(&quot;Loaded existing model fit from&quot;, model_file, &quot;\\n&quot;) } ## Loaded existing model fit from simmodels/W7_fit_memory2biased.RDS # File path for saved model model_file &lt;- &quot;simmodels/W7_fit_memory2memory.RDS&quot; # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(model_file)) {# Fitting memory agent model to memory agent data fit_memory2memory &lt;- mod_memory$sample( data = data_memory, seed = 123, chains = 1, parallel_chains = 1, threads_per_chain = 1, iter_warmup = 2000, iter_sampling = 2000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99 ) fit_memory2memory$save_object(file = model_file) cat(&quot;Generated new model fit and saved to&quot;, model_file, &quot;\\n&quot;) } else { # Load existing results fit_memory2memory &lt;- readRDS(model_file) cat(&quot;Loaded existing model fit from&quot;, model_file, &quot;\\n&quot;) } ## Loaded existing model fit from simmodels/W7_fit_memory2memory.RDS # Display basic summary of the models cat(&quot;Biased model fitted to biased data:\\n&quot;) ## Biased model fitted to biased data: print(fit_biased2biased$summary(c(&quot;thetaM&quot;, &quot;thetaSD&quot;))) ## # A tibble: 2 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 thetaM 1.35 1.35 0.0698 0.0696 1.23 1.46 1.00 3523. 1409. ## 2 thetaSD 0.638 0.636 0.0502 0.0475 0.560 0.725 1.00 3393. 1650. cat(&quot;\\nBiased model fitted to memory data:\\n&quot;) ## ## Biased model fitted to memory data: print(fit_biased2memory$summary(c(&quot;thetaM&quot;, &quot;thetaSD&quot;))) ## # A tibble: 2 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 thetaM 1.92 1.92 0.107 0.110 1.75 2.10 1.00 1831. 1463. ## 2 thetaSD 1.05 1.05 0.0775 0.0739 0.934 1.19 1.00 1510. 1246. cat(&quot;\\nMemory model fitted to biased data:\\n&quot;) ## ## Memory model fitted to biased data: print(fit_memory2biased$summary(c(&quot;biasM&quot;, &quot;betaM&quot;, &quot;tau[1]&quot;, &quot;tau[2]&quot;))) ## # A tibble: 4 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 biasM 1.14 1.13 0.169 0.172 0.865 1.42 1.00 777. 1330. ## 2 betaM 0.261 0.264 0.189 0.186 -0.0544 0.563 1.00 1119. 1464. ## 3 tau[1] 0.594 0.594 0.0787 0.0762 0.466 0.722 1.00 722. 1029. ## 4 tau[2] 0.165 0.145 0.122 0.126 0.0134 0.394 1.01 133. 257. cat(&quot;\\nMemory model fitted to memory data:\\n&quot;) ## ## Memory model fitted to memory data: print(fit_memory2memory$summary(c(&quot;biasM&quot;, &quot;betaM&quot;, &quot;tau[1]&quot;, &quot;tau[2]&quot;))) ## # A tibble: 4 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 biasM -0.878 -0.883 0.207 0.202 -1.21 -0.539 1.00 714. 1354. ## 2 betaM 3.56 3.56 0.226 0.220 3.18 3.91 1.00 1368. 1441. ## 3 tau[1] 0.650 0.655 0.155 0.142 0.382 0.902 1.00 982. 957. ## 4 tau[2] 0.514 0.499 0.293 0.315 0.0654 1.04 1.01 81.3 332. print(&quot;Let&#39;s visualize some key parameters to better understand what the models have learned:&quot;) ## [1] &quot;Let&#39;s visualize some key parameters to better understand what the models have learned:&quot; # Extract posterior samples for key parameters draws_biased2biased &lt;- as_draws_df(fit_biased2biased$draws()) draws_biased2memory &lt;- as_draws_df(fit_biased2memory$draws()) draws_memory2biased &lt;- as_draws_df(fit_memory2biased$draws()) draws_memory2memory &lt;- as_draws_df(fit_memory2memory$draws()) # Prepare data for plotting param_data &lt;- bind_rows( # Biased model parameters tibble( parameter = &quot;Bias (θ)&quot;, value = draws_biased2biased$thetaM, model = &quot;Biased Model&quot;, data = &quot;Biased Data&quot; ), tibble( parameter = &quot;Bias (θ)&quot;, value = draws_biased2memory$thetaM, model = &quot;Biased Model&quot;, data = &quot;Memory Data&quot; ), # Memory model parameters - converting to probability scale for bias tibble( parameter = &quot;Bias (θ)&quot;, value = draws_memory2biased$biasM, model = &quot;Memory Model&quot;, data = &quot;Biased Data&quot; ), tibble( parameter = &quot;Bias (θ)&quot;, value = draws_memory2memory$biasM, model = &quot;Memory Model&quot;, data = &quot;Memory Data&quot; ), # Memory sensitivity parameter tibble( parameter = &quot;Memory Sensitivity (β)&quot;, value = draws_memory2biased$betaM, model = &quot;Memory Model&quot;, data = &quot;Biased Data&quot; ), tibble( parameter = &quot;Memory Sensitivity (β)&quot;, value = draws_memory2memory$betaM, model = &quot;Memory Model&quot;, data = &quot;Memory Data&quot; ) ) # Create reference lines data frame for the true parameter values true_param_lines &lt;- bind_rows( tibble(parameter = &quot;Bias (θ)&quot;, model = &quot;Biased Model&quot;, true_value = rateM), tibble(parameter = &quot;Bias (θ)&quot;, model = &quot;Memory Model&quot;, true_value = biasM), tibble(parameter = &quot;Memory Sensitivity (β)&quot;, model = &quot;Memory Model&quot;, true_value = betaM) ) # Create visualization of posterior distributions ggplot(param_data, aes(x = value, fill = data)) + geom_density(alpha = 0.6) + # Add reference lines only to relevant panels geom_vline(data = true_param_lines, aes(xintercept = true_value), linetype = &quot;dashed&quot;, color = &quot;black&quot;) + facet_grid(model ~ parameter, scales = &quot;free&quot;) + scale_fill_brewer(palette = &quot;Set1&quot;) + labs( title = &quot;Posterior Distributions of Key Parameters&quot;, subtitle = &quot;Comparing model fits to different data types&quot;, x = &quot;Parameter Value&quot;, y = &quot;Density&quot;, fill = &quot;Data Type&quot; ) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) 8.9 Cross-Validation for Model Comparison Now that we’ve fit our models, we can use cross-validation techniques to compare them. We’ll start with PSIS-LOO since it’s computationally efficient, and then validate with true k-fold cross-validation. 8.9.1 PSIS-LOO Comparison # Calculate ELPD using loo Loo_biased2biased &lt;- fit_biased2biased$loo(save_psis = TRUE, cores = 4) Loo_biased2memory &lt;- fit_biased2memory$loo(save_psis = TRUE, cores = 4) Loo_memory2biased &lt;- fit_memory2biased$loo(save_psis = TRUE, cores = 4) Loo_memory2memory &lt;- fit_memory2memory$loo(save_psis = TRUE, cores = 4) # Diagnostic check for LOO reliability cat(&quot;Diagnostics for biased model on biased data:\\n&quot;) ## Diagnostics for biased model on biased data: print(sum(Loo_biased2biased$diagnostics$pareto_k &gt; 0.7)) ## [1] 0 cat(&quot;\\nDiagnostics for biased model on memory data:\\n&quot;) ## ## Diagnostics for biased model on memory data: print(sum(Loo_biased2memory$diagnostics$pareto_k &gt; 0.7)) ## [1] 0 cat(&quot;\\nDiagnostics for memory model on biased data:\\n&quot;) ## ## Diagnostics for memory model on biased data: print(sum(Loo_memory2biased$diagnostics$pareto_k &gt; 0.7)) ## [1] 0 cat(&quot;\\nDiagnostics for memory model on memory data:\\n&quot;) ## ## Diagnostics for memory model on memory data: print(sum(Loo_memory2memory$diagnostics$pareto_k &gt; 0.7)) ## [1] 0 # Extract ELPD values cat(&quot;\\nELPD for each model-data combination:\\n&quot;) ## ## ELPD for each model-data combination: cat(&quot;Biased model on biased data:&quot;, round(Loo_biased2biased$estimates[&quot;elpd_loo&quot;, &quot;Estimate&quot;], 2), &quot;\\n&quot;) ## Biased model on biased data: -6015.11 cat(&quot;Biased model on memory data:&quot;, round(Loo_biased2memory$estimates[&quot;elpd_loo&quot;, &quot;Estimate&quot;], 2), &quot;\\n&quot;) ## Biased model on memory data: -4696.54 cat(&quot;Memory model on biased data:&quot;, round(Loo_memory2biased$estimates[&quot;elpd_loo&quot;, &quot;Estimate&quot;], 2), &quot;\\n&quot;) ## Memory model on biased data: -6019.19 cat(&quot;Memory model on memory data:&quot;, round(Loo_memory2memory$estimates[&quot;elpd_loo&quot;, &quot;Estimate&quot;], 2), &quot;\\n&quot;) ## Memory model on memory data: -4526.47 8.9.2 Visualizing LOO-CV Results To better understand how our models perform, let’s visualize the pointwise differences in ELPD: # Create dataframe of pointwise ELPD differences elpd_diff &lt;- tibble( observation = seq(length(Loo_biased2biased$pointwise[, &quot;elpd_loo&quot;])), biased_data_diff = Loo_biased2biased$pointwise[, &quot;elpd_loo&quot;] - Loo_memory2biased$pointwise[, &quot;elpd_loo&quot;], memory_data_diff = Loo_memory2memory$pointwise[, &quot;elpd_loo&quot;] - Loo_biased2memory$pointwise[, &quot;elpd_loo&quot;] ) # Create long format for easier plotting elpd_long &lt;- elpd_diff %&gt;% pivot_longer( cols = c(biased_data_diff, memory_data_diff), names_to = &quot;comparison&quot;, values_to = &quot;elpd_diff&quot; ) %&gt;% mutate( comparison = case_when( comparison == &quot;biased_data_diff&quot; ~ &quot;Biased Model vs Memory Model\\non Biased Data&quot;, comparison == &quot;memory_data_diff&quot; ~ &quot;Memory Model vs Biased Model\\non Memory Data&quot; ), favors_true_model = case_when( comparison == &quot;Biased Model vs Memory Model\\non Biased Data&quot; &amp; elpd_diff &gt; 0 ~ TRUE, comparison == &quot;Memory Model vs Biased Model\\non Memory Data&quot; &amp; elpd_diff &gt; 0 ~ TRUE, TRUE ~ FALSE ) ) # Create visualization of ELPD differences p1 &lt;- ggplot(elpd_long, aes(x = observation, y = elpd_diff, color = favors_true_model)) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + geom_point(alpha = 0.3, size = 1) + facet_wrap(~comparison) + scale_color_manual(values = c(&quot;TRUE&quot; = &quot;green4&quot;, &quot;FALSE&quot; = &quot;red&quot;)) + labs( title = &quot;Pointwise ELPD Differences Between Models&quot;, subtitle = &quot;Positive values (represented in green) favor the true model (first model in comparison)&quot;, x = &quot;Observation&quot;, y = &quot;ELPD Difference&quot; ) + theme_minimal() + theme(legend.position = &quot;none&quot;) # Create density plots of ELPD differences p2 &lt;- ggplot(elpd_long, aes(x = elpd_diff, fill = comparison)) + geom_density(alpha = 0.5) + geom_vline(xintercept = 0, linetype = &quot;dashed&quot;) + scale_fill_brewer(palette = &quot;Set1&quot;) + labs( title = &quot;Distribution of ELPD Differences&quot;, subtitle = &quot;How consistently does the true model outperform the alternative?&quot;, x = &quot;ELPD Difference&quot;, y = &quot;Density&quot;, fill = &quot;Comparison&quot; ) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) # Combine plots p1 / p2 + plot_layout(heights = c(3, 2)) It’s quite clear that the bulk of the data a equally well explained by the models. And the differences for the biased data are tricky to see in the plot. Yet, we can see that in memory data there are a non trivial amount of data points better explained by the memory model (the true underlying data-generating mechanism). 8.10 Formal Model Comparison Now let’s perform formal model comparison using the loo_compare function, which computes the difference in ELPD between models and the standard error of this difference: # Compare models fitted to biased data comparison_biased &lt;- loo_compare(Loo_biased2biased, Loo_memory2biased) print(&quot;Model comparison for biased data:&quot;) ## [1] &quot;Model comparison for biased data:&quot; print(comparison_biased) ## elpd_diff se_diff ## model1 0.0 0.0 ## model2 -4.1 1.1 # Compare models fitted to memory data comparison_memory &lt;- loo_compare(Loo_memory2memory, Loo_biased2memory) print(&quot;\\nModel comparison for memory data:&quot;) ## [1] &quot;\\nModel comparison for memory data:&quot; print(comparison_memory) ## elpd_diff se_diff ## model1 0.0 0.0 ## model2 -170.1 13.0 # Calculate model weights weights_biased &lt;- loo_model_weights(list( &quot;Biased Model&quot; = Loo_biased2biased, &quot;Memory Model&quot; = Loo_memory2biased )) weights_memory &lt;- loo_model_weights(list( &quot;Memory Model&quot; = Loo_memory2memory, &quot;Biased Model&quot; = Loo_biased2memory )) # Create data for model weights visualization weights_data &lt;- tibble( model = c(&quot;Biased Model&quot;, &quot;Memory Model&quot;, &quot;Memory Model&quot;, &quot;Biased Model&quot;), data_type = c(&quot;Biased Data&quot;, &quot;Biased Data&quot;, &quot;Memory Data&quot;, &quot;Memory Data&quot;), weight = c(weights_biased[1], weights_biased[2], weights_memory[1], weights_memory[2]) ) # Visualize model weights ggplot(weights_data, aes(x = model, y = weight, fill = model)) + geom_col() + facet_wrap(~ data_type) + scale_fill_brewer(palette = &quot;Set1&quot;) + labs( title = &quot;Model Comparison via Stacking Weights&quot;, subtitle = &quot;Higher weights indicate better predictive performance&quot;, x = NULL, y = &quot;Model Weight&quot;, fill = &quot;Model Type&quot; ) + theme_minimal() + theme( legend.position = &quot;bottom&quot;, axis.text.x = element_text(angle = 45, hjust = 1) ) + geom_text(aes(label = scales::percent(weight, accuracy = 0.1)), vjust = -0.5) Here it is clear that formal model comparison can clearly pick the right model. Hurrah! 8.10.1 K-Fold Cross-Validation for Validation While PSIS-LOO is efficient, we need to check how it relates with true cross-validation. First we create new stan models, which separates data into training and test data and include the ability to calculate log-likelihood for test data. This is crucial for cross-validation, as we need to evaluate the model’s performance on unseen data. # Stan model for biased agent with cross-validation capabilities stan_biased_cv_model &lt;- &quot; // Multilevel Biased Agent Model with CV support // // This model has additional structures to handle test data for cross-validation functions{ real normal_lb_rng(real mu, real sigma, real lb) { real p = normal_cdf(lb | mu, sigma); // CDF for bounds real u = uniform_rng(p, 1); return (sigma * inv_Phi(u)) + mu; // Inverse CDF for value } } // Input data - now includes both training and test data data { int&lt;lower = 1&gt; trials; // Number of trials per agent int&lt;lower = 1&gt; agents; // Number of training agents array[trials, agents] int h; // Training choice data int&lt;lower = 1&gt; agents_test; // Number of test agents array[trials, agents_test] int h_test; // Test choice data } // Parameters to be estimated parameters { real thetaM; // Population mean of bias real&lt;lower = 0&gt; thetaSD; // Population SD of bias array[agents] real theta; // Individual agent biases } // Model definition - trained only on training data model { // Population-level priors target += normal_lpdf(thetaM | 0, 1); target += normal_lpdf(thetaSD | 0, .3) - normal_lccdf(0 | 0, .3); // Individual-level parameters target += normal_lpdf(theta | thetaM, thetaSD); // Likelihood for training data only for (i in 1:agents) target += bernoulli_logit_lpmf(h[,i] | theta[i]); } // Calculate log-likelihood for both training and test data generated quantities{ real thetaM_prior; real&lt;lower=0&gt; thetaSD_prior; real&lt;lower=0, upper=1&gt; theta_prior; real&lt;lower=0, upper=1&gt; theta_posterior; int&lt;lower=0, upper = trials&gt; prior_preds; int&lt;lower=0, upper = trials&gt; posterior_preds; // Log-likelihood for training data array[trials, agents] real log_lik; // Log-likelihood for test data - crucial for cross-validation array[trials, agents_test] real log_lik_test; // Generate prior and posterior samples thetaM_prior = normal_rng(0,1); thetaSD_prior = normal_lb_rng(0,0.3,0); theta_prior = inv_logit(normal_rng(thetaM_prior, thetaSD_prior)); theta_posterior = inv_logit(normal_rng(thetaM, thetaSD)); prior_preds = binomial_rng(trials, inv_logit(thetaM_prior)); posterior_preds = binomial_rng(trials, inv_logit(thetaM)); // Calculate log-likelihood for training data for (i in 1:agents){ for (t in 1:trials){ log_lik[t,i] = bernoulli_logit_lpmf(h[t,i] | theta[i]); } } // Calculate log-likelihood for test data // Note: We use population-level estimates for prediction for (i in 1:agents_test){ for (t in 1:trials){ log_lik_test[t,i] = bernoulli_logit_lpmf(h_test[t,i] | thetaM); } } } &quot; # Write the model to a file write_stan_file( stan_biased_cv_model, dir = &quot;stan/&quot;, basename = &quot;W7_MultilevelBias_cv.stan&quot; ) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W7_MultilevelBias_cv.stan&quot; # Compile the model file &lt;- file.path(&quot;stan/W7_MultilevelBias_cv.stan&quot;) mod_biased_cv &lt;- cmdstan_model( file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;) ) # Similarly, define the memory agent model with CV support stan_memory_cv_model &lt;- &quot; // Multilevel Memory Agent Model with CV support // // This model has additional structures to handle test data for cross-validation functions{ real normal_lb_rng(real mu, real sigma, real lb) { real p = normal_cdf(lb | mu, sigma); // CDF for bounds real u = uniform_rng(p, 1); return (sigma * inv_Phi(u)) + mu; // Inverse CDF for value } } // Input data - includes both training and test data data { int&lt;lower = 1&gt; trials; // Number of trials per agent int&lt;lower = 1&gt; agents; // Number of training agents array[trials, agents] int h; // Training choice data array[trials, agents] int other; // Opponent&#39;s choices for training agents int&lt;lower = 1&gt; agents_test; // Number of test agents array[trials, agents_test] int h_test; // Test choice data array[trials, agents_test] int other_test; // Opponent&#39;s choices for test agents } // Parameters to be estimated parameters { real biasM; // Population mean baseline bias real betaM; // Population mean memory sensitivity vector&lt;lower = 0&gt;[2] tau; // Population SDs matrix[2, agents] z_IDs; // Standardized individual parameters cholesky_factor_corr[2] L_u; // Cholesky factor of correlation matrix } // Transformed parameters transformed parameters { // Memory states for training data array[trials, agents] real memory; // Memory states for test data array[trials, agents_test] real memory_test; // Individual parameters matrix[agents,2] IDs; IDs = (diag_pre_multiply(tau, L_u) * z_IDs)&#39;; // Calculate memory states for training data for (agent in 1:agents){ for (trial in 1:trials){ if (trial == 1) { memory[trial, agent] = 0.5; } if (trial &lt; trials){ memory[trial + 1, agent] = memory[trial, agent] + ((other[trial, agent] - memory[trial, agent]) / trial); if (memory[trial + 1, agent] == 0){memory[trial + 1, agent] = 0.01;} if (memory[trial + 1, agent] == 1){memory[trial + 1, agent] = 0.99;} } } } // Calculate memory states for test data for (agent in 1:agents_test){ for (trial in 1:trials){ if (trial == 1) { memory_test[trial, agent] = 0.5; } if (trial &lt; trials){ memory_test[trial + 1, agent] = memory_test[trial, agent] + ((other_test[trial, agent] - memory_test[trial, agent]) / trial); if (memory_test[trial + 1, agent] == 0){memory_test[trial + 1, agent] = 0.01;} if (memory_test[trial + 1, agent] == 1){memory_test[trial + 1, agent] = 0.99;} } } } } // Model definition - trained only on training data model { // Population-level priors target += normal_lpdf(biasM | 0, 1); target += normal_lpdf(tau[1] | 0, .3) - normal_lccdf(0 | 0, .3); target += normal_lpdf(betaM | 0, .3); target += normal_lpdf(tau[2] | 0, .3) - normal_lccdf(0 | 0, .3); target += lkj_corr_cholesky_lpdf(L_u | 2); // Standardized individual parameters target += std_normal_lpdf(to_vector(z_IDs)); // Likelihood for training data only for (agent in 1:agents){ for (trial in 1:trials){ target += bernoulli_logit_lpmf(h[trial, agent] | biasM + IDs[agent, 1] + memory[trial, agent] * (betaM + IDs[agent, 2])); } } } // Calculate log-likelihood for both training and test data generated quantities{ // Prior samples real biasM_prior; real&lt;lower=0&gt; biasSD_prior; real betaM_prior; real&lt;lower=0&gt; betaSD_prior; real bias_prior; real beta_prior; // Posterior predictive samples array[agents] int&lt;lower=0, upper = trials&gt; prior_preds0; array[agents] int&lt;lower=0, upper = trials&gt; prior_preds1; array[agents] int&lt;lower=0, upper = trials&gt; prior_preds2; array[agents] int&lt;lower=0, upper = trials&gt; posterior_preds0; array[agents] int&lt;lower=0, upper = trials&gt; posterior_preds1; array[agents] int&lt;lower=0, upper = trials&gt; posterior_preds2; // Log-likelihood for training data array[trials, agents] real log_lik; // Log-likelihood for test data - crucial for cross-validation array[trials, agents_test] real log_lik_test; // Generate prior samples biasM_prior = normal_rng(0,1); biasSD_prior = normal_lb_rng(0,0.3,0); betaM_prior = normal_rng(0,1); betaSD_prior = normal_lb_rng(0,0.3,0); bias_prior = normal_rng(biasM_prior, biasSD_prior); beta_prior = normal_rng(betaM_prior, betaSD_prior); // Generate predictions for different memory conditions for (i in 1:agents){ prior_preds0[i] = binomial_rng(trials, inv_logit(bias_prior + 0 * beta_prior)); prior_preds1[i] = binomial_rng(trials, inv_logit(bias_prior + 1 * beta_prior)); prior_preds2[i] = binomial_rng(trials, inv_logit(bias_prior + 2 * beta_prior)); posterior_preds0[i] = binomial_rng(trials, inv_logit(biasM + IDs[i,1] + 0 * (betaM + IDs[i,2]))); posterior_preds1[i] = binomial_rng(trials, inv_logit(biasM + IDs[i,1] + 1 * (betaM + IDs[i,2]))); posterior_preds2[i] = binomial_rng(trials, inv_logit(biasM + IDs[i,1] + 2 * (betaM + IDs[i,2]))); // Calculate log-likelihood for training data for (t in 1:trials){ log_lik[t,i] = bernoulli_logit_lpmf(h[t, i] | biasM + IDs[i, 1] + memory[t, i] * (betaM + IDs[i, 2])); } } // Calculate log-likelihood for test data // Note: We use population-level estimates for prediction for (i in 1:agents_test){ for (t in 1:trials){ log_lik_test[t,i] = bernoulli_logit_lpmf(h_test[t,i] | biasM + memory_test[t, i] * betaM); } } } &quot; # Write the model to a file write_stan_file( stan_memory_cv_model, dir = &quot;stan/&quot;, basename = &quot;W7_MultilevelMemory_cv.stan&quot; ) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W7_MultilevelMemory_cv.stan&quot; # Compile the model file &lt;- file.path(&quot;stan/W7_MultilevelMemory_cv.stan&quot;) mod_memory_cv &lt;- cmdstan_model( file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;) ) These CV-ready models extend our original models with additional structures to handle test data and compute separate log-likelihoods for training and test observations. Now, let’s demonstrate how to implement k-fold cross-validation using these models (N.b. we only fit both models to the memory data, I still need to implement the full comparison against biased data as well) # Define file path for saved CV results cv_results_file &lt;- &quot;simmodels/W7_CV_Biased&amp;Memory.RData&quot; # Check if we need to run the CV analysis if (regenerate_simulations || !file.exists(cv_results_file)) { # Load necessary packages for parallelization pacman::p_load(future, furrr) # Set up parallel processing (adjust workers based on your system) plan(multisession, workers = 8) # Split the data into folds (10 folds, grouped by agent) d$fold &lt;- kfold_split_grouped(K = 10, x = d$agent) unique_folds &lt;- unique(d$fold) # Initialize matrices to store log predictive densities # We&#39;ll fill these after parallel processing log_pd_biased_kfold &lt;- matrix(nrow = 1000, ncol = nrow(d)) log_pd_memory_kfold &lt;- matrix(nrow = 1000, ncol = nrow(d)) # Define function to process a single fold process_fold &lt;- function(k) { cat(&quot;Starting processing of fold&quot;, k, &quot;\\n&quot;) # Create training set (all data except fold k) d_train &lt;- d %&gt;% filter(fold != k) # Prepare training data for Stan d_memory1_train &lt;- d_train %&gt;% dplyr::select(agent, memoryChoice) %&gt;% mutate(row = row_number()) %&gt;% pivot_wider(names_from = agent, values_from = memoryChoice) d_memory2_train &lt;- d_train %&gt;% dplyr::select(agent, randomChoice) %&gt;% mutate(row = row_number()) %&gt;% pivot_wider(names_from = agent, values_from = randomChoice) agents_n &lt;- length(unique(d_train$agent)) # Create test set (only fold k) d_test &lt;- d %&gt;% filter(fold == k) d_memory1_test &lt;- d_test %&gt;% dplyr::select(agent, memoryChoice) %&gt;% mutate(row = row_number()) %&gt;% pivot_wider(names_from = agent, values_from = memoryChoice) d_memory2_test &lt;- d_test %&gt;% dplyr::select(agent, randomChoice) %&gt;% mutate(row = row_number()) %&gt;% pivot_wider(names_from = agent, values_from = randomChoice) agents_test_n &lt;- length(unique(d_test$agent)) # Prepare data for Stan model data_memory &lt;- list( trials = trials, agents = agents_n, agents_test = agents_test_n, h = as.matrix(d_memory1_train[, 2:(agents_n + 1)]), other = as.matrix(d_memory2_train[, 2:(agents_n + 1)]), h_test = as.matrix(d_memory1_test[, 2:(agents_test_n + 1)]), other_test = as.matrix(d_memory2_test[, 2:(agents_test_n + 1)]) ) # Fit models on training data and evaluate on test data # Note: removed threads_per_chain parameter fit_biased &lt;- mod_biased_cv$sample( data = data_memory, seed = 123 + k, # Unique seed per fold chains = 1, parallel_chains = 1, threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 1000, refresh = 0, # Suppress progress output in parallel runs max_treedepth = 20, adapt_delta = 0.99 ) fit_memory &lt;- mod_memory_cv$sample( data = data_memory, seed = 456 + k, # Unique seed per fold chains = 1, parallel_chains = 1, threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 1000, refresh = 0, # Suppress progress output in parallel runs max_treedepth = 20, adapt_delta = 0.99 ) # Extract log likelihood for test data points biased_log_lik &lt;- fit_biased$draws(&quot;log_lik_test&quot;, format = &quot;matrix&quot;) memory_log_lik &lt;- fit_memory$draws(&quot;log_lik_test&quot;, format = &quot;matrix&quot;) # Create indices to keep track of which positions in the full matrix these values belong to test_indices &lt;- which(d$fold == k) # Return results for this fold result &lt;- list( fold = k, test_indices = test_indices, biased_log_lik = biased_log_lik, memory_log_lik = memory_log_lik ) cat(&quot;Completed processing of fold&quot;, k, &quot;\\n&quot;) return(result) } # Process all folds in parallel using furrr cat(&quot;Starting parallel processing of all folds\\n&quot;) results &lt;- future_map(unique_folds, process_fold, .options = furrr_options(seed = TRUE)) # Now populate the full matrices with results from each fold for (i in seq_along(results)) { fold_result &lt;- results[[i]] test_indices &lt;- fold_result$test_indices # For each test index, populate the corresponding column in the full matrices for (j in seq_along(test_indices)) { idx &lt;- test_indices[j] log_pd_biased_kfold[, idx] &lt;- fold_result$biased_log_lik[, j] log_pd_memory_kfold[, idx] &lt;- fold_result$memory_log_lik[, j] } } # Save results save(log_pd_biased_kfold, log_pd_memory_kfold, file = cv_results_file) cat(&quot;Generated new cross-validation results and saved to&quot;, cv_results_file, &quot;\\n&quot;) } else { # Load existing results load(cv_results_file) cat(&quot;Loaded existing cross-validation results from&quot;, cv_results_file, &quot;\\n&quot;) } ## Loaded existing cross-validation results from simmodels/W7_CV_Biased&amp;Memory.RData # Load cross-validation results cv_results_file &lt;- &quot;simmodels/W7_CV_Biased&amp;Memory.RData&quot; load(cv_results_file) # Calculate ELPD values from cross-validation results elpd_biased_kfold &lt;- elpd(log_pd_biased_kfold) elpd_memory_kfold &lt;- elpd(log_pd_memory_kfold) # Extract ELPD values and standard errors biased_elpd_value &lt;- elpd_biased_kfold$estimates[&quot;elpd&quot;, &quot;Estimate&quot;] memory_elpd_value &lt;- elpd_memory_kfold$estimates[&quot;elpd&quot;, &quot;Estimate&quot;] biased_se &lt;- elpd_biased_kfold$estimates[&quot;elpd&quot;, &quot;SE&quot;] memory_se &lt;- elpd_memory_kfold$estimates[&quot;elpd&quot;, &quot;SE&quot;] # Print values for comparison cat(&quot;K-fold CV results:\\n&quot;) ## K-fold CV results: cat(&quot;Biased model ELPD:&quot;, biased_elpd_value, &quot;±&quot;, biased_se, &quot;\\n&quot;) ## Biased model ELPD: -5543.23 ± 78.8173 cat(&quot;Memory model ELPD:&quot;, memory_elpd_value, &quot;±&quot;, memory_se, &quot;\\n&quot;) ## Memory model ELPD: -4860.888 ± 66.89472 # Create a results table kfold_results &lt;- tibble( model = c(&quot;Biased Agent Model&quot;, &quot;Memory Agent Model&quot;), elpd = c(biased_elpd_value, memory_elpd_value), se = c(biased_se, memory_se) ) # Calculate model weights from ELPD values max_elpd &lt;- max(c(biased_elpd_value, memory_elpd_value)) relative_elpd &lt;- c(biased_elpd_value, memory_elpd_value) - max_elpd model_weights &lt;- exp(relative_elpd) / sum(exp(relative_elpd)) # Add weights to results kfold_results$weight &lt;- model_weights # Display the results table kfold_results ## # A tibble: 2 × 4 ## model elpd se weight ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Biased Agent Model -5543. 78.8 4.60e-297 ## 2 Memory Agent Model -4861. 66.9 1 e+ 0 8.10.2 Comparing LOO and K-Fold Results # Create comparison table of PSIS-LOO vs K-fold CV loo_results &lt;- tibble( model = c(&quot;Biased Agent Model&quot;, &quot;Memory Agent Model&quot;), loo_elpd = c(Loo_biased2biased$estimates[&quot;elpd_loo&quot;, &quot;Estimate&quot;], Loo_memory2biased$estimates[&quot;elpd_loo&quot;, &quot;Estimate&quot;]), loo_se = c(Loo_biased2biased$estimates[&quot;elpd_loo&quot;, &quot;SE&quot;], Loo_memory2biased$estimates[&quot;elpd_loo&quot;, &quot;SE&quot;]), loo_weight = c(weights_biased[1], weights_biased[2]) ) # Combine with k-fold results comparison_results &lt;- left_join(loo_results, kfold_results, by = &quot;model&quot;) %&gt;% rename(kfold_elpd = elpd, kfold_se = se, kfold_weight = weight) # Display comparison comparison_results %&gt;% mutate( loo_elpd = round(loo_elpd, 1), loo_se = round(loo_se, 2), loo_weight = round(loo_weight, 3), kfold_elpd = round(kfold_elpd, 1), kfold_se = round(kfold_se, 2), kfold_weight = round(kfold_weight, 3) ) %&gt;% knitr::kable( caption = &quot;Comparison of PSIS-LOO and K-fold Cross-Validation Results&quot;, col.names = c(&quot;Model&quot;, &quot;LOO ELPD&quot;, &quot;LOO SE&quot;, &quot;LOO Weight&quot;, &quot;K-fold ELPD&quot;, &quot;K-fold SE&quot;, &quot;K-fold Weight&quot;) ) Table 8.1: Comparison of PSIS-LOO and K-fold Cross-Validation Results Model LOO ELPD LOO SE LOO Weight K-fold ELPD K-fold SE K-fold Weight Biased Agent Model -6015.1 58.82 1 -5543.2 78.82 0 Memory Agent Model -6019.2 58.83 0 -4860.9 66.89 1 Funnily enough cross-validation indicates the wrong model. 8.11 Limitations of Model Comparison Approaches While cross-validation and ELPD provide powerful tools for model comparison, it’s important to understand their limitations: Training vs. Transfer Generalization Cross-validation only assesses a model’s ability to generalize to new data from the same distribution (training generalization). It doesn’t evaluate how well models transfer to different contexts or populations (transfer generalization). Model Misspecification All models are wrong, but some are useful. Cross-validation helps identify which wrong model is most useful for prediction, but doesn’t guarantee we’ve captured the true generating process. Limited Data With limited data, cross-validation estimates can have high variance, especially for complex models. K-fold CV with small k can help mitigate this issue. Computational Cost True cross-validation requires refitting models multiple times, which can be prohibitively expensive for complex Bayesian models. PSIS-LOO offers an efficient approximation but may not always be reliable. Parameter vs. Predictive Focus Model comparison based on predictive performance might select different models than if we were focused on accurate parameter estimation. The “best” model depends on your goals. 8.12 Exercises Compare the models on different subsets of the data (e.g., early vs. late trials). Does the preferred model change depending on which portion of the data you use? Experiment with different priors for the models. How sensitive are the model comparison results to prior choices? Implement a different model (e.g., win-stay-lose-shift) and compare it to the biased and memory models. Which performs best? Explore how the amount of data affects model comparison. How many trials do you need to reliably identify the true model? Investigate the relationship between model complexity and predictive performance in this context. Are there systematic patterns in when simpler models are preferred? 8.12.1 Rant on internal vs external validity However, we need to think carefully about what we mean by “out of sample.” There are actually two distinct types of test sets we might consider: internal and external. Internal test sets come from the same data collection effort as our training data - for example, we might randomly set aside 20% of our matching pennies games to test on. While this approach helps us detect overfitting to specific participants or trials, it cannot tell us how well our model generalizes to truly new contexts. Our test set participants were recruited from the same population, played the game under the same conditions, and were influenced by the same experimental setup as our training participants. External test sets, in contrast, come from genuinely different contexts. For our matching pennies model, this might mean testing on games played: In different cultures or age groups Under time pressure versus relaxed conditions For real money versus just for fun Against human opponents versus computer agents In laboratory versus online settings The distinction matters because cognitive models often capture not just universal mental processes, but also specific strategies that people adopt in particular contexts. A model that perfectly predicts behavior in laboratory matching pennies games might fail entirely when applied to high-stakes poker games, even though both involve similar strategic thinking. This raises deeper questions about what kind of generalization we want our models to achieve. Are we trying to build models that capture universal cognitive processes, or are we content with models that work well within specific contexts? The answer affects not just how we evaluate our models, but how we design them in the first place. In practice, truly external test sets are rare in cognitive science - they require additional data collection under different conditions, which is often impractical. This means we must be humble about our claims of generalization. When we talk about a model’s predictive accuracy, we should be clear that we’re usually measuring its ability to generalize within a specific experimental context, not its ability to capture human cognition in all its diversity. This limitation of internal test sets is one reason why cognitive scientists often complement predictive accuracy metrics with other forms of model evaluation, such as testing theoretical predictions on new tasks or examining whether model parameters correlate sensibly with individual differences. These approaches help us build confidence that our models capture meaningful cognitive processes rather than just statistical patterns specific to our experimental setup. *** "],["mixture-models.html", "Chapter 9 Mixture models 9.1 Visualizing Mixture Distributions: Reaction Time Example 9.2 Theoretical Foundation for Mixture Models 9.3 Implementing a Basic Mixture Model 9.4 Parameter Interpretation 9.5 Multilevel Mixture Models 9.6 Comparing Single-Process and Mixture Models 9.7 Conclusion", " Chapter 9 Mixture models When we model human cognition and behavior, we often find ourselves facing a puzzling reality: people don’t always follow a single, consistent strategy. Consider a person playing our matching pennies game - sometimes they might carefully track their opponent’s patterns, other times they might rely on a simple bias toward choosing “heads,” and occasionally they might respond completely randomly when their attention lapses. Traditional cognitive models that assume a single process are ill-equipped to capture this complexity. Mixture models provide an elegant solution to this challenge. Rather than assuming behavior reflects just one cognitive process, mixture models allow us to combine multiple different processes within a unified modeling framework. In the previous chapter, we explored model comparison techniques that help us select between competing cognitive models. Mixture models approach in a sense reframe that very problem - instead of asking “which model is correct?”, they allow us to ask “how much does each model contribute?” This shift acknowledges the possibility that multiple cognitive processes might coexist, either within a single individual or across a population. 9.0.1 Learning Objectives After completing this chapter, you will be able to: Understand how mixture models combine multiple cognitive processes Implement mixture models for different types of behavioral data using Stan Estimate mixture weights and component-specific parameters Evaluate mixture models through posterior predictive checks Compare mixture models to single-process alternatives 9.0.2 Chapter Roadmap In this chapter, we will: Examine reaction time data to visualize how mixture distributions appear in practice Develop a theoretical foundation for mixture modeling Implement a simple mixture model combining biased and random choice processes (thus going back to our matching pennies example) Evaluate and validate this model through posterior checks Extend to multilevel mixture models that capture individual differences Compare mixture models with traditional single-process approaches 9.1 Visualizing Mixture Distributions: Reaction Time Example Before diving into the mathematics and implementation of mixture models, let’s start with a concrete example that visually demonstrates why we need them. Reaction time (RT) data provides a particularly clear window into the mixture of cognitive processes. In many cognitive tasks, participants are asked to respond as quickly as possible while still being accurate. However, attention fluctuates over time. Let’s simulate a scenario where participants sometimes engage in deliberate thinking (producing a log-normal distribution of RTs) and sometimes experience attentional lapses (producing more or less random responses with a uniform distribution of RTs). # Flag to control whether to regenerate simulations regenerate_simulations &lt;- TRUE # Load necessary packages library(tidyverse) library(here) library(posterior) library(cmdstanr) library(bayesplot) library(patchwork) library(loo) set.seed(123) # For reproducible results # Number of observations n_obs &lt;- 500 # Parameters for the two processes # Process 1: Deliberate thinking (log-normal distribution) mu_delib &lt;- 6.2 # Mean of log(RT) for deliberate process (≈ 500ms) sigma_delib &lt;- 0.2 # SD of log(RT) for deliberate process # Process 2: Attentional lapses (uniform distribution) min_lapse &lt;- 200 # Minimum RT during attentional lapses (ms) max_lapse &lt;- 2000 # Maximum RT during attentional lapses (ms) # Mixture weight (proportion of deliberate responses) pi_delib &lt;- 0.8 # 80% deliberate thinking, 20% attentional lapses # Simulate the mixture process &lt;- rbinom(n_obs, 1, pi_delib) # 1 = deliberate, 0 = lapse # Generate RTs from the appropriate distribution based on process rt &lt;- numeric(n_obs) for (i in 1:n_obs) { if (process[i] == 1) { # Deliberate process - log-normal distribution rt[i] &lt;- rlnorm(1, mu_delib, sigma_delib) } else { # Attentional lapse - uniform distribution rt[i] &lt;- runif(1, min_lapse, max_lapse) } } # Combine into a data frame rt_data &lt;- tibble( observation = 1:n_obs, rt = rt, process = ifelse(process == 1, &quot;Deliberate&quot;, &quot;Attentional Lapse&quot;), log_rt = log(rt) ) # Summary statistics by process rt_summary &lt;- rt_data %&gt;% group_by(process) %&gt;% summarize( count = n(), proportion = n()/n_obs, mean_rt = mean(rt), median_rt = median(rt), sd_rt = sd(rt) ) # Display summary knitr::kable(rt_summary, digits = 2) process count proportion mean_rt median_rt sd_rt Attentional Lapse 94 0.19 1076.04 1106.17 552.71 Deliberate 406 0.81 494.98 490.17 92.08 Now, let’s visualize the reaction time distribution: # Create histogram of reaction times with overlaid density curves p1 &lt;- ggplot() + # Histogram of all data geom_histogram(data = rt_data, aes(x = rt, y = ..density..), bins = 30, fill = &quot;gray80&quot;, color = &quot;black&quot;, alpha = 0.5) + # Density curves for each component geom_density(data = rt_data %&gt;% filter(process == &quot;Deliberate&quot;), aes(x = rt, fill = &quot;Deliberate Thinking&quot;), alpha = 0.4) + # We can&#39;t use geom_density for uniform distribution (it would smooth it) # Instead, overlay the theoretical uniform density geom_segment(aes(x = min_lapse, xend = max_lapse, y = 1/(max_lapse-min_lapse), yend = 1/(max_lapse-min_lapse), color = &quot;Attentional Lapse&quot;), linewidth = 1.5, alpha = 0.8) + # Overall density geom_density(data = rt_data, aes(x = rt), color = &quot;black&quot;, linewidth = 1) + # Aesthetics scale_fill_manual(values = c(&quot;Deliberate Thinking&quot; = &quot;blue&quot;), name = &quot;Process&quot;) + scale_color_manual(values = c(&quot;Attentional Lapse&quot; = &quot;red&quot;), name = &quot;Process&quot;) + labs(title = &quot;Reaction Time Distribution&quot;, subtitle = &quot;A mixture of deliberate thinking and attentional lapses&quot;, x = &quot;Reaction Time (ms)&quot;, y = &quot;Density&quot;) + coord_cartesian(xlim = c(0, 2000)) + theme_minimal() + theme(legend.position = &quot;top&quot;) # Also plot the log RT p2 &lt;- ggplot() + geom_histogram(data = rt_data, aes(x = log_rt, y = ..density..), bins = 30, fill = &quot;gray80&quot;, color = &quot;black&quot;, alpha = 0.5) + geom_density(data = rt_data %&gt;% filter(process == &quot;Deliberate&quot;), aes(x = log_rt, fill = &quot;Deliberate Thinking&quot;), alpha = 0.4) + geom_density(data = rt_data, aes(x = log_rt), color = &quot;black&quot;, linewidth = 1) + scale_fill_manual(values = c(&quot;Deliberate Thinking&quot; = &quot;blue&quot;), name = &quot;Process&quot;) + labs(title = &quot;Log Reaction Time Distribution&quot;, subtitle = &quot;Log transformation highlights the mixture components&quot;, x = &quot;Log Reaction Time&quot;, y = &quot;Density&quot;) + theme_minimal() + theme(legend.position = &quot;top&quot;) # Display both plots p1 / p2 In this example, we can clearly see how the overall reaction time distribution (black line) is a combination of two distinct processes: Deliberate Thinking (Blue): A log-normal distribution centered around 500ms, representing focused cognitive processing of the task. Attentional Lapses (Red Line): A uniform distribution spanning from very quick to very slow responses, representing trials where attention has drifted, leading to either impulsive responses or delayed responses due to mind-wandering. The mixture of these processes creates a complex distribution with a prominent peak (from the deliberate process) and extended tails (from the attentional lapses). A standard single-process model assuming only a log-normal distribution would fail to capture these extended tails, leading to poor fit and potentially misleading conclusions about the cognitive processes involved. This is exactly the situation where mixture models excel. They allow us to represent observed data as coming from a weighted combination of different underlying processes. Next, we’ll formalize this intuition and extend it to decision-making models. 9.2 Theoretical Foundation for Mixture Models 9.2.1 The Mixture Model Framework At their core, mixture models represent data as coming from a weighted combination of different “component” distributions or processes. Mathematically, a mixture model can be expressed as: p(y) = π_1p_1(y) + π_2p_2(y) + … + π_kp_k(y) Where: p(y) is the overall probability of observing data point y p_j(y) is the probability of y according to component model j π_j is the weight or mixing proportion of component j (with all π_j summing to 1) Each component distribution p_j(y) can have its own parameters, and the mixing proportions π_j determine how much each component contributes to the overall model. 9.2.2 Mixture Models and Latent Variables An alternative and often useful way to think about mixture models is through latent (unobserved) variables. We can introduce a latent categorical variable z that indicates which component generated each observation. For example, in a two-component mixture: z_i = 1 means observation i came from component 1 z_i=2 means observation i came from component 2 We can then formulate the mixture model as: p(y_i, z_i) = p(z_i)p(y_i∣z_i) Where p(z_i=j)=π_j is the prior probability of component j, and p(y_i∣z_i = j) is the likelihood of y_i under component j. This latent variable perspective is particularly useful for implementing mixture models in Bayesian frameworks like Stan. 9.2.3 Applications in Cognitive Modeling In cognitive modeling, mixture models can represent several important phenomena: Attentional fluctuations: As in our reaction time example, performance may reflect both focused engagement and attentional lapses Strategy switching: Individuals might switch between different strategies over time Dual-process theories: Behavior might arise from multiple cognitive systems (e.g., automatic vs. controlled) Individual differences: Different individuals might use different strategies Exploration vs. exploitation: Some decisions might reflect exploring new options while others exploit known rewards 9.3 Implementing a Basic Mixture Model Now that we understand the theoretical foundation, let’s implement a simple mixture model for choice data. We’ll model behavior as a mixture of two processes: A biased process that chooses option 1, let’s say “right”, with some probability θ A random process that chooses randomly (50/50) between options 1 and 0, or “right” and “left” This could represent, for example, a person who sometimes carefully performs a task but occasionally responds randomly due to attentional lapses. 9.3.1 Simulating Mixed Strategy Data In previous chapters we generated data to use for fitting models. Let’s use that same data but focus on a particular agent who might be mixing strategies: # Load data from a previous chapter d &lt;- read_csv(&quot;simdata/W3_randomnoise.csv&quot;) # Choose data with some bias (0.8) and noise (0.1) dd &lt;- d %&gt;% subset(rate == 0.8 &amp; noise == 0.1) # Prepare data for Stan data &lt;- list( n = 120, h = dd$choice ) # Display summary statistics mean_choice &lt;- mean(dd$choice) cat(&quot;Mean choice (proportion of &#39;right&#39; choices):&quot;, mean_choice, &quot;\\n&quot;) ## Mean choice (proportion of &#39;right&#39; choices): 0.75 Let’s visualize this data to see if we can detect patterns suggestive of a mixture: # Calculate running proportion of right choices running_prop &lt;- cumsum(dd$choice) / seq_along(dd$choice) # Plot actual choices over time p1 &lt;- ggplot(dd, aes(x = trial)) + geom_point(aes(y = choice), alpha = 0.5) + geom_hline(yintercept = 0.5, linetype = &quot;dashed&quot;, color = &quot;red&quot;) + labs(title = &quot;Sequence of Choices&quot;, subtitle = &quot;0 = left, 1 = right&quot;, x = &quot;Trial&quot;, y = &quot;Choice&quot;) + theme_minimal() # Plot running average p2 &lt;- ggplot(dd, aes(x = trial)) + geom_line(aes(y = running_prop)) + geom_hline(yintercept = 0.5, linetype = &quot;dashed&quot;, color = &quot;red&quot;) + geom_hline(yintercept = 0.8, linetype = &quot;dashed&quot;, color = &quot;blue&quot;) + labs(title = &quot;Running Proportion of Right Choices&quot;, subtitle = &quot;Red: random choice (0.5), Blue: true bias (0.8)&quot;, x = &quot;Trial&quot;, y = &quot;Proportion Right&quot;) + ylim(0, 1) + theme_minimal() p1 / p2 For binary choice data, it’s harder to visually detect a mixture compared to reaction times. The overall proportion of right choices falls between what we’d expect from random choice (0.5) and fully biased choice with rate 0.8, which is consistent with a mixture of these processes. 9.3.2 Stan Implementation of a Mixture Model Now let’s implement a Stan model that represents choices as coming from a mixture of a biased process and a random process: stan_mixture_model &lt;- &quot; // Mixture model for binary choice data // Combines a biased choice process with a random choice process data { int&lt;lower=1&gt; n; // Number of trials array[n] int h; // Choice data (0/1) } parameters { real bias; // Bias parameter for biased process (logit scale) real noise_logit; // Mixing weight for random process (logit scale) } model { // Priors target += normal_lpdf(bias | 0, 1); // Prior for bias parameter (centered at 0.5 in prob scale) target += normal_lpdf(noise_logit | -1, 1); // Prior for noise proportion (favors lower noise) // Mixture likelihood using log_sum_exp for numerical stability target += log_sum_exp( log(inv_logit(noise_logit)) + // Log probability of random process bernoulli_logit_lpmf(h | 0), // Log likelihood under random process (p=0.5) log1m(inv_logit(noise_logit)) + // Log probability of biased process bernoulli_logit_lpmf(h | bias) // Log likelihood under biased process ); } generated quantities { // Transform parameters to probability scale for easier interpretation real&lt;lower=0, upper=1&gt; noise_p = inv_logit(noise_logit); // Proportion of random choices real&lt;lower=0, upper=1&gt; bias_p = inv_logit(bias); // Bias toward right in biased choices // Predicted distributions vector[n] log_lik; array[n] int pred_component; // Which component generated each prediction (1=random, 0=biased) array[n] int pred_choice; // Predicted choices // Calculate log likelihood for each observation (for model comparison) for (i in 1:n) { log_lik[i] = log_sum_exp( log(noise_p) + bernoulli_logit_lpmf(h[i] | 0), log1m(noise_p) + bernoulli_logit_lpmf(h[i] | bias) ); } // Generate posterior predictions for (i in 1:n) { // First determine which component to use pred_component[i] = bernoulli_rng(noise_p); // Then generate prediction from appropriate component if (pred_component[i] == 1) { // Random component pred_choice[i] = bernoulli_rng(0.5); } else { // Biased component pred_choice[i] = bernoulli_rng(bias_p); } } } &quot; # Write the Stan model to a file write_stan_file( stan_mixture_model, dir = &quot;stan/&quot;, basename = &quot;W8_MixtureSingle.stan&quot; ) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W8_MixtureSingle.stan&quot; # Compile the model file &lt;- file.path(&quot;stan/W8_MixtureSingle.stan&quot;) mod_mixture &lt;- cmdstan_model(file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) 9.3.3 Fitting and Evaluating the Mixture Model Now let’s fit the model and evaluate its performance: # File path for saved model results model_file &lt;- &quot;simmodels/W8_singlemixture.RDS&quot; # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(model_file)) { # Fit the model samples &lt;- mod_mixture$sample( data = data, seed = 123, chains = 2, parallel_chains = 2, threads_per_chain = 1, iter_warmup = 2000, iter_sampling = 2000, refresh = 500, max_treedepth = 20, adapt_delta = 0.99, ) # Save the results samples$save_object(file = model_file) cat(&quot;Generated new model fit and saved to&quot;, model_file, &quot;\\n&quot;) } else { # Load existing results samples &lt;- readRDS(model_file) cat(&quot;Loaded existing model fit from&quot;, model_file, &quot;\\n&quot;) } ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 Iteration: 1 / 4000 [ 0%] (Warmup) ## Chain 1 Iteration: 500 / 4000 [ 12%] (Warmup) ## Chain 1 Iteration: 1000 / 4000 [ 25%] (Warmup) ## Chain 1 Iteration: 1500 / 4000 [ 37%] (Warmup) ## Chain 1 Iteration: 2000 / 4000 [ 50%] (Warmup) ## Chain 1 Iteration: 2001 / 4000 [ 50%] (Sampling) ## Chain 1 Iteration: 2500 / 4000 [ 62%] (Sampling) ## Chain 2 Iteration: 1 / 4000 [ 0%] (Warmup) ## Chain 2 Iteration: 500 / 4000 [ 12%] (Warmup) ## Chain 2 Iteration: 1000 / 4000 [ 25%] (Warmup) ## Chain 2 Iteration: 1500 / 4000 [ 37%] (Warmup) ## Chain 2 Iteration: 2000 / 4000 [ 50%] (Warmup) ## Chain 2 Iteration: 2001 / 4000 [ 50%] (Sampling) ## Chain 2 Iteration: 2500 / 4000 [ 62%] (Sampling) ## Chain 1 Iteration: 3000 / 4000 [ 75%] (Sampling) ## Chain 1 Iteration: 3500 / 4000 [ 87%] (Sampling) ## Chain 1 Iteration: 4000 / 4000 [100%] (Sampling) ## Chain 2 Iteration: 3000 / 4000 [ 75%] (Sampling) ## Chain 2 Iteration: 3500 / 4000 [ 87%] (Sampling) ## Chain 1 finished in 0.3 seconds. ## Chain 2 Iteration: 4000 / 4000 [100%] (Sampling) ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## ## Generated new model fit and saved to simmodels/W8_singlemixture.RDS # Extract summary statistics summary &lt;- samples$summary(c(&quot;bias_p&quot;, &quot;noise_p&quot;)) print(summary) ## # A tibble: 2 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 bias_p 0.742 0.743 0.0385 0.0392 0.677 0.803 1.00 2251. 2120. ## 2 noise_p 0.251 0.219 0.153 0.149 0.0544 0.538 1.00 2159. 2020. 9.3.4 Model Diagnostics Let’s check the model diagnostics to ensure our inference is reliable: # Extract draws for diagnostics draws &lt;- as_draws_df(samples$draws()) # Check for convergence with trace plots p1 &lt;- mcmc_trace(draws, pars = c(&quot;bias_p&quot;, &quot;noise_p&quot;)) + ggtitle(&quot;Trace Plots for Mixture Model Parameters&quot;) + theme_minimal() # Examine parameter distributions p2 &lt;- mcmc_hist(draws, pars = c(&quot;bias_p&quot;, &quot;noise_p&quot;)) + ggtitle(&quot;Posterior Distributions&quot;) + theme_minimal() # Display diagnostics p1 p2 # Check for potential divergences cat(&quot;Number of divergent transitions:&quot;, sum(draws$.divergent), &quot;\\n&quot;) ## Number of divergent transitions: 0 9.3.5 Parameter Recovery Analysis To verify that our model can accurately recover parameters, let’s examine how close our inferred parameters are to the true values used in the simulation: # True parameters (from the simulation) true_bias &lt;- 0.8 true_noise &lt;- 0.1 # Calculate parameter recovery metrics bias_error &lt;- mean(draws$bias_p) - true_bias noise_error &lt;- mean(draws$noise_p) - true_noise cat(&quot;Parameter Recovery Results:\\n&quot;) ## Parameter Recovery Results: cat(&quot;Bias parameter - True:&quot;, true_bias, &quot;Estimated:&quot;, round(mean(draws$bias_p), 3), &quot;Error:&quot;, round(bias_error, 3), &quot;\\n&quot;) ## Bias parameter - True: 0.8 Estimated: 0.742 Error: -0.058 cat(&quot;Noise parameter - True:&quot;, true_noise, &quot;Estimated:&quot;, round(mean(draws$noise_p), 3), &quot;Error:&quot;, round(noise_error, 3), &quot;\\n&quot;) ## Noise parameter - True: 0.1 Estimated: 0.251 Error: 0.151 # Generate prior samples in probability space set.seed(123) n_samples &lt;- nrow(draws) prior_samples &lt;- tibble( bias_prior = inv_logit_scaled(rnorm(n_samples, 0, 1)), noise_prior = inv_logit_scaled(rnorm(n_samples, 0, 1)) ) # Prepare data for plotting recovery_data &lt;- tibble( parameter = rep(c(&quot;Bias&quot;, &quot;Noise&quot;), each = nrow(draws)), posterior = c(draws$bias_p, draws$noise_p), prior = c(prior_samples$bias_prior, prior_samples$noise_prior), true_value = rep(c(true_bias, true_noise), each = nrow(draws)), type = &quot;Posterior&quot; ) prior_data &lt;- tibble( parameter = rep(c(&quot;Bias&quot;, &quot;Noise&quot;), each = nrow(prior_samples)), value = c(prior_samples$bias_prior, prior_samples$noise_prior), true_value = rep(c(true_bias, true_noise), each = nrow(prior_samples)), type = &quot;Prior&quot; ) # Visualize parameter recovery with priors ggplot() + # Add prior densities geom_histogram(data = prior_data, aes(x = value, fill = type), alpha = 0.5, bins = 30) + # Add posterior densities geom_histogram(data = recovery_data, aes(x = posterior, fill = type), alpha = 0.5, bins = 30) + # Add true values geom_vline(data = recovery_data %&gt;% distinct(parameter, true_value), aes(xintercept = true_value), linetype = &quot;dashed&quot;, size = 1, color = &quot;black&quot;) + # Facet by parameter facet_wrap(~parameter, scales = &quot;free&quot;) + # Formatting scale_fill_manual(values = c(&quot;Posterior&quot; = &quot;blue&quot;, &quot;Prior&quot; = &quot;red&quot;), name = &quot;Distribution&quot;) + labs(title = &quot;Parameter Recovery Analysis&quot;, subtitle = &quot;Dashed lines show true values used in simulation&quot;, x = &quot;Parameter Value (probability scale)&quot;, y = &quot;Count&quot;) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) The model has recovered the true parameters reasonably well, providing confidence in our approach. In a real project, we’d want to run this across a broad range of parameter values! 9.3.6 Posterior Predictive Checks # Generate posterior and prior predictive samples n_samples &lt;- 100 # First, generate posterior predictive samples post_pred_samples &lt;- matrix(NA, nrow = n_samples, ncol = data$n) post_component_samples &lt;- matrix(NA, nrow = n_samples, ncol = data$n) # Sample from posterior distributions post_sample_indices &lt;- sample(1:nrow(draws), n_samples, replace = TRUE) for(i in 1:n_samples) { idx &lt;- post_sample_indices[i] noise_p_sample &lt;- draws$noise_p[idx] bias_p_sample &lt;- draws$bias_p[idx] for(j in 1:data$n) { # Determine component (0 = biased, 1 = random) component &lt;- rbinom(1, 1, noise_p_sample) post_component_samples[i,j] &lt;- component if(component == 1) { # Random component post_pred_samples[i,j] &lt;- rbinom(1, 1, 0.5) } else { # Biased component post_pred_samples[i,j] &lt;- rbinom(1, 1, bias_p_sample) } } } # Now, generate prior predictive samples in the same way prior_pred_samples &lt;- matrix(NA, nrow = n_samples, ncol = data$n) prior_component_samples &lt;- matrix(NA, nrow = n_samples, ncol = data$n) # Generate samples from prior distributions prior_noise_samples &lt;- inv_logit_scaled(rnorm(n_samples, 0, 1)) prior_bias_samples &lt;- inv_logit_scaled(rnorm(n_samples, 0, 1)) for(i in 1:n_samples) { noise_p_prior &lt;- prior_noise_samples[i] bias_p_prior &lt;- prior_bias_samples[i] for(j in 1:data$n) { # Determine component (0 = biased, 1 = random) component &lt;- rbinom(1, 1, noise_p_prior) prior_component_samples[i,j] &lt;- component if(component == 1) { # Random component prior_pred_samples[i,j] &lt;- rbinom(1, 1, 0.5) } else { # Biased component prior_pred_samples[i,j] &lt;- rbinom(1, 1, bias_p_prior) } } } # Calculate summary statistics from predictive samples # For posterior post_pred_means &lt;- apply(post_pred_samples, 1, mean) post_pred_run_lengths &lt;- apply(post_pred_samples, 1, function(x) mean(rle(x)$lengths)) # For prior prior_pred_means &lt;- apply(prior_pred_samples, 1, mean) prior_pred_run_lengths &lt;- apply(prior_pred_samples, 1, function(x) mean(rle(x)$lengths)) # Calculate observed statistics obs_mean &lt;- mean(data$h) obs_runs &lt;- mean(rle(data$h)$lengths) # Prepare data for visualization pred_means_data &lt;- tibble( mean = c(post_pred_means, prior_pred_means), type = factor(c(rep(&quot;Posterior&quot;, n_samples), rep(&quot;Prior&quot;, n_samples)), levels = c(&quot;Prior&quot;, &quot;Posterior&quot;)) ) pred_runs_data &lt;- tibble( run_length = c(post_pred_run_lengths, prior_pred_run_lengths), type = factor(c(rep(&quot;Posterior&quot;, n_samples), rep(&quot;Prior&quot;, n_samples)), levels = c(&quot;Prior&quot;, &quot;Posterior&quot;)) ) # Visualize posterior and prior predictive checks p1 &lt;- ggplot() + # Add prior predictive distribution geom_histogram(data = pred_means_data %&gt;% filter(type == &quot;Prior&quot;), aes(x = mean, fill = type), bins = 30, alpha = 0.5) + # Add posterior predictive distribution geom_histogram(data = pred_means_data %&gt;% filter(type == &quot;Posterior&quot;), aes(x = mean, fill = type), bins = 30, alpha = 0.5) + # Add observed statistic geom_vline(xintercept = obs_mean, color = &quot;black&quot;, size = 1, linetype = &quot;dashed&quot;) + scale_fill_manual(values = c(&quot;Prior&quot; = &quot;red&quot;, &quot;Posterior&quot; = &quot;blue&quot;), name = &quot;Distribution&quot;) + labs(title = &quot;Predictive Check: Mean&quot;, subtitle = &quot;Dashed line shows observed data value&quot;, x = &quot;Mean (proportion of right choices)&quot;, y = &quot;Count&quot;) + theme_minimal() p2 &lt;- ggplot() + # Add prior predictive distribution geom_histogram(data = pred_runs_data %&gt;% filter(type == &quot;Prior&quot;), aes(x = run_length, fill = type), bins = 30, alpha = 0.5) + # Add posterior predictive distribution geom_histogram(data = pred_runs_data %&gt;% filter(type == &quot;Posterior&quot;), aes(x = run_length, fill = type), bins = 30, alpha = 0.5) + # Add observed statistic geom_vline(xintercept = obs_runs, color = &quot;black&quot;, size = 1, linetype = &quot;dashed&quot;) + scale_fill_manual(values = c(&quot;Prior&quot; = &quot;red&quot;, &quot;Posterior&quot; = &quot;blue&quot;), name = &quot;Distribution&quot;) + labs(title = &quot;Predictive Check: Run Lengths&quot;, subtitle = &quot;Dashed line shows observed data value&quot;, x = &quot;Mean run length&quot;, y = &quot;Count&quot;) + theme_minimal() # Display the plots side by side with shared legend p1 + p2 + plot_layout(guides = &quot;collect&quot;) &amp; theme(legend.position = &quot;bottom&quot;) Our model appears to capture both the overall proportion of right choices and the distribution of run lengths in the observed data. This suggests the mixture model is adequately representing the data-generating process. 9.4 Parameter Interpretation The mixture model provides valuable insights into the cognitive processes underlying the observed behavior: Bias Parameter (θ): The bias parameter (estimated as approximately r round(mean(draws$bias_p), 2)) represents the probability of choosing the right option when the participant is following the biased process. Noise Parameter (π): The noise parameter (estimated as approximately r round(mean(draws$noise_p), 2)) represents the proportion of choices that come from the random process rather than the biased process. This can be interpreted as the frequency of attentional lapses or exploratory behavior. The mixture model thus decomposes behavior into two distinct processes, providing a more nuanced understanding than a single-process model could offer. This has important cognitive implications: for instance, besides the usual focus on “deliberate” reaction times, individual variations attentional lapses might be affected by the experimental condition, or an underlying diagnosis, thus providing richer information. 9.5 Multilevel Mixture Models Now let’s extend our approach to a multilevel (hierarchical) mixture model that can accommodate individual differences across multiple participants. This allows us to estimate both population-level parameters and individual-specific variations. 9.5.1 Stan Implementation stan_multilevel_mixture_model &lt;- &quot; // Multilevel mixture model for binary choice data // Allows individual differences in both bias and mixture weights functions { real normal_lb_rng(real mu, real sigma, real lb) { real p = normal_cdf(lb | mu, sigma); // cdf for bounds real u = uniform_rng(p, 1); return (sigma * inv_Phi(u)) + mu; // inverse cdf for value } } data { int&lt;lower=1&gt; trials; // Number of trials per agent int&lt;lower=1&gt; agents; // Number of agents array[trials, agents] int h; // Choice data (0/1) } parameters { // Population-level parameters real biasM; // Population mean of bias (logit scale) real noiseM; // Population mean of noise proportion (logit scale) // Population standard deviations vector&lt;lower=0&gt;[2] tau; // SDs for [bias, noise] // Individual z-scores (non-centered parameterization) matrix[2, agents] z_IDs; // Correlation matrix cholesky_factor_corr[2] L_u; } transformed parameters { // Individual parameters (constructed from non-centered parameterization) matrix[agents, 2] IDs; IDs = (diag_pre_multiply(tau, L_u) * z_IDs)&#39;; } model { // Priors for population means target += normal_lpdf(biasM | 0, 1); // Prior for population bias mean target += normal_lpdf(noiseM | -1, 0.5); // Prior for population noise mean (favoring lower noise) // Priors for population SDs (half-normal) target += normal_lpdf(tau[1] | 0, 0.3) - normal_lccdf(0 | 0, 0.3); target += normal_lpdf(tau[2] | 0, 0.3) - normal_lccdf(0 | 0, 0.3); // Prior for correlation matrix target += lkj_corr_cholesky_lpdf(L_u | 2); // Prior for individual z-scores target += std_normal_lpdf(to_vector(z_IDs)); // Likelihood for (i in 1:agents) { target += log_sum_exp( log(inv_logit(noiseM + IDs[i,2])) + // Prob of random process for agent i bernoulli_logit_lpmf(h[,i] | 0), // Likelihood under random process log1m(inv_logit(noiseM + IDs[i,2])) + // Prob of biased process for agent i bernoulli_logit_lpmf(h[,i] | biasM + IDs[i,1]) // Likelihood under biased process ); } } generated quantities { // Prior predictive samples real biasM_prior = normal_rng(0, 1); real&lt;lower=0&gt; biasSD_prior = normal_lb_rng(0, 0.3, 0); real noiseM_prior = normal_rng(-1, 0.5); real&lt;lower=0&gt; noiseSD_prior = normal_lb_rng(0, 0.3, 0); // Transform to probability scale for easier interpretation real&lt;lower=0, upper=1&gt; bias_mean = inv_logit(biasM); real&lt;lower=0, upper=1&gt; noise_mean = inv_logit(noiseM); // Correlation between parameters corr_matrix[2] Omega = multiply_lower_tri_self_transpose(L_u); real bias_noise_corr = Omega[1,2]; // For each agent, generate individual parameters and predictions array[agents] real&lt;lower=0, upper=1&gt; agent_bias; array[agents] real&lt;lower=0, upper=1&gt; agent_noise; array[trials, agents] int pred_component; array[trials, agents] int pred_choice; array[trials, agents] real log_lik; // Calculate individual parameters and generate predictions for (i in 1:agents) { // Individual parameters agent_bias[i] = inv_logit(biasM + IDs[i,1]); agent_noise[i] = inv_logit(noiseM + IDs[i,2]); // Log likelihood calculations for (t in 1:trials) { log_lik[t,i] = log_sum_exp( log(agent_noise[i]) + bernoulli_logit_lpmf(h[t,i] | 0), log1m(agent_noise[i]) + bernoulli_logit_lpmf(h[t,i] | logit(agent_bias[i])) ); } // Generate predictions for (t in 1:trials) { pred_component[t,i] = bernoulli_rng(agent_noise[i]); if (pred_component[t,i] == 1) { // Random component pred_choice[t,i] = bernoulli_rng(0.5); } else { // Biased component pred_choice[t,i] = bernoulli_rng(agent_bias[i]); } } } } &quot; # Write the model to a file write_stan_file( stan_multilevel_mixture_model, dir = &quot;stan/&quot;, basename = &quot;W8_MixtureMultilevel.stan&quot; ) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W8_MixtureMultilevel.stan&quot; # Compile the model file &lt;- file.path(&quot;stan/W8_MixtureMultilevel.stan&quot;) mod_multilevel_mixture &lt;- cmdstan_model( file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;) ) 9.5.2 Preparing Data for the Multilevel Model Now we’ll prepare data from multiple agents for the multilevel model: # Set random seed for reproducibility set.seed(12345) # Simulation parameters agents &lt;- 20 trials &lt;- 120 # Population-level parameters rate_M &lt;- 1.386 # Population mean of bias (~0.8 in probability space) rate_SD &lt;- 0.65 # Population SD of bias noise_M &lt;- -2.2 # Population mean of noise (~0.1 in probability space) noise_SD &lt;- 0.5 # Population SD of noise # Print population parameters in probability space for clarity cat(&quot;Population parameters in probability space:\\n&quot;) ## Population parameters in probability space: cat(&quot;Mean rate:&quot;, round(plogis(rate_M), 2), &quot;\\n&quot;) ## Mean rate: 0.8 cat(&quot;Mean noise:&quot;, round(plogis(noise_M), 2), &quot;\\n&quot;) ## Mean noise: 0.1 # Generate random agent parameters agent_rates &lt;- rnorm(agents, rate_M, rate_SD) agent_noises &lt;- rnorm(agents, noise_M, noise_SD) # Convert to probability space for easier interpretation agent_rates_prob &lt;- plogis(agent_rates) agent_noises_prob &lt;- plogis(agent_noises) # Generate choice data generate_agent_data &lt;- function(agent_id, rate, noise, n_trials) { # Convert parameters to probability space rate_p &lt;- plogis(rate) noise_p &lt;- plogis(noise) # Initialize vectors choices &lt;- numeric(n_trials) components &lt;- numeric(n_trials) # 0 = biased, 1 = random for (t in 1:n_trials) { # Determine which component generates this choice component &lt;- rbinom(1, 1, noise_p) components[t] &lt;- component if (component == 1) { # Random component (0.5 probability) choices[t] &lt;- rbinom(1, 1, 0.5) } else { # Biased component choices[t] &lt;- rbinom(1, 1, rate_p) } } return(tibble( agent = agent_id, trial = 1:n_trials, choice = choices, component = components, true_rate = rate, true_rate_prob = rate_p, true_noise = noise, true_noise_prob = noise_p )) } # Generate data for all agents d &lt;- map_df(1:agents, function(id) { generate_agent_data( agent_id = id, rate = agent_rates[id], noise = agent_noises[id], n_trials = trials ) }) # Create matrix where columns are agents, rows are trials h_matrix &lt;- d %&gt;% dplyr::select(agent, trial, choice) %&gt;% pivot_wider( names_from = agent, values_from = choice ) %&gt;% dplyr::select(-trial) %&gt;% as.matrix() # Summary statistics agent_summary &lt;- d %&gt;% group_by(agent, true_rate_prob, true_noise_prob) %&gt;% summarize( mean_choice = mean(choice), prop_component_1 = mean(component), n_trials = n() ) # Prepare data for Stan data_multilevel &lt;- list( trials = trials, agents = agents, h = h_matrix ) # Verify the dimensions of the matrix print(paste(&quot;Matrix dimensions (trials × agents):&quot;, paste(dim(h_matrix), collapse = &quot; × &quot;))) ## [1] &quot;Matrix dimensions (trials × agents): 120 × 20&quot; # Display summary of agent parameters cat(&quot;\\nAgent parameters summary:\\n&quot;) ## ## Agent parameters summary: print(summary(agent_summary)) ## agent true_rate_prob true_noise_prob mean_choice prop_component_1 ## Min. : 1.00 Min. :0.5509 Min. :0.04604 Min. :0.6083 Min. :0.02500 ## 1st Qu.: 5.75 1st Qu.:0.7596 1st Qu.:0.08461 1st Qu.:0.7375 1st Qu.:0.06458 ## Median :10.50 Median :0.8088 Median :0.12743 Median :0.8083 Median :0.12083 ## Mean :10.50 Mean :0.7952 Mean :0.13278 Mean :0.7771 Mean :0.12500 ## 3rd Qu.:15.25 3rd Qu.:0.8562 3rd Qu.:0.19007 3rd Qu.:0.8187 3rd Qu.:0.16042 ## Max. :20.00 Max. :0.9287 Max. :0.24944 Max. :0.8750 Max. :0.30000 ## n_trials ## Min. :120 ## 1st Qu.:120 ## Median :120 ## Mean :120 ## 3rd Qu.:120 ## Max. :120 # Plot agent-level parameters ggplot(agent_summary, aes(x = true_rate_prob, y = true_noise_prob)) + geom_point(alpha = 0.7) + geom_text(aes(label = agent), hjust = -0.3, vjust = 0) + labs( title = &quot;True Agent Parameters&quot;, subtitle = &quot;Each point represents one agent&quot;, x = &quot;Rate (probability of choosing right when using biased component)&quot;, y = &quot;Noise (probability of using random component)&quot;, size = &quot;Mean Choice&quot; ) + theme_minimal() # Plot choice patterns for a few selected agents selected_agents &lt;- c(1, 5, 10, 15, 20) d_selected &lt;- d %&gt;% filter(agent %in% selected_agents) ggplot(d_selected, aes(x = trial, y = choice, color = factor(agent))) + geom_line(alpha = 0.5) + geom_point(aes(shape = factor(component)), size = 2) + scale_shape_manual(values = c(16, 4), labels = c(&quot;Biased&quot;, &quot;Random&quot;), name = &quot;Component&quot;) + labs( title = &quot;Choice Patterns for Selected Agents&quot;, subtitle = &quot;Circles = biased component, X = random component&quot;, x = &quot;Trial&quot;, y = &quot;Choice (0/1)&quot;, color = &quot;Agent&quot; ) + theme_minimal() + facet_wrap(~agent, ncol = 1) 9.5.3 Fitting and Evaluating the Multilevel Model # File path for saved multilevel model results multilevel_model_file &lt;- &quot;simmodels/W8_multimixture.RDS&quot; # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(multilevel_model_file)) { # Fit the multilevel model multilevel_samples &lt;- mod_multilevel_mixture$sample( data = data_multilevel, seed = 123, chains = 2, parallel_chains = 2, threads_per_chain = 2, iter_warmup = 2000, iter_sampling = 2000, refresh = 500, max_treedepth = 20, adapt_delta = 0.99, ) # Save the results multilevel_samples$save_object(file = multilevel_model_file) cat(&quot;Generated new multilevel model fit and saved to&quot;, multilevel_model_file, &quot;\\n&quot;) } else { # Load existing results multilevel_samples &lt;- readRDS(multilevel_model_file) cat(&quot;Loaded existing multilevel model fit from&quot;, multilevel_model_file, &quot;\\n&quot;) } ## Running MCMC with 2 parallel chains, with 2 thread(s) per chain... ## ## Chain 1 Iteration: 1 / 4000 [ 0%] (Warmup) ## Chain 2 Iteration: 1 / 4000 [ 0%] (Warmup) ## Chain 1 Iteration: 500 / 4000 [ 12%] (Warmup) ## Chain 2 Iteration: 500 / 4000 [ 12%] (Warmup) ## Chain 1 Iteration: 1000 / 4000 [ 25%] (Warmup) ## Chain 2 Iteration: 1000 / 4000 [ 25%] (Warmup) ## Chain 1 Iteration: 1500 / 4000 [ 37%] (Warmup) ## Chain 2 Iteration: 1500 / 4000 [ 37%] (Warmup) ## Chain 1 Iteration: 2000 / 4000 [ 50%] (Warmup) ## Chain 1 Iteration: 2001 / 4000 [ 50%] (Sampling) ## Chain 2 Iteration: 2000 / 4000 [ 50%] (Warmup) ## Chain 2 Iteration: 2001 / 4000 [ 50%] (Sampling) ## Chain 1 Iteration: 2500 / 4000 [ 62%] (Sampling) ## Chain 2 Iteration: 2500 / 4000 [ 62%] (Sampling) ## Chain 1 Iteration: 3000 / 4000 [ 75%] (Sampling) ## Chain 2 Iteration: 3000 / 4000 [ 75%] (Sampling) ## Chain 1 Iteration: 3500 / 4000 [ 87%] (Sampling) ## Chain 1 Iteration: 4000 / 4000 [100%] (Sampling) ## Chain 2 Iteration: 3500 / 4000 [ 87%] (Sampling) ## Chain 1 finished in 11.8 seconds. ## Chain 2 Iteration: 4000 / 4000 [100%] (Sampling) ## Chain 2 finished in 13.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 12.7 seconds. ## Total execution time: 13.9 seconds. ## ## Generated new multilevel model fit and saved to simmodels/W8_multimixture.RDS # Summary of population-level parameters summary_ml &lt;- multilevel_samples$summary(c(&quot;bias_mean&quot;, &quot;noise_mean&quot;, &quot;bias_noise_corr&quot;)) print(summary_ml) ## # A tibble: 3 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 bias_mean 0.782 0.783 0.0152 0.0148 0.756 0.805 1.00 1643. 2204. ## 2 noise_mean 0.159 0.153 0.0538 0.0522 0.0823 0.257 1.00 4433. 2085. ## 3 bias_noise_corr -0.00263 0.00248 0.444 0.512 -0.721 0.715 1.00 5316. 2615. 9.5.4 Examining Individual Differences The multilevel mixture model allows us to examine individual differences in both bias and mixture weights: # Extract draws for individual parameters draws_ml &lt;- as_draws_df(multilevel_samples$draws()) # First, we need to extract all the necessary parameters # Population-level parameters biasM &lt;- mean(draws_ml$biasM) noiseM &lt;- mean(draws_ml$noiseM) tau1 &lt;- mean(draws_ml$`tau[1]`) tau2 &lt;- mean(draws_ml$`tau[2]`) # Create a dataframe to store the individual parameters individual_params &lt;- tibble( agent = 1:data_multilevel$agents, bias = numeric(data_multilevel$agents), noise = numeric(data_multilevel$agents) ) # Loop through agents to extract and transform z_IDs to actual parameter values for (i in 1:data_multilevel$agents) { # Extract the z-scores for this agent z1_name &lt;- paste0(&quot;z_IDs[1,&quot;, i, &quot;]&quot;) z2_name &lt;- paste0(&quot;z_IDs[2,&quot;, i, &quot;]&quot;) # Calculate bias and noise based on the multilevel structure # Note: This assumes the model uses z-scores to represent deviations from population means z1 &lt;- mean(draws_ml[[z1_name]]) z2 &lt;- mean(draws_ml[[z2_name]]) # Transform back to original scale individual_params$bias[i] &lt;- plogis(biasM + z1 * tau1) # Convert to probability scale individual_params$noise[i] &lt;- plogis(noiseM + z2 * tau2) # Convert to probability scale } # Get the true values from our generated data frame &#39;d&#39; true_params &lt;- d %&gt;% group_by(agent) %&gt;% summarize( true_rate = first(true_rate), true_rate_prob = first(true_rate_prob), true_noise = first(true_noise), true_noise_prob = first(true_noise_prob) ) # Join with the true parameter values individual_params &lt;- individual_params %&gt;% left_join(true_params, by = &quot;agent&quot;) # Check the result head(individual_params) ## # A tibble: 6 × 7 ## agent bias noise true_rate true_rate_prob true_noise true_noise_prob ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.800 0.151 1.77 0.854 -1.81 0.141 ## 2 2 0.815 0.151 1.85 0.864 -1.47 0.187 ## 3 3 0.758 0.150 1.31 0.788 -2.52 0.0743 ## 4 4 0.800 0.150 1.09 0.749 -2.98 0.0485 ## 5 5 0.840 0.151 1.78 0.856 -3.00 0.0475 ## 6 6 0.709 0.153 0.204 0.551 -1.30 0.215 # Plot individual parameter estimates p1 &lt;- ggplot(individual_params, aes(x = bias, y = noise)) + geom_point(size = 3, alpha = 0.7) + geom_hline(yintercept = mean(draws_ml$noise_mean), linetype = &quot;dashed&quot;) + geom_vline(xintercept = mean(draws_ml$bias_mean), linetype = &quot;dashed&quot;) + labs(title = &quot;Individual Parameter Estimates&quot;, subtitle = &quot;Each point represents one agent&quot;, x = &quot;Bias Parameter (θ)&quot;, y = &quot;Noise Parameter (π)&quot;) + theme_minimal() # Plot correlation between true and estimated parameters p2 &lt;- ggplot(individual_params, aes(x = inv_logit_scaled(true_rate), y = bias)) + geom_point(size = 3, alpha = 0.7) + geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;) + labs(title = &quot;Parameter Recovery: Bias&quot;, subtitle = &quot;True vs. estimated bias parameter&quot;, x = &quot;True Bias&quot;, y = &quot;Estimated Bias&quot;) + theme_minimal() # Display plots p1 p2 9.5.5 Interpreting Parameter Correlations An important advantage of multilevel mixture models is their ability to reveal correlations between parameters across individuals: # Visualize correlation between parameters ggplot() + geom_histogram(aes(x = draws_ml$bias_noise_corr), bins = 30, fill = &quot;steelblue&quot;, alpha = 0.7) + geom_vline(xintercept = 0, linetype = &quot;dashed&quot;) + labs(title = &quot;Correlation Between Bias and Noise Parameters&quot;, subtitle = &quot;Population-level correlation across individuals&quot;, x = &quot;Correlation Coefficient&quot;, y = &quot;Count&quot;) + theme_minimal() The correlation between bias and noise parameters can provide important insights into cognitive processes. For example, a negative correlation might suggest that individuals with stronger biases tend to have fewer random lapses, while a positive correlation could indicate that strong biases are associated with more exploratory behavior. However, in the simulation process we did not include any correlation between the bias and noise parameters, so the correlation we observe here is correctly estimated as centered at 0. 9.6 Comparing Single-Process and Mixture Models Finally, let’s compare our mixture model with a single-process alternative to determine which better captures the observed behavior. 9.6.1 Single-Process Model First, let’s implement a simple single-process model that assumes all choices come from a biased process: # Stan model for single-process biased agent stan_biased_model &lt;- &quot; data { int&lt;lower=1&gt; n; // Number of trials array[n] int h; // Choice data (0/1) } parameters { real bias; // Bias parameter (logit scale) } model { // Prior target += normal_lpdf(bias | 0, 1); // Likelihood (all choices come from biased process) target += bernoulli_logit_lpmf(h | bias); } generated quantities { real&lt;lower=0, upper=1&gt; bias_p = inv_logit(bias); // Bias on probability scale // Log likelihood for model comparison vector[n] log_lik; for (i in 1:n) { log_lik[i] = bernoulli_logit_lpmf(h[i] | bias); } // Posterior predictions array[n] int pred_choice; for (i in 1:n) { pred_choice[i] = bernoulli_rng(bias_p); } } &quot; # Write the model to a file write_stan_file( stan_biased_model, dir = &quot;stan/&quot;, basename = &quot;W8_BiasedSingle.stan&quot; ) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W8_BiasedSingle.stan&quot; # Compile the model file &lt;- file.path(&quot;stan/W8_BiasedSingle.stan&quot;) mod_biased &lt;- cmdstan_model( file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;) ) # File path for saved single-process model results biased_model_file &lt;- &quot;simmodels/W8_biased_single.RDS&quot; # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(biased_model_file)) { # Fit the single-process model biased_samples &lt;- mod_biased$sample( data = data, seed = 123, chains = 2, parallel_chains = 2, threads_per_chain = 2, iter_warmup = 2000, iter_sampling = 2000, refresh = 500, max_treedepth = 20, adapt_delta = 0.99, ) # Save the results biased_samples$save_object(file = biased_model_file) cat(&quot;Generated new single-process model fit and saved to&quot;, biased_model_file, &quot;\\n&quot;) } else { # Load existing results biased_samples &lt;- readRDS(biased_model_file) cat(&quot;Loaded existing single-process model fit from&quot;, biased_model_file, &quot;\\n&quot;) } ## Running MCMC with 2 parallel chains, with 2 thread(s) per chain... ## ## Chain 1 Iteration: 1 / 4000 [ 0%] (Warmup) ## Chain 1 Iteration: 500 / 4000 [ 12%] (Warmup) ## Chain 1 Iteration: 1000 / 4000 [ 25%] (Warmup) ## Chain 1 Iteration: 1500 / 4000 [ 37%] (Warmup) ## Chain 1 Iteration: 2000 / 4000 [ 50%] (Warmup) ## Chain 1 Iteration: 2001 / 4000 [ 50%] (Sampling) ## Chain 1 Iteration: 2500 / 4000 [ 62%] (Sampling) ## Chain 1 Iteration: 3000 / 4000 [ 75%] (Sampling) ## Chain 1 Iteration: 3500 / 4000 [ 87%] (Sampling) ## Chain 1 Iteration: 4000 / 4000 [100%] (Sampling) ## Chain 2 Iteration: 1 / 4000 [ 0%] (Warmup) ## Chain 2 Iteration: 500 / 4000 [ 12%] (Warmup) ## Chain 2 Iteration: 1000 / 4000 [ 25%] (Warmup) ## Chain 2 Iteration: 1500 / 4000 [ 37%] (Warmup) ## Chain 2 Iteration: 2000 / 4000 [ 50%] (Warmup) ## Chain 2 Iteration: 2001 / 4000 [ 50%] (Sampling) ## Chain 2 Iteration: 2500 / 4000 [ 62%] (Sampling) ## Chain 2 Iteration: 3000 / 4000 [ 75%] (Sampling) ## Chain 1 finished in 0.2 seconds. ## Chain 2 Iteration: 3500 / 4000 [ 87%] (Sampling) ## Chain 2 Iteration: 4000 / 4000 [100%] (Sampling) ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Generated new single-process model fit and saved to simmodels/W8_biased_single.RDS # Summary of single-process model summary_biased &lt;- biased_samples$summary(&quot;bias_p&quot;) print(summary_biased) ## # A tibble: 1 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 bias_p 0.740 0.742 0.0385 0.0365 0.673 0.800 1.00 997. 696. 9.6.2 Model Comparison Now let’s compare the single-process model with our mixture model using leave-one-out cross-validation (LOO-CV): # Calculate LOO for both models loo_mixture &lt;- samples$loo() loo_biased &lt;- biased_samples$loo() # Compare models comparison &lt;- loo_compare(loo_mixture, loo_biased) print(comparison) ## elpd_diff se_diff ## model2 0.0 0.0 ## model1 -1.6 1.4 # Calculate model weights weights &lt;- loo_model_weights(list( &quot;Mixture&quot; = loo_mixture, &quot;Single-Process&quot; = loo_biased )) print(weights) ## Method: stacking ## ------ ## weight ## Mixture 0.000 ## Single-Process 1.000 # Visualize model comparison ggplot(tibble( model = names(weights), weight = as.numeric(weights) )) + geom_col(aes(x = model, y = weight, fill = model), alpha = 0.7) + scale_fill_brewer(palette = &quot;Set1&quot;) + labs(title = &quot;Model Comparison via LOO-CV&quot;, subtitle = &quot;Higher weights indicate better predictive performance&quot;, x = NULL, y = &quot;Model Weight&quot;) + theme_minimal() + theme(legend.position = &quot;none&quot;) + geom_text(aes(x = model, y = weight + 0.05, label = scales::percent(weight, accuracy = 0.1))) Based on the model comparison, the single model appears to better capture the data-generating process than the single-process alternative. This might seem counterintuitive, but it highlights that predictive performance is not necessarily the best way of choosing your model. What happens here is that a combination of two binomials can be mathematically reduced to a single binomial (in this case with a lower rate that the biased component). This is why the single-process model performs better in terms of LOO-CV, even though the mixture model is more realistic and provides a richer interpretation of the data. Theory should guide you in this case. 9.6.3 Applications of Mixture Models in Cognitive Science Mixture models have found numerous applications in cognitive science. Here are a few examples that highlight their versatility: Attention and Vigilance: Modeling attentional lapses during sustained attention tasks as a mixture of focused and random responses. Memory: Representing recognition memory as a mixture of more implicit familiarity and more explicit recollection processes, each with distinct characteristics. *Decision Making: Modeling economic choices as combinations of heuristic and deliberative processes, with the proportion varying based on task demands. Learning: Capturing the transition from rule-based to automatic processing during skill acquisition, with the mixture weights shifting over time. Individual Differences: Identifying subgroups of participants who employ qualitatively different strategies to solve the same task. 9.7 Conclusion Mixture models represent a crucial step forward in our cognitive modeling toolkit, allowing us to capture the complexity and variability inherent in human behavior. Through this chapter, we’ve seen how combining multiple cognitive strategies within a single model can provide richer and more realistic accounts of decision-making processes. Several key insights emerge from our exploration of mixture models: Beyond Single-Process Simplifications: Mixture models allow us to move beyond the false choice between oversimplified single-strategy models and intractably complex specifications. By combining a small number of interpretable components, we can capture substantial behavioral complexity while maintaining mathematical and computational tractability. Bayesian Implementation: The Bayesian implementation of mixture models in Stan provides powerful tools for inference. We can estimate not only the parameters of different cognitive strategies but also their relative contributions to behavior and how they might vary across individuals. Model Validation: Mixture models require careful attention to identifiability and validation. Through parameter recovery studies and posterior predictive checks, we’ve seen how to verify that our specifications can reliably recover true parameter values and generate realistic behavioral patterns. "],["win-stay-lose-shift-a-heuristic-decision-strategy.html", "Chapter 10 Win-Stay-Lose-Shift: A Heuristic Decision Strategy 10.1 Introduction 10.2 Generating Data: Simulating WSLS Agents 10.3 Data Processing and Initial Visualization 10.4 Verifying Model Properties and Assumptions 10.5 Modeling a Single WSLS Agent 10.6 Multilevel WSLS Model 10.7 Quality Checks and Parameter Recovery", " Chapter 10 Win-Stay-Lose-Shift: A Heuristic Decision Strategy 10.1 Introduction Win-Stay-Lose-Shift (WSLS) represents one of the simplest yet most fascinating decision-making strategies observed in both human and animal behavior. The core principle is intuitive: if an action leads to success, repeat it; if it leads to failure, try something else. Despite its simplicity, this strategy can produce sophisticated behavioral patterns and proves surprisingly effective in many scenarios. In this chapter, we’ll explore WSLS through computational modeling, building on our previous work with random agents. We’ll see how this apparently simple strategy can capture important aspects of learning and adaptation. Through careful implementation and testing, we’ll develop insights into both the strengths and limitations of WSLS as a model of decision-making. Our exploration will follow several key steps: Implementing the basic WSLS strategy in code Testing it against different opponents Scaling up to multiple agents Analyzing patterns in the resulting data The WSLS strategy differs significantly from our previous random agent models. Rather than making choices based on fixed probabilities, a WSLS agent: Remembers its previous choice Tracks whether that choice was successful Uses this information to determine its next move This creates an interesting form of path dependence - the agent’s choices are shaped by its history of interactions (and we need to ensure that memory is calculated for the correct trial and applied to the following trial). 10.1.1 Loading Required Packages Let’s begin by loading the packages we’ll need for our analysis: # Load necessary packages for simulation and analysis pacman::p_load(tidyverse, # For data manipulation and visualization here, # For file path management posterior, # For working with posterior samples cmdstanr, # For interfacing with Stan brms, # For Bayesian regression models tidybayes, # For working with Bayesian samples loo, # For model comparison patchwork) # For combining multiple plots 10.2 Generating Data: Simulating WSLS Agents Now we’ll set up a simulation environment where WSLS agents interact with random agents. The parameters we define here will shape our simulation. We’ll create 100 agents who each play 120 trials, allowing us to observe both individual behavior and broader patterns across many interactions. Our WSLS agent implementation uses a parameterized approach where: alpha represents a baseline bias toward choosing one option over another betaWin represents the strength of the “stay” response after a win betaLose represents the strength of the “shift” response after a loss noise allows for occasional random deviations from the strategy This parameterization lets us explore variants of the WSLS strategy with different sensitivities to wins and losses. # Define simulation parameters agents &lt;- 100 # Number of agents to simulate trials &lt;- 120 # Number of trials per agent # Noise parameter (probability of random choice regardless of strategy) noise &lt;- 0 # Parameters for random agent (on log-odds scale) rateM &lt;- 1.4 # Population mean bias for random agent rateSD &lt;- 0.3 # Population SD of bias # Parameters for WSLS agent (on log-odds scale) alphaM &lt;- 0 # Population mean baseline bias alphaSD &lt;- 0.1 # Population SD of baseline bias betaWinM &lt;- 1.5 # Population mean win-stay parameter betaWinSD &lt;- 0.3 # Population SD of win-stay parameter betaLoseM &lt;- 1.5 # Population mean lose-shift parameter betaLoseSD &lt;- 0.3 # Population SD of lose-shift parameter Next, we’ll define functions to implement our agent strategies: # Random agent function: makes choices based on bias parameter (on log-odds scale) # with possibility of random noise RandomAgentNoise_f &lt;- function(rate, noise) { # Generate choice based on agent&#39;s bias parameter choice &lt;- rbinom(1, 1, inv_logit_scaled(rate)) # With probability &#39;noise&#39;, override choice with random 50/50 selection if (rbinom(1, 1, noise) == 1) { choice = rbinom(1, 1, 0.5) } return(choice) } # Win-Stay-Lose-Shift agent function: # - alpha: baseline bias parameter # - betaWin: parameter controlling the tendency to repeat winning choices # - betaLose: parameter controlling the tendency to switch after losing # - win: indicator of previous win (+1 if won with choice 1, -1 if won with choice 0, 0 if lost) # - lose: indicator of previous loss (+1 if lost with choice 0, -1 if lost with choice 1, 0 if won) # - noise: probability of making a random choice WSLSAgentNoise_f &lt;- function(alpha, betaWin, betaLose, win, lose, noise) { # Calculate choice probability based on WSLS parameters and previous outcomes rate &lt;- alpha + betaWin * win + betaLose * lose # Generate choice based on calculated probability choice &lt;- rbinom(1, 1, inv_logit_scaled(rate)) # With probability &#39;noise&#39;, override choice with random 50/50 selection if (rbinom(1, 1, noise) == 1) { choice = rbinom(1, 1, 0.5) } return(choice) } Now let’s generate the simulation data. We’ll simulate each agent playing against a random opponent, and track their choices, wins, and losses: # File path for saved simulation results sim_data_file &lt;- &quot;simdata/W9_WSLS_data.RDS&quot; # Check if we need to regenerate the simulation data if (regenerate_simulations || !file.exists(sim_data_file)) { # Initialize dataframe to store results d &lt;- NULL # Loop through all agents for (agent in 1:agents) { # Sample individual agent parameters from population distributions rate &lt;- rnorm(1, rateM, rateSD) # Individual bias for random agent alpha &lt;- rnorm(1, alphaM, alphaSD) # Individual baseline bias for WSLS agent betaWin &lt;- rnorm(1, betaWinM, betaWinSD) # Individual win-stay parameter betaLose &lt;- rnorm(1, betaLoseM, betaLoseSD) # Individual lose-shift parameter # Initialize vectors to store data for this agent randomChoice &lt;- rep(NA, trials) # Choices of random agent wslsChoice &lt;- rep(NA, trials) # Choices of WSLS agent win &lt;- rep(NA, trials) # Win indicator for WSLS agent lose &lt;- rep(NA, trials) # Lose indicator for WSLS agent feedback &lt;- rep(NA, trials) # Whether WSLS agent won (1) or lost (0) # Generate choices for each trial for (trial in 1:trials) { # Random agent makes choice randomChoice[trial] &lt;- RandomAgentNoise_f(rate, noise) # WSLS agent makes first choice randomly, then follows strategy if (trial == 1) { wslsChoice[trial] &lt;- rbinom(1, 1, 0.5) # First choice is random } else { # Use WSLS strategy based on previous outcome wslsChoice[trial] &lt;- WSLSAgentNoise_f( alpha, betaWin, betaLose, win[trial - 1], lose[trial - 1], noise ) } # Determine outcome (1 = WSLS agent wins, 0 = WSLS agent loses) feedback[trial] &lt;- ifelse(wslsChoice[trial] == randomChoice[trial], 1, 0) # Encode win/lose signals for WSLS strategy: # win: +1 if won with choice 1, -1 if won with choice 0, 0 if lost win[trial] &lt;- ifelse(feedback[trial] == 1, ifelse(wslsChoice[trial] == 1, 1, -1), 0) # lose: +1 if lost with choice 0, -1 if lost with choice 1, 0 if won lose[trial] &lt;- ifelse(feedback[trial] == 0, ifelse(wslsChoice[trial] == 1, -1, 1), 0) } # Create data frames for this agent tempRandom &lt;- tibble( agent, trial = seq(trials), choice = randomChoice, rate, noise, rateM, rateSD, alpha, alphaM, alphaSD, betaWin, betaWinM, betaWinSD, betaLose, betaLoseM, betaLoseSD, win, lose, feedback, strategy = &quot;Random&quot; ) tempWSLS &lt;- tibble( agent, trial = seq(trials), choice = wslsChoice, rate, noise, rateM, rateSD, alpha, alphaM, alphaSD, betaWin, betaWinM, betaWinSD, betaLose, betaLoseM, betaLoseSD, win, lose, feedback, strategy = &quot;WSLS&quot; ) # Combine data for both agent types temp &lt;- rbind(tempRandom, tempWSLS) # Append to main dataframe if (agent &gt; 1) { d &lt;- rbind(d, temp) } else { d &lt;- temp } } # Save the simulation results saveRDS(d, sim_data_file) cat(&quot;Generated new simulation data and saved to&quot;, sim_data_file, &quot;\\n&quot;) } else { # Load existing simulation results d &lt;- readRDS(sim_data_file) cat(&quot;Loaded existing simulation data from&quot;, sim_data_file, &quot;\\n&quot;) } ## Loaded existing simulation data from simdata/W9_WSLS_data.RDS 10.3 Data Processing and Initial Visualization Let’s process our data to create useful variables for analysis and visualization: # Process data: add lead/lag variables and calculate cumulative statistics d &lt;- d %&gt;% group_by(agent, strategy) %&gt;% mutate( # Next and previous choices/outcomes nextChoice = lead(choice), # Next choice (for analysis) prevWin = lag(win), # Previous win indicator prevLose = lag(lose), # Previous lose indicator # Performance metrics cumulativerate = cumsum(choice) / seq_along(choice), # Running proportion of &quot;right&quot; choices performance = cumsum(feedback) / seq_along(feedback) # Running proportion of wins ) %&gt;% mutate( # identify win and lose as 0 during first trial prevWin = ifelse(is.na(prevWin), 0, prevWin), prevLose = ifelse(is.na(prevLose), 0, prevLose) ) Now let’s visualize the choice patterns of our agents to get a sense of their behavior: # First visualization: Proportion of &quot;right&quot; choices over time for both strategies p1 &lt;- ggplot(d, aes(trial, cumulativerate, group = agent, color = agent)) + geom_line(alpha = 0.3) + geom_hline(yintercept = 0.5, linetype = &quot;dashed&quot;) + ylim(0, 1) + labs( title = &quot;Choice Patterns by Strategy&quot;, subtitle = &quot;Each line represents one agent&#39;s running proportion of &#39;right&#39; choices&quot;, x = &quot;Trial&quot;, y = &quot;Proportion of &#39;Right&#39; Choices&quot; ) + theme_classic() + facet_wrap(~strategy) + theme(legend.position = &quot;none&quot;) # Hide individual agent legend for clarity # Display the plot p1 The visualization above shows how the choice patterns evolve over time for both the Random and WSLS agents. The random agents show an approximately stable choice pattern (though with individual biases), while the WSLS agents show more varied patterns as they adapt to their opponents. Let’s also examine how each strategy performs against its opponent: # Create visualization for performance against opponent p2a &lt;- ggplot(subset(d, strategy == &quot;Random&quot;), aes(trial, 1 - performance, group = agent, color = agent)) + geom_line(alpha = 0.3) + geom_hline(yintercept = 0.5, linetype = &quot;dashed&quot;) + ylim(0, 1) + labs( title = &quot;Performance of Random Agents&quot;, subtitle = &quot;Lower values indicate better performance against WSLS opponents&quot;, x = &quot;Trial&quot;, y = &quot;Proportion of Losses&quot; ) + theme_classic() + theme(legend.position = &quot;none&quot;) p2b &lt;- ggplot(subset(d, strategy == &quot;WSLS&quot;), aes(trial, performance, group = agent, color = agent)) + geom_line(alpha = 0.3) + geom_hline(yintercept = 0.5, linetype = &quot;dashed&quot;) + ylim(0, 1) + labs( title = &quot;Performance of WSLS Agents&quot;, subtitle = &quot;Higher values indicate better performance against Random opponents&quot;, x = &quot;Trial&quot;, y = &quot;Proportion of Wins&quot; ) + theme_classic() + theme(legend.position = &quot;none&quot;) # Combine plots using patchwork p2a / p2b The performance plots reveal an interesting pattern. The WSLS agents generally maintain a winning percentage above 0.5 (the dashed line), indicating they can effectively exploit the biases in the random agents. This demonstrates a key strength of the WSLS strategy - it can adapt to and take advantage of predictable patterns in opponent behavior. 10.4 Verifying Model Properties and Assumptions Let’s check some key properties of our simulation to ensure it’s working as expected: # Check that win and lose indicators are orthogonal as expected p3 &lt;- ggplot(d, aes(win, lose)) + geom_jitter(alpha = 0.1, width = 0.1, height = 0.1) + labs( title = &quot;Orthogonality of Win and Lose Indicators&quot;, subtitle = &quot;Confirms that win and lose signals are mutually exclusive&quot;, x = &quot;Win Indicator&quot;, y = &quot;Lose Indicator&quot; ) + theme_bw() p3 The plot confirms that our win and lose indicators are mutually exclusive - when the win signal is active (±1), the lose signal is 0, and vice versa. This is important for the proper functioning of our WSLS model. Now let’s check that the WSLS agents are indeed responding to win and lose signals as expected: # Check that WSLS agents respond to win/lose signals appropriately p4 &lt;- d %&gt;% subset(strategy == &quot;WSLS&quot;) %&gt;% mutate(nextChoice = lead(choice)) %&gt;% group_by(agent, win, lose) %&gt;% summarize(heads = mean(nextChoice), .groups = &quot;drop&quot;) %&gt;% ggplot(aes(win, heads)) + geom_point(alpha = 0.5) + labs( title = &quot;WSLS Strategy Implementation Check&quot;, subtitle = &quot;Shows how next choice probability depends on win/lose signals&quot;, x = &quot;Win Signal (-1 = won with &#39;left&#39;, +1 = won with &#39;right&#39;, 0 = lost)&quot;, y = &quot;Probability of Choosing &#39;Right&#39; on Next Trial&quot; ) + theme_bw() + facet_wrap(~lose, labeller = labeller( lose = c(&quot;-1&quot; = &quot;Lose = -1\\n(lost with &#39;right&#39;)&quot;, &quot;0&quot; = &quot;Lose = 0\\n(won)&quot;, &quot;1&quot; = &quot;Lose = 1\\n(lost with &#39;left&#39;)&quot;) )) p4 This plot confirms that our WSLS agents are behaving as expected: When win = +1 (won with ‘right’), the agents tend to choose ‘right’ again When win = -1 (won with ‘left’), the agents tend to choose ‘left’ again When lose = +1 (lost with ‘left’), the agents tend to switch to ‘right’ When lose = -1 (lost with ‘right’), the agents tend to switch to ‘left’ The variation in probabilities reflects the individual differences in our agents’ parameters. 10.5 Modeling a Single WSLS Agent Let’s first build a model to infer the parameters of a single WSLS agent. This will help us understand the basic mechanics before scaling up to the multilevel model: # Select one agent&#39;s data for single-agent model d_a &lt;- d %&gt;% subset( strategy == &quot;WSLS&quot; &amp; agent == 2 # Arbitrarily select agent #2 Note that trial 1 is already excluded ) # Prepare data for Stan model data_wsls_simple &lt;- list( trials = trials, # trials h = d_a$choice, # Choices (0/1) win = d_a$prevWin, # Previous win signal lose = d_a$prevLose # Previous lose signal ) Now let’s define the Stan model for a single WSLS agent: # Stan model code for single WSLS agent stan_wsls_model &lt;- &quot; functions{ // Helper function for generating truncated normal random numbers real normal_lb_rng(real mu, real sigma, real lb) { real p = normal_cdf(lb | mu, sigma); // cdf for bounds real u = uniform_rng(p, 1); return (sigma * inv_Phi(u)) + mu; // inverse cdf for value } } data { int&lt;lower = 1&gt; trials; // Number of trials array[trials] int h; // Choices (0/1) vector[trials] win; // Win signal for each trial vector[trials] lose; // Lose signal for each trial } parameters { real alpha; // Baseline bias parameter real winB; // Win-stay parameter real loseB; // Lose-shift parameter } model { // Priors target += normal_lpdf(alpha | 0, .3); // Prior for baseline bias target += normal_lpdf(winB | 1, 1); // Prior for win-stay parameter target += normal_lpdf(loseB | 1, 1); // Prior for lose-shift parameter // Likelihood: WSLS choice model // Remember that in the first trial we ensured win and lose have a value of 0, // which correspond to a fixed 0.5 probability (0 on logit scale) // since there is no previous outcome to guide the choice target += bernoulli_logit_lpmf(h | alpha + winB * win + loseB * lose); } generated quantities{ // Prior predictive samples real alpha_prior; real winB_prior; real loseB_prior; // Posterior and prior predictions array[trials] int prior_preds; array[trials] int posterior_preds; // Log likelihood for model comparison vector[trials] log_lik; // Generate prior samples alpha_prior = normal_rng(0, 1); winB_prior = normal_rng(0, 1); loseB_prior = normal_rng(0, 1); // Prior predictive simulations for (t in 1:trials) { prior_preds[t] = bernoulli_logit_rng(alpha_prior + winB_prior * win[t] + loseB_prior * lose[t]); } // Posterior predictive simulations for (t in 1:trials) { posterior_preds[t] = bernoulli_logit_rng(alpha + winB * win[t] + loseB * lose[t]); } // Calculate log likelihood for each observation for (t in 1:trials){ log_lik[t] = bernoulli_logit_lpmf(h[t] | alpha + winB * win[t] + loseB * lose[t]); } } &quot; # Write the model to a file write_stan_file( stan_wsls_model, dir = &quot;stan/&quot;, basename = &quot;W9_WSLS.stan&quot;) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W9_WSLS.stan&quot; # Compile the model file &lt;- file.path(&quot;stan/W9_WSLS.stan&quot;) mod_wsls_simple &lt;- cmdstan_model( file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;), pedantic = TRUE ) Now let’s fit the model to our single agent’s data: # File path for saved model results single_model_file &lt;- &quot;simmodels/W9_WSLS_simple.RDS&quot; # Check if we need to rerun the model if (regenerate_simulations || !file.exists(single_model_file)) { # Fit the model samples_wsls_simple &lt;- mod_wsls_simple$sample( data = data_wsls_simple, seed = 123, chains = 2, parallel_chains = 2, threads_per_chain = 1, iter_warmup = 2000, iter_sampling = 2000, refresh = 1000, max_treedepth = 20, adapt_delta = 0.99 ) # Save the results samples_wsls_simple$save_object(file = single_model_file) cat(&quot;Generated new model fit and saved to&quot;, single_model_file, &quot;\\n&quot;) } else { # Load existing results samples_wsls_simple &lt;- readRDS(single_model_file) cat(&quot;Loaded existing model fit from&quot;, single_model_file, &quot;\\n&quot;) } ## Loaded existing model fit from simmodels/W9_WSLS_simple.RDS Let’s examine the parameter estimates and convergence for the single agent model: # Display summary statistics for the parameters summary_stats &lt;- samples_wsls_simple$summary() print(summary_stats) ## # A tibble: 367 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lp__ -58.4 -58.1 1.20 0.997 -60.7 -57.1 1.00 1606. 2161. ## 2 alpha 0.172 0.174 0.224 0.228 -0.190 0.532 1.00 1950. 2217. ## 3 winB 1.16 1.15 0.332 0.337 0.628 1.70 1.00 1845. 2075. ## 4 loseB 1.90 1.90 0.413 0.413 1.24 2.60 1.00 2039. 1774. ## 5 alpha_prior -0.0314 -0.0233 0.993 0.978 -1.69 1.60 1.00 3888. 3857. ## 6 winB_prior 0.0165 0.0208 1.00 1.01 -1.62 1.66 1.00 3845. 3867. ## 7 loseB_prior -0.0254 -0.0333 1.02 1.01 -1.71 1.65 1.00 3926. 3728. ## 8 prior_preds[1] 0.484 0 0.500 0 0 1 1.00 3927. NA ## 9 prior_preds[2] 0.490 0 0.500 0 0 1 1.00 3597. NA ## 10 prior_preds[3] 0.491 0 0.500 0 0 1 1.00 3847. NA ## # ℹ 357 more rows # Extract posterior samples and include sampling of the prior draws_df &lt;- as_draws_df(samples_wsls_simple$draws()) # Create plot grid for parameter recovery p5a &lt;- ggplot(draws_df) + geom_histogram(aes(alpha), fill = &quot;blue&quot;, alpha = 0.3) + geom_histogram(aes(alpha_prior), fill = &quot;red&quot;, alpha = 0.3) + geom_vline(xintercept = d_a$alpha[1], linetype = &quot;dashed&quot;, size = 1) + labs( title = &quot;Baseline Bias Parameter (Alpha)&quot;, subtitle = &quot;Blue = posterior, Red = prior, Dashed line = true value&quot;, x = &quot;Parameter Value&quot;, y = &quot;Density&quot; ) + theme_classic() p5b &lt;- ggplot(draws_df) + geom_histogram(aes(winB), fill = &quot;blue&quot;, alpha = 0.3) + geom_histogram(aes(winB_prior), fill = &quot;red&quot;, alpha = 0.3) + geom_vline(xintercept = d_a$betaWin[1], linetype = &quot;dashed&quot;, size = 1) + labs( title = &quot;Win-Stay Parameter (Beta Win)&quot;, subtitle = &quot;Blue = posterior, Red = prior, Dashed line = true value&quot;, x = &quot;Parameter Value&quot;, y = &quot;Density&quot; ) + theme_classic() p5c &lt;- ggplot(draws_df) + geom_histogram(aes(loseB), fill = &quot;blue&quot;, alpha = 0.3) + geom_histogram(aes(loseB_prior), fill = &quot;red&quot;, alpha = 0.3) + geom_vline(xintercept = d_a$betaLose[1], linetype = &quot;dashed&quot;, size = 1) + labs( title = &quot;Lose-Shift Parameter (Beta Lose)&quot;, subtitle = &quot;Blue = posterior, Red = prior, Dashed line = true value&quot;, x = &quot;Parameter Value&quot;, y = &quot;Density&quot; ) + theme_classic() # Combine plots (p5a | p5b) / p5c The parameter recovery for our single agent looks good. The posterior distributions (blue) are centered around the true parameter values (dashed lines), showing that our model can accurately recover the underlying parameters that generated the agent’s behavior. The posteriors are also substantially narrower than the priors (red), indicating that the data is informative. [MISSING A FULL PARAMETER RECOVERY] 10.6 Multilevel WSLS Model Now let’s scale up to model all agents simultaneously with a multilevel (hierarchical) model. This allows us to estimate both population-level parameters and individual differences: ## Prepare data for multilevel model # Transform data into matrices where columns are agents, rows are trials d_wsls1 &lt;- d %&gt;% subset(strategy == &quot;WSLS&quot;) %&gt;% subset(select = c(agent, choice)) %&gt;% mutate(row = row_number()) %&gt;% pivot_wider(names_from = agent, values_from = choice) d_wsls2 &lt;- d %&gt;% subset(strategy == &quot;WSLS&quot;) %&gt;% subset(select = c(agent, prevWin)) %&gt;% mutate(row = row_number()) %&gt;% pivot_wider(names_from = agent, values_from = prevWin) d_wsls3 &lt;- d %&gt;% subset(strategy == &quot;WSLS&quot;) %&gt;% subset(select = c(agent, prevLose)) %&gt;% mutate(row = row_number()) %&gt;% pivot_wider(names_from = agent, values_from = prevLose) # Create the data list for Stan data_wsls &lt;- list( trials = trials, agents = agents, h = as.matrix(d_wsls1[, 2:(agents + 1)]), # Choice matrix win = as.matrix(d_wsls2[, 2:(agents + 1)]), # Win signal matrix lose = as.matrix(d_wsls3[, 2:(agents + 1)]) # Lose signal matrix ) Now let’s define the multilevel Stan model: # Stan code for multilevel WSLS model stan_wsls_ml_model &lt;- &quot; functions{ real normal_lb_rng(real mu, real sigma, real lb) { real p = normal_cdf(lb | mu, sigma); // cdf for bounds real u = uniform_rng(p, 1); return (sigma * inv_Phi(u)) + mu; // inverse cdf for value } } // The input (data) for the model. data { int&lt;lower = 1&gt; trials; // Number of trials int&lt;lower = 1&gt; agents; // Number of agents array[trials, agents] int h; // Choice data (0/1) array[trials, agents] real win; // Win signals array[trials, agents] real lose; // Lose signals } parameters { // Population-level parameters real winM; // Population mean for win-stay parameter real loseM; // Population mean for lose-shift parameter // Population standard deviations vector&lt;lower = 0&gt;[2] tau; // SDs for [win, lose] parameters // Individual z-scores (non-centered parameterization) matrix[2, agents] z_IDs; // Correlation matrix cholesky_factor_corr[2] L_u; } transformed parameters { // Individual parameters (constructed from non-centered parameterization) matrix[agents, 2] IDs; IDs = (diag_pre_multiply(tau, L_u) * z_IDs)&#39;; } model { // Population-level priors target += normal_lpdf(winM | 0, 1); // Prior for win-stay mean target += normal_lpdf(tau[1] | 0, .3) - normal_lccdf(0 | 0, .3); // Half-normal for SD target += normal_lpdf(loseM | 0, .3); // Prior for lose-shift mean target += normal_lpdf(tau[2] | 0, .3) - normal_lccdf(0 | 0, .3); // Half-normal for SD // Prior for correlation matrix target += lkj_corr_cholesky_lpdf(L_u | 2); // Prior for individual z-scores target += std_normal_lpdf(to_vector(z_IDs)); // Likelihood for (i in 1:agents) target += bernoulli_logit_lpmf(h[,i] | to_vector(win[,i]) * (winM + IDs[i,1]) + to_vector(lose[,i]) * (loseM + IDs[i,2])); } generated quantities{ // Prior predictive samples real winM_prior; real&lt;lower=0&gt; winSD_prior; real loseM_prior; real&lt;lower=0&gt; loseSD_prior; real win_prior; real lose_prior; // Posterior predictive samples for various scenarios array[trials, agents] int prior_preds; array[trials, agents] int posterior_preds; // Log likelihood for model comparison array[trials, agents] real log_lik; // Generate prior samples winM_prior = normal_rng(0, 1); winSD_prior = normal_lb_rng(0, 0.3, 0); loseM_prior = normal_rng(0, 1); loseSD_prior = normal_lb_rng(0, 0.3, 0); win_prior = normal_rng(winM_prior, winSD_prior); lose_prior = normal_rng(loseM_prior, loseSD_prior); // Generate predictions for (i in 1:agents){ // Prior predictive simulations for (t in 1:trials) { prior_preds[t, i] = bernoulli_logit_rng( win[t, i] * win_prior + lose[t, i] * lose_prior ); } // Posterior predictive simulations for (t in 1:trials) { posterior_preds[t, i] = bernoulli_logit_rng( win[t, i] * (winM + IDs[i, 1]) + lose[t, i] * (loseM + IDs[i, 2]) ); } // Calculate log likelihood for each observation for (t in 1:trials){ log_lik[t, i] = bernoulli_logit_lpmf( h[t, i] | win[t, i] * (winM + IDs[i, 1]) + lose[t, i] * (loseM + IDs[i, 2]) ); } } } &quot; # Write the model to a file write_stan_file( stan_wsls_ml_model, dir = &quot;stan/&quot;, basename = &quot;W9_WSLS_ml.stan&quot;) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W9_WSLS_ml.stan&quot; # Compile the model file &lt;- file.path(&quot;stan/W9_WSLS_ml.stan&quot;) mod_wsls_ml &lt;- cmdstan_model( file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;), pedantic = TRUE ) Now let’s fit the multilevel model to all agents: # File path for saved multilevel model results multilevel_model_file &lt;- &quot;simmodels/W9_WSLS_multilevel.RDS&quot; # Check if we need to rerun the model if (regenerate_simulations || !file.exists(multilevel_model_file)) { # Fit the multilevel model samples_wsls_ml &lt;- mod_wsls_ml$sample( data = data_wsls, seed = 123, chains = 2, parallel_chains = 2, threads_per_chain = 1, iter_warmup = 2000, iter_sampling = 2000, refresh = 500, # Reduced refresh rate for cleaner output max_treedepth = 20, adapt_delta = 0.99 ) # Save the results samples_wsls_ml$save_object(file = multilevel_model_file) cat(&quot;Generated new multilevel model fit and saved to&quot;, multilevel_model_file, &quot;\\n&quot;) } else { # Load existing results samples_wsls_ml &lt;- readRDS(multilevel_model_file) cat(&quot;Loaded existing multilevel model fit from&quot;, multilevel_model_file, &quot;\\n&quot;) } ## Loaded existing multilevel model fit from simmodels/W9_WSLS_multilevel.RDS 10.7 Quality Checks and Parameter Recovery 10.7.1 Convergence Diagnostics Let’s examine convergence diagnostics to ensure our model has estimated the parameters reliably: draws_ml &lt;- as_draws_df(samples_wsls_ml$draws()) # Trace plots for convergence checking mcmc_trace(draws_ml, pars = c(&quot;winM&quot;, &quot;loseM&quot;, &quot;tau[1]&quot;, &quot;tau[2]&quot;)) + labs( title = &quot;MCMC Trace Plots for Key Parameters&quot;, subtitle = &quot;Good mixing indicates model convergence&quot; ) # Rank plots for assessing convergence mcmc_rank_hist(draws_ml, pars = c(&quot;winM&quot;, &quot;loseM&quot;, &quot;tau[1]&quot;, &quot;tau[2]&quot;)) + labs( title = &quot;Rank Plots for Key Parameters&quot;, subtitle = &quot;Uniform distributions indicate good mixing&quot; ) The trace plots show the parameter values across iterations. Good mixing (chains overlapping without patterns) indicates convergence. Rank histograms near uniform also suggest good convergence, while U-shaped or inverted-U histograms would indicate poor mixing. 10.7.2 Prior-Posterior Update Visualization Now let’s visualize how our knowledge about the parameters has been updated by the data: # Extract prior samples prior_samples &lt;- tibble( winM_prior = draws_ml$winM_prior, loseM_prior = draws_ml$loseM_prior, winSD_prior = draws_ml$winSD_prior, loseSD_prior = draws_ml$loseSD_prior ) # Create dataframe for prior-posterior comparison update_df &lt;- tibble( Parameter = rep(c(&quot;Win-Stay Mean&quot;, &quot;Lose-Shift Mean&quot;, &quot;Win-Stay SD&quot;, &quot;Lose-Shift SD&quot;), each = nrow(draws_ml), times = 2), Value = c(draws_ml$winM, draws_ml$loseM, draws_ml$`tau[1]`, draws_ml$`tau[2]`, prior_samples$winM_prior, prior_samples$loseM_prior, prior_samples$winSD_prior, prior_samples$loseSD_prior), Distribution = rep(c(&quot;Posterior&quot;, &quot;Prior&quot;), each = 4 * nrow(draws_ml)), True_Value = rep(c(betaWinM, betaLoseM, betaWinSD, betaLoseSD), each = nrow(draws_ml), times = 2) ) # Visualize prior-posterior update ggplot(update_df, aes(x = Value, fill = Distribution)) + geom_histogram(alpha = 0.6) + geom_vline(aes(xintercept = True_Value), color = &quot;black&quot;, linetype = &quot;dashed&quot;) + facet_wrap(~ Parameter, scales = &quot;free&quot;) + scale_fill_manual(values = c(&quot;Prior&quot; = &quot;lightpink&quot;, &quot;Posterior&quot; = &quot;steelblue&quot;)) + labs( title = &quot;Prior vs. Posterior Distributions&quot;, subtitle = &quot;Prior (pink) vs. posterior (blue) distributions with true values (dashed lines)&quot;, x = &quot;Parameter Value&quot;, y = &quot;Density&quot; ) + theme_minimal() This visualization shows how our knowledge about the parameters has been updated by the data. The prior distributions (pink) represent our knowledge before seeing the data, while the posterior distributions (blue) show what we learned after fitting the model to the data. Narrower posteriors centered near the true values indicate that our model effectively learned from the data. 10.7.3 Individual-Level Parameter Recovery One of the key advantages of multilevel modeling is the ability to estimate parameters for individual agents. Let’s extract individual parameters and assess recovery: # Extract individual agent parameters agent_params &lt;- tibble() for (i in 1:agents) { # Extract parameters for this agent win_param &lt;- mean(draws_ml$winM) + mean(draws_ml[[paste0(&quot;IDs[&quot;, i, &quot;,1]&quot;)]]) lose_param &lt;- mean(draws_ml$loseM) + mean(draws_ml[[paste0(&quot;IDs[&quot;, i, &quot;,2]&quot;)]]) # Get true parameters true_data &lt;- filter(d, strategy == &quot;WSLS&quot;, agent == i, trial == 1) true_win &lt;- first(true_data$betaWin) true_lose &lt;- first(true_data$betaLose) # Add to dataframe agent_params &lt;- bind_rows( agent_params, tibble( agent = i, win_estimated = win_param, lose_estimated = lose_param, win_true = true_win, lose_true = true_lose ) ) } # Create comparison plots p1 &lt;- ggplot(agent_params, aes(win_true, win_estimated)) + geom_point(alpha = 0.7) + geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;, se = TRUE) + labs( title = &quot;Individual Win-Stay Parameter Recovery&quot;, x = &quot;True Win-Stay Parameter&quot;, y = &quot;Estimated Win-Stay Parameter&quot; ) + theme_minimal() p2 &lt;- ggplot(agent_params, aes(lose_true, lose_estimated)) + geom_point(alpha = 0.7) + geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;, se = TRUE) + labs( title = &quot;Individual Lose-Shift Parameter Recovery&quot;, x = &quot;True Lose-Shift Parameter&quot;, y = &quot;Estimated Lose-Shift Parameter&quot; ) + theme_minimal() # Calculate recovery statistics win_corr &lt;- cor(agent_params$win_true, agent_params$win_estimated) lose_corr &lt;- cor(agent_params$lose_true, agent_params$lose_estimated) # Display correlation statistics cat(&quot;Correlation between true and estimated Win-Stay parameters:&quot;, round(win_corr, 3), &quot;\\n&quot;) ## Correlation between true and estimated Win-Stay parameters: 0.746 cat(&quot;Correlation between true and estimated Lose-Shift parameters:&quot;, round(lose_corr, 3), &quot;\\n&quot;) ## Correlation between true and estimated Lose-Shift parameters: 0.666 # Create combined plot p1 | p2 These scatter plots show how well our model recovers individual-level parameters. Points near the diagonal line (dashed) indicate accurate parameter recovery, while the red regression line shows the overall relationship between true and estimated values. High correlation coefficients suggest good recovery of individual differences. 10.7.4 Posterior Predictive Checks Posterior predictive checks help us assess whether our model can generate data that resembles the observed data: I’ll modify the posterior predictive checks chunk to use the regeneration_simulations flag for better efficiency. This will save the computed results and reload them if they already exist: rCopy# Posterior predictive checks with regeneration flag # File path for saved predictive check data pred_checks_file &lt;- &quot;simdata/W9_WSLS_predictive_checks.RDS&quot; pred_checks_uncertainty_file &lt;- &quot;simdata/W9_WSLS_predictive_uncertainty.RDS&quot; # Check if we need to recompute the predictive checks if (regenerate_simulations || !file.exists(pred_checks_file) || !file.exists(pred_checks_uncertainty_file)) { # Extract samples for a few selected agents selected_agents &lt;- c(1, 25, 50, 75) # Initialize data structures observed_data &lt;- matrix(NA, nrow = trials-1, ncol = length(selected_agents)) prior_means &lt;- matrix(NA, nrow = trials-1, ncol = length(selected_agents)) posterior_means &lt;- matrix(NA, nrow = trials-1, ncol = length(selected_agents)) # Also track quantiles for uncertainty bands prior_lower &lt;- matrix(NA, nrow = trials-1, ncol = length(selected_agents)) prior_upper &lt;- matrix(NA, nrow = trials-1, ncol = length(selected_agents)) posterior_lower &lt;- matrix(NA, nrow = trials-1, ncol = length(selected_agents)) posterior_upper &lt;- matrix(NA, nrow = trials-1, ncol = length(selected_agents)) # Fill observed data for (i in seq_along(selected_agents)) { agent_idx &lt;- selected_agents[i] observed_data[, i] &lt;- data_wsls$h[1:(trials-1), agent_idx] } # Process the predictions trial by trial to reduce memory usage for (t in 1:(trials-1)) { cat(&quot;Processing trial&quot;, t, &quot;of&quot;, trials-1, &quot;\\n&quot;) # Extract variable names for this trial prior_vars &lt;- paste0(&quot;prior_preds[&quot;, t, &quot;,&quot;, selected_agents, &quot;]&quot;) post_vars &lt;- paste0(&quot;posterior_preds[&quot;, t, &quot;,&quot;, selected_agents, &quot;]&quot;) # Extract predictions for all agents for this trial for (i in seq_along(selected_agents)) { prior_preds &lt;- as.numeric(draws_ml[[prior_vars[i]]]) post_preds &lt;- as.numeric(draws_ml[[post_vars[i]]]) # Calculate means prior_means[t, i] &lt;- mean(prior_preds) posterior_means[t, i] &lt;- mean(post_preds) # Calculate 95% credible intervals prior_lower[t, i] &lt;- quantile(prior_preds, 0.025) prior_upper[t, i] &lt;- quantile(prior_preds, 0.975) posterior_lower[t, i] &lt;- quantile(post_preds, 0.025) posterior_upper[t, i] &lt;- quantile(post_preds, 0.975) } } # Convert to long format for plotting pred_check_data &lt;- tibble( trial = rep(2:trials, length(selected_agents)), # Trial numbers (starting from 2) agent = rep(selected_agents, each = trials-1), observed = c(observed_data), prior_mean = c(prior_means), posterior_mean = c(posterior_means), agent_label = factor(paste(&quot;Agent&quot;, rep(selected_agents, each = trials-1))) ) # Uncertainty data uncertainty_data &lt;- tibble( trial = rep(2:trials, length(selected_agents)), agent = rep(selected_agents, each = trials-1), prior_lower = c(prior_lower), prior_upper = c(prior_upper), posterior_lower = c(posterior_lower), posterior_upper = c(posterior_upper), agent_label = factor(paste(&quot;Agent&quot;, rep(selected_agents, each = trials-1))) ) # Calculate cumulative choice proportions pred_check_cumulative &lt;- pred_check_data %&gt;% group_by(agent_label) %&gt;% mutate( trial_idx = trial - 1, # Index starting from 1 obs_cumulative = cumsum(observed) / seq_along(observed), prior_cumulative = cumsum(prior_mean) / seq_along(prior_mean), posterior_cumulative = cumsum(posterior_mean) / seq_along(posterior_mean) ) # Save the results saveRDS(pred_check_cumulative, pred_checks_file) saveRDS(uncertainty_data, pred_checks_uncertainty_file) cat(&quot;Generated new predictive checks and saved to&quot;, pred_checks_file, &quot;\\n&quot;) } else { # Load existing results pred_check_cumulative &lt;- readRDS(pred_checks_file) uncertainty_data &lt;- readRDS(pred_checks_uncertainty_file) cat(&quot;Loaded existing predictive checks from saved files\\n&quot;) } ## Loaded existing predictive checks from saved files # Plot cumulative choice proportions - Posterior predictive check with uncertainty p1 &lt;- ggplot(pred_check_cumulative, aes(x = trial)) + # Add point-wise uncertainty intervals geom_ribbon(data = uncertainty_data, aes(ymin = posterior_lower, ymax = posterior_upper), fill = &quot;blue&quot;, alpha = 0.2) + # Add observed and predicted lines geom_line(aes(y = obs_cumulative, color = &quot;Observed&quot;), size = 1) + geom_line(aes(y = posterior_cumulative, color = &quot;Posterior Predicted&quot;), size = 1, linetype = &quot;dashed&quot;) + facet_wrap(~ agent_label, ncol = 2) + labs( title = &quot;Posterior Predictive Check with Uncertainty&quot;, subtitle = &quot;Observed (solid) vs. Posterior Predicted (dashed) with 95% CI bands&quot;, x = &quot;Trial&quot;, y = &quot;Proportion of &#39;Right&#39; Choices&quot;, color = &quot;Data Source&quot; ) + scale_color_manual(values = c(&quot;Observed&quot; = &quot;black&quot;, &quot;Posterior Predicted&quot; = &quot;blue&quot;)) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) # Plot cumulative choice proportions - Prior predictive check with uncertainty p2 &lt;- ggplot(pred_check_cumulative, aes(x = trial)) + # Add point-wise uncertainty intervals geom_ribbon(data = uncertainty_data, aes(ymin = prior_lower, ymax = prior_upper), fill = &quot;red&quot;, alpha = 0.2) + # Add observed and predicted lines geom_line(aes(y = obs_cumulative, color = &quot;Observed&quot;), size = 1) + geom_line(aes(y = prior_cumulative, color = &quot;Prior Predicted&quot;), size = 1, linetype = &quot;dashed&quot;) + facet_wrap(~ agent_label, ncol = 2) + labs( title = &quot;Prior Predictive Check with Uncertainty&quot;, subtitle = &quot;Observed (solid) vs. Prior Predicted (dashed) with 95% CI bands&quot;, x = &quot;Trial&quot;, y = &quot;Proportion of &#39;Right&#39; Choices&quot;, color = &quot;Data Source&quot; ) + scale_color_manual(values = c(&quot;Observed&quot; = &quot;black&quot;, &quot;Prior Predicted&quot; = &quot;red&quot;)) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) # Display both plots using patchwork p1 / p2 This posterior predictive check compares the observed choice patterns (solid lines) with those predicted by our model (dashed lines) for a few selected agents. Close alignment indicates that our model captures the key patterns in the data well. 10.7.5 Model Comparison with LOO-CV Finally, let’s compute Leave-One-Out Cross-Validation (LOO-CV) to assess our model’s predictive performance. We’ll also demonstrate how this could be used to compare our WSLS model with a simpler alternative: # Compute LOO for the multilevel WSLS model loo_wsls &lt;- samples_wsls_ml$loo() # Print LOO results print(loo_wsls) ## ## Computed from 4000 by 12000 log-likelihood matrix. ## ## Estimate SE ## elpd_loo -5687.6 62.8 ## p_loo 99.2 1.5 ## looic 11375.1 125.6 ## ------ ## MCSE of elpd_loo is NA. ## MCSE and ESS estimates assume MCMC draws (r_eff in [0.5, 1.6]). ## ## Pareto k diagnostic values: ## Count Pct. Min. ESS ## (-Inf, 0.7] (good) 11900 99.2% 2900 ## (0.7, 1] (bad) 0 0.0% &lt;NA&gt; ## (1, Inf) (very bad) 100 0.8% &lt;NA&gt; ## See help(&#39;pareto-k-diagnostic&#39;) for details. # Check Pareto k diagnostics pareto_k_table &lt;- table(loo_wsls$diagnostics$pareto_k &gt; 0.7) cat(&quot;Number of observations with Pareto k &gt; 0.7:&quot;, pareto_k_table[&quot;TRUE&quot;], &quot;\\n&quot;) ## Number of observations with Pareto k &gt; 0.7: 100 The LOO-CV computation provides an estimate of the model’s expected predictive accuracy. In a full analysis, we would compare this with alternative models to determine which provides the best balance of fit and generalizability. # Visualize distribution of pointwise elpd values elpd_data &lt;- tibble( observation = 1:length(loo_wsls$pointwise[,&quot;elpd_loo&quot;]), elpd = loo_wsls$pointwise[,&quot;elpd_loo&quot;] ) # Create histogram of elpd values ggplot(elpd_data, aes(x = elpd)) + geom_histogram(bins = 30, fill = &quot;steelblue&quot;, color = &quot;black&quot;, alpha = 0.7) + geom_vline(aes(xintercept = mean(elpd)), color = &quot;darkred&quot;, linetype = &quot;dashed&quot;, size = 1) + labs( title = &quot;Distribution of Pointwise Expected Log Predictive Density (ELPD)&quot;, subtitle = &quot;Higher values indicate better prediction for individual observations&quot;, x = &quot;ELPD&quot;, y = &quot;Count&quot; ) + theme_minimal() This visualization shows the distribution of pointwise expected log predictive density (ELPD) values, with the mean indicated by the dashed line. Observations with higher ELPD values are better predicted by our model. A long left tail would suggest some observations are particularly difficult for the model to predict and in a real project we should explore what the issues are. "],["bayesian-models-of-cognition.html", "Chapter 11 Bayesian Models of Cognition 11.1 Learning Objectives 11.2 Chapter Roadmap 11.3 The Bayesian Framework for Cognition 11.4 Visualizing Bayesian Updating 11.5 Bayesian Models in Cognitive Science 11.6 Example: Social Influence in Perceptual Decision-Making 11.7 A Bayesian Integration Model for the Marble Task 11.8 The Mathematical Model 11.9 Examining Belief Distributions for Selected Scenarios 11.10 Weighted Bayesian Integration 11.11 Common Misinterpretations 11.12 Simulating Agents with Different Evidence Weighting Strategies 11.13 Key Observations from the Simulation 11.14 Model Quality Checks 11.15 Prior-Posterior Update Visualization 11.16 Parameter recovery 11.17 Model comparison 11.18 Leave-One-Out Cross-Validation and Model Comparison 11.19 Multilevel Bayesian Models 11.20 Multilevel Bayesian Models for Evidence Integration 11.21 From Single-Agent to Multilevel: Extending Bayesian Cognitive Models 11.22 Fitting the Multilevel Models 11.23 Dynamic Bayesian Evidence Integration: Sequential Updating Models 11.24 Sequential Bayesian Evidence Integration Models", " Chapter 11 Bayesian Models of Cognition 11.0.1 Introduction The impressive power of Bayes theorem and Bayesian approaches to modeling has tempted cognitive scientists into exploring how far they could get in thinking the mind and brain as Bayesian machines. The idea is that the mind is a probabilistic machine that updates its beliefs based on the evidence it receives. The human mind constantly receives input from various sources – direct personal experience, social information from others, prior knowledge, and sensory input. A fundamental question in cognitive science is how these disparate pieces of information are combined to produce coherent beliefs about the world. The Bayesian framework offers a powerful approach to modeling this process. Under this framework, the mind is conceptualized as a probabilistic machine that continuously updates its beliefs based on new evidence. This contrasts with rule-based or purely associative models by emphasizing: Representations of uncertainty: Beliefs are represented as probability distributions, not single values Optimal integration: Information is combined according to its reliability Prior knowledge: New evidence is interpreted in light of existing beliefs In this chapter, we will explore how Bayesian integration can be formalized and used to model cognitive processes. We’ll start with simple models that give equal weight to different information sources, then develop more sophisticated models that allow for differential weighting based on source reliability, and finally consider how beliefs might update over time. This chapter is not a comprehensive review of Bayesian cognitive modeling, but rather a practical introduction to the topic. We’ll focus on simple models that illustrate key concepts and provide a foundation for more advanced applications. To go further in your learning check on Bayesian models of cognition check: Ma, W. J., Kording, K. P., &amp; Goldreich, D. (2023). Bayesian models of perception and action: An introduction. MIT press. Griffiths, T. L., Chater, N., &amp; Tenenbaum, J. B. (Eds.). (2024). Bayesian models of cognition: reverse engineering the mind. MIT Press. N. D. Goodman, J. B. Tenenbaum, and The ProbMods Contributors (2016). Probabilistic Models of Cognition (2nd ed.). Retrieved 2025-3-10 from https://probmods.org/ 11.1 Learning Objectives After completing this chapter, you will be able to: Understand the basic principles of Bayesian information integration Implement models that combine multiple sources of information in a principled Bayesian way Fit and evaluate these models using Stan Differentiate between alternative Bayesian updating schemes Apply Bayesian cognitive models to decision-making data 11.2 Chapter Roadmap In this chapter, we will: Introduce the Bayesian framework for cognitive modeling Implement a simple Bayesian integration model Develop and test a weighted Bayesian model that allows for different source reliability Explore temporal Bayesian updating Extend our models to multilevel structures that capture individual differences Compare alternative Bayesian models and evaluate their cognitive implications 11.3 The Bayesian Framework for Cognition Bayesian models of cognition explore the idea that the mind operates according to principles similar to Bayes’ theorem, combining different sources of evidence to form updated beliefs. Most commonly this is framed in terms of prior beliefs being updated with new evidence to form updated posterior beliefs. Formally: P(belief | evidence) ∝ P(evidence | belief) × P(belief) Where: P(belief | evidence) is the posterior belief after observing evidence P(evidence | belief) is the likelihood of observing the evidence given a belief P(belief) is the prior belief before observing evidence In cognitive terms, this means people integrate new information with existing knowledge, giving more weight to reliable information sources and less weight to unreliable ones. Yet, there is nothing mathematically special about the prior and the likelihood. They are just two sources of information that are combined in a way that is consistent with the rules of probability. Any other combination of information sources can be modeled with the same theorem. Note that a more traditional formula for Bayes Theorem would be P(belief | evidence) = [P(evidence | belief) × P(belief)] / P(evidence) where the product of prior and likelihood is normalized by P(evidence) (bringing it back to a probability scale). That’s why we used a ∝ symbol in the formula above, to indicate that we are not considering the normalization constant, and that the posterior is only proportional (and not exactly equal) to the multiplication of the two sources of information. Nevertheless, this is a first useful approximation of the theorem, which we can build on in the rest of the chapter. *** 11.4 Visualizing Bayesian Updating To better understand Bayesian updating, let’s create a conceptual diagram: # Create a visualization of Bayesian updating process # Function to create Bayesian updating visualization create_bayesian_updating_diagram &lt;- function() { # Create example data # Prior (starting with uniform distribution) x &lt;- seq(0, 1, by = 0.01) prior &lt;- dbeta(x, 1, 1) # Likelihood (evidence suggesting higher probability) likelihood &lt;- dbeta(x, 7, 3) # Posterior (combines prior and likelihood) posterior &lt;- dbeta(x, 8, 4) # 1+7, 1+3 # Create data frame for plotting plot_data &lt;- data.frame( x = rep(x, 3), density = c(prior, likelihood, posterior), distribution = factor(rep(c(&quot;Prior&quot;, &quot;Likelihood&quot;, &quot;Posterior&quot;), each = length(x)), levels = c(&quot;Prior&quot;, &quot;Likelihood&quot;, &quot;Posterior&quot;)) ) # Create main plot showing distributions p1 &lt;- ggplot(plot_data, aes(x = x, y = density, color = distribution, linetype = distribution)) + geom_line(size = 1.2) + scale_color_manual(values = c(&quot;Prior&quot; = &quot;blue&quot;, &quot;Likelihood&quot; = &quot;red&quot;, &quot;Posterior&quot; = &quot;purple&quot;)) + scale_linetype_manual(values = c(&quot;Prior&quot; = &quot;dashed&quot;, &quot;Likelihood&quot; = &quot;dotted&quot;, &quot;Posterior&quot; = &quot;solid&quot;)) + labs(title = &quot;Bayesian Updating Process&quot;, subtitle = &quot;Combining prior beliefs with new evidence&quot;, x = &quot;Belief (probability)&quot;, y = &quot;Probability Density&quot;, color = &quot;Distribution&quot;, linetype = &quot;Distribution&quot;) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) + annotate(&quot;text&quot;, x = 0.1, y = 0.9, label = &quot;Low certainty\\nprior belief&quot;, color = &quot;blue&quot;, hjust = 0) + annotate(&quot;text&quot;, x = 0.8, y = 1.5, label = &quot;New evidence\\nsuggests high\\nprobability&quot;, color = &quot;red&quot;, hjust = 1) + annotate(&quot;text&quot;, x = 0.65, y = 2.2, label = &quot;Updated belief\\ncombines both\\nsources&quot;, color = &quot;purple&quot;, hjust = 0) # Create flow diagram to illustrate process flow_data &lt;- data.frame( x = c(1, 2, 3), y = c(1, 1, 1), label = c(&quot;Prior\\nBelief&quot;, &quot;Evidence&quot;, &quot;Posterior\\nBelief&quot;), box_color = c(&quot;blue&quot;, &quot;red&quot;, &quot;purple&quot;) ) arrow_data &lt;- data.frame( x = c(1.3, 2.3), xend = c(1.7, 2.7), y = c(1, 1), yend = c(1, 1) ) p2 &lt;- ggplot() + # Add boxes for process stages geom_rect(data = flow_data, aes(xmin = x - 0.3, xmax = x + 0.3, ymin = y - 0.3, ymax = y + 0.3, fill = box_color), color = &quot;black&quot;, alpha = 0.3) + # Add text labels geom_text(data = flow_data, aes(x = x, y = y, label = label), size = 3.5) + # Add arrows geom_segment(data = arrow_data, aes(x = x, y = y, xend = xend, yend = yend), arrow = arrow(length = unit(0.2, &quot;cm&quot;), type = &quot;closed&quot;)) + # Add the operation being performed annotate(&quot;text&quot;, x = 1.5, y = 1.2, label = &quot;×&quot;, size = 6) + annotate(&quot;text&quot;, x = 2.5, y = 1.2, label = &quot;∝&quot;, size = 5) + # Formatting scale_fill_manual(values = c(&quot;blue&quot;, &quot;red&quot;, &quot;purple&quot;)) + theme_void() + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Bayesian Inference Flow&quot;) # Combine plots vertically combined_plot &lt;- p1 / p2 + plot_layout(heights = c(4, 1)) return(combined_plot) } # Generate and display the diagram create_bayesian_updating_diagram() This diagram illustrates the key elements of Bayesian updating: Prior belief (blue dashed line): Our initial uncertainty about a phenomenon, before seeing evidence Likelihood (red dotted line): The pattern of evidence we observe Posterior belief (purple solid line): Our updated belief after combining prior and evidence Notice how the posterior distribution: Is narrower than either the prior or likelihood alone (indicating increased certainty) Sits between the prior and likelihood, but closer to the likelihood (as the evidence was fairly strong) Has its peak shifted from the prior toward the likelihood (reflecting belief updating) The bottom diagram shows the algebraic process: we multiply the prior by the likelihood, then normalize to get the posterior belief. 11.5 Bayesian Models in Cognitive Science Bayesian cognitive models have been successfully applied to a wide range of phenomena: Perception: How we combine multiple sensory cues (visual, auditory, tactile) to form a unified percept Learning: How we update our knowledge from observation and instruction Decision-making: How we weigh different sources of evidence when making choices Social cognition: How we integrate others’ opinions with our own knowledge Language: How we disambiguate words and sentences based on context Psychopathology: How crucial aspects of conditions like schizophrenia and autism can be understood in terms of atypical Bayesian inference (e.g. atypical weights given to different sources of information, or hyper-precise priors or hyper-precise likelihood). 11.6 Example: Social Influence in Perceptual Decision-Making To ground our discussion in a cognitive science context, let’s consider a simplified version of a recent study examining how people with and without schizophrenia integrate information from different sources (Simonsen et al., 2021). In this task, participants needed to guess the color of the next marble drawn from a jar. They received information from two sources: Direct evidence: A small sample of 8 marbles drawn from the jar (e.g., 6 blue and 2 red marbles) Social evidence: The choices and confidence ratings of four other people who had seen their own independent samples from the jar This paradigm allows researchers to examine how individuals integrate their own direct perceptual evidence with socially transmitted information — a fundamental process in human cognition that may be altered in certain clinical conditions and be involved in generating some aspects of their psychopathology. For simplicity, we’ll focus on a binary version where participants must guess whether the next marble will be blue or red, and we’ll examine how they integrate their direct sample with social information from just what one other person has chosen. Further, at every trial the participants are given a new jar with a potentially different proportion of blue and red marbles, so there is no learning involved. 11.7 A Bayesian Integration Model for the Marble Task In a fully Bayesian approach, participants would: Use direct evidence to form a belief about the proportion of blue marbles in the jar Use social evidence to form another belief about the same proportion Combine these beliefs in a principled way to make their final judgment 11.7.1 Intuitive Explanation Using Beta Distributions The beta distribution provides an elegant way to represent beliefs about proportions (like the proportion of blue marbles in a jar): The beta distribution is defined by two parameters, traditionally called α (alpha) and β (beta). These parameters have an intuitive interpretation: you can think of α as the number of “successes” you’ve observed (e.g., blue marbles) plus 1, and β as the number of “failures” (e.g., red marbles) plus 1. So a Beta(1,1) distribution represents a uniform belief - no prior knowledge about the proportion. After observing evidence, you simply add the counts to these parameters: If you see 6 blue and 2 red marbles, your updated belief is Beta(1+6, 1+2) = Beta(7, 3) This distribution has its peak at 7/(7+3) = 0.7, reflecting your belief that the true proportion is around 70% blue To combine multiple sources of evidence, you simply add all the counts together: If direct evidence gives Beta(7, 3) and social evidence suggests Beta(2, 4) Your combined belief is Beta(7+2, 3+4) = Beta(9, 7) This has its peak at 9/(9+7) = 0.56, reflecting a compromise between the two sources The beauty of this approach is that it automatically weights evidence by its strength (amount of data) and properly represents uncertainty through the width of the distribution. 11.8 The Mathematical Model For our marble task, the Bayesian inference process involves: 11.8.1 Evidence Representation Direct evidence: Observing blue1 blue marbles and red1 red marbles out of total1 trials Social evidence: Inferring blue2 blue marbles and red2 red marbles from social information. If we consider only their choice: red corresponds to the sampling of one red marble; blue corresponds to the sampling of one blue marble. If we consider their confidence, we might try to make this correspond to the marbles the sampled: “Clear blue” might imply 8 blue marbles; maybe blue might imply 6 blue and 2 red marbles; “maybe red” might imply 6 red and 2 blue marbles; “clear red” might imply 8 red marbles. Alternatively we can keep it more uncertain and reduce the assumed sample to 0 blue out of 3, 1 blue out of 3, 2 blue marbles out of 3, or 3 blue marbles out of 3. This intrinsically models the added uncertainty in observing the other’s choice and not their samples. 11.8.2 Integration The integrated belief is represented by a posterior beta distribution: Beta(α + blue1 + blue2, β + red1 + red2) Where α and β are prior parameters (typically 1 each for a uniform prior) 11.8.3 Decision Final choice (blue or red) depends on whether the expected value of this distribution is above 0.5 Confidence depends on the concentration of the distribution 11.8.4 Implementation in R # Beta-binomial model for Bayesian integration in the marble task # # This function implements a Bayesian integration model for combining direct and social evidence # about the proportion of blue marbles in a jar. It uses the beta-binomial model, which is # particularly suitable for reasoning about proportions. # # Parameters: # alpha_prior: Prior alpha parameter (conceptually: prior blue marbles + 1) # beta_prior: Prior beta parameter (conceptually: prior red marbles + 1) # blue1: Number of blue marbles in direct evidence # total1: Total marbles in direct evidence # blue2: Effective blue marbles from social evidence # total2: Effective total marbles from social evidence # # Returns: # List with posterior parameters and statistics for decision-making betaBinomialModel &lt;- function(alpha_prior, beta_prior, blue1, total1, blue2, total2) { # Calculate red marbles for each source red1 &lt;- total1 - blue1 # Number of red marbles in direct evidence red2 &lt;- total2 - blue2 # Inferred number of red marbles from social evidence # The key insight of Bayesian integration: simply add up all evidence counts # This automatically gives more weight to sources with more data alpha_post &lt;- alpha_prior + blue1 + blue2 # Posterior alpha (total blues + prior) beta_post &lt;- beta_prior + red1 + red2 # Posterior beta (total reds + prior) # Calculate posterior statistics expected_rate &lt;- alpha_post / (alpha_post + beta_post) # Mean of beta distribution # Variance has a simple formula for beta distributions # Lower variance = higher confidence in our estimate variance &lt;- (alpha_post * beta_post) / ((alpha_post + beta_post)^2 * (alpha_post + beta_post + 1)) # Calculate 95% credible interval using beta quantile functions # This gives us bounds within which we believe the true proportion lies ci_lower &lt;- qbeta(0.025, alpha_post, beta_post) ci_upper &lt;- qbeta(0.975, alpha_post, beta_post) # Calculate confidence based on variance # Higher variance = lower confidence; transform to 0-1 scale confidence &lt;- 1 - (2 * sqrt(variance)) confidence &lt;- max(0, min(1, confidence)) # Bound between 0 and 1 # Make decision based on whether expected rate exceeds 0.5 # If P(blue) &gt; 0.5, choose blue; otherwise choose red choice &lt;- ifelse(expected_rate &gt; 0.5, &quot;Blue&quot;, &quot;Red&quot;) # Return all calculated parameters in a structured list return(list( alpha_post = alpha_post, beta_post = beta_post, expected_rate = expected_rate, variance = variance, ci_lower = ci_lower, ci_upper = ci_upper, confidence = confidence, choice = choice )) } 11.8.5 Simulating Experimental Scenarios We’ll create a comprehensive set of scenarios by varying both direct evidence (number of blue marbles observed directly) and social evidence (number of blue marbles inferred from social information). # Set total counts for direct and social evidence total1 &lt;- 8 # Total marbles in direct evidence total2 &lt;- 3 # Total evidence units in social evidence # Create all possible combinations of direct and social evidence scenarios &lt;- expand_grid( blue1 = seq(0, 8, 1), # Direct evidence: 0 to 8 blue marbles blue2 = seq(0, 3, 1) # Social evidence: 0 to 3 blue marbles (confidence levels) ) %&gt;% mutate( red1 = total1 - blue1, # Calculate red marbles for direct evidence red2 = total2 - blue2 # Calculate implied red marbles for social evidence ) # Process all scenarios to generate summary statistics sim_data &lt;- map_dfr(1:nrow(scenarios), function(i) { # Extract scenario parameters blue1 &lt;- scenarios$blue1[i] red1 &lt;- scenarios$red1[i] blue2 &lt;- scenarios$blue2[i] red2 &lt;- scenarios$red2[i] # Calculate Bayesian integration using our model result &lt;- betaBinomialModel(1, 1, blue1, total1, blue2, total2) # Return summary data for this scenario tibble( blue1 = blue1, red1 = red1, blue2 = blue2, red2 = red2, expected_rate = result$expected_rate, variance = result$variance, ci_lower = result$ci_lower, ci_upper = result$ci_upper, choice = result$choice, confidence = result$confidence ) }) # Convert social evidence to meaningful labels for better visualization sim_data$social_evidence &lt;- factor(sim_data$blue2, levels = c(0, 1, 2, 3), labels = c(&quot;Clear Red&quot;, &quot;Maybe Red&quot;, &quot;Maybe Blue&quot;, &quot;Clear Blue&quot;)) 11.8.6 Visualizing Bayesian Integration Let’s examine how expected proportion and uncertainty vary across different evidence combinations: # Create two plot panels to visualize model behavior across all evidence combinations p1 &lt;- ggplot(sim_data, aes(x = blue1, y = expected_rate, color = social_evidence, group = social_evidence)) + # Add credible intervals to show uncertainty geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper, fill = social_evidence), alpha = 0.2, color = NA) + geom_line(size = 1) + geom_point(size = 3) + geom_hline(yintercept = 0.5, linetype = &quot;dashed&quot;, color = &quot;gray50&quot;) + scale_x_continuous(breaks = 0:8) + scale_color_brewer(palette = &quot;Set1&quot;) + scale_fill_brewer(palette = &quot;Set1&quot;) + labs(title = &quot;Bayesian Integration of Direct and Social Evidence&quot;, subtitle = &quot;Expected proportion with 95% credible intervals&quot;, x = &quot;Number of Blue Marbles in Direct Sample (out of 8)&quot;, y = &quot;Expected Proportion of Blue Marbles&quot;, color = &quot;Social Evidence&quot;, fill = &quot;Social Evidence&quot;) + theme_bw() + coord_cartesian(ylim = c(0, 1)) # Display plot p1 A few notes about the plot: Evidence integration: The expected proportion of blue marbles (top plot) varies with both direct and social evidence. I would normally expect a non-linear interaction: when direct evidence is ambiguous (e.g., 4 blue out of 8), social evidence should have a stronger effect on the final belief. However, the effect is subtle if any. Evidence Interaction: It may be hard to see, but the influence of social evidence is strongest when direct evidence is ambiguous (around 4 blue marbles) and weakest at the extremes (0 or 8 blue marbles). This reflects the Bayesian property that stronger evidence dominates weaker evidence. Credible intervals: The 95% credible intervals (shaded regions) show our uncertainty about the true proportion. These intervals narrow with more evidence, indicating increased confidence in our estimates. This is better seen in the lower plot than in the upper one. Notice how the variance is highest when direct evidence is ambiguous (around 4 blue marbles) and lowest at the extremes (as they combine congruent evidence from both sources). 11.9 Examining Belief Distributions for Selected Scenarios While the summary statistics give us a high-level view, examining the full posterior distributions provides deeper insight into how evidence is combined. Let’s visualize the complete probability distributions for a selected subset of scenarios: # Function to generate Beta distributions for all components of a Bayesian model # This function returns the prior, likelihood, and posterior distributions # for a given scenario of blue and red marbles simpleBayesianModel_f &lt;- function(blue1, red1, blue2, red2) { # Prior parameters (uniform prior) alpha_prior &lt;- 1 beta_prior &lt;- 1 # Calculate parameters for each distribution # Direct evidence distribution alpha_direct &lt;- alpha_prior + blue1 beta_direct &lt;- beta_prior + red1 # Social evidence distribution alpha_social &lt;- alpha_prior + blue2 beta_social &lt;- beta_prior + red2 # Posterior distribution (combined evidence) alpha_post &lt;- alpha_prior + blue1 + blue2 beta_post &lt;- beta_prior + red1 + red2 # Create a grid of theta values (possible proportions of blue marbles) theta &lt;- seq(0.001, 0.999, length.out = 200) # Calculate densities for each distribution prior_density &lt;- dbeta(theta, alpha_prior, beta_prior) direct_density &lt;- dbeta(theta, alpha_direct, beta_direct) social_density &lt;- dbeta(theta, alpha_social, beta_social) posterior_density &lt;- dbeta(theta, alpha_post, beta_post) # Return dataframe with all distributions return(data.frame( theta = theta, prior = prior_density, direct = direct_density, social = social_density, posterior = posterior_density )) } # Select a few representative scenarios selected_scenarios &lt;- expand_grid( blue1 = c(1, 4, 7), # Different levels of direct evidence blue2 = c(0, 1, 2, 3) # Different levels of social evidence ) # Generate distributions for each scenario distribution_data &lt;- do.call(rbind, lapply(1:nrow(selected_scenarios), function(i) { blue1 &lt;- selected_scenarios$blue1[i] blue2 &lt;- selected_scenarios$blue2[i] # Generate distributions dist_df &lt;- simpleBayesianModel_f(blue1, total1 - blue1, blue2, total2 - blue2) # Add scenario information dist_df$blue1 &lt;- blue1 dist_df$blue2 &lt;- blue2 dist_df$social_evidence &lt;- factor(blue2, levels = c(0, 1, 2, 3), labels = c(&quot;Clear Red&quot;, &quot;Maybe Red&quot;, &quot;Maybe Blue&quot;, &quot;Clear Blue&quot;)) return(dist_df) })) # Modify the plotting function p_evidence_combination &lt;- ggplot(distribution_data) + # Prior distribution with clear emphasis geom_line(aes(x = theta, y = prior), color = &quot;gray50&quot;, linetype = &quot;solid&quot;, size = 0.5) + # Posterior distribution with consistent coloring geom_area(aes(x = theta, y = posterior), fill = &quot;purple&quot;, alpha = 0.3) + geom_line(aes(x = theta, y = posterior), color = &quot;purple&quot;, size = 1.2) + # Direct and social evidence distributions geom_line(aes(x = theta, y = direct, color = &quot;Direct Evidence&quot;), size = 1, alpha = 0.7, linetype = &quot;dashed&quot;) + geom_line(aes(x = theta, y = social, color = &quot;Social Evidence&quot;), size = 1, alpha = 0.7, linetype = &quot;dashed&quot;) + # Facet by direct and social evidence levels facet_grid(blue2 ~ blue1, labeller = labeller( blue1 = function(x) paste(&quot;Direct: &quot;, x, &quot; Blue&quot;), blue2 = function(x) paste(&quot;Social: &quot;, x, &quot; Blue&quot;) )) + # Aesthetics scale_color_manual(values = c( &quot;Direct Evidence&quot; = &quot;blue&quot;, &quot;Social Evidence&quot; = &quot;red&quot; )) + # Labels and theme labs( title = &quot;Bayesian Evidence Integration: Distribution Combination&quot;, subtitle = &quot;Merging direct and social evidence into a posterior belief&quot;, x = &quot;Proportion of Blue Marbles&quot;, y = &quot;Probability Density&quot;, color = &quot;Evidence Type&quot; ) + theme_minimal() + theme( legend.position = &quot;bottom&quot;, strip.text = element_text(size = 8), axis.text = element_text(size = 6) ) # Display the plot print(p_evidence_combination) This comprehensive visualization shows how the different probability distributions interact: Prior distribution (gray line): Our initial uniform belief about the proportion of blue marbles. Direct evidence distribution (blue dashed line): Belief based solely on our direct observation of marbles. Notice how it becomes more concentrated with more extreme evidence (e.g., 1 or 7 blue marbles). Social evidence distribution (red dashed line): Belief based solely on social information. This is generally less concentrated than the direct evidence distribution since it’s based on lower evidence (0-3 vs. 0-8). Posterior distribution (purple area): The final belief that results from combining all information sources. Notice how it tends to lie between the direct and social evidence distributions, but is typically narrower than either, reflecting increased certainty from combining information, unless the evidence is in conflict. 11.10 Weighted Bayesian Integration In real cognitive systems, people often weight information sources differently based on their reliability or relevance. Let’s implement a weighted Bayesian model that allows for differential weighting of evidence sources. 11.10.1 The Mathematical Model Our weighted Bayesian integration model extends the simple model by introducing weight parameters for each information source: Start with prior: Beta(α₀, β₀) Observe direct evidence: k₁ blue marbles out of n₁ total Observe social evidence: k₂ blue marbles out of n₂ total Apply weights: w₁ for direct evidence, w₂ for social evidence Posterior: Beta(α₀ + w₁·k₁ + w₂·k₂, β₀ + w₁·(n₁-k₁) + w₂·(n₂-k₂)) The weights represent the degree to which each information source influences the final belief. A weight of 2.0 means you treat that evidence as if you had observed twice as many marbles as you actually did (as more reliable than what the current evidence would warrant), while a weight of 0.5 means you treat it as half as informative. From a cognitive perspective, they might reflect judgments about reliability, relevance, or attentional focus. 11.10.2 Implementation # Weighted Beta-Binomial model for evidence integration # # This function extends our basic model by allowing different weights for each # evidence source. This can represent differences in perceived reliability, # attention, or individual cognitive tendencies. # # Parameters: # alpha_prior, beta_prior: Prior parameters (typically 1,1 for uniform prior) # blue1, total1: Direct evidence (blue marbles and total) # blue2, total2: Social evidence (blue signals and total) # weight_direct, weight_social: Relative weights for each evidence source # # Returns: # List with model results and statistics weightedBetaBinomial &lt;- function(alpha_prior, beta_prior, blue1, total1, blue2, total2, weight_direct, weight_social) { # Calculate red marbles for each source red1 &lt;- total1 - blue1 # Number of red marbles in direct evidence red2 &lt;- total2 - blue2 # Number of red signals in social evidence # Apply weights to evidence (this is the key step) # Weighting effectively scales the &quot;sample size&quot; of each information source weighted_blue1 &lt;- blue1 * weight_direct # Weighted blue count from direct evidence weighted_red1 &lt;- red1 * weight_direct # Weighted red count from direct evidence weighted_blue2 &lt;- blue2 * weight_social # Weighted blue count from social evidence weighted_red2 &lt;- red2 * weight_social # Weighted red count from social evidence # Calculate posterior parameters by adding weighted evidence alpha_post &lt;- alpha_prior + weighted_blue1 + weighted_blue2 # Posterior alpha parameter beta_post &lt;- beta_prior + weighted_red1 + weighted_red2 # Posterior beta parameter # Calculate statistics from posterior beta distribution expected_rate &lt;- alpha_post / (alpha_post + beta_post) # Mean of beta distribution # Calculate variance (lower variance = higher confidence) variance &lt;- (alpha_post * beta_post) / ((alpha_post + beta_post)^2 * (alpha_post + beta_post + 1)) # Calculate 95% credible interval ci_lower &lt;- qbeta(0.025, alpha_post, beta_post) ci_upper &lt;- qbeta(0.975, alpha_post, beta_post) # Calculate decision and confidence decision &lt;- ifelse(rbinom(1, 1, expected_rate) == 1, &quot;Blue&quot;, &quot;Red&quot;) # Decision based on most likely color confidence &lt;- 1 - (2 * sqrt(variance)) # Confidence based on certainty confidence &lt;- max(0, min(1, confidence)) # Bound between 0 and 1 # Return all calculated parameters in a structured list return(list( alpha_post = alpha_post, beta_post = beta_post, expected_rate = expected_rate, variance = variance, ci_lower = ci_lower, ci_upper = ci_upper, decision = decision, confidence = confidence )) } 11.10.3 Visualizing Weighted Bayesian Integration Let’s create a comprehensive visualization showing how different weights affect belief formation: # Create improved visualization using small multiples weighted_belief_plot &lt;- function() { # Define grid of parameters to visualize w1_values &lt;- seq(0, 2, by = 0.2) # Weight for source 1 w2_values &lt;- seq(0, 2, by = 0.2) # Weight for source 2 source1_values &lt;- seq(0, 8, by = 1) # Source 1 values source2_values &lt;- seq(0, 3, by = 1) # Source 2 values # Generate data plot_data &lt;- expand_grid( w1 = w1_values, w2 = w2_values, Source1 = source1_values, Source2 = source2_values ) %&gt;% mutate( belief = pmap_dbl(list(w1, w2, Source1, Source2), function(w1, w2, s1, s2) { # Calculate Beta parameters alpha_prior &lt;- 1 beta_prior &lt;- 1 alpha_post &lt;- alpha_prior + w1 * s1 + w2 * s2 beta_post &lt;- beta_prior + w1 * (8 - s1) + w2*(3 - s2) # Return expected value alpha_post / (alpha_post + beta_post) }) ) # Create visualization p &lt;- ggplot(plot_data, aes(x = Source1, y = belief, color = Source2, group = Source2)) + geom_line() + facet_grid(w1 ~ w2, labeller = labeller( w1 = function(x) paste(&quot;w1 =&quot;, x), w2 = function(x) paste(&quot;w2 =&quot;, x) )) + scale_color_viridis_c(option = &quot;plasma&quot;) + labs( title = &quot;Weighted Bayesian Integration of Two Evidence Sources&quot;, x = &quot;Direct Evidence (Blue Marbles)&quot;, y = &quot;Belief in the next pick being a blue marble&quot;, color = &quot;Social Evidence (Blue Marbles)&quot; ) + theme_minimal() + theme( strip.background = element_rect(fill = &quot;gray90&quot;), strip.text = element_text(size = 10, face = &quot;bold&quot;), panel.grid.minor = element_blank(), panel.border = element_rect(color = &quot;black&quot;, fill = NA) ) return(p) } # Generate and display the plot weighted_belief_plot() The visualization showcases weighted Bayesian integration: First, when both weights (w1 and w2) are low (top left panels), beliefs remain moderate regardless of the evidence values, representing high uncertainty. As weights increase (moving right and down), beliefs become more extreme, showing increased confidence in the integrated evidence. Second, the slope of the lines indicates the relative influence of each source. Steeper slopes (bottom right panels) demonstrate that Source1 has stronger influence on belief when both weights are high, while the spacing between lines shows the impact of Source2. Third, when weights are asymmetric (e.g., high w1 and low w2), the belief is dominated by the source with the higher weight, essentially ignoring evidence from the other source. This illustrates how selective attention to certain evidence sources can be modeled as differential weighting in a Bayesian framework. 11.10.4 Resolving Conflicting Evidence To further understand how weighted Bayesian integration resolves conflicts between evidence sources, let’s examine two specific conflict scenarios: # Define conflict scenarios scenario1 &lt;- list(blue1 = 7, total1 = 8, blue2 = 0, total2 = 3) # Direct: blue, Social: red scenario2 &lt;- list(blue1 = 5, total1 = 8, blue2 = 0, total2 = 3) # Direct: red, Social: blue # Create function to evaluate scenarios across weight combinations evaluate_conflict &lt;- function(scenario) { weight_grid &lt;- expand_grid( weight_direct = seq(0, 2, by = 0.2), weight_social = seq(0, 2, by = 0.2) ) # Calculate results for each weight combination results &lt;- pmap_dfr(weight_grid, function(weight_direct, weight_social) { result &lt;- weightedBetaBinomial( 1, 1, scenario$blue1, scenario$total1, scenario$blue2, scenario$total2, weight_direct, weight_social ) tibble( weight_direct = weight_direct, weight_social = weight_social, expected_rate = result$expected_rate, decision = result$decision ) }) return(results) } # Calculate results conflict1_results &lt;- evaluate_conflict(scenario1) conflict2_results &lt;- evaluate_conflict(scenario2) # Create visualizations p1 &lt;- ggplot(conflict1_results, aes(x = weight_direct, y = weight_social)) + geom_tile(aes(fill = expected_rate)) + geom_contour(aes(z = expected_rate), breaks = 0.5, color = &quot;black&quot;, size = 1) + scale_fill_gradient2( low = &quot;red&quot;, mid = &quot;white&quot;, high = &quot;blue&quot;, midpoint = 0.5, limits = c(0, 1) ) + labs( title = &quot;Scenario 1: Strong Direct Evidence for Blue (7 out of 8) \\nvs. Strong Social Evidence for Red (0/3)&quot;, subtitle = &quot;Black line shows decision boundary (expected rate = 0.5)&quot;, x = &quot;Weight for Direct Evidence&quot;, y = &quot;Weight for Social Evidence&quot;, fill = &quot;Expected\\nRate&quot; ) + theme_minimal() + coord_fixed() p2 &lt;- ggplot(conflict2_results, aes(x = weight_direct, y = weight_social)) + geom_tile(aes(fill = expected_rate)) + geom_contour(aes(z = expected_rate), breaks = 0.5, color = &quot;black&quot;, size = 1) + scale_fill_gradient2( low = &quot;red&quot;, mid = &quot;white&quot;, high = &quot;blue&quot;, midpoint = 0.5, limits = c(0, 1) ) + labs( title = &quot;Scenario 1: Weak Direct Evidence for Blue (5 out of 8) \\nvs. Strong Social Evidence for Red (0/3)&quot;, subtitle = &quot;Black line shows decision boundary (expected rate = 0.5)&quot;, x = &quot;Weight for Direct Evidence&quot;, y = &quot;Weight for Social Evidence&quot;, fill = &quot;Expected\\nRate&quot; ) + theme_minimal() + coord_fixed() # Display plots p1 / p2 These visualizations illustrate how different weight combinations resolve conflicts between evidence sources: Decision boundary: The black line represents combinations of weights that lead to equal evidence for red and blue (expected rate = 0.5). Weight combinations above this line lead to a “blue” decision, while those below lead to a “red” decision. Relative evidence strength: The slope of the decision boundary reflects the relative strength of the evidence sources. A steeper slope indicates that direct evidence is stronger relative to social evidence. Individual differences: Different individuals might give different weights to evidence sources, leading to different decisions even when faced with identical evidence. This provides a mechanistic explanation for individual variation in decision-making. 11.11 Common Misinterpretations 11.11.1 Weight Interpretation Weights effectively scale the relative importance of each source of evidence. A weight of 0 means ignoring that evidence source entirely. A weight of 1 means treating the evidence as observed, at face value. Weights above 1 amplify the evidence, while weights below 1 dampen it. A negative weight would make the agent invert the direction of the evidence (if more evidence for red, they’d tend to pick blue). Remember that weights moderate the evidence, so a strong weight doesn’t guarantee a strong influence if the evidence itself is weak. 11.11.2 Integration vs. Averaging Bayesian integration is not simply the averaging of evidence across sources, because it naturally includes how precise the evidence is (how narrow the distribution). Normally, this would happen when we multiply the distributions involved. The Beta-Binomial model handles this automatically by incorporating sample sizes (the n of marbles). 11.11.3 Interpreting Confidence There is something tricky in this model when it comes to confidence. We can say that a belief that the next sample is going to be blue with a 0.8 (average) probability more confident than one with a 0.6 (average) probability. We can also say that a belief that the next sample is going to be blue with a 0.8 (95% CIs 0.5-1) probability is less confident than a belief with a 0.6 (95% CIs 0.55-0.65) probability. We need to keep these two aspects separate. The first one is about the average probability, the second one is about the uncertainty around that average probability. In the code above we only call the second confidence and use entropy of the posterior distribution to quantify it. 11.12 Simulating Agents with Different Evidence Weighting Strategies To prepare for our model fitting, we’ll simulate three distinct agents: Balanced Agent: This agent treats both direct and social evidence at face value, applying equal weights (w_direct = 1.0, w_social = 1.0). This represents an unbiased integration of information. Self-Focused Agent: This agent overweights their own direct evidence (w_direct = 1.5) while underweighting social evidence (w_social = 0.5). This represents someone who trusts their own observations more than information from others. Socially-Influenced Agent: This agent does the opposite, overweighting social evidence (w_social = 2.0) while underweighting their own direct evidence (w_direct = 0.7). This might represent someone who is highly responsive to social information. Let’s generate decisions for these three agents in an experiment exposing them to all possible evidence combinations and visualize how their different weighting strategies affect their beliefs and choices. # Simulation of agents with different weighting strategies # This code generates decisions for three agents with different approaches to weighting evidence # Define our three agent types with their respective weights agents &lt;- tibble( agent_type = c(&quot;Balanced&quot;, &quot;Self-Focused&quot;, &quot;Socially-Influenced&quot;), weight_direct = c(1.0, 1.5, 0.7), # Weight for direct evidence weight_social = c(1.0, 0.5, 2.0) # Weight for social evidence ) # Create all possible evidence combinations # Direct evidence: 0-8 blue marbles out of 8 total # Social evidence: 0-3 signals (representing confidence levels) evidence_combinations &lt;- expand_grid( blue1 = 0:8, # Direct evidence: number of blue marbles seen blue2 = 0:3 # Social evidence: strength of blue evidence ) %&gt;% mutate( total1 = 8, # Total marbles in direct evidence total2 = 3 # Total strength units in social evidence ) generate_agent_decisions &lt;- function(weight_direct, weight_social, evidence_df, n_samples = 5) { # Create a data frame that repeats each evidence combination n_samples times repeated_evidence &lt;- evidence_df %&gt;% slice(rep(1:n(), each = n_samples)) %&gt;% # Add a sample_id to distinguish between repetitions of the same combination group_by(blue1, blue2, total1, total2) %&gt;% mutate(sample_id = 1:n()) %&gt;% ungroup() # Apply our weighted Bayesian model to each evidence combination decisions &lt;- pmap_dfr(repeated_evidence, function(blue1, blue2, total1, total2, sample_id) { # Calculate Bayesian integration with the agent&#39;s specific weights result &lt;- weightedBetaBinomial( alpha_prior = 1, beta_prior = 1, blue1 = blue1, total1 = total1, blue2 = blue2, total2 = total2, weight_direct = weight_direct, weight_social = weight_social ) # Return key decision metrics tibble( sample_id = sample_id, blue1 = blue1, blue2 = blue2, total1 = total1, total2 = total2, expected_rate = result$expected_rate, # Probability the next marble is blue choice = result$decision, # Final decision (Blue or Red) choice_binary = ifelse(result$decision == &quot;Blue&quot;, 1, 0), confidence = result$confidence # Confidence in decision ) }) return(decisions) } # When generating data for weighted Bayesian model simulation simulation_results &lt;- map_dfr(1:nrow(agents), function(i) { # Extract this agent&#39;s parameters agent_data &lt;- agents[i, ] # Generate decisions for this agent with multiple samples decisions &lt;- generate_agent_decisions( agent_data$weight_direct, agent_data$weight_social, evidence_combinations, n_samples = 5 # Generate 5 samples per evidence combination ) # Add agent identifier decisions$agent_type &lt;- agent_data$agent_type return(decisions) }) # Add descriptive labels for visualization simulation_results &lt;- simulation_results %&gt;% mutate( # Create descriptive labels for social evidence social_evidence = factor( blue2, levels = 0:3, labels = c(&quot;Clear Red&quot;, &quot;Maybe Red&quot;, &quot;Maybe Blue&quot;, &quot;Clear Blue&quot;) ), # Create factor for agent type to control plotting order agent_type = factor( agent_type, levels = c(&quot;Balanced&quot;, &quot;Self-Focused&quot;, &quot;Socially-Influenced&quot;) ) ) # Let&#39;s examine a sample of the generated data head(simulation_results) ## # A tibble: 6 × 11 ## sample_id blue1 blue2 total1 total2 expected_rate choice choice_binary confidence ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 0 8 3 0.0769 Red 0 0.858 ## 2 2 0 0 8 3 0.0769 Red 0 0.858 ## 3 3 0 0 8 3 0.0769 Red 0 0.858 ## 4 4 0 0 8 3 0.0769 Red 0 0.858 ## 5 5 0 0 8 3 0.0769 Red 0 0.858 ## 6 1 0 1 8 3 0.154 Red 0 0.807 ## # ℹ 2 more variables: agent_type &lt;fct&gt;, social_evidence &lt;fct&gt; Now let’s create visualizations to compare how these different agents make decisions based on the same evidence: # Visualization 1: Expected probability across evidence combinations p1 &lt;- ggplot(simulation_results, aes(x = blue1, y = expected_rate, color = social_evidence, group = social_evidence)) + # Draw a line for each social evidence level geom_line(size = 1) + # Add points to show discrete evidence combinations geom_point(size = 2) + # Add a reference line at 0.5 (decision boundary) geom_hline(yintercept = 0.5, linetype = &quot;dashed&quot;, color = &quot;gray50&quot;) + # Facet by agent type facet_wrap(~ agent_type, ncol = 1) + # Customize colors and labels scale_color_brewer(palette = &quot;Set1&quot;) + scale_x_continuous(breaks = 0:8) + labs( title = &quot;How Different Agents Integrate Evidence&quot;, subtitle = &quot;Expected probability of blue marble across evidence combinations&quot;, x = &quot;Number of Blue Marbles in Direct Sample (out of 8)&quot;, y = &quot;Expected Probability of Blue&quot;, color = &quot;Social Evidence&quot; ) + theme_bw() + theme(legend.position = &quot;bottom&quot;) # Visualization 2: Decision boundaries for each agent # Create a simplified dataset showing just the decision (Blue/Red) decision_data &lt;- simulation_results %&gt;% mutate(decision_value = ifelse(choice == &quot;Blue&quot;, 1, 0)) p2 &lt;- ggplot(decision_data, aes(x = blue1, y = blue2)) + # Create tiles colored by decision geom_tile(aes(fill = choice)) + # Add decision boundary contour line stat_contour(aes(z = decision_value), breaks = 0.5, color = &quot;black&quot;, size = 1) + # Facet by agent type facet_wrap(~ agent_type) + # Customize colors and labels scale_fill_manual(values = c(&quot;Red&quot; = &quot;firebrick&quot;, &quot;Blue&quot; = &quot;royalblue&quot;)) + scale_x_continuous(breaks = 0:8) + scale_y_continuous(breaks = 0:3) + labs( title = &quot;Decision Boundaries Across Agents&quot;, subtitle = &quot;Black line shows where agents switch from choosing red to blue&quot;, x = &quot;Number of Blue Marbles in Direct Evidence (out of 8)&quot;, y = &quot;Number of Blue Signals in Social Evidence (out of 3)&quot;, fill = &quot;Decision&quot; ) + theme_bw() # Visualization 3: Confidence levels p3 &lt;- ggplot(simulation_results, aes(x = blue1, y = confidence, color = social_evidence, group = social_evidence)) + geom_line(size = 1) + geom_point(size = 2) + facet_wrap(~ agent_type, ncol = 1) + scale_color_brewer(palette = &quot;Set1&quot;) + scale_x_continuous(breaks = 0:8) + labs( title = &quot;Confidence Across Evidence Combinations&quot;, subtitle = &quot;Higher values indicate greater confidence in decision&quot;, x = &quot;Number of Blue Marbles in Direct Sample (out of 8)&quot;, y = &quot;Decision Confidence&quot;, color = &quot;Social Evidence&quot; ) + theme_bw() + theme(legend.position = &quot;bottom&quot;) # Display the visualizations p1 p2 p3 11.13 Key Observations from the Simulation Our simulation highlights several important aspects of Bayesian evidence integration with different weighting strategies: Evidence Thresholds: The decision boundaries (Visualization 2) clearly show how much evidence each agent requires to switch from choosing red to blue. The Self-Focused agent needs less direct evidence when social evidence supports blue, compared to the Socially-Influenced agent. Influence of Social Evidence: In the first visualization, we can observe how the lines for different social evidence levels are spaced. For the Socially-Influenced agent, these lines are widely spaced, indicating that social evidence strongly affects their beliefs. For the Self-Focused agent, the lines are closer together, showing less impact from social evidence. Confidence Patterns: The third visualization reveals how confidence varies across evidence combinations and agent types. All agents are most confident when evidence is strong and consistent across sources, but they differ in how they handle conflicting evidence. Decision Regions: The Self-Focused agent has a larger region where they choose blue based primarily on direct evidence, while the Socially-Influenced agent has more regions where social evidence can override moderate direct evidence. These patterns highlight the profound impact that evidence weighting can have on decision-making, even when agents are all using the same underlying Bayesian integration mechanism. In the next section, we’ll implement these agents in Stan to perform more sophisticated parameter estimation. Now, let’s define our Stan models to implement: a simple bayesian agent (equivalent to assuming both weights to be 1); and a weighted bayesian agent (explicitly inferring weights for direct and social evidence). # Simple Beta-Binomial Stan model (no weights) SimpleAgent_stan &lt;- &quot; // Bayesian integration model relying on a beta-binomial distribution // to preserve all uncertainty // All evidence is taken at face value (equal weights) data { int&lt;lower=1&gt; N; // Number of decisions array[N] int&lt;lower=0, upper=1&gt; choice; // Choices (0=red, 1=blue) array[N] int&lt;lower=0&gt; blue1; // Direct evidence (blue marbles) array[N] int&lt;lower=0&gt; total1; // Total direct evidence (total marbles) array[N] int&lt;lower=0&gt; blue2; // Social evidence (blue signals) array[N] int&lt;lower=0&gt; total2; // Total social evidence (total signals) } parameters{ real&lt;lower = 0&gt; alpha_prior; // Prior alpha parameter real&lt;lower = 0&gt; beta_prior; // Prior beta parameter } model { target += lognormal_lpdf(alpha_prior | 0, 1); // Prior on alpha_prior, the agent bias towards blue target += lognormal_lpdf(beta_prior | 0, 1); // Prior on beta_prior, the agent bias towards red // Each observation is a separate decision for (i in 1:N) { // Calculate Beta parameters for posterior belief distribution real alpha_post = alpha_prior + blue1[i] + blue2[i]; real beta_post = beta_prior + (total1[i] - blue1[i]) + (total2[i] - blue2[i]); // Use beta_binomial distribution which integrates over all possible values // of the rate parameter weighted by their posterior probability target += beta_binomial_lpmf(choice[i] | 1, alpha_post, beta_post); } } generated quantities { // Log likelihood for model comparison vector[N] log_lik; // Prior and posterior predictive checks array[N] int prior_pred_choice; array[N] int posterior_pred_choice; for (i in 1:N) { // For prior predictions, use uniform prior (Beta(1,1)) prior_pred_choice[i] = beta_binomial_rng(1, 1, 1); // For posterior predictions, use integrated evidence real alpha_post = alpha_prior + blue1[i] + blue2[i]; real beta_post = beta_prior + (total1[i] - blue1[i]) + (total2[i] - blue2[i]); // Generate predictions using the complete beta-binomial model posterior_pred_choice[i] = beta_binomial_rng(1, alpha_post, beta_post); // Log likelihood calculation using beta-binomial log_lik[i] = beta_binomial_lpmf(choice[i] | 1, alpha_post, beta_post); } } &quot; # Weighted Beta-Binomial Stan model WeightedAgent_stan &lt;- &quot; data { int&lt;lower=1&gt; N; // Number of decisions array[N] int&lt;lower=0, upper=1&gt; choice; // Choices (0=red, 1=blue) array[N] int&lt;lower=0&gt; blue1; // Direct evidence (blue marbles) array[N] int&lt;lower=0&gt; total1; // Total direct evidence array[N] int&lt;lower=0&gt; blue2; // Social evidence (blue signals) array[N] int&lt;lower=0&gt; total2; // Total social evidence } parameters { real&lt;lower = 0&gt; alpha_prior; // Prior alpha parameter real&lt;lower = 0&gt; beta_prior; // Prior beta parameter real&lt;lower=0&gt; total_weight; // Total influence of all evidence real&lt;lower=0, upper=1&gt; weight_prop; // Proportion of weight for direct evidence } transformed parameters { real&lt;lower=0&gt; weight_direct = total_weight * weight_prop; real&lt;lower=0&gt; weight_social = total_weight * (1 - weight_prop); } model { // Priors target += lognormal_lpdf(alpha_prior | 0, 1); // Prior on alpha_prior target += lognormal_lpdf(beta_prior | 0, 1); // Prior on beta_prior target += lognormal_lpdf(total_weight | .8, .4); // Centered around 2 with reasonable spread and always positive target += beta_lpdf(weight_prop | 1, 1); // Uniform prior on proportion // Each observation is a separate decision for (i in 1:N) { // For this specific decision: real weighted_blue1 = blue1[i] * weight_direct; real weighted_red1 = (total1[i] - blue1[i]) * weight_direct; real weighted_blue2 = blue2[i] * weight_social; real weighted_red2 = (total2[i] - blue2[i]) * weight_social; // Calculate Beta parameters for this decision real alpha_post = alpha_prior + weighted_blue1 + weighted_blue2; real beta_post = beta_prior + weighted_red1 + weighted_red2; // Use beta_binomial distribution to integrate over the full posterior target += beta_binomial_lpmf(choice[i] | 1, alpha_post, beta_post); } } generated quantities { // Log likelihood and predictions vector[N] log_lik; array[N] int posterior_pred_choice; array[N] int prior_pred_choice; // Sample the agent&#39;s preconceptions real alpha_prior_prior = lognormal_rng(0, 1); real beta_prior_prior = lognormal_rng(0, 1); // Sample from priors for the reparameterized model real&lt;lower = 0&gt; total_weight_prior = lognormal_rng(.8, .4); real weight_prop_prior = beta_rng(1, 1); // Derive the implied direct and social weights from the prior samples real weight_direct_prior = total_weight_prior * weight_prop_prior; real weight_social_prior = total_weight_prior * (1 - weight_prop_prior); // Posterior predictions and log-likelihood for (i in 1:N) { // Posterior predictions using the weighted evidence real weighted_blue1 = blue1[i] * weight_direct; real weighted_red1 = (total1[i] - blue1[i]) * weight_direct; real weighted_blue2 = blue2[i] * weight_social; real weighted_red2 = (total2[i] - blue2[i]) * weight_social; real alpha_post = alpha_prior + weighted_blue1 + weighted_blue2; real beta_post = beta_prior + weighted_red1 + weighted_red2; // Log likelihood using beta_binomial log_lik[i] = beta_binomial_lpmf(choice[i] | 1, alpha_post, beta_post); // Generate predictions from the full posterior posterior_pred_choice[i] = beta_binomial_rng(1, alpha_post, beta_post); // Prior predictions using the prior-derived weights real prior_weighted_blue1 = blue1[i] * weight_direct_prior; real prior_weighted_red1 = (total1[i] - blue1[i]) * weight_direct_prior; real prior_weighted_blue2 = blue2[i] * weight_social_prior; real prior_weighted_red2 = (total2[i] - blue2[i]) * weight_social_prior; real alpha_prior_preds = alpha_prior + prior_weighted_blue1 + prior_weighted_blue2; real beta_prior_preds = beta_prior + prior_weighted_red1 + prior_weighted_red2; // Generate predictions from the prior prior_pred_choice[i] = beta_binomial_rng(1, alpha_prior, beta_prior); } } &quot; # Write the models to files write_stan_file( SimpleAgent_stan, dir = &quot;stan/&quot;, basename = &quot;W10 _beta_binomial.stan&quot; ) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W10 _beta_binomial.stan&quot; write_stan_file( WeightedAgent_stan, dir = &quot;stan/&quot;, basename = &quot;W10 _weighted_beta_binomial.stan&quot; ) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W10 _weighted_beta_binomial.stan&quot; # Prepare simulation data for Stan fitting # Convert &#39;Blue&#39; and &#39;Red&#39; choices to binary format (1 for Blue, 0 for Red) sim_data_for_stan &lt;- simulation_results %&gt;% mutate( choice_binary = as.integer(choice == &quot;Blue&quot;), total1 = 8, # Total marbles in direct evidence (constant) total2 = 3 # Total signals in social evidence (constant) ) # Split data by agent type balanced_data &lt;- sim_data_for_stan %&gt;% filter(agent_type == &quot;Balanced&quot;) self_focused_data &lt;- sim_data_for_stan %&gt;% filter(agent_type == &quot;Self-Focused&quot;) socially_influenced_data &lt;- sim_data_for_stan %&gt;% filter(agent_type == &quot;Socially-Influenced&quot;) # Function to prepare data for Stan prepare_stan_data &lt;- function(df) { list( N = nrow(df), choice = df$choice_binary, blue1 = df$blue1, total1 = df$total1, blue2 = df$blue2, total2 = df$total2 ) } # Prepare Stan data for each agent stan_data_balanced &lt;- prepare_stan_data(balanced_data) stan_data_self_focused &lt;- prepare_stan_data(self_focused_data) stan_data_socially_influenced &lt;- prepare_stan_data(socially_influenced_data) # Compile the Stan models file_simple &lt;- file.path(&quot;stan/W10 _beta_binomial.stan&quot;) file_weighted &lt;- file.path(&quot;stan/W10 _weighted_beta_binomial.stan&quot;) # Check if we need to regenerate simulation results if (regenerate_simulations) { # Compile models mod_simple &lt;- cmdstan_model(file_simple, cpp_options = list(stan_threads = TRUE)) mod_weighted &lt;- cmdstan_model(file_weighted, cpp_options = list(stan_threads = TRUE)) # Fit simple model to each agent&#39;s data fit_simple_balanced &lt;- mod_simple$sample( data = stan_data_balanced, seed = 123, chains = 2, parallel_chains = 2, threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 1000, refresh = 0 # Set to 500 or so to see progress ) fit_simple_self_focused &lt;- mod_simple$sample( data = stan_data_self_focused, seed = 124, chains = 2, parallel_chains = 2, threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 1000, refresh = 0 ) fit_simple_socially_influenced &lt;- mod_simple$sample( data = stan_data_socially_influenced, seed = 125, chains = 2, parallel_chains = 2, threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 1000, refresh = 0 ) # Fit weighted model to each agent&#39;s data fit_weighted_balanced &lt;- mod_weighted$sample( data = stan_data_balanced, seed = 124, chains = 2, parallel_chains = 2, threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 1000, refresh = 0 ) fit_weighted_self_focused &lt;- mod_weighted$sample( data = stan_data_self_focused, seed = 127, chains = 2, parallel_chains = 2, threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 1000, refresh = 0 ) fit_weighted_socially_influenced &lt;- mod_weighted$sample( data = stan_data_socially_influenced, seed = 128, chains = 2, parallel_chains = 2, threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 1000, refresh = 0 ) # Save model fits for future use fit_simple_balanced$save_object(&quot;simmodels/fit_simple_balanced.rds&quot;) fit_simple_self_focused$save_object(&quot;simmodels/fit_simple_self_focused.rds&quot;) fit_simple_socially_influenced$save_object(&quot;simmodels/fit_simple_socially_influenced.rds&quot;) fit_weighted_balanced$save_object(&quot;simmodels/fit_weighted_balanced.rds&quot;) fit_weighted_self_focused$save_object(&quot;simmodels/fit_weighted_self_focused.rds&quot;) fit_weighted_socially_influenced$save_object(&quot;simmodels/fit_weighted_socially_influenced.rds&quot;) cat(&quot;Generated and saved new model fits\\n&quot;) } else { # Load existing model fits fit_simple_balanced &lt;- readRDS(&quot;simmodels/fit_simple_balanced.rds&quot;) fit_simple_self_focused &lt;- readRDS(&quot;simmodels/fit_simple_self_focused.rds&quot;) fit_simple_socially_influenced &lt;- readRDS(&quot;simmodels/fit_simple_socially_influenced.rds&quot;) fit_weighted_balanced &lt;- readRDS(&quot;simmodels/fit_weighted_balanced.rds&quot;) fit_weighted_self_focused &lt;- readRDS(&quot;simmodels/fit_weighted_self_focused.rds&quot;) fit_weighted_socially_influenced &lt;- readRDS(&quot;simmodels/fit_weighted_socially_influenced.rds&quot;) cat(&quot;Loaded existing model fits\\n&quot;) } ## Loaded existing model fits 11.14 Model Quality Checks 11.14.1 Overview Model quality checks are crucial for understanding how well our Bayesian models capture the underlying data-generating process. We’ll use three primary techniques: Prior Predictive Checks Posterior Predictive Checks Prior-Posterior Update Visualization # Function to create trace and rank plots for a model create_diagnostic_plots &lt;- function(fit, model_name) { # Extract posterior draws draws &lt;- as_draws_df(fit$draws()) trace_data &lt;- data.frame( Iteration = rep(1:(nrow(draws)/length(unique(draws$.chain))), length(unique(draws$.chain))), Chain = draws$.chain, weight_direct = draws$weight_direct, weight_social = draws$weight_social, total_weight = draws$total_weight, weight_prop = draws$weight_prop ) # Create trace plot trace_plot1 &lt;- ggplot(trace_data, aes(x = Iteration, y = weight_direct, color = factor(Chain))) + geom_line() + labs(title = paste(&quot;Trace Plot for weight_direct&quot;), x = &quot;Iteration&quot;, y = &quot;weight_direct&quot;, color = &quot;Chain&quot;) + theme_minimal() + theme(plot.title = element_text(hjust = 0.5, face = &quot;bold&quot;)) trace_plot2 &lt;- ggplot(trace_data, aes(x = Iteration, y = weight_social, color = factor(Chain))) + geom_line() + labs(title = paste(&quot;Trace Plot for weight_social&quot;), x = &quot;Iteration&quot;, y = &quot;weight_direct&quot;, color = &quot;Chain&quot;) + theme_minimal() + theme(plot.title = element_text(hjust = 0.5, face = &quot;bold&quot;)) trace_plot3 &lt;- ggplot(trace_data, aes(x = Iteration, y = total_weight, color = factor(Chain))) + geom_line() + labs(title = paste(&quot;Trace Plot for total_weight&quot;), x = &quot;Iteration&quot;, y = &quot;weight_direct&quot;, color = &quot;Chain&quot;) + theme_minimal() + theme(plot.title = element_text(hjust = 0.5, face = &quot;bold&quot;)) trace_plot4 &lt;- ggplot(trace_data, aes(x = Iteration, y = weight_prop, color = factor(Chain))) + geom_line() + labs(title = paste(&quot;Trace Plot for weight_prop&quot;), x = &quot;Iteration&quot;, y = &quot;weight_direct&quot;, color = &quot;Chain&quot;) + theme_minimal() + theme(plot.title = element_text(hjust = 0.5, face = &quot;bold&quot;)) # Combine plots using patchwork combined_trace_plot &lt;- (trace_plot1 + trace_plot2) / (trace_plot3 + trace_plot4) + plot_annotation(title = paste(&quot;Trace Plots for&quot;, model_name)) # Return the plots return(combined_trace_plot) } # Generate diagnostic plots for each model create_diagnostic_plots(fit_weighted_balanced, &quot;Balanced Model&quot;) create_diagnostic_plots(fit_weighted_self_focused, &quot;Self-Focused Model&quot;) create_diagnostic_plots(fit_weighted_socially_influenced, &quot;Socially-Influenced Model&quot;) 11.14.2 Prior and Posterior Predictive Checks Prior predictive checks help us understand what our model assumes about the world before seeing any data. They answer the question: “What kind of data would we expect to see if we only used our prior beliefs?” Posterior predictive checks are the same, but after having seen the data. This helps us assess whether the model can generate data that looks similar to our observed data. plot_predictive_checks &lt;- function(stan_fit, simulation_results, model_name = &quot;Simple Balanced&quot;, param_name = &quot;prior_pred_choice&quot;) { # Extract predictive samples pred_samples &lt;- stan_fit$draws(param_name, format = &quot;data.frame&quot;) # Get the number of samples and observations n_samples &lt;- nrow(pred_samples) n_obs &lt;- ncol(pred_samples) - 3 # Subtract chain, iteration, and draw columns # Convert to long format long_pred &lt;- pred_samples %&gt;% dplyr::select(-.chain, -.iteration, -.draw) %&gt;% # Remove metadata columns pivot_longer( cols = everything(), names_to = &quot;obs_id&quot;, values_to = &quot;choice&quot; ) %&gt;% mutate(obs_id = parse_number(obs_id)) # Extract observation number # Join with the original simulation data to get evidence levels # First, add an observation ID to the simulation data sim_with_id &lt;- simulation_results %&gt;% mutate(obs_id = row_number()) # Join predictions with evidence levels long_pred_with_evidence &lt;- long_pred %&gt;% left_join( sim_with_id %&gt;% dplyr::select(obs_id, blue1, blue2), by = &quot;obs_id&quot; ) # Summarize proportion of 1s per evidence combination pred_summary &lt;- long_pred_with_evidence %&gt;% group_by(blue1, blue2) %&gt;% summarize( proportion = mean(choice, na.rm = TRUE), n = n(), se = sqrt((proportion * (1 - proportion)) / n), # Binomial SE lower = proportion - 1.96 * se, upper = proportion + 1.96 * se, .groups = &quot;drop&quot; ) # Generate title based on parameter name title &lt;- ifelse(param_name == &quot;prior_pred_choice&quot;, paste0(&quot;Prior Predictive Check for &quot;, model_name), paste0(&quot;Posterior Predictive Check for &quot;, model_name)) # Create plot ggplot(pred_summary, aes(x = blue1, y = proportion, color = factor(blue2), group = blue2)) + geom_line() + geom_point() + geom_ribbon(aes(ymin = lower, ymax = upper, fill = factor(blue2)), alpha = 0.2, color = NA) + ylim(0, 1) + labs(title = title, x = &quot;Direct Evidence (Blue Marbles)&quot;, y = &quot;Proportion of Choice = Blue&quot;, color = &quot;Social Evidence&quot;, fill = &quot;Social Evidence&quot;) + theme_minimal() } # Generate all plots prior_simple_balanced &lt;- plot_predictive_checks(fit_simple_balanced, simulation_results, &quot;Simple Balanced&quot;, &quot;prior_pred_choice&quot;) prior_simple_self_focused &lt;- plot_predictive_checks(fit_simple_self_focused, simulation_results, &quot;Simple Self Focused&quot;, &quot;prior_pred_choice&quot;) prior_simple_socially_influenced &lt;- plot_predictive_checks(fit_simple_socially_influenced, simulation_results, &quot;Simple Socially Influenced&quot;, &quot;prior_pred_choice&quot;) #prior_weighted_balanced &lt;- plot_predictive_checks(fit_weighted_balanced, simulation_results, &quot;Weighted Balanced&quot;, &quot;prior_pred_choice&quot;) #prior_weighted_self_focused &lt;- plot_predictive_checks(fit_weighted_self_focused, simulation_results, &quot;Weighted Self Focused&quot;, &quot;prior_pred_choice&quot;) #prior_weighted_socially_influenced &lt;- plot_predictive_checks(fit_weighted_socially_influenced, simulation_results, &quot;Weighted Socially Influenced&quot;, &quot;prior_pred_choice&quot;) posterior_simple_balanced &lt;- plot_predictive_checks(fit_simple_balanced, simulation_results, &quot;Simple Balanced&quot;, &quot;posterior_pred_choice&quot;) posterior_simple_self_focused &lt;- plot_predictive_checks(fit_simple_self_focused, simulation_results, &quot;Simple Self Focused&quot;, &quot;posterior_pred_choice&quot;) posterior_simple_socially_influenced &lt;- plot_predictive_checks(fit_simple_socially_influenced, simulation_results, &quot;Simple Socially Influenced&quot;, &quot;posterior_pred_choice&quot;) posterior_weighted_balanced &lt;- plot_predictive_checks(fit_weighted_balanced, simulation_results, &quot;Weighted Balanced&quot;, &quot;posterior_pred_choice&quot;) posterior_weighted_self_focused &lt;- plot_predictive_checks(fit_weighted_self_focused, simulation_results, &quot;Weighted Self Focused&quot;, &quot;posterior_pred_choice&quot;) posterior_weighted_socially_influenced &lt;- plot_predictive_checks(fit_weighted_socially_influenced, simulation_results, &quot;Weighted Socially Influenced&quot;, &quot;posterior_pred_choice&quot;) # Arrange Prior Predictive Checks in a Grid prior_grid &lt;- (prior_simple_balanced + prior_simple_self_focused + prior_simple_socially_influenced) +#/ (prior_weighted_balanced + prior_weighted_self_focused + prior_weighted_socially_influenced) + plot_annotation(title = &quot;Prior Predictive Checks&quot;) # Arrange Posterior Predictive Checks in a Grid posterior_grid &lt;- (posterior_simple_balanced + posterior_simple_self_focused + posterior_simple_socially_influenced) / (posterior_weighted_balanced + posterior_weighted_self_focused + posterior_weighted_socially_influenced) + plot_annotation(title = &quot;Posterior Predictive Checks&quot;) # Display the grids print(prior_grid) print(posterior_grid) 11.15 Prior-Posterior Update Visualization This visualization shows how our beliefs change after observing data, comparing the prior and posterior distributions for key parameters. # Function to plot prior-posterior updates for reparameterized model plot_reparameterized_updates &lt;- function(fit_list, true_params_list, model_names) { # Create dataframe for posterior values posterior_df &lt;- tibble() # Process each model for (i in seq_along(fit_list)) { fit &lt;- fit_list[[i]] model_name &lt;- model_names[i] # Extract posterior draws draws_df &lt;- as_draws_df(fit$draws()) # Check which parameterization is used (old or new) if (all(c(&quot;total_weight&quot;, &quot;weight_prop&quot;) %in% names(draws_df))) { # New parameterization - extract parameters directly temp_df &lt;- tibble( model_name = model_name, parameter = &quot;total_weight&quot;, value = draws_df$total_weight, distribution = &quot;Posterior&quot; ) posterior_df &lt;- bind_rows(posterior_df, temp_df) temp_df &lt;- tibble( model_name = model_name, parameter = &quot;weight_prop&quot;, value = draws_df$weight_prop, distribution = &quot;Posterior&quot; ) posterior_df &lt;- bind_rows(posterior_df, temp_df) # Also calculate the derived parameters for comparison with true values temp_df &lt;- tibble( model_name = model_name, parameter = &quot;weight_direct&quot;, value = draws_df$total_weight * draws_df$weight_prop, distribution = &quot;Posterior (derived)&quot; ) posterior_df &lt;- bind_rows(posterior_df, temp_df) temp_df &lt;- tibble( model_name = model_name, parameter = &quot;weight_social&quot;, value = draws_df$total_weight * (1 - draws_df$weight_prop), distribution = &quot;Posterior (derived)&quot; ) posterior_df &lt;- bind_rows(posterior_df, temp_df) } else if (all(c(&quot;weight_direct&quot;, &quot;weight_social&quot;) %in% names(draws_df))) { # Old parameterization - extract and calculate equivalent new parameters temp_df &lt;- tibble( model_name = model_name, parameter = &quot;weight_direct&quot;, value = draws_df$weight_direct, distribution = &quot;Posterior&quot; ) posterior_df &lt;- bind_rows(posterior_df, temp_df) temp_df &lt;- tibble( model_name = model_name, parameter = &quot;weight_social&quot;, value = draws_df$weight_social, distribution = &quot;Posterior&quot; ) posterior_df &lt;- bind_rows(posterior_df, temp_df) # Calculate the equivalent new parameters total_weight &lt;- draws_df$weight_direct + draws_df$weight_social weight_prop &lt;- draws_df$weight_direct / total_weight temp_df &lt;- tibble( model_name = model_name, parameter = &quot;total_weight&quot;, value = total_weight, distribution = &quot;Posterior (derived)&quot; ) posterior_df &lt;- bind_rows(posterior_df, temp_df) temp_df &lt;- tibble( model_name = model_name, parameter = &quot;weight_prop&quot;, value = weight_prop, distribution = &quot;Posterior (derived)&quot; ) posterior_df &lt;- bind_rows(posterior_df, temp_df) } else { warning(paste(&quot;Unknown parameterization in model&quot;, model_name)) } } # Generate prior samples based on recommended priors for new parameterization prior_df &lt;- tibble() for (i in seq_along(model_names)) { model_name &lt;- model_names[i] # Number of prior samples to match posterior n_samples &lt;- 2000 # Generate prior samples - gamma(2,1) for total_weight and beta(1,1) for weight_prop total_weight_prior &lt;- rgamma(n_samples, shape = 2, rate = 1) weight_prop_prior &lt;- rbeta(n_samples, 1, 1) # For the new parameterization temp_df &lt;- tibble( model_name = model_name, parameter = &quot;total_weight&quot;, value = total_weight_prior, distribution = &quot;Prior&quot; ) prior_df &lt;- bind_rows(prior_df, temp_df) temp_df &lt;- tibble( model_name = model_name, parameter = &quot;weight_prop&quot;, value = weight_prop_prior, distribution = &quot;Prior&quot; ) prior_df &lt;- bind_rows(prior_df, temp_df) # Calculate derived parameters for the old parameterization weight_direct_prior &lt;- total_weight_prior * weight_prop_prior weight_social_prior &lt;- total_weight_prior * (1 - weight_prop_prior) temp_df &lt;- tibble( model_name = model_name, parameter = &quot;weight_direct&quot;, value = weight_direct_prior, distribution = &quot;Prior (derived)&quot; ) prior_df &lt;- bind_rows(prior_df, temp_df) temp_df &lt;- tibble( model_name = model_name, parameter = &quot;weight_social&quot;, value = weight_social_prior, distribution = &quot;Prior (derived)&quot; ) prior_df &lt;- bind_rows(prior_df, temp_df) } # Combine prior and posterior combined_df &lt;- bind_rows(prior_df, posterior_df) # Convert true parameter values true_values_df &lt;- map2_dfr(true_params_list, model_names, function(params, model_name) { # Extract original parameters weight_direct &lt;- params$weight_direct weight_social &lt;- params$weight_social # Calculate new parameterization total_weight &lt;- weight_direct + weight_social weight_prop &lt;- weight_direct / total_weight tibble( model_name = model_name, parameter = c(&quot;weight_direct&quot;, &quot;weight_social&quot;, &quot;total_weight&quot;, &quot;weight_prop&quot;), value = c(weight_direct, weight_social, total_weight, weight_prop) ) }) # Create plots for different parameter sets # 1. New parameterization (total_weight and weight_prop) p1 &lt;- combined_df %&gt;% filter(parameter %in% c(&quot;total_weight&quot;, &quot;weight_prop&quot;)) %&gt;% ggplot(aes(x = value, fill = distribution, color = distribution)) + geom_density(alpha = 0.3, linewidth = 1.2) + facet_grid(model_name ~ parameter, scales = &quot;free&quot;) + scale_fill_manual(values = c(&quot;Prior&quot; = &quot;#E63946&quot;, &quot;Prior (derived)&quot; = &quot;#E67946&quot;, &quot;Posterior&quot; = &quot;#1D3557&quot;, &quot;Posterior (derived)&quot; = &quot;#1D5587&quot;)) + scale_color_manual(values = c(&quot;Prior&quot; = &quot;#E63946&quot;, &quot;Prior (derived)&quot; = &quot;#E67946&quot;, &quot;Posterior&quot; = &quot;#1D3557&quot;, &quot;Posterior (derived)&quot; = &quot;#1D5587&quot;)) + geom_vline(data = true_values_df %&gt;% filter(parameter %in% c(&quot;total_weight&quot;, &quot;weight_prop&quot;)), aes(xintercept = value), color = &quot;#2A9D8F&quot;, linetype = &quot;dashed&quot;, linewidth = 1.2) + labs(title = &quot;Prior vs. Posterior: New Parameterization&quot;, subtitle = &quot;Green dashed lines indicate true parameter values&quot;, x = &quot;Parameter Value&quot;, y = &quot;Density&quot;, fill = &quot;Distribution&quot;, color = &quot;Distribution&quot;) + theme_minimal(base_size = 14) + theme(legend.position = &quot;top&quot;) # 2. Original parameterization (weight_direct and weight_social) p2 &lt;- combined_df %&gt;% filter(parameter %in% c(&quot;weight_direct&quot;, &quot;weight_social&quot;)) %&gt;% ggplot(aes(x = value, fill = distribution, color = distribution)) + geom_density(alpha = 0.3, linewidth = 1.2) + facet_grid(model_name ~ parameter, scales = &quot;free&quot;) + scale_fill_manual(values = c(&quot;Prior&quot; = &quot;#E63946&quot;, &quot;Prior (derived)&quot; = &quot;#E67946&quot;, &quot;Posterior&quot; = &quot;#1D3557&quot;, &quot;Posterior (derived)&quot; = &quot;#1D5587&quot;)) + scale_color_manual(values = c(&quot;Prior&quot; = &quot;#E63946&quot;, &quot;Prior (derived)&quot; = &quot;#E67946&quot;, &quot;Posterior&quot; = &quot;#1D3557&quot;, &quot;Posterior (derived)&quot; = &quot;#1D5587&quot;)) + geom_vline(data = true_values_df %&gt;% filter(parameter %in% c(&quot;weight_direct&quot;, &quot;weight_social&quot;)), aes(xintercept = value), color = &quot;#2A9D8F&quot;, linetype = &quot;dashed&quot;, linewidth = 1.2) + labs(title = &quot;Prior vs. Posterior: Original Parameterization&quot;, subtitle = &quot;Green dashed lines indicate true parameter values&quot;, x = &quot;Parameter Value&quot;, y = &quot;Density&quot;, fill = &quot;Distribution&quot;, color = &quot;Distribution&quot;) + theme_minimal(base_size = 14) + theme(legend.position = &quot;top&quot;) # Return both plots return(list(new_params = p1, old_params = p2)) } fit_list &lt;- list( fit_weighted_balanced, fit_weighted_self_focused, fit_weighted_socially_influenced ) true_params_list &lt;- list( list(weight_direct = 1, weight_social = 1), list(weight_direct = 1.5, weight_social = 0.5), list(weight_direct = 0.7, weight_social = 2) ) model_names &lt;- c(&quot;Weighted Balanced&quot;, &quot;Weighted Self-Focused&quot;, &quot;Weighted Socially Influenced&quot;) # Generate the plots plots &lt;- plot_reparameterized_updates(fit_list, true_params_list, model_names) # Display the plots print(plots$new_params) print(plots$old_params) # Save the plots ggsave(&quot;prior_posterior_new_params.pdf&quot;, plots$new_params, width = 12, height = 10) ggsave(&quot;prior_posterior_old_params.pdf&quot;, plots$old_params, width = 12, height = 10) 11.16 Parameter recovery ## Parameter recovery # Set random seed for reproducibility set.seed(123) ## Set up parallel processing future::plan(multisession, workers = parallel::detectCores() - 1) # Define parameter grid for thorough testing weight_values &lt;- c(0, 0.2, 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2) n_trials &lt;- c(1, 2, 3, 4, 5) # Number of times full combination of levels is repeated # Create a grid of all parameter combinations to test param_grid &lt;- expand_grid(w1 = weight_values, w2 = weight_values, trials = n_trials) # Define evidence combinations evidence_combinations &lt;- expand_grid( blue1 = 0:8, # Direct evidence: number of blue marbles seen blue2 = 0:3, # Social evidence: strength of blue evidence total1 = 8, # Total marbles in direct evidence (constant) total2 = 3 # Total strength units in social evidence (constant) ) # Function to generate decisions across all evidence combinations for a given agent generate_agent_decisions &lt;- function(weight_direct, weight_social, evidence_df, n_samples = 5) { # Create a data frame that repeats each evidence combination n_samples times repeated_evidence &lt;- evidence_df %&gt;% slice(rep(1:n(), each = n_samples)) %&gt;% # Add a sample_id to distinguish between repetitions of the same combination group_by(blue1, blue2, total1, total2) %&gt;% mutate(sample_id = 1:n()) %&gt;% ungroup() # Apply our weighted Bayesian model to each evidence combination decisions &lt;- pmap_dfr(repeated_evidence, function(blue1, blue2, total1, total2, sample_id) { # Calculate Bayesian integration with the agent&#39;s specific weights result &lt;- weightedBetaBinomial( alpha_prior = 1, beta_prior = 1, blue1 = blue1, total1 = total1, blue2 = blue2, total2 = total2, weight_direct = weight_direct, weight_social = weight_social ) # Return key decision metrics tibble( sample_id = sample_id, blue1 = blue1, blue2 = blue2, total1 = total1, total2 = total2, expected_rate = result$expected_rate, # Probability the next marble is blue choice = result$decision, # Final decision (Blue or Red) choice_binary = ifelse(result$decision == &quot;Blue&quot;, 1, 0), confidence = result$confidence # Confidence in decision ) }) return(decisions) } # Function to prepare Stan data prepare_stan_data &lt;- function(df) { list( N = nrow(df), choice = df$choice_binary, blue1 = df$blue1, total1 = df$total1, blue2 = df$blue2, total2 = df$total2 ) } # Compile the Stan model file_weighted &lt;- file.path(&quot;stan/W10 _weighted_beta_binomial.stan&quot;) mod_weighted &lt;- cmdstan_model(file_weighted, cpp_options = list(stan_threads = TRUE)) # Function to fit model using cmdstanr fit_model &lt;- function(data) { stan_data &lt;- prepare_stan_data(data) fit &lt;- mod_weighted$sample( data = stan_data, seed = 126, chains = 2, parallel_chains = 2, threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 1000, refresh = 0 ) return(fit) } # Run simulations and model fitting in parallel results &lt;- param_grid %&gt;% mutate( # Generate synthetic data for each parameter combination data = future_pmap(list(w1, w2, trials), function(w1, w2, t) { generate_agent_decisions(w1, w2, evidence_combinations, t) }, .options = furrr_options(seed = TRUE)), # Fit model to each dataset fit = future_map(data, fit_model, .progress = TRUE) ) ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.4 seconds. ## Chain 2 finished in 1.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.3 seconds. ## Total execution time: 1.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.5 seconds. ## Chain 1 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.0 seconds. ## Chain 2 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.1 seconds. ## Chain 2 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 1.0 seconds. ## Chain 1 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.0 seconds. ## Chain 2 finished in 1.1 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.2 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.0 seconds. ## Chain 2 finished in 1.1 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.1 seconds. ## Total execution time: 1.2 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.7 seconds. ## Chain 1 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.0 seconds. ## Chain 2 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.1 seconds. ## Chain 2 finished in 1.1 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.1 seconds. ## Total execution time: 1.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.2 seconds. ## Chain 2 finished in 1.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.2 seconds. ## Total execution time: 1.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.3 seconds. ## Chain 1 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.0 seconds. ## Chain 2 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.0 seconds. ## Chain 2 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.4 seconds. ## Chain 1 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.7 seconds. ## Chain 1 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.2 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.8 seconds. ## Chain 1 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.1 seconds. ## Chain 2 finished in 1.1 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.1 seconds. ## Total execution time: 1.2 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.9 seconds. ## Chain 1 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.0 seconds. ## Chain 2 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.0 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.2 seconds. ## Chain 2 finished in 1.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.3 seconds. ## Total execution time: 1.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.2 seconds. ## Chain 2 finished in 1.1 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.1 seconds. ## Total execution time: 1.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.1 seconds. ## Chain 2 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.1 seconds. ## Total execution time: 1.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.7 seconds. ## Chain 1 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.9 seconds. ## Chain 1 finished in 1.1 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.2 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.7 seconds. ## Chain 1 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.0 seconds. ## Chain 2 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.2 seconds. ## Chain 1 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.5 seconds. ## Chain 1 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.5 seconds. ## Chain 1 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.9 seconds. ## Chain 1 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.6 seconds. ## Chain 1 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.0 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.5 seconds. ## Chain 1 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.0 seconds. ## Chain 2 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.4 seconds. ## Chain 1 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 1.0 seconds. ## Chain 1 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 1.0 seconds. ## Chain 1 finished in 1.1 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.0 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.0 seconds. ## Chain 2 finished in 1.1 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.1 seconds. ## Total execution time: 1.2 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.6 seconds. ## Chain 1 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.0 seconds. ## Chain 2 finished in 1.1 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.1 seconds. ## Total execution time: 1.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.1 seconds. ## Chain 2 finished in 1.1 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.1 seconds. ## Total execution time: 1.2 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.4 seconds. ## Chain 2 finished in 1.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.4 seconds. ## Total execution time: 1.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.7 seconds. ## Chain 1 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.0 seconds. ## Chain 2 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.2 seconds. ## Chain 1 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 1.0 seconds. ## Chain 1 finished in 1.1 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.1 seconds. ## Total execution time: 1.2 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.0 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.2 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.0 seconds. ## Chain 2 finished in 1.1 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.1 seconds. ## Total execution time: 1.2 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.5 seconds. ## Chain 1 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 1.0 seconds. ## Chain 1 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.2 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.4 seconds. ## Chain 1 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.1 seconds. ## Chain 2 finished in 1.1 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.1 seconds. ## Total execution time: 1.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.7 seconds. ## Chain 1 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.0 seconds. ## Chain 2 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 1.1 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.2 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.6 seconds. ## Chain 1 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.0 seconds. ## Chain 2 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.0 seconds. ## Chain 2 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.2 seconds. ## Chain 1 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 1.0 seconds. ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.9 seconds. ## Chain 1 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.0 seconds. ## Chain 2 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.1 seconds. ## Chain 2 finished in 1.1 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.1 seconds. ## Total execution time: 1.2 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.0 seconds. ## Chain 2 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.1 seconds. ## Chain 2 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.7 seconds. ## Chain 1 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.0 seconds. ## Chain 2 finished in 1.1 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.2 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.9 seconds. ## Chain 1 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.2 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.3 seconds. ## Chain 1 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.3 seconds. ## Chain 1 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.6 seconds. ## Chain 1 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.5 seconds. ## Chain 1 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.4 seconds. ## Chain 1 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.5 seconds. ## Chain 1 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.8 seconds. ## Chain 1 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.2 seconds. ## Chain 1 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.8 seconds. ## Chain 1 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.5 seconds. ## Chain 1 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.0 seconds. ## Chain 2 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.1 seconds. ## Chain 2 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.2 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.1 seconds. ## Chain 2 finished in 1.1 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.1 seconds. ## Total execution time: 1.2 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.9 seconds. ## Chain 1 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.2 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.8 seconds. ## Chain 1 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.7 seconds. ## Chain 1 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.5 seconds. ## Chain 1 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.7 seconds. ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.9 seconds. ## Chain 1 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.2 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.5 seconds. ## Chain 1 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.7 seconds. ## Chain 1 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.0 seconds. ## Chain 2 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.4 seconds. ## Chain 1 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.3 seconds. ## Chain 2 finished in 1.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.2 seconds. ## Total execution time: 1.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.8 seconds. ## Chain 1 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.1 seconds. ## Chain 2 finished in 1.1 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.1 seconds. ## Total execution time: 1.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.9 seconds. ## Chain 1 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.0 seconds. ## Chain 2 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.9 seconds. ## Chain 1 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.2 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.6 seconds. ## Chain 1 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.0 seconds. ## Chain 2 finished in 1.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.1 seconds. ## Total execution time: 1.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.0 seconds. ## Chain 2 finished in 1.1 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.1 seconds. ## Total execution time: 1.2 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 1.2 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.1 seconds. ## Chain 2 finished in 1.1 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.1 seconds. ## Total execution time: 1.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.5 seconds. ## Chain 1 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.8 seconds. ## Chain 1 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.7 seconds. ## Chain 1 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.5 seconds. ## Chain 1 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.8 seconds. ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.7 seconds. ## Chain 1 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.5 seconds. ## Chain 1 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.0 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.8 seconds. ## Chain 1 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.8 seconds. ## Chain 1 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.3 seconds. ## Chain 2 finished in 1.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.3 seconds. ## Total execution time: 1.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 1.0 seconds. ## Chain 2 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.2 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 1.1 seconds. ## Chain 1 finished in 1.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.2 seconds. ## Total execution time: 1.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 1.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 1.1 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.6 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.9 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.5 seconds. ## Chain 1 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.3 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.3 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.6 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.6 seconds. ## Total execution time: 0.7 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.9 seconds. ## Chain 2 finished in 0.9 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.9 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 2 finished in 0.8 seconds. ## Chain 1 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 1.0 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.4 seconds. ## Chain 2 finished in 0.4 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.4 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. ## Chain 2 finished in 0.5 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 0.6 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Chain 2 finished in 0.7 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.7 seconds. ## Total execution time: 0.8 seconds. ## ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.8 seconds. ## Chain 2 finished in 0.8 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 0.9 seconds. # Extract both old and new parameterization results results &lt;- results %&gt;% mutate( # First extract parameters in the new parameterization total_weight_est = map_dbl(fit, ~mean(as_draws_df(.x$draws())$total_weight)), weight_prop_est = map_dbl(fit, ~mean(as_draws_df(.x$draws())$weight_prop)), # Calculate the traditional parameters from the new parameterization weight_direct_est = total_weight_est * weight_prop_est, weight_social_est = total_weight_est * (1 - weight_prop_est), # Calculate the true parameters in the new parameterization true_total_weight = w1 + w2, true_weight_prop = w1 / (w1 + w2), # Also extract uncertainty estimates total_weight_sd = map_dbl(fit, ~sd(as_draws_df(.x$draws())$total_weight)), weight_prop_sd = map_dbl(fit, ~sd(as_draws_df(.x$draws())$weight_prop)), weight_direct_sd = map_dbl(fit, function(x) { draws &lt;- as_draws_df(x$draws()) sd(draws$total_weight * draws$weight_prop) }), weight_social_sd = map_dbl(fit, function(x) { draws &lt;- as_draws_df(x$draws()) sd(draws$total_weight * (1 - draws$weight_prop)) }) ) # Create functions to visualize parameter recovery for both parameterizations plot_recovery_original &lt;- function(results_df) { # Visualize direct weight recovery p1 &lt;- results_df %&gt;% ggplot(aes(x = w1, y = weight_direct_est, color = factor(trials))) + geom_point() + geom_errorbar(aes(ymin = weight_direct_est - weight_direct_sd, ymax = weight_direct_est + weight_direct_sd), width = 0.1, alpha = 0.5) + geom_abline(slope = 1, intercept = 0, linetype = &quot;dashed&quot;) + facet_wrap(~ w2, labeller = labeller(w2 = function(x) paste(&quot;Social Weight =&quot;, x))) + labs(title = &quot;Direct Weight Parameter Recovery&quot;, x = &quot;True Direct Weight&quot;, y = &quot;Estimated Direct Weight&quot;, color = &quot;Number of\\nTrials per\\nCondition&quot;) + theme_minimal() + theme(legend.position = &quot;right&quot;) # Visualize social weight recovery p2 &lt;- results_df %&gt;% ggplot(aes(x = w2, y = weight_social_est, color = factor(trials))) + geom_point() + geom_errorbar(aes(ymin = weight_social_est - weight_social_sd, ymax = weight_social_est + weight_social_sd), width = 0.1, alpha = 0.5) + geom_abline(slope = 1, intercept = 0, linetype = &quot;dashed&quot;) + facet_wrap(~ w1, labeller = labeller(w1 = function(x) paste(&quot;Direct Weight =&quot;, x))) + labs(title = &quot;Social Weight Parameter Recovery&quot;, x = &quot;True Social Weight&quot;, y = &quot;Estimated Social Weight&quot;, color = &quot;Number of\\nTrials per\\nCondition&quot;) + theme_minimal() + theme(legend.position = &quot;right&quot;) return(list(direct = p1, social = p2)) } plot_recovery_new &lt;- function(results_df) { # Visualize total weight recovery # First, create discrete categories for weight proportion to avoid using continuous variable in facet_wrap results_with_categories &lt;- results_df %&gt;% filter(true_total_weight &gt; 0) %&gt;% # Avoid division by zero issues mutate(weight_prop_cat = cut(true_weight_prop, breaks = c(0, 0.25, 0.5, 0.75, 1.0), labels = c(&quot;0-0.25&quot;, &quot;0.25-0.5&quot;, &quot;0.5-0.75&quot;, &quot;0.75-1.0&quot;), include.lowest = TRUE)) p1 &lt;- results_with_categories %&gt;% ggplot(aes(x = true_total_weight, y = total_weight_est, color = factor(trials))) + geom_point() + geom_errorbar(aes(ymin = total_weight_est - total_weight_sd, ymax = total_weight_est + total_weight_sd), width = 0.1, alpha = 0.5) + geom_abline(slope = 1, intercept = 0, linetype = &quot;dashed&quot;) + facet_wrap(~ weight_prop_cat, labeller = labeller( weight_prop_cat = function(x) paste(&quot;Weight Proportion =&quot;, x) )) + labs(title = &quot;Total Weight Parameter Recovery&quot;, x = &quot;True Total Weight&quot;, y = &quot;Estimated Total Weight&quot;, color = &quot;Number of\\nTrials per\\nCondition&quot;) + theme_minimal() + theme(legend.position = &quot;right&quot;) # Visualize weight proportion recovery # Create discrete categories for total weight results_with_categories &lt;- results_df %&gt;% filter(true_total_weight &gt; 0) %&gt;% # Avoid division by zero issues mutate(total_weight_cat = cut(true_total_weight, breaks = c(0, 0.5, 1.0, 1.5, 2.0), labels = c(&quot;0-0.5&quot;, &quot;0.5-1.0&quot;, &quot;1.0-1.5&quot;, &quot;1.5-2.0&quot;), include.lowest = TRUE)) p2 &lt;- results_with_categories %&gt;% ggplot(aes(x = true_weight_prop, y = weight_prop_est, color = factor(trials))) + geom_point() + geom_errorbar(aes(ymin = weight_prop_est - weight_prop_sd, ymax = weight_prop_est + weight_prop_sd), width = 0.01, alpha = 0.5) + geom_abline(slope = 1, intercept = 0, linetype = &quot;dashed&quot;) + facet_wrap(~ total_weight_cat, labeller = labeller( total_weight_cat = function(x) paste(&quot;Total Weight =&quot;, x) )) + labs(title = &quot;Weight Proportion Parameter Recovery&quot;, x = &quot;True Weight Proportion&quot;, y = &quot;Estimated Weight Proportion&quot;, color = &quot;Number of\\nTrials per\\nCondition&quot;) + theme_minimal() + theme(legend.position = &quot;right&quot;) return(list(total = p1, prop = p2)) } # Generate plots original_recovery_plots &lt;- plot_recovery_original(results) new_recovery_plots &lt;- plot_recovery_new(results) # Display plots for new parameterization new_recovery_plots$total new_recovery_plots$prop # Display plots for original parameterization original_recovery_plots$direct original_recovery_plots$social # Analysis of recovery quality by parameter combination recovery_summary &lt;- results %&gt;% mutate( # Calculate error metrics for original parameterization error_direct = abs(weight_direct_est - w1), error_social = abs(weight_social_est - w2), rel_error_direct = ifelse(w1 &gt; 0, error_direct / w1, NA), rel_error_social = ifelse(w2 &gt; 0, error_social / w2, NA), # Calculate error metrics for new parameterization error_total = abs(total_weight_est - true_total_weight), error_prop = abs(weight_prop_est - true_weight_prop), rel_error_total = ifelse(true_total_weight &gt; 0, error_total / true_total_weight, NA), rel_error_prop = ifelse(true_weight_prop &gt; 0, error_prop / true_weight_prop, NA) ) %&gt;% group_by(trials) %&gt;% summarize( mean_error_direct = mean(error_direct, na.rm = TRUE), mean_error_social = mean(error_social, na.rm = TRUE), mean_rel_error_direct = mean(rel_error_direct, na.rm = TRUE), mean_rel_error_social = mean(rel_error_social, na.rm = TRUE), mean_error_total = mean(error_total, na.rm = TRUE), mean_error_prop = mean(error_prop, na.rm = TRUE), mean_rel_error_total = mean(rel_error_total, na.rm = TRUE), mean_rel_error_prop = mean(rel_error_prop, na.rm = TRUE), .groups = &quot;drop&quot; ) # Display summary table knitr::kable(recovery_summary, digits = 3, caption = &quot;Parameter Recovery Quality by Number of Trials&quot;) Table 11.1: Parameter Recovery Quality by Number of Trials trials mean_error_direct mean_error_social mean_rel_error_direct mean_rel_error_social mean_error_total mean_error_prop mean_rel_error_total mean_rel_error_prop 1 0.436 0.560 0.652 0.833 0.784 0.151 0.774 0.339 2 0.431 0.524 0.742 0.742 0.795 0.126 0.768 0.315 3 0.404 0.487 0.627 0.740 0.780 0.102 0.764 0.211 4 0.389 0.442 0.611 0.736 0.740 0.097 0.719 0.208 5 0.350 0.451 0.569 0.666 0.721 0.087 0.690 0.184 # Create a summary visualization showing how recovery improves with more trials p_summary &lt;- recovery_summary %&gt;% pivot_longer( cols = starts_with(&quot;mean_&quot;), names_to = &quot;metric&quot;, values_to = &quot;value&quot; ) %&gt;% mutate( parameter_type = case_when( grepl(&quot;direct&quot;, metric) ~ &quot;Direct Weight&quot;, grepl(&quot;social&quot;, metric) ~ &quot;Social Weight&quot;, grepl(&quot;total&quot;, metric) ~ &quot;Total Weight&quot;, grepl(&quot;prop&quot;, metric) ~ &quot;Weight Proportion&quot; ), error_type = case_when( grepl(&quot;rel_error&quot;, metric) ~ &quot;Relative Error&quot;, TRUE ~ &quot;Absolute Error&quot; ) ) %&gt;% ggplot(aes(x = trials, y = value, color = parameter_type, linetype = error_type)) + geom_line(size = 1) + geom_point(size = 3) + facet_wrap(~ error_type, scales = &quot;free_y&quot;) + labs( title = &quot;Parameter Recovery Improvement with Increased Trials&quot;, x = &quot;Number of Trials per Evidence Combination&quot;, y = &quot;Mean Error&quot;, color = &quot;Parameter&quot;, linetype = &quot;Error Type&quot; ) + theme_minimal() p_summary 11.17 Model comparison 11.18 Leave-One-Out Cross-Validation and Model Comparison In this section, we’ll explore how to compare the simple Bayesian agent (where weights are equal) and the weighted Bayesian agent (where weights can differ) using Leave-One-Out Cross-Validation (LOO-CV). We’ll leverage the models we’ve already fitted to our three simulated agent types: Balanced, Self-Focused, and Socially-Influenced. 11.18.1 Understanding LOO Cross-Validation in Bayesian Framework LOO-CV is a powerful method for model comparison that estimates how well a model will predict new, unseen data. At its core, LOO-CV works by: Leaving out one observation at a time Fitting the model on all remaining observations Predicting the left-out observation using that model Repeating for all observations and aggregating the results In a Bayesian context, exact LOO-CV would require refitting our model N times (where N is the number of observations), which is computationally expensive. Instead, we use Pareto-Smoothed Importance Sampling (PSIS-LOO), which approximates LOO-CV from a single model fit. The key insight of PSIS-LOO is that we can use importance sampling to approximate how the posterior would change if an observation were removed: \\[p(\\theta | y_{-i}) \\approx \\frac{p(\\theta | y)}{p(y_i | \\theta)} \\propto \\frac{p(\\theta | y)}{p(y_i | \\theta)}\\] where \\(p(\\theta | y_{-i})\\) is the posterior without observation \\(i\\), and \\(p(\\theta | y)\\) is the full posterior. 11.18.2 Step-by-Step Implementation of LOO-CV Let’s apply LOO-CV to compare our models across the three scenarios. # Load the loo package library(loo) # Function to extract log-likelihood and compute LOO compute_loo &lt;- function(model_fit) { # Extract log-likelihood values log_lik &lt;- model_fit$draws(&quot;log_lik&quot;, format = &quot;matrix&quot;) # Compute LOO-CV using PSIS loo_result &lt;- loo(log_lik) return(loo_result) } # Compute LOO for each model and scenario loo_simple_balanced &lt;- compute_loo(fit_simple_balanced) loo_simple_self_focused &lt;- compute_loo(fit_simple_self_focused) loo_simple_socially_influenced &lt;- compute_loo(fit_simple_socially_influenced) loo_weighted_balanced &lt;- compute_loo(fit_weighted_balanced) loo_weighted_self_focused &lt;- compute_loo(fit_weighted_self_focused) loo_weighted_socially_influenced &lt;- compute_loo(fit_weighted_socially_influenced) 11.18.3 Understanding PSIS-LOO Diagnostics Before we compare models, it’s important to check the reliability of our LOO estimates. PSIS-LOO provides diagnostics through the Pareto k values: # Function to check Pareto k diagnostics check_pareto_k &lt;- function(loo_result, model_name) { # Extract Pareto k values pareto_k &lt;- loo_result$diagnostics$pareto_k # Count problematic k values n_k_high &lt;- sum(pareto_k &gt; 0.7) n_k_medium &lt;- sum(pareto_k &gt; 0.5 &amp; pareto_k &lt;= 0.7) # Proportion of problematic observations prop_problematic &lt;- (n_k_high + n_k_medium) / length(pareto_k) # Create diagnostic summary summary_df &lt;- tibble( model = model_name, total_obs = length(pareto_k), k_high = n_k_high, k_medium = n_k_medium, prop_problematic = prop_problematic, reliability = case_when( prop_problematic == 0 ~ &quot;Excellent&quot;, prop_problematic &lt; 0.05 ~ &quot;Good&quot;, prop_problematic &lt; 0.1 ~ &quot;Fair&quot;, TRUE ~ &quot;Poor&quot; ) ) return(summary_df) } # Check diagnostics for all models diagnostics &lt;- bind_rows( check_pareto_k(loo_simple_balanced, &quot;Simple - Balanced&quot;), check_pareto_k(loo_simple_self_focused, &quot;Simple - Self-Focused&quot;), check_pareto_k(loo_simple_socially_influenced, &quot;Simple - Socially-Influenced&quot;), check_pareto_k(loo_weighted_balanced, &quot;Weighted - Balanced&quot;), check_pareto_k(loo_weighted_self_focused, &quot;Weighted - Self-Focused&quot;), check_pareto_k(loo_weighted_socially_influenced, &quot;Weighted - Socially-Influenced&quot;) ) # Display diagnostics table knitr::kable(diagnostics, digits = 3, caption = &quot;PSIS-LOO Reliability Diagnostics&quot;) Table 11.2: PSIS-LOO Reliability Diagnostics model total_obs k_high k_medium prop_problematic reliability Simple - Balanced 180 180 0 1 Poor Simple - Self-Focused 180 180 0 1 Poor Simple - Socially-Influenced 180 180 0 1 Poor Weighted - Balanced 180 0 0 0 Excellent Weighted - Self-Focused 180 0 0 0 Excellent Weighted - Socially-Influenced 180 0 0 0 Excellent 11.18.4 Model Comparison for Each Scenario Now we can compare the models within each scenario: # Function to compare models and create visualization compare_scenario_models &lt;- function(loo_simple, loo_weighted, scenario_name) { # Compare models comparison &lt;- loo_compare(loo_simple, loo_weighted) # Calculate model weights weights &lt;- loo_model_weights(list( &quot;Simple Bayesian&quot; = loo_simple, &quot;Weighted Bayesian&quot; = loo_weighted )) # Print comparison cat(&quot;\\nModel comparison for&quot;, scenario_name, &quot;scenario:\\n&quot;) print(comparison) # Print weights cat(&quot;\\nModel weights for&quot;, scenario_name, &quot;scenario:\\n&quot;) print(weights) # Create comparison dataframe comparison_df &lt;- as.data.frame(comparison) comparison_df$model &lt;- rownames(comparison_df) rownames(comparison_df) &lt;- NULL comparison_df$scenario &lt;- scenario_name # Create weights dataframe weights_df &lt;- tibble( model = names(weights), weight = as.numeric(weights), scenario = scenario_name ) # Return both dataframes return(list(comparison = comparison_df, weights = weights_df)) } # Perform comparisons for each scenario balanced_comparison &lt;- compare_scenario_models( loo_simple_balanced, loo_weighted_balanced, &quot;Balanced&quot; ) ## ## Model comparison for Balanced scenario: ## elpd_diff se_diff ## model1 0.0 0.0 ## model2 -1.0 0.3 ## ## Model weights for Balanced scenario: ## Method: stacking ## ------ ## weight ## Simple Bayesian 1.000 ## Weighted Bayesian 0.000 self_focused_comparison &lt;- compare_scenario_models( loo_simple_self_focused, loo_weighted_self_focused, &quot;Self-Focused&quot; ) ## ## Model comparison for Self-Focused scenario: ## elpd_diff se_diff ## model2 0.0 0.0 ## model1 -3.1 2.6 ## ## Model weights for Self-Focused scenario: ## Method: stacking ## ------ ## weight ## Simple Bayesian 0.085 ## Weighted Bayesian 0.915 socially_influenced_comparison &lt;- compare_scenario_models( loo_simple_socially_influenced, loo_weighted_socially_influenced, &quot;Socially-Influenced&quot; ) ## ## Model comparison for Socially-Influenced scenario: ## elpd_diff se_diff ## model2 0.0 0.0 ## model1 -6.2 3.5 ## ## Model weights for Socially-Influenced scenario: ## Method: stacking ## ------ ## weight ## Simple Bayesian 0.000 ## Weighted Bayesian 1.000 # Combine comparison results all_comparisons &lt;- bind_rows( balanced_comparison$comparison, self_focused_comparison$comparison, socially_influenced_comparison$comparison ) all_weights &lt;- bind_rows( balanced_comparison$weights, self_focused_comparison$weights, socially_influenced_comparison$weights ) 11.18.5 Visualizing the Comparison Results Let’s create informative visualizations to better understand the comparison results: # Plot ELPD differences p1 &lt;- ggplot(all_comparisons, aes(x = model, y = elpd_diff, fill = model)) + geom_col() + geom_errorbar(aes(ymin = elpd_diff - se_diff, ymax = elpd_diff + se_diff), width = 0.2) + facet_wrap(~ scenario, scales = &quot;free_y&quot;) + labs( title = &quot;Model Comparison via LOO-CV&quot;, subtitle = &quot;Higher ELPD difference is better; error bars show ±1 SE&quot;, x = NULL, y = &quot;ELPD Difference&quot; ) + scale_fill_brewer(palette = &quot;Set1&quot;) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) # Plot model weights p2 &lt;- ggplot(all_weights, aes(x = model, y = weight, fill = model)) + geom_col() + geom_text(aes(label = scales::percent(weight, accuracy = 0.1)), vjust = -0.5, size = 4) + facet_wrap(~ scenario) + labs( title = &quot;Model Weights Based on LOO-CV&quot;, subtitle = &quot;Higher weights indicate better predictive performance&quot;, x = NULL, y = &quot;Model Weight&quot; ) + scale_fill_brewer(palette = &quot;Set1&quot;) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) + ylim(0, 1) # Display plots p1 + p2 # Create a summary table of results summary_table &lt;- all_weights %&gt;% pivot_wider(names_from = model, values_from = weight) %&gt;% mutate( winning_model = case_when( `Simple Bayesian` &gt; `Weighted Bayesian` ~ &quot;Simple Bayesian&quot;, `Weighted Bayesian` &gt; `Simple Bayesian` ~ &quot;Weighted Bayesian&quot;, TRUE ~ &quot;Tie&quot; ), weight_difference = abs(`Simple Bayesian` - `Weighted Bayesian`), evidence_strength = case_when( weight_difference &lt; 0.1 ~ &quot;Weak&quot;, weight_difference &lt; 0.3 ~ &quot;Moderate&quot;, weight_difference &lt; 0.6 ~ &quot;Strong&quot;, TRUE ~ &quot;Very Strong&quot; ) ) # Display summary table knitr::kable(summary_table, digits = 3, caption = &quot;Summary of Model Comparison Results&quot;) Table 11.3: Summary of Model Comparison Results scenario Simple Bayesian Weighted Bayesian winning_model weight_difference evidence_strength Balanced 1.000 0.000 Simple Bayesian 1.00 Very Strong Self-Focused 0.085 0.915 Weighted Bayesian 0.83 Very Strong Socially-Influenced 0.000 1.000 Weighted Bayesian 1.00 Very Strong 11.18.6 Understanding the Results Now let’s take a deeper look at what these LOO comparisons tell us: 11.18.6.1 1. Balanced Agent Scenario In the Balanced Agent scenario (where both direct and social evidence are weighted equally), we expect the simple Bayesian model to perform well, since it assumes equal weights by design. If our LOO comparison shows the weighted model doesn’t provide much advantage, this confirms our expectations - the additional complexity of differential weighting isn’t justified when the true process gives equal weight to evidence sources. 11.18.6.2 2. Self-Focused Agent Scenario For the Self-Focused Agent (who overweights direct evidence and underweights social evidence), we expect the weighted Bayesian model to outperform the simple model. If the LOO comparison shows a substantial advantage for the weighted model, it suggests that capturing the differential weighting of evidence is important for predicting this agent’s behavior. 11.18.6.3 3. Socially-Influenced Agent Scenario Similarly, for the Socially-Influenced Agent (who overweights social evidence), we expect the weighted model to have an advantage. The size of this advantage indicates how crucial it is to account for the specific weighting pattern to understand this agent’s decision-making process. 11.18.7 The Mathematics Behind LOO-CV Let’s look at the mathematical foundations of LOO-CV to better understand what’s happening: Log Predictive Density: For each observation \\(i\\), the log predictive density is: \\[\\log p(y_i | y_{-i}) = \\log \\int p(y_i | \\theta) p(\\theta | y_{-i}) d\\theta\\] This represents how well we can predict observation \\(i\\) using a model trained on all other observations. PSIS-LOO Approximation: Since we don’t want to refit our model for each observation, we use importance sampling: \\[\\log p(y_i | y_{-i}) \\approx \\log \\frac{\\sum_{j=1}^S w_i^j p(y_i | \\theta^j)}{\\sum_{j=1}^S w_i^j}\\] where \\(w_i^j \\propto \\frac{1}{p(y_i | \\theta^j)}\\) are importance weights and \\(\\theta^j\\) are samples from the full posterior. Expected Log Predictive Density (ELPD): The overall measure of model predictive accuracy is: \\[\\text{ELPD} = \\sum_{i=1}^N \\log p(y_i | y_{-i})\\] Higher ELPD values indicate better predictive performance. 11.18.8 Examining Pointwise Contributions to LOO To understand where model differences arise, we can look at the pointwise contributions to LOO: # Extract pointwise values pointwise_balanced &lt;- tibble( observation = 1:length(loo_simple_balanced$pointwise[,&quot;elpd_loo&quot;]), simple = loo_simple_balanced$pointwise[,&quot;elpd_loo&quot;], weighted = loo_weighted_balanced$pointwise[,&quot;elpd_loo&quot;], difference = weighted - simple, scenario = &quot;Balanced&quot; ) pointwise_self_focused &lt;- tibble( observation = 1:length(loo_simple_self_focused$pointwise[,&quot;elpd_loo&quot;]), simple = loo_simple_self_focused$pointwise[,&quot;elpd_loo&quot;], weighted = loo_weighted_self_focused$pointwise[,&quot;elpd_loo&quot;], difference = weighted - simple, scenario = &quot;Self-Focused&quot; ) pointwise_socially_influenced &lt;- tibble( observation = 1:length(loo_simple_socially_influenced$pointwise[,&quot;elpd_loo&quot;]), simple = loo_simple_socially_influenced$pointwise[,&quot;elpd_loo&quot;], weighted = loo_weighted_socially_influenced$pointwise[,&quot;elpd_loo&quot;], difference = weighted - simple, scenario = &quot;Socially-Influenced&quot; ) # Combine pointwise data all_pointwise &lt;- bind_rows( pointwise_balanced, pointwise_self_focused, pointwise_socially_influenced ) # Plot pointwise differences ggplot(all_pointwise, aes(x = observation, y = difference)) + geom_col(aes(fill = difference &gt; 0)) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + facet_wrap(~ scenario, scales = &quot;free_x&quot;) + scale_fill_manual(values = c(&quot;TRUE&quot; = &quot;green4&quot;, &quot;FALSE&quot; = &quot;firebrick&quot;), name = &quot;Weighted Better?&quot;) + labs( title = &quot;Pointwise Differences in ELPD Between Models&quot;, subtitle = &quot;Green bars indicate observations where the weighted model performs better&quot;, x = &quot;Observation&quot;, y = &quot;ELPD Difference (Weighted - Simple)&quot; ) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) 11.19 Multilevel Bayesian Models In the previous sections, we explored how individuals integrate direct and social evidence using Bayesian principles. However, our models assumed that all individuals use the same weighting strategy. In reality, people vary in how they weigh different sources of information - some may trust their own observations more, while others may be more influenced by social information. Multilevel (hierarchical) models allow us to capture this individual variation while still leveraging the commonalities across individuals. They offer several advantages: They model individual differences explicitly They improve parameter estimation for individuals with limited data They allow us to examine correlations between individual parameters They provide population-level insights about general tendencies In this section, we’ll develop multilevel versions of both our simple beta-binomial and weighted beta-binomial models. 11.19.1 Simulating Data from Multiple Agents First, let’s simulate a population of agents with varying evidence-weighting parameters: # Simulation parameters n_agents &lt;- 20 # Number of agents per model n_trials_per_agent &lt;- 36 # Number of evidence combinations per agent # Define population parameters for simple model (equal weights with varying scaling) simple_population_scaling_mean &lt;- 1.0 # Mean scaling factor (log-scale) simple_population_scaling_sd &lt;- 0.3 # SD of scaling factor (log-scale) # Define population parameters for weighted model weighted_population_scaling_mean &lt;- 1.5 # Mean scaling factor (log-scale) weighted_population_scaling_sd &lt;- 0.3 # SD of scaling factor (log-scale) weighted_population_ratio_mean &lt;- 1 # Mean weight ratio (log-scale, 0 = equal weights) weighted_population_ratio_sd &lt;- 0.5 # SD of weight ratio (log-scale) # Generate agent parameters for simple model simple_agents &lt;- tibble( agent_id = 1:n_agents, model_type = &quot;simple&quot;, # Generate log-normal scaling factors log_scaling = rnorm(n_agents, simple_population_scaling_mean, simple_population_scaling_sd), scaling_factor = exp(log_scaling), # For simple model, weight ratio is always 1 (equal weights) weight_ratio = rep(1, n_agents), # Calculate the actual weights weight_direct = scaling_factor * weight_ratio / (1 + weight_ratio), weight_social = scaling_factor / (1 + weight_ratio) ) # Generate agent parameters for weighted model weighted_agents &lt;- tibble( agent_id = n_agents + (1:n_agents), # Continue numbering from simple agents model_type = &quot;weighted&quot;, # Generate log-normal scaling factors log_scaling = rnorm(n_agents, weighted_population_scaling_mean, weighted_population_scaling_sd), scaling_factor = exp(log_scaling), # Generate log-normal weight ratios log_weight_ratio = rnorm(n_agents, weighted_population_ratio_mean, weighted_population_ratio_sd), weight_ratio = exp(log_weight_ratio), # Calculate the actual weights weight_direct = scaling_factor * weight_ratio / (1 + weight_ratio), weight_social = scaling_factor / (1 + weight_ratio) ) # Combine agent parameters all_agents &lt;- bind_rows(simple_agents, weighted_agents) # Print summary of agent parameters agent_summary &lt;- all_agents %&gt;% group_by(model_type) %&gt;% summarize( n = n(), mean_scaling = mean(scaling_factor), sd_scaling = sd(scaling_factor), mean_ratio = mean(weight_ratio), sd_ratio = sd(weight_ratio), mean_direct = mean(weight_direct), sd_direct = sd(weight_direct), mean_social = mean(weight_social), sd_social = sd(weight_social) ) print(agent_summary) ## # A tibble: 2 × 10 ## model_type n mean_scaling sd_scaling mean_ratio sd_ratio mean_direct sd_direct ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 simple 20 2.94 0.878 1 0 1.47 0.439 ## 2 weighted 20 4.56 1.08 3.28 1.67 3.37 0.881 ## # ℹ 2 more variables: mean_social &lt;dbl&gt;, sd_social &lt;dbl&gt; # Create all possible evidence combinations evidence_combinations &lt;- expand_grid( blue1 = 0:8, # Direct evidence: 0-8 blue marbles out of 8 blue2 = 0:3, # Social evidence: 0-3 blue signals out of 3 total1 = 8, # Total marbles in direct evidence (constant) total2 = 3 # Total signals in social evidence (constant) ) # Function to generate agent decisions based on their parameters generate_agent_decisions &lt;- function(agent_data, evidence_df, n_samples = 5) { # Extract agent parameters agent_id &lt;- agent_data$agent_id model_type &lt;- agent_data$model_type weight_direct &lt;- agent_data$weight_direct weight_social &lt;- agent_data$weight_social # Create a data frame that repeats each evidence combination n_samples times repeated_evidence &lt;- evidence_df %&gt;% slice(rep(1:n(), each = n_samples)) %&gt;% group_by(blue1, blue2, total1, total2) %&gt;% mutate(sample_id = 1:n()) %&gt;% ungroup() # Generate decisions for each evidence combination decisions &lt;- pmap_dfr(repeated_evidence, function(blue1, blue2, total1, total2, sample_id) { # Calculate weighted evidence weighted_blue1 &lt;- blue1 * weight_direct weighted_red1 &lt;- (total1 - blue1) * weight_direct weighted_blue2 &lt;- blue2 * weight_social weighted_red2 &lt;- (total2 - blue2) * weight_social # Calculate Beta parameters alpha_post &lt;- 1 + weighted_blue1 + weighted_blue2 beta_post &lt;- 1 + weighted_red1 + weighted_red2 # Expected probability expected_rate &lt;- alpha_post / (alpha_post + beta_post) # Make choice choice &lt;- rbinom(1, 1, expected_rate) # Return decision data tibble( agent_id = agent_id, model_type = model_type, sample_id = sample_id, blue1 = blue1, blue2 = blue2, total1 = total1, total2 = total2, expected_rate = expected_rate, choice = choice, # Include true parameter values for reference true_weight_direct = weight_direct, true_weight_social = weight_social, true_weight_ratio = weight_direct / weight_social, true_scaling_factor = weight_direct + weight_social ) }) return(decisions) } # Generate decisions for all agents multilevel_sim_data &lt;- map_dfr(1:nrow(all_agents), function(i) { generate_agent_decisions(all_agents[i, ], evidence_combinations) }) # Add descriptive labels multilevel_sim_data &lt;- multilevel_sim_data %&gt;% mutate( social_evidence = factor( blue2, levels = 0:3, labels = c(&quot;Clear Red&quot;, &quot;Weak Red&quot;, &quot;Weak Blue&quot;, &quot;Clear Blue&quot;) ) ) # Visualize decision patterns for selected agents # Take a sample of agents from each model type selected_simple_agents &lt;- sample(unique(simple_agents$agent_id), 3) selected_weighted_agents &lt;- sample(unique(weighted_agents$agent_id), 3) selected_agents &lt;- c(selected_simple_agents, selected_weighted_agents) # Create plot decision_plot &lt;- multilevel_sim_data %&gt;% filter(agent_id %in% selected_agents) %&gt;% ggplot(aes(x = blue1, y = expected_rate, color = social_evidence, group = social_evidence)) + geom_line(size = 1) + geom_point(size = 2) + geom_hline(yintercept = 0.5, linetype = &quot;dashed&quot;, color = &quot;gray50&quot;) + facet_wrap(~ model_type + agent_id, ncol = 3) + labs( title = &quot;Decision Patterns: Simple vs. Weighted Integration&quot;, subtitle = &quot;Simple model shows parallel curves (equal weights), weighted model shows varying influence of social evidence&quot;, x = &quot;Blue Marbles in Direct Evidence (out of 8)&quot;, y = &quot;Probability of Choosing Blue&quot;, color = &quot;Social Evidence&quot; ) + theme_bw() + theme(legend.position = &quot;bottom&quot;) # Display plot print(decision_plot) # Print summary of dataset cat(&quot;Generated&quot;, nrow(multilevel_sim_data), &quot;observations from&quot;, n_agents * 2, &quot;agents (&quot;, n_agents, &quot;per model type)\\n&quot;) ## Generated 7200 observations from 40 agents ( 20 per model type) # Create data structure for Stan fitting stan_data_multilevel &lt;- list( N = nrow(multilevel_sim_data), J = n_agents * 2, agent_id = multilevel_sim_data$agent_id, choice = multilevel_sim_data$choice, blue1 = multilevel_sim_data$blue1, total1 = multilevel_sim_data$total1, blue2 = multilevel_sim_data$blue2, total2 = multilevel_sim_data$total2 ) 11.19.2 Understanding the Simulated Data The simulation generates data from two types of agents: Simple Integration Agents: These agents weight direct and social evidence equally, but with varying overall scaling factors. This creates individual differences in how strongly evidence affects beliefs, but without preferential weighting of sources. Weighted Integration Agents: These agents can weight direct and social evidence differently. Some might trust their direct evidence more, others might be more influenced by social information. The key visual difference in their decision patterns is: Simple integration agents show parallel curves for different social evidence levels. The spacing between curves is consistent across all levels of direct evidence, indicating equal influence. Weighted integration agents show varying spacing between curves. When an agent weights social evidence more heavily, the curves are more separated; when direct evidence is weighted more, the curves converge. By generating data from both models, we can: Verify our model-fitting procedure can recover the true parameters Test whether our model comparison methods correctly identify which integration strategy generated each dataset Assess how robustly we can detect differential weighting of evidence sources In the next section, we’ll fit both our multilevel models to this data and compare their performance.. Looking at these visualizations, we can see clear individual differences in how agents integrate evidence: Some agents give more weight to direct evidence, requiring less direct evidence to choose “blue” regardless of social evidence Others are more influenced by social information, showing greater spacing between the different social evidence lines These differences create unique decision boundaries for each agent, where they transition from choosing red to blue 11.20 Multilevel Bayesian Models for Evidence Integration In this section, we implement two multilevel Bayesian models that capture different hypotheses about how individuals integrate evidence from multiple sources. Both models allow for individual differences, but they differ in what aspects of evidence integration can vary across individuals. 11.20.1 Model 1: Simple Evidence Integration with Individual Scaling Our first model implements a cognitively simple integration strategy where all evidence sources are weighted equally (taken at “face value”), but the overall impact of evidence can vary across individuals: Each information source (direct and social evidence) receives equal relative weight in the integration process However, the overall scaling of evidence can vary between individuals This represents individuals who treat all evidence sources as equally reliable, but differ in how strongly any evidence influences their beliefs Mathematically, this means that for individual j: - Direct evidence weight = scaling_factor[j] × 0.5 - Social evidence weight = scaling_factor[j] × 0.5 This model captures the hypothesis that individuals differ in their overall sensitivity to evidence, but not in how they relatively weight different sources. Some individuals might be more conservative (low scaling factor), requiring more evidence to shift their beliefs, while others might be more responsive to evidence overall (high scaling factor). 11.20.2 Model 2: Weighted Evidence Integration Our second model implements a more complex integration strategy where both the overall impact of evidence and the relative weighting of different evidence sources can vary across individuals: Each individual can give different weights to direct versus social evidence The overall scaling of evidence can also vary between individuals This represents individuals who may trust certain evidence sources more than others We parameterize this model using two key parameters for each individual j: - scaling_factor[j]: The total weight given to all evidence - weight_ratio[j]: The ratio of direct evidence weight to social evidence weight From these, we derive the actual weights: - Direct evidence weight = scaling_factor[j] × weight_ratio[j] / (1 + weight_ratio[j]) - Social evidence weight = scaling_factor[j] / (1 + weight_ratio[j]) This parameterization ensures that the sum of weights equals the scaling factor, while the ratio between weights is determined by the weight ratio. 11.20.3 Why Allow Scaling to Vary in the Simple Model? Including individual variation in the scaling factor for the simple model serves several important purposes: Fair Comparison: It ensures that the comparison between models focuses specifically on differential weighting rather than just the presence of individual differences. The key question becomes “Do individuals weight evidence sources differently?” rather than “Do individuals vary in how they use evidence?” Statistical Control: The scaling parameter serves as a statistical control, ensuring that any evidence for differential weighting isn’t just capturing overall differences in evidence sensitivity. Nested Model Structure: It creates a proper nested model relationship - the simple model is a special case of the weighted model where the weight ratio is constrained to be 1.0 (equal weights) for everyone. This approach allows us to conduct a more precise test of our cognitive hypothesis about differential weighting of evidence sources, while accounting for individual differences in overall evidence use that likely exist regardless of weighting strategy. 11.21 From Single-Agent to Multilevel: Extending Bayesian Cognitive Models When moving from single-agent to multilevel modeling, we need to extend our Stan code to capture both population-level patterns and individual differences. This transformation requires careful consideration of parameter structure, prior specification, and computational efficiency. Let’s explore how we adapted our single-agent models into multilevel versions. 11.21.1 Key Components of the Multilevel Extension Parameterizing Individual Differences In our single-agent models, we had straightforward parameters like total_weight and weight_prop (for the weighted model) or just a scaling factor (for the simple model). For multilevel modeling, we need to create parameters that vary across individuals while maintaining population coherence. For the simple integration model: // Single-agent version parameters { real&lt;lower=0&gt; total_weight; // Overall scaling of evidence } // Multilevel version parameters { real mu_scaling; // Population mean (log scale) real&lt;lower=0&gt; sigma_scaling; // Population SD vector[J] z_scaling; // Standardized individual deviations } transformed parameters { vector&lt;lower=0&gt;[J] scaling_factor; // Individual scaling factors for (j in 1:J) { scaling_factor[j] = exp(mu_scaling + z_scaling[j] * sigma_scaling); } } Note several key changes: We now have population-level parameters (mu_scaling, sigma_scaling) that describe the distribution from which individual parameters are drawn We use non-centered parameterization with standardized z-scores to improve sampling efficiency We work in log space to ensure positive scaling factors 11.21.2 2. Hierarchical Prior Structure Priors also need to be restructured in a hierarchical fashion: // Single-agent version target += lognormal_lpdf(total_weight | .8, .4); // Prior for scaling // Multilevel version target += normal_lpdf(mu_scaling | 0, 1); // Prior for population mean target += exponential_lpdf(sigma_scaling | 2); // Prior for between-subject variability target += std_normal_lpdf(z_scaling); // Prior for standardized deviations The prior structure now has: Priors on population means Priors on population variances Standard normal priors on the standardized individual deviations This creates a proper hierarchical structure where individual parameters are partially pooled toward the population mean, with the degree of pooling determined by the population variance. 11.21.3 3. Handling Data from Multiple Individuals The data structure must be modified to associate observations with specific individuals: // Single-agent version data { int&lt;lower=1&gt; N; // Number of observations array[N] int&lt;lower=0, upper=1&gt; choice; // Choices (0=red, 1=blue) // Other data... } // Multilevel version data { int&lt;lower=1&gt; N; // Number of observations int&lt;lower=1&gt; J; // Number of subjects array[N] int&lt;lower=1, upper=J&gt; agent_id; // Agent ID for each observation array[N] int&lt;lower=0, upper=1&gt; choice; // Choices (0=red, 1=blue) // Other data... } The key addition is agent_id, which maps each observation to its corresponding agent. This allows us to apply the correct individual-level parameters to each observation. 11.21.4 4. Likelihood Specification The likelihood must be adapted to use the appropriate individual-level parameters: // Single-agent version for (i in 1:N) { real weighted_blue1 = blue1[i] * weight_direct; // ...additional code... choice[i] ~ bernoulli(expected_prob); } // Multilevel version for (i in 1:N) { real w_direct = weight_direct[agent_id[i]]; // Get parameters for this individual real w_social = weight_social[agent_id[i]]; real weighted_blue1 = blue1[i] * w_direct; // ...additional code... choice[i] ~ bernoulli(expected_prob); } We now index individual parameters by agent_id[i] to ensure each observation uses the correct agent’s parameters. 11.21.5 Why These Changes Matter 11.21.5.1 Computational Efficiency: Non-Centered Parameterization The non-centered parameterization (using z-scores) is critical for efficient sampling in hierarchical models. When individual parameters are close to the population mean or when population variance is small, direct parameterization can cause the sampler to get stuck in a difficult geometry called the “funnel” problem. By separating the individual effects into standardized z-scores, we create better sampling geometry and improve convergence. This is why we use: scaling_factor[j] = exp(mu_scaling + z_scaling[j] * sigma_scaling); instead of directly sampling individual parameters. 11.21.5.2 Working in Log Space for Bounded Parameters For parameters that must be positive (like scaling factors), working in log space ensures we maintain proper bounds while allowing the parameter to vary freely on the unconstrained scale: // This ensures scaling_factor is always positive scaling_factor[j] = exp(mu_scaling + z_scaling[j] * sigma_scaling); Similarly, for parameters constrained between 0 and 1 (like weight_prop), we use the logit transformation. Now we are ready for the full implementation of our multilevel Bayesian models for evidence integration. # Stan model for multilevel simple beta-binomial multilevel_simple_stan &lt;- &quot; // Multilevel Simple Beta-Binomial Model // This model assumes equal weights for evidence sources (taking evidence at face value) // but allows for individual variation in overall responsiveness data { int&lt;lower=1&gt; N; // Total number of observations int&lt;lower=1&gt; J; // Number of subjects array[N] int&lt;lower=1, upper=J&gt; agent_id; // Agent ID for each observation array[N] int&lt;lower=0, upper=1&gt; choice; // Choices (0=red, 1=blue) array[N] int&lt;lower=0&gt; blue1; // Direct evidence (blue marbles) array[N] int&lt;lower=0&gt; total1; // Total direct evidence array[N] int&lt;lower=0&gt; blue2; // Social evidence (blue signals) array[N] int&lt;lower=0&gt; total2; // Total social evidence } parameters { // Population-level parameters for agents&#39; preconceptions real mu_alpha_prior; // Population mean for alpha prior real&lt;lower=0&gt; sigma_alpha_prior; // Population SD for alpha prior real mu_beta_prior; // Population mean for beta prior real&lt;lower=0&gt; sigma_beta_prior; // Population SD for beta prior // Population-level parameter for overall scaling real mu_scaling; // Population mean scaling factor (log scale) real&lt;lower=0&gt; sigma_scaling; // Population SD of scaling // Individual-level (random) effects vector[J] z_alpha_prior; // Standardized individual deviations for alpha prior vector[J] z_beta_prior; // Standardized individual deviations for beta prior vector[J] z_scaling; // Standardized individual deviations } transformed parameters { // Individual-level parameters vector&lt;lower=0&gt;[J] scaling_factor; // Individual scaling factors vector&lt;lower=0&gt;[J] alpha_prior; // Individual alpha prior vector&lt;lower=0&gt;[J] beta_prior; // Individual beta prior // Non-centered parameterization for scaling factor for (j in 1:J) { alpha_prior[j] = exp(mu_alpha_prior + z_alpha_prior[j] * sigma_alpha_prior); beta_prior[j] = exp(mu_beta_prior + z_beta_prior[j] * sigma_beta_prior); scaling_factor[j] = exp(mu_scaling + z_scaling[j] * sigma_scaling); } } model { // Priors for population parameters target += lognormal_lpdf(mu_alpha_prior | 0, 1); // Prior for population mean of alpha prior target += exponential_lpdf(sigma_alpha_prior | 1); // Prior for population SD of alpha prior target += lognormal_lpdf(mu_beta_prior | 0, 1); // Prior for population mean of beta prior target += exponential_lpdf(sigma_beta_prior | 1); // Prior for population SD of beta prior target += normal_lpdf(mu_scaling | 0, 1); // Prior for log scaling factor target += exponential_lpdf(sigma_scaling | 2); // Prior for between-subject variability // Prior for standardized random effects z_scaling ~ std_normal(); // Standard normal prior z_alpha_prior ~ std_normal(); // Standard normal prior z_beta_prior ~ std_normal(); // Standard normal prior // Likelihood for (i in 1:N) { // Calculate the individual scaling factor real scale = scaling_factor[agent_id[i]]; // Simple integration - weights both evidence sources equally but applies individual scaling // Both direct and social evidence get weight = 1.0 * scaling_factor real weighted_blue1 = blue1[i] * scale; real weighted_red1 = (total1[i] - blue1[i]) * scale; real weighted_blue2 = blue2[i] * scale; real weighted_red2 = (total2[i] - blue2[i]) * scale; // Calculate Beta parameters for posterior real alpha_post = alpha_prior[agent_id[i]] + weighted_blue1 + weighted_blue2; real beta_post = beta_prior[agent_id[i]] + weighted_red1 + weighted_red2; // Use beta-binomial distribution to model the choice target += beta_binomial_lpmf(choice[i] | 1, alpha_post, beta_post); } } generated quantities { // Population parameters on natural scale real population_scaling = exp(mu_scaling); // Log likelihood for model comparison vector[N] log_lik; // Population and individual predictions array[N] int pred_choice; for (i in 1:N) { // Calculate the individual scaling factor real scale = scaling_factor[agent_id[i]]; // Calculate weighted evidence real weighted_blue1 = blue1[i] * scale; real weighted_red1 = (total1[i] - blue1[i]) * scale; real weighted_blue2 = blue2[i] * scale; real weighted_red2 = (total2[i] - blue2[i]) * scale; // Calculate Beta parameters real alpha_post = alpha_prior[agent_id[i]] + weighted_blue1 + weighted_blue2; real beta_post = beta_prior[agent_id[i]] + weighted_red1 + weighted_red2; // Generate predictions using beta-binomial pred_choice[i] = beta_binomial_rng(1, alpha_post, beta_post); // Calculate log likelihood log_lik[i] = beta_binomial_lpmf(choice[i] | 1, alpha_post, beta_post); } } &quot; # Write the model to a file write_stan_file( multilevel_simple_stan, dir = &quot;stan/&quot;, basename = &quot;W10_multilevel_simple_beta_binomial.stan&quot; ) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W10_multilevel_simple_beta_binomial.stan&quot; 11.21.6 Implementing the Multilevel Weighted Beta-Binomial Model in Stan Now let’s implement the multilevel weighted beta-binomial model, which allows both population-level estimates of evidence weights and individual variations around these population means. multilevel_weighted_stan &lt;- &quot; // Multilevel Weighted Beta-Binomial Model // This model allows different weights for different evidence sources // Using total_weight and weight_prop parameterization data { int&lt;lower=1&gt; N; // Total number of observations int&lt;lower=1&gt; J; // Number of subjects array[N] int&lt;lower=1, upper=J&gt; agent_id; // Agent ID for each observation array[N] int&lt;lower=0, upper=1&gt; choice; // Choices (0=red, 1=blue) array[N] int&lt;lower=0&gt; blue1; // Direct evidence (blue marbles) array[N] int&lt;lower=0&gt; total1; // Total direct evidence array[N] int&lt;lower=0&gt; blue2; // Social evidence (blue signals) array[N] int&lt;lower=0&gt; total2; // Total social evidence } parameters { // Population-level parameters for agents&#39; preconceptions real mu_alpha_prior; // Population mean for alpha prior real&lt;lower=0&gt; sigma_alpha_prior; // Population SD for alpha prior real mu_beta_prior; // Population mean for beta prior real&lt;lower=0&gt; sigma_beta_prior; // Population SD for beta prior // Population-level parameters real mu_weight_ratio; // Population mean for relative weight (direct/social) - log scale real mu_scaling; // Population mean for overall scaling - log scale // Population-level standard deviations real&lt;lower=0&gt; sigma_weight_ratio; // Between-subject variability in relative weighting real&lt;lower=0&gt; sigma_scaling; // Between-subject variability in scaling // Individual-level (random) effects vector[J] z_weight_ratio; // Standardized individual weight ratio deviations vector[J] z_scaling; // Standardized individual scaling deviations } transformed parameters { // Individual-level parameters vector&lt;lower=0&gt;[J] weight_ratio; // Individual relative weights (direct/social) vector&lt;lower=0&gt;[J] scaling_factor; // Individual overall scaling factors vector&lt;lower=0&gt;[J] weight_direct; // Individual weights for direct evidence vector&lt;lower=0&gt;[J] weight_social; // Individual weights for social evidence // Non-centered parameterization for (j in 1:J) { // Transform standardized parameters to natural scale weight_ratio[j] = exp(mu_weight_ratio + z_weight_ratio[j] * sigma_weight_ratio); scaling_factor[j] = exp(mu_scaling + z_scaling[j] * sigma_scaling); // Calculate individual weights // The sum of weights is determined by the scaling factor // The ratio between weights is determined by weight_ratio weight_direct[j] = scaling_factor[j] * weight_ratio[j] / (1 + weight_ratio[j]); weight_social[j] = scaling_factor[j] / (1 + weight_ratio[j]); } } model { // Priors for population parameters mu_weight_ratio ~ normal(0, 1); // Prior for log weight ratio centered at 0 (equal weights) mu_scaling ~ normal(0, 1); // Prior for log scaling factor sigma_weight_ratio ~ exponential(2); // Prior for between-subject variability sigma_scaling ~ exponential(2); // Prior for scaling variability // Priors for individual random effects z_weight_ratio ~ std_normal(); // Standard normal prior for weight ratio z-scores z_scaling ~ std_normal(); // Standard normal prior for scaling z-scores z_alpha_prior ~ std_normal(); // Standard normal prior z_beta_prior ~ std_normal(); // Standard normal prior // Likelihood for (i in 1:N) { // Get weights for this person real w_direct = weight_direct[agent_id[i]]; real w_social = weight_social[agent_id[i]]; // Calculate weighted evidence real weighted_blue1 = blue1[i] * w_direct; real weighted_red1 = (total1[i] - blue1[i]) * w_direct; real weighted_blue2 = blue2[i] * w_social; real weighted_red2 = (total2[i] - blue2[i]) * w_social; // Calculate Beta parameters for Bayesian integration real alpha_post = alpha_prior[agent_id[i]] + weighted_blue1 + weighted_blue2; real beta_post = beta_prior[agent_id[i]] + weighted_red1 + weighted_red2; // Use beta-binomial distribution to model the choice target += beta_binomial_lpmf(choice[i] | 1, alpha_post, beta_post); } } generated quantities { // Convert population parameters to original weight scale for interpretation real population_ratio = exp(mu_weight_ratio); real population_scaling = exp(mu_scaling); real population_weight_direct = population_scaling * population_ratio / (1 + population_ratio); real population_weight_social = population_scaling / (1 + population_ratio); // Log likelihood for model comparison vector[N] log_lik; // Population and individual predictions array[N] int pred_choice; for (i in 1:N) { // Get weights for this person real w_direct = weight_direct[agent_id[i]]; real w_social = weight_social[agent_id[i]]; // Calculate weighted evidence real weighted_blue1 = blue1[i] * w_direct; real weighted_red1 = (total1[i] - blue1[i]) * w_direct; real weighted_blue2 = blue2[i] * w_social; real weighted_red2 = (total2[i] - blue2[i]) * w_social; // Calculate Beta parameters real alpha_post = alpha_prior[agent_id[i]] + weighted_blue1 + weighted_blue2; real beta_post = beta_prior[agent_id[i]] + weighted_red1 + weighted_red2; // Generate predictions using beta-binomial pred_choice[i] = beta_binomial_rng(1, alpha_post, beta_post); // Calculate log likelihood log_lik[i] = beta_binomial_lpmf(choice[i] | 1, alpha_post, beta_post); } } &quot; # Write the model to a file write_stan_file( multilevel_weighted_stan, dir = &quot;stan/&quot;, basename = &quot;W10_multilevel_weighted_beta_binomial.stan&quot; ) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W10_multilevel_weighted_beta_binomial.stan&quot; 11.22 Fitting the Multilevel Models Now that we’ve generated data from both simple and weighted integration strategies, we can fit our two multilevel models to this data. This will allow us to: Evaluate our ability to recover the true parameters Compare the models to determine which better explains the observed decisions Assess whether we can correctly identify which cognitive strategy generated each agent’s data We’ll fit both models to the full dataset, which contains a mixture of simple and weighted integration agents. This represents a realistic scenario where we don’t know in advance which strategy each individual is using. # Fitting the Multilevel Models to Simulated Data # We&#39;ll fit both the simple and weighted integration models to our simulated data # Create file paths for Stan models file_simple_ml &lt;- file.path(&quot;stan/W10_multilevel_simple_beta_binomial.stan&quot;) file_weighted_ml &lt;- file.path(&quot;stan/W10_multilevel_weighted_beta_binomial.stan&quot;) # Check if we need to regenerate model fits or load existing ones if (regenerate_simulations) { # Compile Stan models mod_simple_ml &lt;- cmdstan_model( file_simple_ml, cpp_options = list(stan_threads = TRUE) ) mod_weighted_ml &lt;- cmdstan_model( file_weighted_ml, cpp_options = list(stan_threads = TRUE) ) # Fit the simple multilevel model # This model assumes equal weights for evidence sources but allows individual scaling cat(&quot;Fitting the simple multilevel model...\\n&quot;) fit_simple_ml &lt;- mod_simple_ml$sample( data = stan_data_multilevel, # Data for all agents (both types) seed = 242, # Seed for reproducibility chains = 2, # Number of MCMC chains parallel_chains = 2, # Run chains in parallel threads_per_chain = 1, # Stan threading iter_warmup = 1000, # Warmup iterations iter_sampling = 1000, # Sampling iterations refresh = 100, # Progress update frequency adapt_delta = 0.9, # Adaptation parameter for HMC max_treedepth = 12 # Maximum tree depth for HMC ) # Fit the weighted multilevel model # This model allows different weights for different evidence sources cat(&quot;Fitting the weighted multilevel model...\\n&quot;) fit_weighted_ml &lt;- mod_weighted_ml$sample( data = stan_data_multilevel, # Same data as simple model seed = 143, # Different seed chains = 2, # Number of MCMC chains parallel_chains = 2, # Run chains in parallel threads_per_chain = 1, # Stan threading iter_warmup = 1000, # Warmup iterations iter_sampling = 1000, # Sampling iterations refresh = 100, # Progress update frequency adapt_delta = 0.95, # Higher adapt_delta for stability max_treedepth = 12 # Maximum tree depth for HMC ) # Save model fits for future use fit_simple_ml$save_object(&quot;simmodels/fit_multilevel_simple_mixed.rds&quot;) fit_weighted_ml$save_object(&quot;simmodels/fit_multilevel_weighted_mixed.rds&quot;) cat(&quot;Models fitted and saved.\\n&quot;) } else { # Load existing model fits fit_simple_ml &lt;- readRDS(&quot;simmodels/fit_multilevel_simple_mixed.rds&quot;) fit_weighted_ml &lt;- readRDS(&quot;simmodels/fit_multilevel_weighted_mixed.rds&quot;) cat(&quot;Loaded existing model fits.\\n&quot;) } ## Loaded existing model fits. # Check for convergence issues # For simple model simple_summary &lt;- fit_simple_ml$summary() simple_rhat_issues &lt;- simple_summary %&gt;% filter(rhat &gt; 1.05) %&gt;% nrow() # For weighted model weighted_summary &lt;- fit_weighted_ml$summary() weighted_rhat_issues &lt;- weighted_summary %&gt;% filter(rhat &gt; 1.05) %&gt;% nrow() # Print convergence summary cat(&quot;Convergence check:\\n&quot;) ## Convergence check: cat(&quot;Simple model parameters with Rhat &gt; 1.05:&quot;, simple_rhat_issues, &quot;out of&quot;, nrow(simple_summary), &quot;\\n&quot;) ## Simple model parameters with Rhat &gt; 1.05: 0 out of 14484 cat(&quot;Weighted model parameters with Rhat &gt; 1.05:&quot;, weighted_rhat_issues, &quot;out of&quot;, nrow(weighted_summary), &quot;\\n&quot;) ## Weighted model parameters with Rhat &gt; 1.05: 0 out of 14649 # Extract posterior samples for key parameters # From simple model draws_simple &lt;- as_draws_df(fit_simple_ml$draws()) population_scaling_simple &lt;- mean(exp(draws_simple$mu_scaling)) population_scaling_sd_simple &lt;- mean(draws_simple$sigma_scaling) # From weighted model draws_weighted &lt;- as_draws_df(fit_weighted_ml$draws()) population_ratio_weighted &lt;- mean(exp(draws_weighted$mu_weight_ratio)) population_scaling_weighted &lt;- mean(exp(draws_weighted$mu_scaling)) population_ratio_sd_weighted &lt;- mean(draws_weighted$sigma_weight_ratio) population_scaling_sd_weighted &lt;- mean(draws_weighted$sigma_scaling) # Print population-level parameter estimates cat(&quot;\\nPopulation parameter estimates:\\n&quot;) ## ## Population parameter estimates: cat(&quot;Simple model:\\n&quot;) ## Simple model: cat(&quot; Mean scaling factor:&quot;, round(population_scaling_simple, 2), &quot;\\n&quot;) ## Mean scaling factor: 2.3 cat(&quot; SD of log scaling:&quot;, round(population_scaling_sd_simple, 2), &quot;\\n\\n&quot;) ## SD of log scaling: 0.19 cat(&quot;Weighted model:\\n&quot;) ## Weighted model: cat(&quot; Mean scaling factor:&quot;, round(population_scaling_weighted, 2), &quot;\\n&quot;) ## Mean scaling factor: 3.29 cat(&quot; Mean weight ratio (direct/social):&quot;, round(population_ratio_weighted, 2), &quot;\\n&quot;) ## Mean weight ratio (direct/social): 1.56 cat(&quot; SD of log scaling:&quot;, round(population_scaling_sd_weighted, 2), &quot;\\n&quot;) ## SD of log scaling: 0.18 cat(&quot; SD of log weight ratio:&quot;, round(population_ratio_sd_weighted, 2), &quot;\\n&quot;) ## SD of log weight ratio: 0.4 11.22.1 Parameter Recovery Analysis Now that we’ve fitted both models, let’s examine how well we can recover the true individual parameters. This is a crucial step in validating our models - if we can’t recover the parameters that generated our data, we might need to refine our models or collect more data. # Extract individual parameter estimates from both models # For simple model (scaling factor only) scaling_factor_simple &lt;- matrix(NA, nrow = nrow(draws_simple), ncol = n_agents * 2) for (j in 1:(n_agents * 2)) { scaling_factor_simple[, j] &lt;- draws_simple[[paste0(&quot;scaling_factor[&quot;, j, &quot;]&quot;)]] } # Calculate posterior means scaling_factor_simple_est &lt;- colMeans(scaling_factor_simple) # For weighted model (scaling factor and weight ratio) scaling_factor_weighted &lt;- matrix(NA, nrow = nrow(draws_weighted), ncol = n_agents * 2) weight_ratio_weighted &lt;- matrix(NA, nrow = nrow(draws_weighted), ncol = n_agents * 2) weight_direct_weighted &lt;- matrix(NA, nrow = nrow(draws_weighted), ncol = n_agents * 2) weight_social_weighted &lt;- matrix(NA, nrow = nrow(draws_weighted), ncol = n_agents * 2) for (j in 1:(n_agents * 2)) { scaling_factor_weighted[, j] &lt;- draws_weighted[[paste0(&quot;scaling_factor[&quot;, j, &quot;]&quot;)]] weight_ratio_weighted[, j] &lt;- draws_weighted[[paste0(&quot;weight_ratio[&quot;, j, &quot;]&quot;)]] weight_direct_weighted[, j] &lt;- draws_weighted[[paste0(&quot;weight_direct[&quot;, j, &quot;]&quot;)]] weight_social_weighted[, j] &lt;- draws_weighted[[paste0(&quot;weight_social[&quot;, j, &quot;]&quot;)]] } # Calculate posterior means scaling_factor_weighted_est &lt;- colMeans(scaling_factor_weighted) weight_ratio_weighted_est &lt;- colMeans(weight_ratio_weighted) weight_direct_weighted_est &lt;- colMeans(weight_direct_weighted) weight_social_weighted_est &lt;- colMeans(weight_social_weighted) # Create dataframe for recovery analysis recovery_data &lt;- tibble( agent_id = 1:(n_agents * 2), model_type = all_agents$model_type, # True parameters true_scaling_factor = all_agents$scaling_factor, true_weight_ratio = all_agents$weight_ratio, true_weight_direct = all_agents$weight_direct, true_weight_social = all_agents$weight_social, # Estimated from simple model est_scaling_simple = scaling_factor_simple_est, # Estimated from weighted model est_scaling_weighted = scaling_factor_weighted_est, est_ratio_weighted = weight_ratio_weighted_est, est_direct_weighted = weight_direct_weighted_est, est_social_weighted = weight_social_weighted_est ) # Calculate recovery accuracy metrics recovery_data &lt;- recovery_data %&gt;% mutate( # Error in scaling factor estimates error_scaling_simple = est_scaling_simple - true_scaling_factor, pct_error_scaling_simple = 100 * error_scaling_simple / true_scaling_factor, error_scaling_weighted = est_scaling_weighted - true_scaling_factor, pct_error_scaling_weighted = 100 * error_scaling_weighted / true_scaling_factor, # Error in weight ratio estimates (only for weighted model) error_ratio_weighted = est_ratio_weighted - true_weight_ratio, pct_error_ratio_weighted = 100 * error_ratio_weighted / true_weight_ratio, # Error in direct/social weight estimates error_direct_weighted = est_direct_weighted - true_weight_direct, pct_error_direct_weighted = 100 * error_direct_weighted / true_weight_direct, error_social_weighted = est_social_weighted - true_weight_social, pct_error_social_weighted = 100 * error_social_weighted / true_weight_social ) # Summarize recovery errors by agent type recovery_summary &lt;- recovery_data %&gt;% group_by(model_type) %&gt;% summarize( # Simple model scaling recovery mean_abs_error_scaling_simple = mean(abs(error_scaling_simple)), mean_abs_pct_error_scaling_simple = mean(abs(pct_error_scaling_simple)), # Weighted model param recovery mean_abs_error_scaling_weighted = mean(abs(error_scaling_weighted)), mean_abs_pct_error_scaling_weighted = mean(abs(pct_error_scaling_weighted)), mean_abs_error_ratio_weighted = mean(abs(error_ratio_weighted)), mean_abs_pct_error_ratio_weighted = mean(abs(pct_error_ratio_weighted)), mean_abs_error_direct_weighted = mean(abs(error_direct_weighted)), mean_abs_pct_error_direct_weighted = mean(abs(pct_error_direct_weighted)), mean_abs_error_social_weighted = mean(abs(error_social_weighted)), mean_abs_pct_error_social_weighted = mean(abs(pct_error_social_weighted)), .groups = &quot;drop&quot; ) # Print recovery summary knitr::kable(recovery_summary, digits = 2, caption = &quot;Parameter Recovery Accuracy by Agent Type&quot;) Table 11.4: Parameter Recovery Accuracy by Agent Type model_type mean_abs_error_scaling_simple mean_abs_pct_error_scaling_simple mean_abs_error_scaling_weighted mean_abs_pct_error_scaling_weighted mean_abs_error_ratio_weighted mean_abs_pct_error_ratio_weighted mean_abs_error_direct_weighted mean_abs_pct_error_direct_weighted mean_abs_error_social_weighted mean_abs_pct_error_social_weighted simple 0.80 24.90 0.82 33.42 0.45 45.02 0.57 47.45 0.35 25.37 weighted 2.15 44.02 1.24 24.22 1.49 37.50 1.27 35.13 0.41 45.28 # Create visualizations of parameter recovery # 1. Scaling factor recovery p1 &lt;- ggplot(recovery_data, aes(x = true_scaling_factor, y = est_scaling_simple, color = model_type)) + geom_point(size = 3, alpha = 0.7) + geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;) + labs( title = &quot;Scaling Factor Recovery (Simple Model)&quot;, subtitle = &quot;How well can the simple model recover the true scaling factors?&quot;, x = &quot;True Scaling Factor&quot;, y = &quot;Estimated Scaling Factor&quot;, color = &quot;Agent Type&quot; ) + theme_minimal() p2 &lt;- ggplot(recovery_data, aes(x = true_scaling_factor, y = est_scaling_weighted, color = model_type)) + geom_point(size = 3, alpha = 0.7) + geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;) + labs( title = &quot;Scaling Factor Recovery (Weighted Model)&quot;, subtitle = &quot;How well can the weighted model recover the true scaling factors?&quot;, x = &quot;True Scaling Factor&quot;, y = &quot;Estimated Scaling Factor&quot;, color = &quot;Agent Type&quot; ) + theme_minimal() # 2. Weight ratio recovery (weighted model only) p3 &lt;- ggplot(recovery_data, aes(x = true_weight_ratio, y = est_ratio_weighted, color = model_type)) + geom_point(size = 3, alpha = 0.7) + geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;) + labs( title = &quot;Weight Ratio Recovery (Weighted Model)&quot;, subtitle = &quot;How well can the weighted model recover the true weight ratios?&quot;, x = &quot;True Weight Ratio (Direct/Social)&quot;, y = &quot;Estimated Weight Ratio&quot;, color = &quot;Agent Type&quot; ) + theme_minimal() # Arrange plots recovery_plots &lt;- p1 + p2 + p3 + plot_layout(ncol = 2) print(recovery_plots) 11.22.2 Model Comparison Now that we’ve fitted both models, we can formally compare them to see which better explains the observed data. We’ll use Leave-One-Out Cross-Validation (LOO-CV) to estimate each model’s predictive accuracy. In a real application, we wouldn’t know in advance whether individuals use simple or weighted integration strategies. Model comparison helps us determine which cognitive model is more consistent with observed behavior. # Calculate LOO-CV for model comparison # This evaluates how well each model predicts held-out data # For simple model loo_simple &lt;- fit_simple_ml$loo() # For weighted model loo_weighted &lt;- fit_weighted_ml$loo() # Compare models loo_comparison &lt;- loo_compare(loo_simple, loo_weighted) print(loo_comparison) ## elpd_diff se_diff ## model2 0.0 0.0 ## model1 -21.6 6.7 # Calculate model weights model_weights &lt;- loo_model_weights(list( &quot;Simple Integration&quot; = loo_simple, &quot;Weighted Integration&quot; = loo_weighted )) # Print model weights print(model_weights) ## Method: stacking ## ------ ## weight ## Simple Integration 0.028 ## Weighted Integration 0.972 # Create a visualization of model comparison model_comp_data &lt;- tibble( model = names(model_weights), weight = as.numeric(model_weights) ) p_model_comp &lt;- ggplot(model_comp_data, aes(x = model, y = weight, fill = model)) + geom_col() + geom_text(aes(label = scales::percent(weight, accuracy = 0.1)), vjust = -0.5, size = 5) + labs( title = &quot;Model Comparison Using LOO-CV&quot;, subtitle = &quot;Higher weights indicate better predictive performance&quot;, x = NULL, y = &quot;Model Weight&quot; ) + scale_fill_brewer(palette = &quot;Set1&quot;) + theme_minimal() + theme(legend.position = &quot;none&quot;) print(p_model_comp) # Compare model performance by agent type # Calculate pointwise ELPD values for each model elpd_simple &lt;- loo_simple$pointwise[, &quot;elpd_loo&quot;] elpd_weighted &lt;- loo_weighted$pointwise[, &quot;elpd_loo&quot;] # Aggregate by agent elpd_by_agent &lt;- multilevel_sim_data %&gt;% dplyr::select(agent_id, model_type) %&gt;% distinct() %&gt;% mutate( elpd_simple = NA_real_, elpd_weighted = NA_real_ ) # Calculate ELPD sums by agent for (j in 1:nrow(elpd_by_agent)) { agent &lt;- elpd_by_agent$agent_id[j] # Find rows for this agent agent_rows &lt;- which(multilevel_sim_data$agent_id == agent) # Sum ELPD values for this agent elpd_by_agent$elpd_simple[j] &lt;- sum(elpd_simple[agent_rows]) elpd_by_agent$elpd_weighted[j] &lt;- sum(elpd_weighted[agent_rows]) } # Calculate ELPD difference (positive = weighted model is better) elpd_by_agent &lt;- elpd_by_agent %&gt;% mutate( elpd_diff = elpd_weighted - elpd_simple, better_model = ifelse(elpd_diff &gt; 0, &quot;Weighted&quot;, &quot;Simple&quot;) ) # Create visualization of model preference by agent type p_agent_comp &lt;- ggplot(elpd_by_agent, aes(x = agent_id, y = elpd_diff, color = model_type)) + geom_point() + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + labs( title = &quot;Model Preference by Agent Type&quot;, subtitle = &quot;Positive values favor the weighted model; negative values favor the simple model&quot;, x = &quot;True Agent Type&quot;, y = &quot;ELPD Difference (Weighted - Simple)&quot;, color = &quot;Preferred Model&quot; ) + theme_minimal() print(p_agent_comp) # Calculate classification accuracy # How often does the better-fitting model match the true generating model? classification &lt;- elpd_by_agent %&gt;% mutate( # For simple agents, the simple model should be better correct_classification = case_when( model_type == &quot;simple&quot; &amp; better_model == &quot;Simple&quot; ~ TRUE, model_type == &quot;weighted&quot; &amp; better_model == &quot;Weighted&quot; ~ TRUE, TRUE ~ FALSE ) ) # Calculate overall accuracy and by agent type overall_accuracy &lt;- mean(classification$correct_classification) accuracy_by_type &lt;- classification %&gt;% group_by(model_type) %&gt;% summarize( n = n(), correct = sum(correct_classification), accuracy = correct / n, .groups = &quot;drop&quot; ) # Print classification results cat(&quot;\\nModel Classification Accuracy:\\n&quot;) ## ## Model Classification Accuracy: cat(&quot;Overall accuracy:&quot;, scales::percent(overall_accuracy), &quot;\\n\\n&quot;) ## Overall accuracy: 70% print(knitr::kable(accuracy_by_type, caption = &quot;Classification Accuracy by Agent Type&quot;)) ## ## ## Table: (\\#tab:unnamed-chunk-64)Classification Accuracy by Agent Type ## ## |model_type | n| correct| accuracy| ## |:----------|--:|-------:|--------:| ## |simple | 20| 14| 0.7| ## |weighted | 20| 14| 0.7| 11.23 Dynamic Bayesian Evidence Integration: Sequential Updating Models In real-world learning scenarios, people continuously update their beliefs as they gather new evidence. While our previous models considered decision-making based on static evidence, a more realistic approach is to incorporate sequential updating where beliefs evolve over time. Let’s develop an extension of our Bayesian evidence integration models that captures how agents dynamically update their beliefs across trials. 11.23.1 Sequential Bayesian Updating: The Theoretical Framework In sequential Bayesian updating, an agent’s posterior belief from one trial becomes the prior for the next trial. This creates a continuous learning process where the agent’s beliefs evolve over time based on observed evidence. The key components of a sequential updating model are: Initial prior belief - The agent’s belief before encountering any evidence Trial-by-trial updating - How beliefs are updated after each new piece of evidence Response mechanism - How updated beliefs translate into observable choices Let’s implement this framework in Stan, starting with the single-agent version and then extending to a multilevel model. 11.23.2 Single-Agent Sequential Updating Model # Stan code for a sequential Bayesian updating model sequential_updating_stan &lt;- &quot; // Sequential Bayesian Updating Model // This model tracks how an agent updates beliefs across a sequence of trials data { int&lt;lower=1&gt; T; // Number of trials array[T] int&lt;lower=0, upper=1&gt; choice; // Choices (0=red, 1=blue) array[T] int&lt;lower=0&gt; blue1; // Direct evidence (blue marbles) on each trial array[T] int&lt;lower=0&gt; total1; // Total direct evidence on each trial array[T] int&lt;lower=0&gt; blue2; // Social evidence (blue signals) on each trial array[T] int&lt;lower=0&gt; total2; // Total social evidence on each trial } parameters { real&lt;lower=0&gt; total_weight; // Overall weight given to evidence real&lt;lower=0, upper=1&gt; weight_prop; // Proportion of weight for direct evidence real&lt;lower=0&gt; learning; // Learning rate parameter } transformed parameters { // Calculate weights for each evidence source real weight_direct = total_weight * weight_prop; real weight_social = total_weight * (1 - weight_prop); // Variables to track belief updating across trials vector&lt;lower=0, upper=1&gt;[T] belief; // Belief in blue on each trial vector&lt;lower=0&gt;[T] alpha_param; // Beta distribution alpha parameter vector&lt;lower=0&gt;[T] beta_param; // Beta distribution beta parameter // Initial belief parameters (uniform prior) alpha_param[1] = 1.0; beta_param[1] = 1.0; // Calculate belief for first trial belief[1] = alpha_param[1] / (alpha_param[1] + beta_param[1]); // Update beliefs across trials for (t in 2:T) { // Calculate weighted evidence from previous trial real weighted_blue1 = blue1[t-1] * weight_direct; real weighted_red1 = (total1[t-1] - blue1[t-1]) * weight_direct; real weighted_blue2 = blue2[t-1] * weight_social; real weighted_red2 = (total2[t-1] - blue2[t-1]) * weight_social; // Update belief with learning rate // alpha controls how much new evidence affects the belief alpha_param[t] = alpha_param[t-1] + learning * (weighted_blue1 + weighted_blue2); beta_param[t] = beta_param[t-1] + learning * (weighted_red1 + weighted_red2); // Calculate updated belief belief[t] = alpha_param[t] / (alpha_param[t] + beta_param[t]); } } model { // Priors for parameters target += lognormal_lpdf(total_weight | 0, 0.5); // Prior centered around 1.0 target += beta_lpdf(weight_prop | 1, 1); // Uniform prior on proportion target += lognormal_lpdf(alpha | -1, 0.5); // Prior on learning rate (typically &lt; 1) // Likelihood for (t in 1:T) { // Model choice as a function of current belief target += bernoulli_lpmf(choice[t] | belief[t]); } } generated quantities { // Log likelihood for model comparison vector[T] log_lik; // Posterior predictions array[T] int pred_choice; for (t in 1:T) { // Generate predicted choices pred_choice[t] = bernoulli_rng(belief[t]); // Calculate log likelihood log_lik[t] = bernoulli_lpmf(choice[t] | belief[t]); } } &quot; # Write the model to a file write_stan_file( sequential_updating_stan, dir = &quot;stan/&quot;, basename = &quot;10_sequential_updating.stan&quot; ) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/10_sequential_updating.stan&quot; 11.23.3 Multilevel Sequential Updating Model Now let’s extend this to a multilevel model that captures individual differences in learning rates and evidence weighting: # Stan code for a multilevel sequential Bayesian updating model multilevel_sequential_stan &lt;- &quot; // Multilevel Sequential Bayesian Updating Model // This model captures individual differences in sequential belief updating data { int&lt;lower=1&gt; N; // Total number of observations int&lt;lower=1&gt; J; // Number of agents int&lt;lower=1&gt; T; // Maximum number of trials per agent array[N] int&lt;lower=1, upper=J&gt; agent_id; // Agent ID for each observation array[N] int&lt;lower=1, upper=T&gt; trial_id; // Trial number for each observation array[N] int&lt;lower=0, upper=1&gt; choice; // Choices (0=red, 1=blue) array[N] int&lt;lower=0&gt; blue1; // Direct evidence (blue marbles) array[N] int&lt;lower=0&gt; total1; // Total direct evidence array[N] int&lt;lower=0&gt; blue2; // Social evidence (blue signals) array[N] int&lt;lower=0&gt; total2; // Total social evidence // Additional data for tracking trial sequences array[J] int&lt;lower=1, upper=T&gt; trials_per_agent; // Number of trials for each agent } parameters { // Population-level parameters real mu_total_weight; // Population mean log total weight real mu_weight_prop_logit; // Population mean logit weight proportion real mu_learning_log; // Population mean log learning rate // Population-level standard deviations vector&lt;lower=0&gt;[3] tau; // SDs for [total_weight, weight_prop, alpha] // Correlation matrix for individual parameters (optional) cholesky_factor_corr[3] L_Omega; // Cholesky factor of correlation matrix // Individual-level variations (non-centered parameterization) matrix[3, J] z; // Standardized individual parameters } transformed parameters { // Individual-level parameters vector&lt;lower=0&gt;[J] total_weight; // Total evidence weight for each agent vector&lt;lower=0, upper=1&gt;[J] weight_prop; // Weight proportion for each agent vector&lt;lower=0&gt;[J] learning; // Learning rate for each agent vector&lt;lower=0&gt;[J] weight_direct; // Direct evidence weight for each agent vector&lt;lower=0&gt;[J] weight_social; // Social evidence weight for each agent // Individual beliefs for each trial // We&#39;ll use a ragged structure due to varying trial counts array[J, T] real belief; // Belief in blue for each agent on each trial // Transform parameters to natural scale matrix[3, J] theta = diag_pre_multiply(tau, L_Omega) * z; // Non-centered parameterization for (j in 1:J) { // Transform individual parameters to appropriate scales total_weight[j] = exp(mu_total_weight + theta[1, j]); weight_prop[j] = inv_logit(mu_weight_prop_logit + theta[2, j]); learning[j] = exp(mu_learning_log + theta[3, j]); // Calculate derived weights weight_direct[j] = total_weight[j] * weight_prop[j]; weight_social[j] = total_weight[j] * (1 - weight_prop[j]); // Initialize belief tracking for each agent real alpha_param = 1.0; // Initial beta distribution parameters real beta_param = 1.0; // Calculate initial belief belief[j, 1] = alpha_param / (alpha_param + beta_param); // Process trials for this agent (skipping the first trial since we initialized it above) for (t in 2:trials_per_agent[j]) { // Find the previous trial&#39;s data for this agent int prev_idx = 0; // Search for previous trial (this is a simplification; more efficient approaches exist) for (i in 1:N) { if (agent_id[i] == j &amp;&amp; trial_id[i] == t-1) { prev_idx = i; break; } } if (prev_idx &gt; 0) { // Calculate weighted evidence from previous trial real weighted_blue1 = blue1[prev_idx] * weight_direct[j]; real weighted_red1 = (total1[prev_idx] - blue1[prev_idx]) * weight_direct[j]; real weighted_blue2 = blue2[prev_idx] * weight_social[j]; real weighted_red2 = (total2[prev_idx] - blue2[prev_idx]) * weight_social[j]; // Update belief parameters with learning rate alpha_param = alpha_param + alpha[j] * (weighted_blue1 + weighted_blue2); beta_param = beta_param + alpha[j] * (weighted_red1 + weighted_red2); } // Calculate updated belief belief[j, t] = alpha_param / (alpha_param + beta_param); } } } model { // Priors for population parameters target += normal_lpdf(mu_total_weight | 0, 1); // Population log total weight target += normal_lpdf(mu_weight_prop_logit | 0, 1); // Population logit weight proportion target += normal_lpdf(mu_alpha_log | -1, 1); // Population log learning rate // Priors for population standard deviations target += exponential_lpdf(tau | 2); // Conservative prior for SDs // Prior for correlation matrix target += lkj_corr_cholesky_lpdf(L_Omega | 2); // LKJ prior on correlations // Prior for standardized individual parameters target += std_normal_lpdf(to_vector(z)); // Standard normal prior on z-scores // Likelihood for (i in 1:N) { int j = agent_id[i]; // Agent ID int t = trial_id[i]; // Trial number // Model choice as a function of current belief target += bernoulli_lpmf(choice[i] | belief[j, t]); } } generated quantities { // Transform population parameters to natural scale for interpretation real&lt;lower=0&gt; pop_total_weight = exp(mu_total_weight); real&lt;lower=0, upper=1&gt; pop_weight_prop = inv_logit(mu_weight_prop_logit); real&lt;lower=0&gt; pop_alpha = exp(mu_alpha_log); real&lt;lower=0&gt; pop_weight_direct = pop_total_weight * pop_weight_prop; real&lt;lower=0&gt; pop_weight_social = pop_total_weight * (1 - pop_weight_prop); // Correlation matrix for individual differences matrix[3, 3] Omega = multiply_lower_tri_self_transpose(L_Omega); // Log likelihood for model comparison vector[N] log_lik; // Posterior predictions array[N] int pred_choice; for (i in 1:N) { int j = agent_id[i]; int t = trial_id[i]; // Generate predicted choices pred_choice[i] = bernoulli_rng(belief[j, t]); // Calculate log likelihood log_lik[i] = bernoulli_lpmf(choice[i] | belief[j, t]); } } &quot; # Write the model to a file write_stan_file( multilevel_sequential_stan, dir = &quot;stan/&quot;, basename = &quot;10_multilevel_sequential_updating.stan&quot; ) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/10_multilevel_sequential_updating.stan&quot; 11.23.4 Generating Data for the Sequential Model To test our sequential updating model, we need to generate data that involves a sequence of decisions where beliefs are updated over time. Here’s how we can simulate such data: # Function to simulate sequential updating agent behavior simulate_sequential_agent &lt;- function(n_trials, weight_direct, weight_social, learning_rate, p_blue_transitions = c(0.7, 0.3)) { # p_blue_transitions = c(p(blue|previous=blue), p(blue|previous=red)) # This creates a Markov process for the underlying jar probabilities # Initialize results results &lt;- tibble( trial = 1:n_trials, true_p_blue = NA_real_, # True probability of blue jar_state = NA_integer_, # Which jar: 1=mostly blue, 0=mostly red blue1 = NA_integer_, # Direct evidence (blue marbles) total1 = NA_integer_, # Total direct evidence blue2 = NA_integer_, # Social evidence (blue signals) total2 = NA_integer_, # Total social evidence belief = NA_real_, # Agent&#39;s belief that next marble is blue choice = NA_integer_ # Agent&#39;s choice (1=blue, 0=red) ) # Initialize belief tracking variables alpha_param &lt;- 1.0 beta_param &lt;- 1.0 # Set initial jar state randomly results$jar_state[1] &lt;- rbinom(1, 1, 0.5) results$true_p_blue[1] &lt;- ifelse(results$jar_state[1] == 1, 0.8, 0.2) # Generate the sequence of jar states (Markov process) for (t in 2:n_trials) { # Transition probability depends on previous state p_blue &lt;- p_blue_transitions[2 - results$jar_state[t-1]] results$jar_state[t] &lt;- rbinom(1, 1, p_blue) results$true_p_blue[t] &lt;- ifelse(results$jar_state[t] == 1, 0.8, 0.2) } # Generate evidence and choices for each trial for (t in 1:n_trials) { # Generate direct evidence (8 marbles per trial) results$total1[t] &lt;- 8 results$blue1[t] &lt;- rbinom(1, results$total1[t], results$true_p_blue[t]) # Generate social evidence (3 signals per trial) results$total2[t] &lt;- 3 results$blue2[t] &lt;- rbinom(1, results$total2[t], results$true_p_blue[t]) # Calculate current belief based on previous evidence if (t == 1) { # First trial - start with uniform prior results$belief[t] &lt;- 0.5 } else { # Calculate weighted evidence from previous trial weighted_blue1 &lt;- results$blue1[t-1] * weight_direct weighted_red1 &lt;- (results$total1[t-1] - results$blue1[t-1]) * weight_direct weighted_blue2 &lt;- results$blue2[t-1] * weight_social weighted_red2 &lt;- (results$total2[t-1] - results$blue2[t-1]) * weight_social # Update belief parameters with learning rate alpha_param &lt;- alpha_param + learning_rate * (weighted_blue1 + weighted_blue2) beta_param &lt;- beta_param + learning_rate * (weighted_red1 + weighted_red2) # Calculate updated belief results$belief[t] &lt;- alpha_param / (alpha_param + beta_param) } # Generate choice based on current belief results$choice[t] &lt;- rbinom(1, 1, results$belief[t]) } return(results) } # Simulate data for multiple agents with different parameters set.seed(42) # Number of agents and trials n_agents &lt;- 20 n_trials &lt;- 50 # Create agent parameters agent_params &lt;- tibble( agent_id = 1:n_agents, # Generate random parameters weight_direct = rlnorm(n_agents, meanlog = 0, sdlog = 0.3), weight_social = rlnorm(n_agents, meanlog = -0.2, sdlog = 0.3), learning_rate = rlnorm(n_agents, meanlog = -1, sdlog = 0.5) # Typically &lt; 1 ) # Simulate data for all agents sequential_sim_data &lt;- map_dfr(1:n_agents, function(i) { agent_data &lt;- simulate_sequential_agent( n_trials = n_trials, weight_direct = agent_params$weight_direct[i], weight_social = agent_params$weight_social[i], learning_rate = agent_params$learning_rate[i] ) # Add agent ID agent_data$agent_id &lt;- i return(agent_data) }) # View data summary sequential_summary &lt;- sequential_sim_data %&gt;% group_by(agent_id) %&gt;% summarize( n_trials = n(), mean_belief = mean(belief), prop_blue_choice = mean(choice), accuracy = mean(choice == jar_state), .groups = &quot;drop&quot; ) # Join with true parameters sequential_summary &lt;- sequential_summary %&gt;% left_join(agent_params, by = &quot;agent_id&quot;) # Print summary head(sequential_summary) ## # A tibble: 6 × 8 ## agent_id n_trials mean_belief prop_blue_choice accuracy weight_direct weight_social ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 50 0.497 0.58 0.4 1.51 0.747 ## 2 2 50 0.460 0.46 0.52 0.844 0.480 ## 3 3 50 0.491 0.54 0.42 1.12 0.778 ## 4 4 50 0.421 0.44 0.56 1.21 1.18 ## 5 5 50 0.493 0.46 0.54 1.13 1.45 ## 6 6 50 0.586 0.58 0.46 0.969 0.720 ## # ℹ 1 more variable: learning_rate &lt;dbl&gt; # Visualize belief updating for a few agents selected_agents &lt;- c(1, 5, 10) p_belief_updating &lt;- sequential_sim_data %&gt;% filter(agent_id %in% selected_agents) %&gt;% ggplot(aes(x = trial, y = belief, color = factor(agent_id), group = agent_id)) + geom_line(size = 1) + geom_point(aes(shape = factor(jar_state)), size = 3) + scale_shape_manual(values = c(&quot;0&quot; = 16, &quot;1&quot; = 17), labels = c(&quot;Mostly Red Jar&quot;, &quot;Mostly Blue Jar&quot;)) + ylim(0, 1) + labs( title = &quot;Sequential Belief Updating&quot;, subtitle = &quot;Belief evolution across trials with changing jar probabilities&quot;, x = &quot;Trial&quot;, y = &quot;Belief (Probability of Blue)&quot;, color = &quot;Agent ID&quot;, shape = &quot;True Jar State&quot; ) + theme_minimal() # Display the plot print(p_belief_updating) # Prepare data for Stan fitting stan_data_sequential &lt;- list( N = nrow(sequential_sim_data), J = n_agents, T = n_trials, agent_id = sequential_sim_data$agent_id, trial_id = sequential_sim_data$trial, choice = sequential_sim_data$choice, blue1 = sequential_sim_data$blue1, total1 = sequential_sim_data$total1, blue2 = sequential_sim_data$blue2, total2 = sequential_sim_data$total2, trials_per_agent = rep(n_trials, n_agents) ) 11.23.5 Fitting and Evaluating the Sequential Models Now we can fit the models to our simulated data: # Check if we need to regenerate model fits if (regenerate_simulations) { # Compile Stan models mod_seq_single &lt;- cmdstan_model( file.path(&quot;stan/10_sequential_updating.stan&quot;), cpp_options = list(stan_threads = TRUE) ) mod_seq_multilevel &lt;- cmdstan_model( file.path(&quot;stan/10_multilevel_sequential_updating.stan&quot;), cpp_options = list(stan_threads = TRUE) ) # Fit the single-agent model to one agent&#39;s data # For demonstration, we&#39;ll use the first agent agent1_data &lt;- filter(sequential_sim_data, agent_id == 1) stan_data_single &lt;- list( T = nrow(agent1_data), choice = agent1_data$choice, blue1 = agent1_data$blue1, total1 = agent1_data$total1, blue2 = agent1_data$blue2, total2 = agent1_data$total2 ) fit_seq_single &lt;- mod_seq_single$sample( data = stan_data_single, seed = 42, chains = 2, parallel_chains = 2, threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 1000, refresh = 200, adapt_delta = 0.9 ) # Fit the multilevel model to all agents&#39; data fit_seq_multilevel &lt;- mod_seq_multilevel$sample( data = stan_data_sequential, seed = 43, chains = 2, parallel_chains = 2, iter_warmup = 1000, iter_sampling = 1000, threads_per_chain = 1, refresh = 200, adapt_delta = 0.95 ) # Save model fits fit_seq_single$save_object(&quot;simmodels/fit_sequential_single.rds&quot;) fit_seq_multilevel$save_object(&quot;simmodels/fit_sequential_multilevel.rds&quot;) cat(&quot;Models fitted and saved.\\n&quot;) } else { # Load existing model fits fit_seq_single &lt;- readRDS(&quot;simmodels/fit_sequential_single.rds&quot;) fit_seq_multilevel &lt;- readRDS(&quot;simmodels/fit_sequential_multilevel.rds&quot;) cat(&quot;Loaded existing model fits.\\n&quot;) } ## Loaded existing model fits. # Check for convergence issues cat(&quot;Checking convergence for single-agent model:\\n&quot;) ## Checking convergence for single-agent model: print(fit_seq_single$summary(c(&quot;total_weight&quot;, &quot;weight_prop&quot;, &quot;alpha&quot;))) ## # A tibble: 3 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 total_weight 1.03 0.895 0.576 0.428 0.415 2.12 1.00 1650. 1273. ## 2 weight_prop 0.500 0.498 0.285 0.369 0.0646 0.950 1.00 1986. 1225. ## 3 alpha 0.369 0.325 0.193 0.166 0.144 0.751 1.00 1802. 1400. cat(&quot;\\nChecking convergence for multilevel model (population parameters):\\n&quot;) ## ## Checking convergence for multilevel model (population parameters): print(fit_seq_multilevel$summary(c(&quot;pop_total_weight&quot;, &quot;pop_weight_prop&quot;, &quot;pop_alpha&quot;))) ## # A tibble: 3 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 pop_total_weight 1.89 1.31 2.49 1.01 0.342 5.10 1.00 2640. 1624. ## 2 pop_weight_prop 0.660 0.672 0.150 0.164 0.391 0.884 0.999 3008. 1583. ## 3 pop_alpha 0.626 0.456 0.601 0.354 0.124 1.73 0.999 2611. 1643. # Extract posterior samples draws_seq_single &lt;- as_draws_df(fit_seq_single$draws()) draws_seq_multilevel &lt;- as_draws_df(fit_seq_multilevel$draws()) # Examine parameter recovery for the single agent agent1_params &lt;- agent_params %&gt;% filter(agent_id == 1) agent1_recovery &lt;- tibble( parameter = c(&quot;weight_direct&quot;, &quot;weight_social&quot;, &quot;learning_rate&quot;), true_value = c( agent1_params$weight_direct, agent1_params$weight_social, agent1_params$learning_rate ), estimated = c( mean(draws_seq_single$weight_direct), mean(draws_seq_single$weight_social), mean(draws_seq_single$alpha) ), error = estimated - true_value, pct_error = 100 * error / true_value ) # Print recovery results cat(&quot;\\nParameter recovery for Agent 1:\\n&quot;) ## ## Parameter recovery for Agent 1: print(agent1_recovery) ## # A tibble: 3 × 5 ## parameter true_value estimated error pct_error ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 weight_direct 1.51 0.522 -0.987 -65.4 ## 2 weight_social 0.747 0.506 -0.241 -32.3 ## 3 learning_rate 0.408 0.369 -0.0387 -9.49 # Evaluate population-level recovery pop_recovery &lt;- tibble( parameter = c(&quot;mean_weight_direct&quot;, &quot;mean_weight_social&quot;, &quot;mean_learning_rate&quot;), true_value = c( mean(agent_params$weight_direct), mean(agent_params$weight_social), mean(agent_params$learning_rate) ), estimated = c( mean(draws_seq_multilevel$pop_weight_direct), mean(draws_seq_multilevel$pop_weight_social), mean(draws_seq_multilevel$pop_alpha) ), error = estimated - true_value, pct_error = 100 * error / true_value ) # Print population recovery results cat(&quot;\\nPopulation parameter recovery:\\n&quot;) ## ## Population parameter recovery: print(pop_recovery) ## # A tibble: 3 × 5 ## parameter true_value estimated error pct_error ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 mean_weight_direct 1.13 1.23 0.0945 8.33 ## 2 mean_weight_social 0.795 0.657 -0.138 -17.3 ## 3 mean_learning_rate 0.409 0.626 0.218 53.3 # Extract individual parameter estimates n_chains &lt;- length(unique(draws_seq_multilevel$.chain)) n_iterations &lt;- nrow(draws_seq_multilevel) / n_chains n_samples &lt;- n_chains * n_iterations weight_direct_samples &lt;- matrix(NA, nrow = n_samples, ncol = n_agents) weight_social_samples &lt;- matrix(NA, nrow = n_samples, ncol = n_agents) alpha_samples &lt;- matrix(NA, nrow = n_samples, ncol = n_agents) for (j in 1:n_agents) { weight_direct_samples[, j] &lt;- draws_seq_multilevel[[paste0(&quot;weight_direct[&quot;, j, &quot;]&quot;)]] weight_social_samples[, j] &lt;- draws_seq_multilevel[[paste0(&quot;weight_social[&quot;, j, &quot;]&quot;)]] alpha_samples[, j] &lt;- draws_seq_multilevel[[paste0(&quot;alpha[&quot;, j, &quot;]&quot;)]] } # Calculate posterior means weight_direct_est &lt;- colMeans(weight_direct_samples) weight_social_est &lt;- colMeans(weight_social_samples) alpha_est &lt;- colMeans(alpha_samples) # Create recovery data for all agents recovery_data &lt;- tibble( agent_id = 1:n_agents, # True parameters true_weight_direct = agent_params$weight_direct, true_weight_social = agent_params$weight_social, true_learning_rate = agent_params$learning_rate, # Estimated parameters est_weight_direct = weight_direct_est, est_weight_social = weight_social_est, est_learning_rate = alpha_est ) # Calculate recovery metrics recovery_data &lt;- recovery_data %&gt;% mutate( error_direct = est_weight_direct - true_weight_direct, error_social = est_weight_social - true_weight_social, error_learning = est_learning_rate - true_learning_rate, pct_error_direct = 100 * error_direct / true_weight_direct, pct_error_social = 100 * error_social / true_weight_social, pct_error_learning = 100 * error_learning / true_learning_rate ) # Create recovery plots p_recovery &lt;- recovery_data %&gt;% pivot_longer( cols = c(starts_with(&quot;true_&quot;), starts_with(&quot;est_&quot;)), names_to = c(&quot;type&quot;, &quot;parameter&quot;), names_pattern = &quot;(true|est)_(.*)&quot; ) %&gt;% pivot_wider( names_from = type, values_from = value ) %&gt;% mutate( parameter = factor( parameter, levels = c(&quot;weight_direct&quot;, &quot;weight_social&quot;, &quot;learning_rate&quot;), labels = c(&quot;Direct Evidence Weight&quot;, &quot;Social Evidence Weight&quot;, &quot;Learning Rate&quot;) ) ) %&gt;% ggplot(aes(x = true, y = est)) + geom_point(size = 3, alpha = 0.7) + geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;) + facet_wrap(~ parameter, scales = &quot;free&quot;) + labs( title = &quot;Parameter Recovery for Sequential Updating Model&quot;, subtitle = &quot;Comparing true parameter values to posterior means&quot;, x = &quot;True Parameter Value&quot;, y = &quot;Estimated Parameter Value&quot; ) + theme_minimal() # Display recovery plot print(p_recovery) # Examine posterior predictive checks # Extract belief trajectories belief_samples &lt;- array(NA, dim = c(n_samples, n_agents, n_trials)) for (j in 1:n_agents) { for (t in 1:n_trials) { belief_samples[, j, t] &lt;- draws_seq_multilevel[[paste0(&quot;belief[&quot;, j, &quot;,&quot;, t, &quot;]&quot;)]] } } # Calculate mean and credible intervals for beliefs belief_summary &lt;- tibble( agent_id = rep(rep(1:n_agents, each = n_trials), 3), trial = rep(rep(1:n_trials, n_agents), 3), statistic = rep(c(&quot;mean&quot;, &quot;lower&quot;, &quot;upper&quot;), each = n_agents * n_trials), value = c( # Mean belief across samples apply(belief_samples, c(2, 3), mean) %&gt;% as.vector(), # Lower 95% CI apply(belief_samples, c(2, 3), quantile, 0.025) %&gt;% as.vector(), # Upper 95% CI apply(belief_samples, c(2, 3), quantile, 0.975) %&gt;% as.vector() ) ) # Reshape for plotting belief_wider &lt;- belief_summary %&gt;% pivot_wider( names_from = statistic, values_from = value ) # Visualize belief trajectories for selected agents p_belief_trajectories &lt;- belief_wider %&gt;% filter(agent_id %in% selected_agents) %&gt;% left_join(sequential_sim_data %&gt;% dplyr::select(agent_id, trial, true_p_blue, jar_state), by = c(&quot;agent_id&quot;, &quot;trial&quot;)) %&gt;% ggplot() + # Add true jar state as background geom_rect(aes(xmin = trial - 0.5, xmax = trial + 0.5, ymin = -Inf, ymax = Inf, fill = factor(jar_state)), alpha = 0.2) + # Add posterior belief intervals geom_ribbon(aes(x = trial, ymin = lower, ymax = upper, group = agent_id), alpha = 0.3, fill = &quot;blue&quot;) + # Add mean posterior belief geom_line(aes(x = trial, y = mean, color = factor(agent_id)), size = 1) + # Add true belief from simulation geom_line(data = sequential_sim_data %&gt;% filter(agent_id %in% selected_agents), aes(x = trial, y = belief, group = agent_id), linetype = &quot;dashed&quot;) + # Add true choices geom_point(data = sequential_sim_data %&gt;% filter(agent_id %in% selected_agents), aes(x = trial, y = choice, shape = &quot;Actual Choice&quot;), size = 2) + # Styling facet_wrap(~ agent_id, ncol = 1) + scale_fill_manual(values = c(&quot;0&quot; = &quot;pink&quot;, &quot;1&quot; = &quot;lightblue&quot;), labels = c(&quot;Mostly Red Jar&quot;, &quot;Mostly Blue Jar&quot;), name = &quot;True Jar State&quot;) + scale_color_brewer(palette = &quot;Dark2&quot;, name = &quot;Agent ID&quot;) + scale_shape_manual(values = c(&quot;Actual Choice&quot; = 4)) + ylim(0, 1) + labs( title = &quot;Belief Updating Over Time&quot;, subtitle = &quot;Blue ribbons show 95% credible intervals of estimated beliefs\\nDashed lines show true simulated beliefs&quot;, x = &quot;Trial&quot;, y = &quot;Belief/Choice Probability&quot;, shape = &quot;&quot; ) + theme_minimal() # Display belief trajectories print(p_belief_trajectories) # Compare learning styles across agents learning_styles &lt;- recovery_data %&gt;% mutate( relative_weight = est_weight_direct / est_weight_social, learning_speed = est_learning_rate ) p_learning_styles &lt;- ggplot(learning_styles, aes(x = relative_weight, y = learning_speed)) + geom_point(size = 3, alpha = 0.7) + geom_text(aes(label = agent_id), hjust = -0.3, vjust = 0.3, size = 3) + labs( title = &quot;Individual Learning Styles&quot;, subtitle = &quot;Mapping agents by their evidence weighting and learning speed&quot;, x = &quot;Relative Weight (Direct/Social)&quot;, y = &quot;Learning Rate&quot; ) + theme_minimal() print(p_learning_styles) # Calculate correlations between estimated parameters param_correlations &lt;- cor( cbind( learning_styles$est_weight_direct, learning_styles$est_weight_social, learning_styles$est_learning_rate ) ) colnames(param_correlations) &lt;- rownames(param_correlations) &lt;- c(&quot;Weight (Direct)&quot;, &quot;Weight (Social)&quot;, &quot;Learning Rate&quot;) # Extract population-level parameter correlations from the model correlation_matrix &lt;- matrix(NA, nrow = 3, ncol = 3) for (i in 1:3) { for (j in 1:3) { correlation_matrix[i, j] &lt;- mean(draws_seq_multilevel[[paste0(&quot;Omega[&quot;, i, &quot;,&quot;, j, &quot;]&quot;)]]) } } colnames(correlation_matrix) &lt;- rownames(correlation_matrix) &lt;- c(&quot;Total Weight&quot;, &quot;Weight Prop&quot;, &quot;Learning Rate&quot;) cat(&quot;\\nEstimated parameter correlations:\\n&quot;) ## ## Estimated parameter correlations: print(param_correlations) ## Weight (Direct) Weight (Social) Learning Rate ## Weight (Direct) 1.00000000 0.7527066 0.08451286 ## Weight (Social) 0.75270656 1.0000000 0.14876461 ## Learning Rate 0.08451286 0.1487646 1.00000000 cat(&quot;\\nPopulation-level parameter correlations:\\n&quot;) ## ## Population-level parameter correlations: print(correlation_matrix) ## Total Weight Weight Prop Learning Rate ## Total Weight 1.000000000 0.017304172 0.007257079 ## Weight Prop 0.017304172 1.000000000 0.008023128 ## Learning Rate 0.007257079 0.008023128 1.000000000 11.24 Sequential Bayesian Evidence Integration Models 11.24.1 Understanding Belief Updating Over Time Real-world learning rarely happens all at once - it’s a dynamic process where our beliefs evolve as we gather new evidence over time. The sequential Bayesian models we’ve developed capture this dynamic learning process by tracking how beliefs are updated from trial to trial. 11.24.2 The Sequential Updating Framework Our sequential updating models build on the static evidence integration models from earlier, but with a crucial difference: beliefs are continuously updated based on new evidence. This creates a recursive structure where: The agent starts with some initial belief (prior) After observing evidence, they update their belief (posterior) This posterior becomes the prior for the next trial The process repeats for each new piece of evidence The key parameters that govern this updating process are: Evidence weights (weight_direct and weight_social): How much influence each type of evidence has Learning rate (alpha): How quickly beliefs change in response to new evidence The learning rate parameter is particularly important - it determines whether an agent is conservative (low learning rate) or responsive (high learning rate) to new information. A learning rate near 1.0 means the agent fully incorporates new evidence, while a rate closer to 0 means the agent makes only small adjustments to beliefs. 11.24.3 Mathematical Formulation For each trial t, the agent’s belief is updated according to: α_t = α_t − 1 + λ × (w_d × E_d, t − 1 + w_s × E_s, t − 1) β_t = β_t − 1 + λ × (w_d × (T_d, t−1−E_d, t−1) + w_s × (T_s,t−1−E_s,t−1)) Belief_t = α_t α_t + β_t Where: α_t and β_t are the parameters of the Beta distribution representing the belief at trial t λ is the learning rate w_d and w_s are the weights for direct and social evidence E_{d,t−1} and E_{s,t-1} are the counts of blue marbles/signals in the previous trial T_{d,t-1} and T_{s,t-1} are the total counts of marbles/signals in the previous trial 11.24.4 Moving from Single-Agent to Multilevel The multilevel extension allows us to model individual differences in learning while still leveraging the commonalities across individuals. This approach: Captures individual learning styles: Some people may learn faster, others may weight certain evidence types more heavily Models population distributions: Helps understand the typical learning patterns and the range of variation Improves parameter estimation: Especially for individuals with limited or noisy data The multilevel structure adds substantial complexity to the model implementation, requiring careful handling of: Trial sequences: Each agent has their own sequence of trials and updating process Parameter correlations: Learning rate might correlate with evidence weighting Computational efficiency: Sequential updating creates dependencies that make parallelization challenging Interpreting Model Results Our simulation and model fitting reveal several important insights: Parameter Recovery The model successfully recovers the key cognitive parameters: Evidence weights: How much individuals trust different information sources Learning rate: How quickly they update their beliefs This validates that our model can meaningfully measure these cognitive processes from observed choices. Learning Style Differences The scatterplot of learning styles shows a two-dimensional space of cognitive strategies: The x-axis represents relative weighting of direct vs. social evidence The y-axis represents learning speed (how quickly beliefs change) This creates a typology of learners: Fast direct learners: Rapidly update based primarily on their own observations Cautious social learners: Slowly incorporate information, with emphasis on social cues Balanced adapters: Moderate learning rate with equal weighting of evidence sources Belief Trajectories The plots of belief trajectories over time reveal how individuals track changing environmental statistics: The shaded regions show the model’s uncertainty about beliefs The comparison with true simulated beliefs validates the model’s ability to recover learning dynamics The background coloring shows how beliefs align with true environmental states (jar probabilities) Parameter Correlations The correlation matrix reveals relationships between cognitive parameters: A negative correlation between learning rate and total evidence weight would suggest compensatory strategies (fast updating with conservative evidence weighting, or slow updating with strong evidence weighting) Correlations between direct and social weights might indicate general trust or skepticism toward evidence regenerate_simulations &lt;- FALSE pacman::p_load( tidyverse, future, purrr, furrr, patchwork, brms, cmdstanr ) "],["introduction-to-categorization-models.html", "Chapter 12 Introduction to Categorization Models 12.1 The Fundamental Problem of Categorization 12.2 Why Model Categorization? 12.3 Historical Development of Categorization Models 12.4 The Three Model Classes: Core Assumptions and Predictions 12.5 Our Implementation Approach 12.6 The Generalized Context Model (GCM) 12.7 Multilevel Generalized Context Model (GCM) 12.8 GCM in Context: Strengths and Limitations 12.9 Some notes about student-produced models of categorization", " Chapter 12 Introduction to Categorization Models 12.1 The Fundamental Problem of Categorization Categorization is one of the most fundamental cognitive abilities that humans possess. From early childhood, we learn to organize the world around us into meaningful categories: distinguishing food from non-food, safe from dangerous, or one letter from another. This process of assigning objects to categories is complex, involving perception, memory, attention, and decision-making. How do humans learn categories and make categorization decisions? This seemingly simple question has generated decades of research and theoretical debate in cognitive science. The answer matters not only for understanding human cognition but also for developing artificial intelligence systems, designing educational interventions, and understanding cognitive disorders that affect categorization abilities. 12.2 Why Model Categorization? Computational models offer a powerful way to formalize theories about categorization. By implementing these theories as computer algorithms, we can: Explore the consequences of different theoretical assumptions Make precise predictions about human behavior in categorization tasks Test competing theories against empirical data In this module, we will explore three major approaches to modeling categorization: Exemplar models - which propose that categories are represented by storing individual examples Prototype models - which suggest categories are represented by an abstract “average” or central tendency Rule-based models - which posit that categories are represented by explicit rules or decision boundaries Each approach captures different aspects of human categorization and has generated substantial empirical research. We’ll implement computational versions of each model type, allowing us to compare their behavior and predictions. 12.3 Historical Development of Categorization Models The computational modeling of categorization has a rich history in cognitive psychology: 12.3.1 Early Views: The Classical Approach Early theories of categorization followed what is now called the classical view, where categories were defined by necessary and sufficient features. In this view, category membership is an all-or-nothing affair: an object either satisfies the criteria or it doesn’t. While intuitive, this approach struggled to explain many aspects of human categorization, such as: Graded category membership (some members seem “more typical” than others) Unclear boundaries between categories Context-dependent categorization Family resemblance structures (where no single feature is necessary) 12.3.2 The Prototype Revolution In the 1970s, Eleanor Rosch’s pioneering work on prototypes challenged the classical view. She demonstrated that categories appear to be organized around central tendencies or “prototypes,” with membership determined by similarity to these prototypes. Objects closer to the prototype are categorized more quickly and consistently. Prototype models formalize this idea by representing categories as their central tendency in a psychological feature space. New items are classified based on their similarity to these prototypes. 12.3.3 The Exemplar Alternative In the late 1970s and 1980s, researchers like Douglas Medin and Robert Nosofsky proposed that rather than abstracting prototypes, people might store individual exemplars of categories and make judgments based on similarity to these stored examples. The Generalized Context Model (GCM), developed by Nosofsky, became the standard exemplar model. It proposes that categorization decisions are based on the summed similarity of a new stimulus to all stored exemplars of each category, weighted by attention to different stimulus dimensions. 12.3.4 Rule-Based Models While similarity-based approaches (both prototype and exemplar) gained prominence, other researchers argued that people sometimes use explicit rules for categorization. Rule-based models propose that categorization involves applying decision rules that partition the stimulus space. Models like Bayesian particle filters, multinomial trees, and also COVIS (COmpetition between Verbal and Implicit Systems) and RULEX (RULe-plus-EXception) formalize these ideas, suggesting that rule learning and application form a core part of human categorization, particularly for well-defined categories. Hybrid Approaches More recent work has focused on hybrid models that incorporate elements of multiple approaches. Models like SUSTAIN (Supervised and Unsupervised STratified Adaptive Incremental Network) and ATRIUM (Attention to Rules and Instances in a Unified Model) propose that humans can flexibly switch between strategies or that different systems operate in parallel. 12.4 The Three Model Classes: Core Assumptions and Predictions Let’s examine the core assumptions of the three model classes we’ll be implementing: 12.4.1 Exemplar Models: Individual Instances as Category Representations Core Assumption: Categories are represented by storing individual examples (exemplars) in memory. Key Features: No abstraction: All encountered exemplars are stored Categorization involves computing similarity to all stored exemplars Similarity is often modeled using an exponential decay function of distance Different dimensions can receive different attention weights Memory effects (recency, frequency) can influence categorization Predictions: Sensitivity to specific training examples Better handling of exceptions and atypical category members Categorization of new items depends on their similarity to specific remembered examples Learning can be incremental, one example at a time 12.4.2 Prototype Models: Categories as Central Tendencies Core Assumption: Categories are represented by their central tendencies or prototypes. Key Features: Abstraction: Information about individual examples is integrated into a summary representation Only the prototype (and possibly variance information) is stored for each category Categorization involves computing similarity to category prototypes Can be updated incrementally (as in Kalman filter approaches) Predictions: The most typical (average) category members will be categorized most easily Less sensitivity to specific examples More efficient in terms of memory requirements May struggle with categories that have complex structures (e.g., non-linear boundaries) 12.4.3 Rule-Based Models: Categories as Decision Boundaries Core Assumption: Categories are represented by explicit rules that specify the necessary and sufficient conditions for category membership. Key Features: Rules partition the stimulus space into regions corresponding to different categories *Rules can be simple (single dimension) or complex (combining multiple dimensions) Focus is on relevant dimensions while irrelevant dimensions are ignored Rule selection often involves hypothesis testing Predictions: Sharp category boundaries Fast decisions once rules are learned Selective attention to rule-relevant dimensions Less sensitivity to similarity once rules are established Often involves a period of hypothesis testing and rule discovery In this module, we’ll implement computational versions of these models and apply them to simulated and real data, allowing us to directly compare their behavior and evaluate their cognitive plausibility. 12.5 Our Implementation Approach We’ll implement three models representing each major theoretical approach: Generalized Context Model (GCM) - A canonical exemplar model where categorization is based on similarity to all stored examples Kalman Filter Prototype Model - A dynamic prototype model that incrementally updates category representations as new examples are encountered Bayesian Particle Filter for Rules - A rule-based model that maintains and updates a distribution over potential categorization rules Each implementation will follow a similar structure: Core model function that simulates the categorization process Stan implementation for parameter estimation Simulation framework for generating synthetic data Parameter recovery analyses Model comparison framework Let’s begin with the Generalized Context Model (GCM), which has been the gold standard exemplar model in the field. 12.6 The Generalized Context Model (GCM) 12.6.1 Mathematical Foundations and Cognitive Principles The Generalized Context Model (GCM), developed by Robert Nosofsky in the 1980s, represents one of the most influential exemplar-based approaches to categorization. Unlike models that abstract category information into prototypes or rules, the GCM proposes that people store individual exemplars in memory and make categorization decisions based on similarity to these stored examples. 12.6.2 Core Assumptions of the GCM Memory for Instances: Every encountered example is stored in memory with its category label. Similarity Computation: Categorization decisions are based on computing the similarity between a new stimulus and all stored exemplars. Selective Attention: Attention can be distributed differently across stimulus dimensions, effectively stretching or shrinking the psychological space. Choice Probability: The probability of assigning a stimulus to a category is proportional to its summed similarity to exemplars of that category. 12.6.3 Mathematical Formulation The GCM is formalized through the following equations: Distance Calculation The psychological distance between two stimuli in a multidimensional space is computed as: \\[d_{ij} = \\left[ \\sum_{m=1}^{M} w_m |x_{im} - x_{jm}|^r \\right]^{\\frac{1}{r}}\\] Where: \\(d_{ij}\\) is the distance between stimuli i and j \\(x_{im}\\) is the value of stimulus i on dimension m \\(w_m\\) is the attention weight for dimension m (constrained so that \\(\\sum_{m} w_m = 1\\)) r determines the distance metric (typically 1 for city-block or 2 for Euclidean) In our implementation, we’ll use the city-block metric (r=1), which is appropriate for separable stimulus dimensions: # Distance distance &lt;- function(vect1, vect2, w) { return(sum(w * abs(vect1 - vect2))) } Similarity Computation Similarity is an exponentially decreasing function of distance: \\[\\eta_{ij} = e^{-c \\cdot d_{ij}}\\] Where: \\(\\eta_{ij}\\) is the similarity between stimuli i and j c is the sensitivity parameter that determines how quickly similarity decreases with distance A higher value of c means similarity decreases more rapidly with distance. This can represent factors like increased discriminability, reduced generalization, or greater specificity in memory. # Similarity similarity &lt;- function(distance, c) { return(exp(-c * distance)) } Let’s assess how similarity changes with distance for different values of the sensitivity parameter c: # Let&#39;s assess similarity dd &lt;- tibble( expand_grid( distance = c(0,.1,.2, .3,.4,.5,1,1.5,2,3,4,5,6), c = c(0.1, 0.2, 0.5, 0.7, 1, 1.5, 2, 3, 4, 5, 6))) %&gt;% mutate( similarity = similarity(distance, c) ) dd %&gt;% mutate(c = factor(c)) %&gt;% ggplot() + geom_line(aes(distance, similarity, group = c, color = c)) + theme_bw() From this visualization, we can observe: With low c values (e.g., 0.1), similarity remains high even for distant stimuli With high c values (e.g., 6.0), similarity drops rapidly even for small distances At intermediate values, there’s a more gradual decrease in similarity Category Response Probability The probability of assigning a stimulus i to category A is: \\[P(A|i) = \\frac{\\beta \\sum_{j \\in A} \\eta_{ij}}{\\beta \\sum_{j \\in A} \\eta_{ij} + (1-\\beta) \\sum_{j \\in B} \\eta_{ij}}\\] Where: \\(\\sum_{j \\in A} \\eta_{ij}\\) is the summed similarity to all exemplars in category A \\(\\beta\\) is a potential response bias for category A (often set to 0.5 for unbiased responding) In our implementation, we calculate this probability as shown in the GCM agent function below. 12.6.4 The GCM Agent Implementation With distance and similarity functions in place, we now need to implement an agent that can: Observe stimuli Store them into categories according to feedback Compare new stimuli to stored exemplars to assess which category is more likely The agent operates in two modes: Cold start: If not enough stimuli have been observed to have exemplars in each category, the agent picks a category at random Similarity-based choice: Otherwise, it calculates similarity to each category’s exemplars and makes a probabilistic choice ### generative model ### gcm &lt;- function(w, c, obs, cat_one, quiet = TRUE) { # create an empty list to save probability of saying &quot;1&quot; for each trial r &lt;- c() ntrials &lt;- nrow(obs) for (i in 1:ntrials) { # If quiet is FALSE, print every ten trials if (!quiet &amp;&amp; i %% 10 == 0) { print(paste(&quot;i =&quot;, i)) } # if this is the first trial, or there any category with no exemplars seen yet, set the choice to random if (i == 1 || sum(cat_one[1:(i - 1)]) == 0 || sum(cat_one[1:(i - 1)]) == (i - 1)) { r &lt;- c(r, .5) } else { similarities &lt;- c() # for each previously seen stimulus assess distance and similarity for (e in 1:(i - 1)) { sim &lt;- similarity(distance(obs[i, ], obs[e, ], w), c) similarities &lt;- c(similarities, sim) } # Calculate prob of saying &quot;1&quot; by dividing similarity to 1 by the sum of similarity to 1 and to 2 numerator &lt;- mean(similarities[cat_one[1:(i - 1)] == 1]) denominator &lt;- mean(similarities[cat_one[1:(i - 1)] == 1]) + mean(similarities[cat_one[1:(i - 1)] == 0]) r &lt;- c(r, numerator / denominator) } } return(rbinom(ntrials, 1, r)) } Let’s break down this implementation step by step: Initialization: The function sets up an empty vector r to store the probability of assigning each stimulus to category 1. Trial Loop: For each trial (stimulus presentation): First Trial or Missing Categories: If this is the first trial or if one of the categories hasn’t been observed yet, the model can’t make an informed decision and defaults to random guessing (probability 0.5). Similarity Computation: For later trials, the model computes the similarity between the current stimulus and all previously observed stimuli. Response Probability: The model calculates the probability of assigning the stimulus to category 1 based on the relative similarity to exemplars of each category. Note that we use the mean similarity rather than the sum. This is a common variant that helps the model maintain stable predictions as the number of exemplars grows. Decision: Finally, the model generates actual binary decisions by sampling from a Bernoulli distribution with the calculated probabilities. This implementation captures the core cognitive processes proposed by the GCM: Storage of exemplars in memory Computation of similarity to all stored exemplars Categorization based on relative similarity to different categories Probabilistic response generation 12.6.5 Simulating Behavior with the GCM To understand how the GCM behaves under different conditions, we can simulate its behavior on categorization tasks. First, we need to set up a task environment with stimuli, features, and category assignments. We will rely on a setup from Kruschke 1993. Participants go through 8 blocks of trials. Within each block, participants see 8 stimuli, the same across every block. Each stimulus has to be categorised (classified) as belonging to either category A or category B. After categorizing a stimulus, feedback is given, so that the participant knows which category the stimulus truly belongs to. The stimuli differ along two continuous dimensions (more specifically the height of a square and the position of a line with it). # Defining the stimuli, their height and position features, and their category stimulus &lt;- c(5,3,7,1,8,2,6,4) height &lt;- c(1,1, 2,2, 3,3, 4,4) position &lt;- c(2,3, 1,4, 1,4, 2,3) category &lt;- as.factor(c(0,0, 1,0, 1,0, 1,1)) # Making this into a tibble stimuli &lt;- tibble(stimulus, height, position, category) # Plotting to make sure all looks right ggplot(stimuli, aes(position, height, color = category, label = stimulus)) + geom_point(shape = 16, size = 3) + geom_label() + geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;) + theme_bw() # Generating the sequence of stimuli in the full experiment sequence &lt;- c() for (i in 1:8) { temp &lt;- sample(seq(8), 8, replace = F) sequence &lt;- append(sequence, temp) } experiment &lt;- tibble(stimulus = sequence, height = NA, position = NA, category = NA) for (i in seq(nrow(experiment))) { experiment$height[i] &lt;- stimuli$height[stimuli$stimulus == experiment$stimulus[i]] experiment$position[i] &lt;- stimuli$position[stimuli$stimulus == experiment$stimulus[i]] experiment$category[i] &lt;- as.numeric(as.character(stimuli$category[stimuli$stimulus == experiment$stimulus[i]])) } This setup creates a categorization task with: 8 unique stimuli varying along two dimensions (height and position) 2 categories (0 and 1) A randomized sequence of 64 trials (8 blocks of 8 stimuli each) The visualization helps us understand the category structure: stimuli are arranged in a 4×2 grid, with category membership indicated by color. The dashed line shows that the categories are not perfectly linearly separable. Now we can simulate how agents with different parameter settings would perform on this task: # Function to simulate responses with different parameter settings simulate_responses &lt;- function(agent, w, c) { observations &lt;- experiment %&gt;% dplyr::select(c(&quot;height&quot;, &quot;position&quot;)) category &lt;- experiment$category if (w == &quot;equal&quot;) { weight &lt;- rep(1 / 2, 2) } else if (w == &quot;skewed1&quot;) { weight &lt;- c(0, 1) } else if (w == &quot;skewed2&quot;) { weight &lt;- c(0.1, 0.9) } # simulate responses responses &lt;- gcm( weight, c, observations, category ) tmp_simulated_responses &lt;- experiment %&gt;% mutate( trial = seq(nrow(experiment)), sim_response = responses, correct = ifelse(category == sim_response, 1, 0), performance = cumsum(correct) / seq_along(correct), c = c, w = w, agent = agent ) return(tmp_simulated_responses) } This function allows us to: Vary attention weight settings (equal or skewed toward different dimensions) Vary the sensitivity parameter (c) Generate responses for multiple simulated agents Track performance (accuracy) over trials Let’s run simulations across a range of parameter values: if (regenerate_simulations) { # simulate responses across parameter combinations plan(multisession, workers = availableCores()) param_df &lt;- dplyr::tibble( expand_grid( agent = 1:10, c = seq(.1, 2, 0.2), w = c(&quot;equal&quot;, &quot;skewed1&quot;, &quot;skewed2&quot;) ) ) simulated_responses &lt;- future_pmap_dfr(param_df, simulate_responses, .options = furrr_options(seed = TRUE) ) # Save model fits write_csv(simulated_responses, &quot;simdata/W11_gcm_single_simulated_responses.csv&quot;) cat(&quot;Simulation saved.\\n&quot;) } else { # Load existing model fits simulated_responses &lt;- read_csv(&quot;simdata/W11_gcm_single_simulated_responses.csv&quot;) cat(&quot;Simulation loaded.\\n&quot;) } ## Simulation loaded. We can then visualize how different parameter settings affect learning and performance: p3 &lt;- simulated_responses %&gt;% mutate(w = as.factor(w)) %&gt;% ggplot(aes(trial, performance, group = w, color = w)) + geom_smooth() + theme_bw() + facet_wrap(c ~ .) p4 &lt;- simulated_responses %&gt;% mutate(c = as.factor(c)) %&gt;% ggplot(aes(trial, performance, group = c, color = c)) + geom_smooth() + theme_bw() + facet_wrap(w ~ .) p3 + p4 From these plots, we can observe: Attention Weights Matter: For this particular category structure, the equal weights condition generally performs better than the highly skewed weight conditions. This makes sense given that both dimensions are relevant for categorization in our stimulus set. Sensitivity Parameter Affects Performance: The optimal value of c depends on the category structure. Too low (&lt; 0.5) and the model overgeneralizes; too high (&gt; 1.5) and it may be too specific. Values around 1.0-1.5 typically produce the best performance with our stimulus set. Interaction Between Parameters: The effect of the sensitivity parameter c often depends on the attention weight distribution. For example, with skewed weights, higher c values may be needed to compensate for the lost information from the ignored dimension. 12.6.6 Implementing the GCM in Stan for Parameter Estimation While simulating the GCM helps us understand its behavior, we ultimately want to estimate its parameters from observed categorization data. To do this, we implement the GCM in Stan, allowing for Bayesian parameter estimation. gcm_single_stan &lt;- &quot; // Generalized Context Model (GCM) data { int&lt;lower=1&gt; ntrials; // number of trials int&lt;lower=1&gt; nfeatures; // number of predefined relevant features array[ntrials] int&lt;lower=0, upper=1&gt; cat_one; // true responses on a trial by trial basis array[ntrials] int&lt;lower=0, upper=1&gt; y; // decisions on a trial by trial basis array[ntrials, nfeatures] real obs; // stimuli as vectors of features real&lt;lower=0, upper=1&gt; b; // initial bias for category one over two // priors vector[nfeatures] w_prior_values; // concentration parameters for dirichlet distribution &lt;lower=1&gt; array[2] real c_prior_values; // mean and variance for logit-normal distribution } transformed data { array[ntrials] int&lt;lower=0, upper=1&gt; cat_two; // dummy variable for category two over cat 1 array[sum(cat_one)] int&lt;lower=1, upper=ntrials&gt; cat_one_idx; // array of which stimuli are cat 1 array[ntrials-sum(cat_one)] int&lt;lower=1, upper=ntrials&gt; cat_two_idx; // array of which stimuli are cat 2 int idx_one = 1; // Initializing int idx_two = 1; for (i in 1:ntrials){ cat_two[i] = abs(cat_one[i]-1); if (cat_one[i]==1){ cat_one_idx[idx_one] = i; idx_one +=1; } else { cat_two_idx[idx_two] = i; idx_two += 1; } } } parameters { simplex[nfeatures] w; // simplex means sum(w)=1 real logit_c; } transformed parameters { // parameter c real&lt;lower=0, upper=2&gt; c = inv_logit(logit_c)*2; // times 2 as c is bounded between 0 and 2 // parameter r (probability of response = category 1) array[ntrials] real&lt;lower=0.0001, upper=0.9999&gt; r; array[ntrials] real rr; for (i in 1:ntrials) { // calculate distance from obs to all exemplars array[(i-1)] real exemplar_sim; for (e in 1:(i-1)){ array[nfeatures] real tmp_dist; for (j in 1:nfeatures) { tmp_dist[j] = w[j]*abs(obs[e,j] - obs[i,j]); } exemplar_sim[e] = exp(-c * sum(tmp_dist)); } if (sum(cat_one[:(i-1)])==0 || sum(cat_two[:(i-1)])==0){ // if there are no examplars in one of the categories r[i] = 0.5; } else { // calculate similarity array[2] real similarities; array[sum(cat_one[:(i-1)])] int tmp_idx_one = cat_one_idx[:sum(cat_one[:(i-1)])]; array[sum(cat_two[:(i-1)])] int tmp_idx_two = cat_two_idx[:sum(cat_two[:(i-1)])]; similarities[1] = sum(exemplar_sim[tmp_idx_one]); similarities[2] = sum(exemplar_sim[tmp_idx_two]); // calculate r[i] rr[i] = (b*similarities[1]) / (b*similarities[1] + (1-b)*similarities[2]); // to make the sampling work if (rr[i] &gt; 0.9999){ r[i] = 0.9999; } else if (rr[i] &lt; 0.0001) { r[i] = 0.0001; } else if (rr[i] &gt; 0.0001 &amp;&amp; rr[i] &lt; 0.9999) { r[i] = rr[i]; } else { r[i] = 0.5; } } } } model { // Priors target += dirichlet_lpdf(w | w_prior_values); target += normal_lpdf(logit_c | c_prior_values[1], c_prior_values[2]); // Decision Data target += bernoulli_lpmf(y | r); } generated quantities { // Prior samples, posterior predictions, and log likelihood calculations... real w_prior; real c_prior; w_prior = dirichlet_rng(w | w_prior_values); c_prior = inv_logit(normal_rng(c_prior_values[1], c_prior_values[2])); }&quot; # Write the model to a file write_stan_file( gcm_single_stan, dir = &quot;stan/&quot;, basename = &quot;W11_gcm_single.stan&quot; ) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W11_gcm_single.stan&quot; The Stan implementation follows the same mathematical principles as our R implementation but is structured for parameter estimation. Key differences include: Parameter Constraints: Attention weights are implemented as a simplex, ensuring they sum to 1, and the sensitivity parameter c is constrained to a reasonable range. Preprocessing: The transformed data block creates useful indices for efficient computation. Numerical Stability: The implementation includes steps to avoid numerical issues, such as bounding probabilities away from 0 and 1. Priors: Priors for attention weights (Dirichlet) and sensitivity (normal on logit scale) are needed for Bayesian inference. Let’s fit this model to some of our simulated data to see how well it recovers the true generating parameters: d &lt;- simulated_responses %&gt;% subset( c == &quot;1.1&quot; &amp; w == &quot;equal&quot; ) gcm_data &lt;- list( ntrials = nrow(d), nfeatures = 2, cat_one = d$category, y = d$sim_response, obs = as.matrix(d[, c(&quot;height&quot;, &quot;position&quot;)]), b = 0.5, w_prior_values = c(1, 1), c_prior_values = c(0, 1) ) # Check if we need to regenerate model fits if (regenerate_simulations) { # Compile Stan models mod_gcm_single &lt;- cmdstan_model( file.path(&quot;stan/W11_gcm_single.stan&quot;), cpp_options = list(stan_threads = TRUE) ) samples_gcm &lt;- mod_gcm_single$sample( data = gcm_data, seed = 123, chains = 1, parallel_chains = 1, threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 1000, refresh = 500 ) # Save model fits samples_gcm$save_object(&quot;simmodels/W11_gcm_single.rds&quot;) cat(&quot;Models fitted and saved.\\n&quot;) } else { # Load existing model fits samples_gcm &lt;- readRDS(&quot;simmodels/W11_gcm_single.rds&quot;) cat(&quot;Loaded existing model fits.\\n&quot;) } ## Loaded existing model fits. Now we can check the parameter recovery: samples_gcm$summary() ## # A tibble: 1,285 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lp__ -372. -372. 0.997 0.732 -374. -371. 1.00 401. 658. ## 2 w[1] 0.490 0.489 0.0563 0.0557 0.393 0.581 0.999 883. 492. ## 3 w[2] 0.510 0.511 0.0563 0.0557 0.419 0.607 0.999 883. 492. ## 4 logit_c 0.193 0.194 0.166 0.163 -0.0917 0.464 1.00 814. 571. ## 5 c 1.10 1.10 0.0817 0.0804 0.954 1.23 1.00 813. 571. ## 6 r[1] 0.5 0.5 0 0 0.5 0.5 NA NA NA ## 7 r[2] 0.5 0.5 0 0 0.5 0.5 NA NA NA ## 8 r[3] 0.369 0.370 0.0169 0.0174 0.341 0.396 1.00 745. 478. ## 9 r[4] 0.655 0.656 0.0226 0.0214 0.616 0.691 1.00 894. 549. ## 10 r[5] 0.491 0.491 0.0502 0.0491 0.406 0.572 0.999 882. 454. ## # ℹ 1,275 more rows draws_df &lt;- as_draws_df(samples_gcm$draws()) draws_df &lt;- draws_df %&gt;% mutate(c_prior = rnorm(nrow(draws_df), 1,1), w_prior1 = rdirichlet(nrow(draws_df), c(1, 1))[,1], w_prior2 = rdirichlet(nrow(draws_df), c(1, 1))[,2]) ggplot(draws_df) + geom_histogram(aes(c), alpha = 0.6, fill = &quot;lightblue&quot;) + geom_histogram(aes(c_prior), alpha = 0.6, fill = &quot;pink&quot;) + geom_vline(xintercept = d$c[1]) + theme_bw() ggplot(draws_df) + geom_histogram(aes(`w[1]`), alpha = 0.6, fill = &quot;lightblue&quot;) + geom_histogram(aes(`w_prior1`), alpha = 0.6, fill = &quot;pink&quot;) + geom_vline(xintercept = 0.5) + theme_bw() ggplot(draws_df) + geom_histogram(aes(`w[2]`), alpha = 0.6, fill = &quot;lightblue&quot;) + geom_histogram(aes(`w_prior2`), alpha = 0.6, fill = &quot;pink&quot;) + geom_vline(xintercept = 0.5) + theme_bw() Recovery seems decent. Let’s look at the larger picture, but looping through the different combinations of parameter values. if (regenerate_simulations) { pacman::p_load(future, purrr, furrr) plan(multisession, workers = availableCores()) sim_d_and_fit &lt;- function(agent, scaling, weight1, weight2) { temp &lt;- simulated_responses %&gt;% subset( c == scaling &amp; w1 == weight1 &amp; w2 == weight2 &amp; agent == agent ) data &lt;- list( ntrials = nrow(temp), nfeatures = 2, cat_one = temp$category, y = temp$sim_response, obs = as.matrix(temp[, c(&quot;height&quot;, &quot;position&quot;)]), b = 0.5, w_prior_values = c(1, 1), c_prior_values = c(0, 1) ) samples_gcm &lt;- mod_gcm_single$sample( data = data, seed = 123, chains = 1, parallel_chains = 1, threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 1000, refresh = 500 ) draws_df &lt;- as_draws_df(samples_gcm$draws()) temp &lt;- tibble(trueC = scaling, trueW1 = weight1, trueW2 = weight2, agent = agent, estC = draws_df$c, estW1 = draws_df$`w[1]`, estW2 = draws_df$`w[2]` ) return(temp) } temp &lt;- tibble(unique(simulated_responses[,c(&quot;agent&quot;, &quot;c&quot;, &quot;w1&quot;, &quot;w2&quot;)])) %&gt;% rename( scaling = c, weight1 = w1, weight2 = w2 ) recovery_df &lt;- future_pmap_dfr(temp, sim_d_and_fit, .options = furrr_options(seed = TRUE)) # Save model fits write_csv(recovery_df, &quot;simdata/W11_GCM_single_recoverydf.csv&quot;) cat(&quot;Models fitted and saved.\\n&quot;) } else { # Load existing model fits recovery_df &lt;- read_csv(&quot;simdata/W11_GCM_single_recoverydf.csv&quot;) cat(&quot;Loaded existing model fits.\\n&quot;) } Time to visualize recovery_df &lt;- read_csv(&quot;simdata/W11_GCM_single_recoverydf.csv&quot;) vline_data &lt;- data.frame(trueC = unique(recovery_df$trueC)) p1 &lt;- ggplot(recovery_df) + geom_density(aes(estC), alpha = 0.3) + facet_wrap(. ~ trueC) + geom_vline(data = vline_data, aes(xintercept = trueC)) + theme_bw() vline_data &lt;- data.frame(trueW1 = unique(recovery_df$trueW1)) p2 &lt;- ggplot(recovery_df) + geom_density(aes(estW1), alpha = 0.3) + facet_wrap(. ~ trueW1) + geom_vline(data = vline_data, aes(xintercept = trueW1)) + theme_bw() vline_data &lt;- data.frame(trueW2 = unique(recovery_df$trueW2)) p3 &lt;- ggplot(recovery_df) + geom_density(aes(estW2), alpha = 0.3) + facet_wrap(. ~ trueW2) + geom_vline(data = vline_data, aes(xintercept = trueW2)) + theme_bw() p1 / p2 / p3 12.6.7 Key Parameters and Their Cognitive Interpretations The GCM has two primary parameters. Attention Weights (w) The attention weights determine how much each stimulus dimension contributes to similarity calculations: Higher weight on a dimension means greater attention to that dimension Attention is selective - increasing weight on one dimension necessarily decreases weight on others Attention can adapt to category structures, focusing on dimensions that discriminate between categories Sensitivity Parameter (c) The sensitivity parameter determines how quickly similarity decreases with distance: Higher c values mean similarity decreases more rapidly with distance Lower c values lead to more generalization across similar stimuli This parameter can be variously interpreted as representing: Perceptual discriminability: How easily differences can be perceived Memory distinctiveness: How separately items are stored in memory Categorization specificity: How narrowly categories are defined This multiplicity of interpretation should keep your critical senses on high alert. Math is oblivious of cognitive interpretation. All the interpretive work, triangulating with other evidence sources, making theoretical assumptions explicit, etc. is on you. 12.7 Multilevel Generalized Context Model (GCM) 12.7.1 The Need for a Multilevel Approach We are often interested in understanding both individual differences and group-level patterns in categorization. Here is where the multilevel (or hierarchical) approach comes in handy [See chapter 7]. The multilevel GCM extends the standard model by assuming that individual participant parameters are drawn from group-level distributions. This approach bridges the gap between treating all participants identically (ignoring individual differences) and treating all participants completely independently (ignoring shared patterns). 12.7.2 Mathematical Formulation of the Multilevel GCM The multilevel GCM extends the standard model in the following ways: Population-Level Parameters A population mean for the sensitivity parameter: \\(\\mu_c\\) A population standard deviation for the sensitivity parameter: \\(\\sigma_c\\) A population distribution for attention weights: \\(\\alpha = \\kappa \\cdot \\omega\\), where: \\(\\omega\\) is a population-level simplex (summing to 1) \\(\\kappa\\) is a concentration parameter controlling individual variation Individual-Level Parameters Each participant j has a sensitivity parameter \\(c_j \\sim \\text{Normal}(\\mu_c, \\sigma_c)\\) Each participant j has attention weights \\(w_j \\sim \\text{Dirichlet}(\\kappa \\cdot \\omega)\\) Probability of Categorization The probability calculations follow the same principles as the standard GCM but are performed separately for each participant. 12.7.3 Simulating Data for a Multilevel GCM To understand the multilevel GCM, let’s first simulate data from agents with varying parameters. We need to simulate hierarchical simplices for the attention weights: simulate_dirichlet &lt;- function(weights, kappa, agents){ w_n &lt;- length(weights) w_ind &lt;- rdirichlet(agents, weights * kappa) w_ind_df &lt;- tibble( agent = as.factor(rep(seq(agents), each = w_n)), value = c(w_ind), weight = rep(seq(w_n), agents) ) return(w_ind_df) } d &lt;- simulate_dirichlet(weights = c(0.5, 0.5), kappa = 100, agents = 10) p1 &lt;- ggplot(d, aes(weight, value, group = agent, color = agent)) + geom_point() + geom_line(linetype = &quot;dashed&quot;, alpha = 0.5) + ylim(0,1) + theme_bw() d &lt;- simulate_dirichlet(weights = c(0.5, 0.5), kappa = 1, agents = 10) p2 &lt;- ggplot(d, aes(weight, value, group = agent, color = agent)) + geom_point() + geom_line(linetype = &quot;dashed&quot;, alpha = 0.5) + ylim(0,1) + theme_bw() p1 + p2 These visualizations show how the concentration parameter \\(\\kappa\\) κ controls individual variation: With high \\(\\kappa\\) (e.g., 100), individual attention weights cluster tightly around the population mean With low \\(\\kappa\\) (e.g., 1), individual attention weights vary widely around the population mean Now we can simulate responses from multiple agents with parameters drawn from group-level distributions: simulate_responses &lt;- function(agent, w, c) { observations &lt;- experiment %&gt;% dplyr::select(c(&quot;height&quot;, &quot;position&quot;)) category &lt;- experiment$category # simulate responses responses &lt;- gcm( w, c, observations, category ) tmp_simulated_responses &lt;- experiment %&gt;% mutate( trial = seq(nrow(experiment)), sim_response = responses, correct = ifelse(category == sim_response, 1, 0), performance = cumsum(correct) / seq_along(correct), c = c, w1 = w[1], w2 = w[2], agent = agent ) return(tmp_simulated_responses) } simulate_ml_responses &lt;- function(agents, scalingM, scalingSD, weights, kappa) { w_ind &lt;- rdirichlet(agents, weights * kappa) c_ind &lt;- rnorm(agents, scalingM, scalingSD) for (i in 1:agents) { tmp &lt;- simulate_responses(i, w = c(w_ind[i,1:2]), c = c_ind[i]) if (i == 1) { simulated_responses &lt;- tmp } else { simulated_responses &lt;- rbind(simulated_responses, tmp) } } return(simulated_responses) } # Simulation parameters agents &lt;- 10 scalingM &lt;- 1 scalingSD &lt;- 0.1 weights &lt;- c(0.5,0.5) kappa &lt;- 1 # Simulate and visualize d &lt;- simulate_ml_responses(agents, scalingM, scalingSD, weights, kappa) ggplot(d, aes(trial, performance)) + geom_smooth() + geom_line(aes(group = agent, color = agent), alpha = 0.3) + theme_bw() This plot shows learning curves for multiple simulated agents. We can observe: Individual differences in learning trajectories A general group trend toward improved performance at first and a plateau after Variability in asymptotic performance levels 12.7.4 Implementing the Multilevel GCM in Stan The Stan implementation of the multilevel GCM extends the standard implementation by adding population-level parameters and hierarchical structure: gcm_ml_stan &lt;- &quot; // Generalized Context Model (GCM) - multilevel version data { int&lt;lower=1&gt; nsubjects; // number of subjects int&lt;lower=1&gt; ntrials; // number of trials int&lt;lower=1&gt; nfeatures; // number of predefined relevant features array[ntrials] int&lt;lower=0, upper=1&gt; cat_one; // true responses on a trial by trial basis array[ntrials, nsubjects] int&lt;lower=0, upper=1&gt; y; // decisions on a trial by trial basis array[ntrials, nfeatures] real obs; // stimuli as vectors of features assuming all participants get the same sequence real&lt;lower=0, upper=1&gt; b; // initial bias for category one over two // priors vector[nfeatures] w_prior_values; // concentration parameters for dirichlet distribution &lt;lower=1&gt; array[2] real c_prior_values; // mean and variance for logit-normal distribution } transformed data { // assuming all participants get the same sequence array[ntrials] int&lt;lower=0, upper=1&gt; cat_two; // dummy variable for category two over cat 1 array[sum(cat_one)] int&lt;lower=1, upper = ntrials&gt; cat_one_idx; // array of which stimuli are cat 1 array[ntrials - sum(cat_one)] int&lt;lower = 1, upper = ntrials&gt; cat_two_idx; // array of which stimuli are cat 2 int idx_one = 1; // Initializing int idx_two = 1; for (i in 1:ntrials){ cat_two[i] = abs(cat_one[i]-1); if (cat_one[i]==1){ cat_one_idx[idx_one] = i; idx_one +=1; } else { cat_two_idx[idx_two] = i; idx_two += 1; } } } parameters { real logit_c_M; // Pop Mean of the scaling parameter (how fast similarity decrease with distance). real&lt;lower = 0&gt; logit_c_SD; // Pop SD of the scaling parameter (how fast similarity decrease with distance). vector[nsubjects] logit_c; // scaling parameter (how fast similarity decrease with distance). simplex[nfeatures] weight; // simplex means sum(w)=1 real&lt;lower=0&gt; kappa; array[nsubjects] simplex[nfeatures] w_ind; // weight parameter (how much attention should be paid to feature 1 related to feature 2 - summing up to 1) } transformed parameters { // parameter w vector[nfeatures] alpha = kappa * weight; // parameter c vector&lt;lower=0,upper=2&gt;[nsubjects] c = inv_logit(logit_c)*2; // times 2 as c is bounded between 0 and 2 // parameter r (probability of response = category 1) array[ntrials, nsubjects] real&lt;lower=0.0001, upper=0.9999&gt; r; array[ntrials, nsubjects] real rr; for (sub in 1:nsubjects) { for (trial in 1:ntrials) { // calculate distance from obs to all exemplars array[(trial-1)] real exemplar_sim; for (e in 1:(trial-1)){ array[nfeatures] real tmp_dist; for (feature in 1:nfeatures) { tmp_dist[feature] = w_ind[sub,feature]*abs(obs[e,feature] - obs[trial,feature]); } exemplar_sim[e] = exp(-c[sub] * sum(tmp_dist)); } if (sum(cat_one[:(trial-1)])==0 || sum(cat_two[:(trial-1)])==0){ // if there are no examplars in one of the categories r[trial,sub] = 0.5; } else { // calculate similarity array[2] real similarities; array[sum(cat_one[:(trial-1)])] int tmp_idx_one = cat_one_idx[:sum(cat_one[:(trial-1)])]; array[sum(cat_two[:(trial-1)])] int tmp_idx_two = cat_two_idx[:sum(cat_two[:(trial-1)])]; similarities[1] = mean(exemplar_sim[tmp_idx_one]); similarities[2] = mean(exemplar_sim[tmp_idx_two]); // calculate r rr[trial,sub] = (b*similarities[1]) / (b*similarities[1] + (1-b)*similarities[2]); // to make the sampling work if (rr[trial,sub] &gt; 0.9999){ r[trial,sub] = 0.9999; } else if (rr[trial,sub] &lt; 0.0001) { r[trial,sub] = 0.0001; } else if (rr[trial,sub] &gt; 0.0001 &amp;&amp; rr[trial,sub] &lt; 0.9999) { r[trial,sub] = rr[trial,sub]; } else { r[trial,sub] = 0.5; }}}}} model { // Priors target += exponential_lpdf(kappa | 0.1); target += dirichlet_lpdf(weight | w_prior_values); target += normal_lpdf(logit_c_M | c_prior_values[1], c_prior_values[2]); target += normal_lpdf(logit_c_SD | 0, 1) - normal_lccdf(0 | 0, 1); target += normal_lpdf(logit_c | logit_c_M, logit_c_SD); // Decision Data for (sub in 1:nsubjects){ target += dirichlet_lpdf(w_ind[sub] | alpha); for (trial in 1:ntrials){ target += bernoulli_lpmf(y[trial,sub] | r[trial,sub]); } } }&quot; # Write the model to a file write_stan_file( gcm_ml_stan, dir = &quot;stan/&quot;, basename = &quot;W11_gcm_ml.stan&quot; ) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W11_gcm_ml.stan&quot; Key features of this multilevel implementation include: Population-Level Parameters: logit_c_M and logit_c_SD: The mean and standard deviation of the logit-transformed sensitivity parameter weight: The population-level attention weight simplex kappa: The concentration parameter controlling individual variation in attention weights Individual-Level Parameters: logit_c: Individual sensitivity parameters (logit-transformed) w_ind: Individual attention weight simplices Hierarchical Structure: Individual sensitivity parameters are drawn from a population normal distribution Individual attention weights are drawn from a population Dirichlet distribution The Dirichlet concentration is controlled by the kappa parameter Parameter Transformations: alpha = kappa * weight: The concentration parameters for the Dirichlet distribution c = inv_logit(logit_c)*2: Transformation from logit scale to the bounded [0,2] range 12.7.5 Fitting the Multilevel GCM To fit the multilevel model to our simulated data, we need to prepare the data in the correct format: ## Fit the simulated data d1 &lt;- d[,c(&quot;agent&quot;,&quot;trial&quot;,&quot;sim_response&quot;)] %&gt;% pivot_wider( names_from = agent, values_from = c(sim_response)) gcm_ml_data &lt;- list( nsubjects = agents, ntrials = nrow(experiment), nfeatures = 2, cat_one = experiment$category, y = as.matrix(d1[, 2:(agents + 1)]), obs = as.matrix(experiment[, c(&quot;height&quot;, &quot;position&quot;)]), b = 0.5, w_prior_values = c(1, 1), c_prior_values = c(0, 1) ) # Check if we need to regenerate model fits if (regenerate_simulations) { # Compile Stan models mod_GCM_ml &lt;- cmdstan_model( file.path(&quot;stan/W11_gcm_ml.stan&quot;), cpp_options = list(stan_threads = TRUE) ) # Fit the multilevel model to all agents&#39; data samples_gcm_ml &lt;- mod_GCM_ml$sample( data = gcm_ml_data, seed = 123, chains = 2, parallel_chains = 2, threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 1000, refresh = 1000 ) # Save model fits samples_gcm_ml$save_object(&quot;simmodels/samples_gcm_ml.rds&quot;) cat(&quot;Models fitted and saved.\\n&quot;) } else { # Load existing model fits samples_gcm_ml &lt;- readRDS(&quot;simmodels/samples_gcm_ml.rds&quot;) cat(&quot;Loaded existing model fits.\\n&quot;) } ## Loaded existing model fits. After fitting, we can examine the posterior distributions of both population and individual parameters: draws &lt;- as_draws_df(samples_gcm_ml) ggplot(draws, aes(.iteration, logit_c_M, group = .chain, color = .chain)) + geom_line(alpha = 0.5) + theme_classic() ggplot(draws, aes(.iteration, logit_c_SD, group = .chain, color = .chain)) + geom_line(alpha = 0.5) + theme_classic() ggplot(draws, aes(.iteration, `weight[1]`, group = .chain, color = .chain)) + geom_line(alpha = 0.5) + theme_classic() ggplot(draws, aes(.iteration, kappa, group = .chain, color = .chain)) + geom_line(alpha = 0.5) + theme_classic() We can also create visualizations showing the posterior distributions compared to the true parameter values: draws %&gt;% mutate(c_prior = rnorm(nrow(draws), 1,1)) %&gt;% ggplot() + geom_histogram(aes(logit_c_M), alpha = 0.6, fill = &quot;lightblue&quot;) + geom_histogram(aes(c_prior), alpha = 0.6, fill = &quot;pink&quot;) + geom_vline(xintercept = logit_scaled(scalingM,0, 2)) + theme_bw() draws %&gt;% mutate(c_prior = TruncatedNormal::rtnorm(nrow(draws), 0, 1, lb = 0)) %&gt;% ggplot() + geom_histogram(aes(logit_c_SD), alpha = 0.6, fill = &quot;lightblue&quot;) + geom_histogram(aes(c_prior), alpha = 0.6, fill = &quot;pink&quot;) + geom_vline(xintercept = 0.5) + theme_bw() draws %&gt;% mutate(`w_prior[1]` = rdirichlet(nrow(draws), c(1, 1))[,1]) %&gt;% ggplot() + geom_histogram(aes(`weight[1]`), alpha = 0.6, fill = &quot;lightblue&quot;) + geom_histogram(aes(`w_prior[1]`), alpha = 0.6, fill = &quot;pink&quot;) + geom_vline(xintercept = 0.5) + theme_bw() draws %&gt;% mutate(`kappa_prior` = rexp(nrow(draws), 0.1)) %&gt;% ggplot() + geom_histogram(aes(`kappa`), alpha = 0.6, fill = &quot;lightblue&quot;) + geom_histogram(aes(`kappa_prior`), alpha = 0.6, fill = &quot;pink&quot;) + geom_vline(xintercept = 1) + theme_bw() Parameter Recovery in the Multilevel GCM As with the standard GCM, parameter recovery analysis is crucial for validating our multilevel implementation. Successful parameter recovery would demonstrate that our model can accurately estimate both individual and population parameters from observed categorization data. For a comprehensive parameter recovery analysis, we would: Simulate data across a grid of population parameter values Fit the multilevel GCM to each simulated dataset Compare recovered parameters to the true generating values Assess recovery of both population parameters and individual variability The recovery of each parameter type has different implications: Sensitivity (c): Successful recovery indicates we can accurately measure generalization gradients Attention Weights (w): Successful recovery indicates we can measure how attention is allocated across dimensions Concentration (kappa): Successful recovery indicates we can measure the degree of individual variation in attention allocation 12.8 GCM in Context: Strengths and Limitations Like all models, the GCM has both strengths and limitations as an account of human categorization: 12.8.1 Strengths Empirical Support: The GCM has successfully fit human categorization data across numerous experiments. Psychological Plausibility: The model’s parameters correspond to psychologically meaningful constructs (attention, sensitivity). Flexibility: The GCM can account for a wide range of category structures and learning phenomena. Integration with Memory: The model naturally connects categorization to memory for specific instances. 12.8.2 Limitations Memory Requirements: Storing all exemplars becomes computationally intensive for large datasets. Complex Categories: The GCM can struggle with categories defined by complex rules or relations. Prior Knowledge: The standard GCM doesn’t account for how prior knowledge affects categorization (unless we simulate previous exposure to exemplars). Category Construction: The model focuses on categorization decisions rather than how categories are initially constructed. In the next sections, we’ll explore alternative approaches - prototype models and rule-based models - that address some of these limitations while introducing their own strengths and challenges. 12.9 Some notes about student-produced models of categorization Attempting to identify features Attempting to associate 1 of the features to one of the outcome dimensions (e.g. green as danger), test hypothesis, stay if positive, try different trait if negative. [missing details on how to define the hypothesized association, and how much evidence needed to shift] Remembering the rule and/or remembering the single exemplars Focusing on 1 feature 1 outcome vs trying to incorporate additional features Using arbitrary associations btw features and outcomes vs. relying on intuition (raised arms are dangerous) "],["prototype-based-models-of-categorization.html", "Chapter 13 Prototype-Based Models of Categorization 13.1 Theoretical Foundations 13.2 Prototype Models vs. Exemplar Models 13.3 The Kalman Filter Approach to Prototype Learning 13.4 Implementing the Kalman Filter Prototype Model 13.5 Simulating Categorization Behavior with the Prototype Model 13.6 Visualizing Prototype Learning 13.7 Comparing Prototype and Exemplar Models 13.8 Implementing the Prototype Model in Stan 13.9 Parameter Recovery Analysis for the Prototype Model 13.10 Cognitive Insights from the Prototype Model 13.11 Contrasting with Exemplar Models 13.12 Strengths of the Prototype Approach 13.13 Limitations of the Prototype Approach 13.14 Extensions to the Basic Model 13.15 Conclusion: The Prototype and Exemplar Debate", " Chapter 13 Prototype-Based Models of Categorization 13.1 Theoretical Foundations Prototype theory emerged as an alternative to both the classical view of categories (based on necessary and sufficient conditions) and exemplar-based accounts. The core idea is elegantly simple: rather than storing all individual exemplars in memory, the cognitive system abstracts a summary representation—a prototype—for each category. New items are then categorized based on their similarity to these prototypes. 13.1.1 Core Assumptions of Prototype Models Category Abstraction: Categories are represented by central tendencies or prototypes rather than collections of exemplars Economical Representation: Only prototype information is stored, not individual exemplars Similarity-Based Decisions: Categorization is based on similarity to category prototypes Typicality Effects: Items more similar to the prototype are processed more fluently and judged as more typical Unlike exemplar models, which maintain that people store individual instances, prototype models propose a more economical representation: the cognitive system extracts and stores the central tendency of the category. This provides a cognitively efficient way to represent categories while capturing many of the phenomena observed in human categorization. 13.2 Prototype Models vs. Exemplar Models The debate between prototype and exemplar theories has been one of the most productive in cognitive psychology. The key differences include: Aspect Prototype Models Exemplar Models Representation Abstract summary (central tendency) Collection of individual instances Memory Requirements Economical (one prototype per category) Potentially high (all exemplars) Typicality Prediction Items similar to prototype are most typical Items similar to many exemplars are most typical Category Boundaries Smoother, based on distance from prototypes Potentially more complex, based on similarity to all exemplars Unusual Members May not influence the prototype much Explicitly represented and influence decisions 13.3 The Kalman Filter Approach to Prototype Learning Traditional prototype models often assumed a static prototype computed as the average of all category members. However, this doesn’t capture the dynamic nature of human learning, where we continuously update our category representations as we encounter new examples. The Kalman filter provides an elegant mathematical framework for implementing a dynamic prototype model. Originally developed for tracking physical systems, the Kalman filter is ideal for modeling prototype learning because it: Updates incrementally with each new observation Balances prior knowledge with new evidence Maintains uncertainty about the prototype location Adjusts learning rate based on certainty 13.3.1 Mathematical Formulation of the Kalman Filter Prototype Model The Kalman filter prototype model tracks each category’s prototype as a probability distribution rather than a fixed point. Specifically, for each category, we maintain: A prototype location (mean vector μ) An uncertainty measure (covariance matrix Σ) 13.3.1.1 1. Initialization For each category C, we initialize: - Prototype location: μ₀ (often set to the first exemplar or a prior expectation) - Uncertainty: Σ₀ (high initial uncertainty) 13.3.1.2 2. Update Equations When a new exemplar x is observed for category C, we update the prototype and uncertainty as follows: Prototype Update: μₜ = μₜ₋₁ + K(x - μₜ₋₁) Uncertainty Update: Σₜ = (I - K)Σₜ₋₁ Kalman Gain: K = Σₜ₋₁(Σₜ₋₁ + R)⁻¹ Where: - μₜ is the updated prototype location - Σₜ is the updated uncertainty - K is the Kalman gain (learning rate) - R is the observation noise (constant or learned) The Kalman gain K is crucial—it determines how much weight to give to the new observation versus the existing prototype. When uncertainty is high, K is larger, giving more weight to new observations. As uncertainty decreases with more observations, K decreases, making the prototype more stable. 13.3.1.3 3. Categorization Decision Given multiple categories, the probability of assigning a new stimulus x to category C is given by: P(C|x) ∝ exp(-0.5(x - μ)ᵀ(Σ + R)⁻¹(x - μ)) This is based on the multivariate normal density, essentially measuring how likely the observation x is under category C’s prototype distribution. 13.4 Implementing the Kalman Filter Prototype Model Let’s implement a Kalman filter prototype model in R. For simplicity, we’ll start with a univariate case and then extend to the multivariate case needed for our categorization task. 13.4.1 Univariate Kalman Filter (for clear explanation) library(mvtnorm) # Control regeneration of simulations/fits regenerate_simulations &lt;- FALSE # Set to FALSE to load saved results # Simple univariate Kalman filter for tracking a prototype kalman_update &lt;- function(mu_prev, sigma_prev, observation, r) { # Calculate Kalman gain k &lt;- sigma_prev / (sigma_prev + r) # Update mean (prototype location) mu_new &lt;- mu_prev + k * (observation - mu_prev) # Update variance (uncertainty) sigma_new &lt;- (1 - k) * sigma_prev # Return updated values return(list(mu = mu_new, sigma = sigma_new, k = k)) } This function: 1. Takes the current prototype (mu_prev), uncertainty (sigma_prev), and a new observation 2. Calculates the Kalman gain (k) based on current uncertainty 3. Updates the prototype by moving it toward the observation, weighted by the gain 4. Updates the uncertainty, which always decreases with more observations 5. Returns the updated prototype, uncertainty, and gain The beauty of the Kalman filter is that the gain automatically adapts: when uncertainty is high (early learning), the gain is high, leading to larger updates. As uncertainty decreases (more observations), the gain decreases, making the prototype more stable. 13.4.2 Multivariate Kalman Filter for Categorization For categorization tasks with multiple feature dimensions, we need a multivariate version of the Kalman filter: # Multivariate Kalman filter for tracking category prototypes multivariate_kalman_update &lt;- function(mu_prev, sigma_prev, observation, r_matrix) { # Ensure inputs are matrices/vectors mu_prev &lt;- as.numeric(mu_prev) observation &lt;- as.numeric(observation) sigma_prev &lt;- as.matrix(sigma_prev) r_matrix &lt;- as.matrix(r_matrix) n_dim &lt;- length(mu_prev) I &lt;- diag(n_dim) # Identity matrix # Calculate Kalman gain # Use tryCatch for potential singularity, though unlikely with added R S_inv &lt;- tryCatch(solve(sigma_prev + r_matrix), error = function(e) { warning(&quot;Matrix inversion failed in Kalman gain calculation. Using pseudo-inverse or adding jitter might help.&quot;) MASS::ginv(sigma_prev + r_matrix) # Fallback using pseudo-inverse }) k_matrix &lt;- sigma_prev %*% S_inv # Update mean (prototype location) innovation &lt;- observation - mu_prev mu_new &lt;- mu_prev + k_matrix %*% innovation # Update covariance (uncertainty) using Joseph form # sigma_new = (I - K) * sigma_prev * (I - K)&#39; + K * R * K&#39; IK_term &lt;- (I - k_matrix) sigma_new &lt;- IK_term %*% sigma_prev %*% t(IK_term) + k_matrix %*% r_matrix %*% t(k_matrix) # Ensure symmetry (numerical precision can sometimes cause minor asymmetry) sigma_new &lt;- (sigma_new + t(sigma_new)) / 2 # Return updated values return(list(mu = as.numeric(mu_new), sigma = sigma_new, k = k_matrix)) } For our categorization task, we’ll use this to track prototypes for each category: # Prototype model using Kalman filter for categorization prototype_kalman &lt;- function(r_value, obs, cat_one, quiet = TRUE) { # Create empty vector for response probabilities response_probs &lt;- c() n_trials &lt;- nrow(obs) n_features &lt;- ncol(obs) # Initialize prototypes for each category (Consistent with Stan) prototype_cat_0 &lt;- list( mu = rep(0, n_features), # Initial prototype location (aligned with Stan) sigma = diag(10, n_features) # Initial uncertainty (high, aligned with Stan) ) prototype_cat_1 &lt;- list( mu = rep(0, n_features), # Initial prototype location (aligned with Stan) sigma = diag(10, n_features) # Initial uncertainty (high, aligned with Stan) ) # Observation noise (fixed for simplicity, could be learned) r_matrix &lt;- diag(r_value, n_features) # Log-Sum-Exp function for stable normalization log_sum_exp &lt;- function(v) { max_v &lt;- max(v) max_v + log(sum(exp(v - max_v))) } # Process each trial for (i in 1:n_trials) { # Debug info if (!quiet &amp;&amp; i %% 10 == 0) { print(paste(&quot;i =&quot;, i)) } current_obs &lt;- as.numeric(obs[i, ]) # Calculate response probability based on current prototypes using MVN density # Calculate covariance matrices including observation noise cov_cat_0 &lt;- prototype_cat_0$sigma + r_matrix cov_cat_1 &lt;- prototype_cat_1$sigma + r_matrix # Calculate log determinants # Use determinant() from base R, ensure log = TRUE log_det_0 &lt;- determinant(cov_cat_0, logarithm = TRUE)$modulus[1] log_det_1 &lt;- determinant(cov_cat_1, logarithm = TRUE)$modulus[1] # Calculate Mahalanobis squared distances # Use tryCatch for potential singularity if cov matrix becomes ill-conditioned prec_cat_0 &lt;- tryCatch(solve(cov_cat_0), error = function(e) MASS::ginv(cov_cat_0)) prec_cat_1 &lt;- tryCatch(solve(cov_cat_1), error = function(e) MASS::ginv(cov_cat_1)) diff0 &lt;- current_obs - prototype_cat_0$mu diff1 &lt;- current_obs - prototype_cat_1$mu dist_sq_0 &lt;- sum(diff0 * (prec_cat_0 %*% diff0)) # Quadratic form: t(diff) %*% prec %*% diff dist_sq_1 &lt;- sum(diff1 * (prec_cat_1 %*% diff1)) # Calculate log probabilities (proportional to MVN density) log_prob_0 &lt;- -0.5 * (dist_sq_0 + log_det_0 + n_features * log(2 * pi)) log_prob_1 &lt;- -0.5 * (dist_sq_1 + log_det_1 + n_features * log(2 * pi)) # Normalize using log-sum-exp to get probability of category 1 # Handle cases where one log_prob might be -Inf (if cov matrix was singular) if (!is.finite(log_prob_0) &amp;&amp; !is.finite(log_prob_1)) { prob_cat_1 &lt;- 0.5 # Undefined, default to 0.5 } else if (!is.finite(log_prob_0)) { prob_cat_1 &lt;- 1.0 # Only category 1 has finite probability } else if (!is.finite(log_prob_1)) { prob_cat_1 &lt;- 0.0 # Only category 0 has finite probability } else { prob_cat_1 &lt;- exp(log_prob_1 - log_sum_exp(c(log_prob_0, log_prob_1))) } # Ensure probability is within bounds prob_cat_1 &lt;- max(min(prob_cat_1, 0.999), 0.001) response_probs &lt;- c(response_probs, prob_cat_1) # Update prototype for the correct category after decision if (i &lt; n_trials) { # No need to update after the last trial if (cat_one[i] == 1) { # Update category 1 prototype using revised update function update &lt;- multivariate_kalman_update( prototype_cat_1$mu, prototype_cat_1$sigma, current_obs, r_matrix ) prototype_cat_1$mu &lt;- update$mu prototype_cat_1$sigma &lt;- update$sigma } else { # Update category 0 prototype using revised update function update &lt;- multivariate_kalman_update( prototype_cat_0$mu, prototype_cat_0$sigma, current_obs, r_matrix ) prototype_cat_0$mu &lt;- update$mu prototype_cat_0$sigma &lt;- update$sigma } } } # Return simulated binary responses based on calculated probabilities return(rbinom(n_trials, 1, response_probs)) } Let’s break down this implementation: Initialization: We start with uninformative prototypes (centered at 0.5 with high uncertainty) The observation noise (r_matrix) determines how much variance we expect around the prototype Decision Process: For each new stimulus, we calculate its Mahalanobis distance to each category’s prototype The Mahalanobis distance accounts for both the prototype location and uncertainty We convert these distances to probabilities using a softmax function Prototype Update: After each trial, we update the prototype of the correct category using the Kalman filter The update moves the prototype toward the new observation and reduces uncertainty The amount of movement depends on the current uncertainty level Learning Dynamics: Early in learning, large updates occur due to high uncertainty As learning progresses, updates become smaller, stabilizing the prototypes Eventually, the prototypes converge to the category centers 13.4.3 The Observation Noise Parameter Small r_value: Assumes observations are very precise; prototypes move less and become more certain quickly Large r_value: Assumes observations have high variability; prototypes move more and remain uncertain longer This parameter can be interpreted as representing the learner’s assumptions about category variability or their perceptual noise. 13.5 Simulating Categorization Behavior with the Prototype Model Let’s now simulate categorization behavior using our prototype model with the same experimental setup we used for the GCM: # Function to simulate responses using the prototype model simulate_prototype_responses &lt;- function(agent, r_value) { # Ensure using correct columns from experiment data observations &lt;- as.matrix(experiment %&gt;% dplyr::select(all_of(c(&quot;height&quot;, &quot;position&quot;)))) category &lt;- experiment$category # Assuming category is 0/1 # Simulate responses using the REVISED prototype_kalman function responses &lt;- prototype_kalman( r_value, observations, category ) # Record results tmp_simulated_responses &lt;- experiment %&gt;% mutate( trial = 1:n(), # Ensure trial sequence is correct sim_response = responses, correct = ifelse(category == sim_response, 1, 0), performance = cumsum(correct) / trial, r_value = r_value, agent = agent ) return(tmp_simulated_responses) } # Simulate responses across different r_values plan(multisession, workers = availableCores()) param_df &lt;- dplyr::tibble( expand_grid( agent = 1:10, r_value = c(0.1, 0.5, 1.0, 2.0, 5.0) ) ) prototype_responses &lt;- future_pmap_dfr(param_df, simulate_prototype_responses, .options = furrr_options(seed = TRUE) ) We can visualize how the observation noise parameter affects performance: prototype_responses %&gt;% mutate(r_value = as.factor(r_value)) %&gt;% ggplot(aes(trial, performance, group = interaction(agent, r_value), color = r_value)) + stat_summary(fun = mean, geom = &quot;line&quot;, alpha = 0.8) + # Plot average performance stat_summary(fun.data = mean_se, geom = &quot;ribbon&quot;, alpha = 0.2, aes(fill = r_value)) + # Show SE ribbon theme_bw() + labs( title = &quot;Categorization Performance with Revised Prototype Model&quot;, subtitle = &quot;Effect of observation noise parameter (r_value)&quot;, x = &quot;Trial&quot;, y = &quot;Proportion Correct (Average over Agents)&quot;, color = &quot;r-value&quot;, fill = &quot;r-value&quot; ) 13.6 Visualizing Prototype Learning To better understand how prototypes evolve over time, let’s visualize the prototype locations and uncertainty throughout learning: track_prototypes &lt;- function(r_value, obs, cat_one) { n_trials &lt;- nrow(obs) n_features &lt;- ncol(obs) # Initialize prototypes (Aligned with simulation/Stan) prototype_cat_0 &lt;- list( mu = rep(0, n_features), sigma = diag(10, n_features) ) prototype_cat_1 &lt;- list( mu = rep(0, n_features), sigma = diag(10, n_features) ) # Observation noise r_matrix &lt;- diag(r_value, n_features) # Storage for tracking prototype evolution prototype_history &lt;- tibble( trial = integer(), category = integer(), feature1_mean = numeric(), # Corresponds to height feature2_mean = numeric(), # Corresponds to position cov_matrix = list() # Store the full 2x2 covariance matrix ) # Process each trial for (i in 1:n_trials) { # Store current prototype state before update prototype_history &lt;- prototype_history %&gt;% add_row( trial = i, category = 0, feature1_mean = prototype_cat_0$mu[1], # Assuming mu[1] is height feature2_mean = prototype_cat_0$mu[2], # Assuming mu[2] is position cov_matrix = list(prototype_cat_0$sigma) # Store the matrix ) %&gt;% add_row( trial = i, category = 1, feature1_mean = prototype_cat_1$mu[1], feature2_mean = prototype_cat_1$mu[2], cov_matrix = list(prototype_cat_1$sigma) # Store the matrix ) # Update prototype for the correct category using REVISED update function # Use &lt;= to include update based on the last trial&#39;s feedback if (i &lt;= n_trials) { current_obs &lt;- as.numeric(obs[i, ]) if (cat_one[i] == 1) { # Update category 1 prototype update &lt;- multivariate_kalman_update( prototype_cat_1$mu, prototype_cat_1$sigma, current_obs, r_matrix ) prototype_cat_1$mu &lt;- update$mu prototype_cat_1$sigma &lt;- update$sigma } else { # Update category 0 prototype update &lt;- multivariate_kalman_update( prototype_cat_0$mu, prototype_cat_0$sigma, current_obs, r_matrix ) prototype_cat_0$mu &lt;- update$mu prototype_cat_0$sigma &lt;- update$sigma } } } # Add final state after last update (state at trial n_trials + 1) prototype_history &lt;- prototype_history %&gt;% add_row( trial = n_trials + 1, category = 0, feature1_mean = prototype_cat_0$mu[1], feature2_mean = prototype_cat_0$mu[2], cov_matrix = list(prototype_cat_0$sigma) ) %&gt;% add_row( trial = n_trials + 1, category = 1, feature1_mean = prototype_cat_1$mu[1], feature2_mean = prototype_cat_1$mu[2], cov_matrix = list(prototype_cat_1$sigma) ) return(prototype_history) } # Track prototypes for visualization (using the actual experiment data) # Determine number of trials from experiment data if possible, otherwise use nrow(obs) n_trials_actual &lt;- if (&quot;id&quot; %in% names(experiment)) max(experiment$id) else nrow(experiment) prototype_trajectory &lt;- track_prototypes( r_value = 1.0, # Example r_value obs = as.matrix(experiment[, c(&quot;height&quot;, &quot;position&quot;)]), cat_one = experiment$category ) # Helper function to create ellipse data from mean and covariance # Requires the ellipse package get_ellipse &lt;- function(mu, sigma, level = 0.68) { if (!requireNamespace(&quot;ellipse&quot;, quietly = TRUE)) { warning(&quot;Package &#39;ellipse&#39; needed for detailed uncertainty ellipses. Install it via install.packages(&#39;ellipse&#39;).&quot;) return(NULL) # Return NULL if package not available } # Ensure mu is numeric vector and sigma is matrix mu &lt;- as.numeric(mu) sigma &lt;- as.matrix(sigma) # Check dimensions if(length(mu) != 2 || !all(dim(sigma) == c(2,2))) { warning(&quot;Ellipse plotting requires 2D mean vector and 2x2 covariance matrix.&quot;) return(NULL) } # Ensure covariance is positive definite for ellipse calculation # Add small jitter if needed eigen_vals &lt;- eigen(sigma, symmetric = TRUE, only.values = TRUE)$values if(any(eigen_vals &lt;= 1e-6)) { warning(&quot;Covariance matrix may not be positive definite for ellipse plotting. Adding jitter.&quot;) sigma &lt;- sigma + diag(ncol(sigma)) * 1e-6 } # Calculate points on the ellipse boundary ellipse_points &lt;- tryCatch(ellipse::ellipse(sigma, centre = mu, level = level), error = function(e) { warning(paste(&quot;Ellipse calculation failed:&quot;, e$message)) NULL }) if (is.null(ellipse_points)) return(NULL) # Convert to data frame with expected names (feature1=height, feature2=position) as.data.frame(ellipse_points) %&gt;% setNames(c(&quot;feature1_mean&quot;, &quot;feature2_mean&quot;)) # Ensure names match plot aesthetics } # Visualize prototype evolution # Get final prototype states final_prototypes &lt;- prototype_trajectory %&gt;% filter(trial == max(trial)) # Create ellipse data for final states # Use rowwise and list columns carefully ellipse_data_list &lt;- final_prototypes %&gt;% rowwise() %&gt;% # Pass mu in the order expected by get_ellipse (feature1=height, feature2=position) mutate(ellipse_df = list(get_ellipse(c(feature1_mean, feature2_mean), cov_matrix[[1]]))) %&gt;% ungroup() %&gt;% dplyr::select(category, ellipse_df) %&gt;% filter(!sapply(ellipse_df, is.null)) # Remove rows where ellipse failed # Check if ellipse data was successfully generated if (nrow(ellipse_data_list) &gt; 0) { ellipse_data_unnested &lt;- ellipse_data_list %&gt;% unnest(ellipse_df) } else { warning(&quot;Could not generate ellipse data for plotting.&quot;) ellipse_data_unnested &lt;- NULL # Set to NULL if empty } # Base plot p_trajectory &lt;- ggplot() + # Plot stimuli geom_point(data = stimuli, aes(position, height, color = as.factor(category)), size = 4, alpha = 0.3) + # Plot prototype trajectory (path during trials 1 to n_trials) geom_path(data = prototype_trajectory %&gt;% filter(trial &lt;= n_trials_actual), # Use actual number of trials aes(feature2_mean, feature1_mean, group = category, color = as.factor(category)), linetype = &quot;dashed&quot;, arrow = arrow(type = &quot;closed&quot;, length = unit(0.1, &quot;inches&quot;))) + # Plot final prototype means (state after last trial) geom_point(data = final_prototypes, aes(feature2_mean, feature1_mean, color = as.factor(category)), size = 3) + # Labels and theme scale_color_discrete(name = &quot;Category&quot;) + labs( title = &quot;Prototype Learning with Kalman Filter (Revised)&quot;, subtitle = &quot;Dashed lines show prototype trajectory, ellipses show final 68% uncertainty&quot;, x = &quot;Position&quot;, y = &quot;Height&quot; ) + theme_minimal() + coord_cartesian(xlim = range(stimuli$position, prototype_trajectory$feature2_mean, na.rm=T), ylim = range(stimuli$height, prototype_trajectory$feature1_mean, na.rm=T)) # Add ellipses if data exists if (!is.null(ellipse_data_unnested)) { # Plot ellipses using geom_path, mapping x to feature2_mean and y to feature1_mean p_trajectory &lt;- p_trajectory + geom_path(data = ellipse_data_unnested, aes(feature2_mean, feature1_mean, color = as.factor(category)), alpha = 0.7) } # Print the final plot print(p_trajectory) This visualization shows: 1. The stimulus space with actual category members 2. The trajectory of each prototype as it updates with new observations 3. The final prototype locations with uncertainty ellipses Notice how the prototypes start at (2.5,2.5) and gradually move toward the center of each category. The uncertainty ellipses show the model’s confidence about each prototype’s location. 13.7 Comparing Prototype and Exemplar Models To directly compare the prototype (Kalman filter) model with the exemplar (GCM) model, we can: Fit both models to the same data Compare their fit using methods like LOO-CV Analyze which model better captures human categorization patterns One way to visualize the difference between these models is to look at their decision boundaries: # Create a grid of points in the stimulus space grid_points &lt;- expand.grid( position = seq(min(stimuli$position) - 0.5, max(stimuli$position) + 0.5, length.out = 50), height = seq(min(stimuli$height) - 0.5, max(stimuli$height) + 0.5, length.out = 50) ) # Function to get prototype model predictions for grid points get_prototype_predictions &lt;- function(r_value, training_obs, training_cat) { # Train the model on observed data n_features &lt;- ncol(training_obs) n_trials &lt;- nrow(training_obs) # Initialize prototypes prototype_cat_0 &lt;- list( mu = rep(0, n_features), sigma = diag(10, n_features) ) prototype_cat_1 &lt;- list( mu = rep(0, n_features), sigma = diag(10, n_features) ) # Observation noise r_matrix &lt;- diag(r_value, n_features) # Train model on observed data for (i in 1:n_trials) { if (training_cat[i] == 1) { # Update category 1 prototype update &lt;- multivariate_kalman_update( prototype_cat_1$mu, prototype_cat_1$sigma, as.numeric(training_obs[i, ]), r_matrix ) prototype_cat_1$mu &lt;- update$mu prototype_cat_1$sigma &lt;- update$sigma } else { # Update category 0 prototype update &lt;- multivariate_kalman_update( prototype_cat_0$mu, prototype_cat_0$sigma, as.numeric(training_obs[i, ]), r_matrix ) prototype_cat_0$mu &lt;- update$mu prototype_cat_0$sigma &lt;- update$sigma } } # Get predictions for grid points predictions &lt;- apply(as.matrix(grid_points), 1, function(point) { # Calculate distances to prototypes sigma_cat_0 &lt;- prototype_cat_0$sigma + r_matrix dist_cat_0 &lt;- mahalanobis(point, prototype_cat_0$mu, solve(sigma_cat_0)) sigma_cat_1 &lt;- prototype_cat_1$sigma + r_matrix dist_cat_1 &lt;- mahalanobis(point, prototype_cat_1$mu, solve(sigma_cat_1)) # Convert to probability prob_cat_1 &lt;- exp(-0.5 * dist_cat_1) / (exp(-0.5 * dist_cat_0) + exp(-0.5 * dist_cat_1)) return(prob_cat_1) }) return(predictions) } # Function to get GCM predictions for grid points get_gcm_predictions &lt;- function(w, c, training_obs, training_cat) { # Get predictions for grid points predictions &lt;- apply(as.matrix(grid_points), 1, function(point) { similarities &lt;- numeric(nrow(training_obs)) # Calculate similarity to all training exemplars for (i in 1:nrow(training_obs)) { sim &lt;- similarity(distance(point, as.numeric(training_obs[i,]), w), c) similarities[i] &lt;- sim } # Calculate probability of category 1 numerator &lt;- mean(similarities[training_cat == 1]) denominator &lt;- mean(similarities[training_cat == 1]) + mean(similarities[training_cat == 0]) if (denominator == 0) return(0.5) # Avoid division by zero return(numerator / denominator) }) return(predictions) } # Get predictions for both models prototype_preds &lt;- get_prototype_predictions( r_value = 1.0, training_obs = as.matrix(stimuli[, c(&quot;height&quot;, &quot;position&quot;)]), training_cat = as.numeric(as.character(stimuli$category)) ) gcm_preds &lt;- get_gcm_predictions( w = c(0.5, 0.5), c = 1.0, training_obs = as.matrix(stimuli[, c(&quot;height&quot;, &quot;position&quot;)]), training_cat = as.numeric(as.character(stimuli$category)) ) # Create visualization data decision_data &lt;- grid_points %&gt;% mutate( prototype_prob = prototype_preds, gcm_prob = gcm_preds, prototype_decision = prototype_prob &gt; 0.5, gcm_decision = gcm_prob &gt; 0.5 ) # Visualize decision boundaries p1 &lt;- ggplot() + # Background colors for decision regions geom_tile(data = decision_data, aes(position, height, fill = prototype_decision), alpha = 0.3) + # Decision boundary contour stat_contour(data = decision_data, aes(position, height, z = prototype_prob), breaks = 0.5, color = &quot;black&quot;, size = 1) + # Actual stimuli geom_point(data = stimuli, aes(position, height, color = category), size = 3) + # Labels and theme scale_fill_manual(values = c(&quot;FALSE&quot; = &quot;tomato&quot;, &quot;TRUE&quot; = &quot;skyblue&quot;)) + labs( title = &quot;Prototype Model Decision Boundary&quot;, x = &quot;Position&quot;, y = &quot;Height&quot;, fill = &quot;Category 1&quot;, color = &quot;True Category&quot; ) + theme_minimal() + theme(legend.position = &quot;none&quot;) p2 &lt;- ggplot() + # Background colors for decision regions geom_tile(data = decision_data, aes(position, height, fill = gcm_decision), alpha = 0.3) + # Decision boundary contour stat_contour(data = decision_data, aes(position, height, z = gcm_prob), breaks = 0.5, color = &quot;black&quot;, size = 1) + # Actual stimuli geom_point(data = stimuli, aes(position, height, color = category), size = 3) + # Labels and theme scale_fill_manual(values = c(&quot;FALSE&quot; = &quot;tomato&quot;, &quot;TRUE&quot; = &quot;skyblue&quot;)) + labs( title = &quot;Exemplar Model (GCM) Decision Boundary&quot;, x = &quot;Position&quot;, y = &quot;Height&quot;, fill = &quot;Category 1&quot;, color = &quot;True Category&quot; ) + theme_minimal() + theme(legend.position = &quot;none&quot;) # Compare the two models side by side p1 + p2 While the example is not really fit for this comparison (sorry, I didn’t think of it earlier on!), the visualization highlights a key difference between prototype and exemplar models: the shape of their decision boundaries. The prototype model tends to create smoother, more regular boundaries based on distance from centroids. The exemplar model can create more complex boundaries that conform to the specific distribution of training examples. 13.8 Implementing the Prototype Model in Stan To estimate the parameters of our prototype model from observed categorization data, we’ll implement it in Stan: prototype_single_stan &lt;- &quot; // Prototype Model using Kalman Filter // This model implements a dynamic prototype-based categorization approach // where category representations are updated sequentially as new examples are observed data { int&lt;lower=1&gt; ntrials; // Number of trials int&lt;lower=1&gt; nfeatures; // Number of feature dimensions array[ntrials] int&lt;lower=0, upper=1&gt; cat_one; // True category labels array[ntrials] int&lt;lower=0, upper=1&gt; y; // Observed decisions array[ntrials, nfeatures] real obs; // Stimulus features real&lt;lower=0, upper=1&gt; b; // Response bias (usually 0.5 for no bias) } parameters { // We model observation noise on log scale for better sampling real log_r; // Log observation noise parameter } transformed parameters { // Transform log_r back to natural scale with reasonable bounds real&lt;lower=0.01, upper=10.0&gt; r_value = exp(log_r); // Response probabilities for each trial array[ntrials] real&lt;lower=0.001, upper=0.999&gt; p; // Initialize prototype means for each category // We use the first observation of each category if available, // otherwise start with the midpoint of the feature space vector[nfeatures] mu_cat0; vector[nfeatures] mu_cat1; // Initialize uncertainty matrices (covariance) for each prototype // High initial values reflect uncertainty before seeing examples matrix[nfeatures, nfeatures] sigma_cat0; matrix[nfeatures, nfeatures] sigma_cat1; // Set initial values based on first observations { // Find first examples of each category int first_cat0 = 0; int first_cat1 = 0; for (i in 1:ntrials) { if (cat_one[i] == 0 &amp;&amp; first_cat0 == 0) first_cat0 = i; if (cat_one[i] == 1 &amp;&amp; first_cat1 == 0) first_cat1 = i; if (first_cat0 &gt; 0 &amp;&amp; first_cat1 &gt; 0) break; } // Initialize means - if no examples of a category, use middle values if (first_cat0 &gt; 0) { for (j in 1:nfeatures) mu_cat0[j] = obs[first_cat0, j]; } else { mu_cat0 = rep_vector(2.5, nfeatures); // Middle of typical feature range } if (first_cat1 &gt; 0) { for (j in 1:nfeatures) mu_cat1[j] = obs[first_cat1, j]; } else { mu_cat1 = rep_vector(2.5, nfeatures); // Middle of typical feature range } // Initialize uncertainty matrices sigma_cat0 = diag_matrix(rep_vector(5.0, nfeatures)); sigma_cat1 = diag_matrix(rep_vector(5.0, nfeatures)); } // Observation noise matrix - constant for all observations matrix[nfeatures, nfeatures] r_matrix = diag_matrix(rep_vector(r_value, nfeatures)); // Process trials sequentially to simulate the learning process for (i in 1:ntrials) { // Extract current observation vector[nfeatures] current_obs = to_vector(obs[i]); // Calculate response probability based on current prototypes if (i == 1 || sum(cat_one[1:(i-1)]) == 0 || sum(cat_one[1:(i-1)]) == (i-1)) { // No examples of one category, use response bias p[i] = b; } else { // Calculate distances to prototypes using Mahalanobis distance // Add observation noise to account for perceptual/memory variance matrix[nfeatures, nfeatures] cov_cat0 = sigma_cat0 + r_matrix; matrix[nfeatures, nfeatures] cov_cat1 = sigma_cat1 + r_matrix; // Compute precision matrices for efficiency and stability matrix[nfeatures, nfeatures] prec_cat0 = inverse_spd(cov_cat0); matrix[nfeatures, nfeatures] prec_cat1 = inverse_spd(cov_cat1); // Mahalanobis distance calculation - quadratic form vector[nfeatures] diff0 = current_obs - mu_cat0; vector[nfeatures] diff1 = current_obs - mu_cat1; real dist_cat0 = dot_product(diff0, prec_cat0 * diff0); real dist_cat1 = dot_product(diff1, prec_cat1 * diff1); // Log determinants for normalization term in multivariate normal real logdet_cat0 = log_determinant(cov_cat0); real logdet_cat1 = log_determinant(cov_cat1); // Calculate log probabilities with complete multivariate normal formula real log_p0 = -0.5 * (dist_cat0 + logdet_cat0 + nfeatures * log(2 * pi())) + log(1-b); real log_p1 = -0.5 * (dist_cat1 + logdet_cat1 + nfeatures * log(2 * pi())) + log(b); // Convert to probability using log-sum-exp for numerical stability p[i] = exp(log_p1 - log_sum_exp(log_p0, log_p1)); // Bound probabilities for numerical stability p[i] = fmax(fmin(p[i], 0.999), 0.001); } // After making a prediction, update the prototype of the correct category // This simulates learning from feedback (except on the last trial) if (i &lt; ntrials) { // Select which prototype to update based on true category if (cat_one[i] == 1) { // Update category 1 prototype using Kalman filter equations // Innovation: difference between observation and current mean vector[nfeatures] innovation = current_obs - mu_cat1; // Combined uncertainty (prior uncertainty + observation noise) matrix[nfeatures, nfeatures] S = sigma_cat1 + r_matrix; // Kalman gain calculation matrix[nfeatures, nfeatures] K = sigma_cat1 * inverse_spd(S); // Update mean (prototype location) mu_cat1 = mu_cat1 + K * innovation; // Update covariance (uncertainty) // Joseph form ensures numerical stability and symmetry matrix[nfeatures, nfeatures] I = diag_matrix(rep_vector(1.0, nfeatures)); sigma_cat1 = (I - K) * sigma_cat1 * (I - K)&#39; + K * r_matrix * K&#39;; } else { // Update category 0 prototype - same procedure vector[nfeatures] innovation = current_obs - mu_cat0; matrix[nfeatures, nfeatures] S = sigma_cat0 + r_matrix; matrix[nfeatures, nfeatures] K = sigma_cat0 * inverse_spd(S); mu_cat0 = mu_cat0 + K * innovation; matrix[nfeatures, nfeatures] I = diag_matrix(rep_vector(1.0, nfeatures)); sigma_cat0 = (I - K) * sigma_cat0 * (I - K)&#39; + K * r_matrix * K&#39;; } } } } model { // Prior for log observation noise // Normal prior centered at 0 (r_value = 1) with reasonable spread target += normal_lpdf(log_r | 0, 1); // Likelihood: model choices as a function of calculated probabilities target += bernoulli_lpmf(y | p); } generated quantities { // Log likelihood for model comparison array[ntrials] real log_lik; for (i in 1:ntrials) { log_lik[i] = bernoulli_lpmf(y[i] | p[i]); } // Final prototype locations for interpretation array[nfeatures] real final_prototype_cat0 = to_array_1d(mu_cat0); array[nfeatures] real final_prototype_cat1 = to_array_1d(mu_cat1); // Final uncertainty (diagonal elements of covariance matrix) array[nfeatures] real final_uncertainty_cat0; array[nfeatures] real final_uncertainty_cat1; for (j in 1:nfeatures) { final_uncertainty_cat0[j] = sqrt(sigma_cat0[j,j]); final_uncertainty_cat1[j] = sqrt(sigma_cat1[j,j]); } // Generate predictions for posterior predictive checks array[ntrials] int pred; for (i in 1:ntrials) { pred[i] = bernoulli_rng(p[i]); } // Observation noise on original scale for interpretation real observation_noise = r_value; }&quot; # Write the model to a file write_stan_file( prototype_single_stan, dir = &quot;stan/&quot;, basename = &quot;W12_prototype_single.stan&quot; ) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W12_prototype_single.stan&quot; prototype_single_stan &lt;- cmdstan_model( file.path(&quot;stan/W12_prototype_single.stan&quot;), cpp_options = list(stan_threads = TRUE) ) This Stan implementation: Takes observed categorization decisions and stimuli as input Estimates the observation noise parameter from the data Implements the same Kalman filter prototype updating as our R model Calculates response probabilities based on similarity to prototypes Returns final prototype locations and predictions We can fit this model to behavioral data and compare it to the GCM to see which better describes human categorization behavior. 13.9 Parameter Recovery Analysis for the Prototype Model To validate our prototype model implementation, we should perform parameter recovery analysis. This involves: Generating synthetic data with known parameter values Fitting the model to recover these parameters Comparing recovered parameters to the true generating values Here, we’ll focus on recovering the observation noise parameter (r_value), which is the key parameter in our Kalman filter prototype model: # Function to simulate data with known r_value generate_prototype_data &lt;- function(true_r_value, n_trials = 300) { # Generate new stimuli features and categories features &lt;- matrix(0, nrow = n_trials, ncol = 2) category &lt;- rep(0:1, each = n_trials/2) # Generate data around two centroids for (i in 1:n_trials) { if (category[i] == 0) { features[i,] &lt;- c(-1.5, -1.5) + rnorm(2, 0, 0.8) } else { features[i,] &lt;- c(1.5, 1.5) + rnorm(2, 0, 0.8) } } # Generate responses using the prototype model with known r_value responses &lt;- prototype_kalman(true_r_value, features, category) return(list( responses = responses, true_r_value = true_r_value, obs = features, cat_one = category )) } # Better function to prepare data for Stan prepare_prototype_stan_data &lt;- function(data) { list( ntrials = length(data$responses), nfeatures = ncol(data$obs), cat_one = data$cat_one, y = data$responses, obs = data$obs, b = 0.5 # Assuming no response bias ) } if (regenerate_simulations) { # Generate synthetic data across a range of r_values r_values_to_test &lt;- c(0.1, 0.5, 1.0, 2.0, 3.0, 4.0, 5.0) recovery_results &lt;- tibble( true_r_value = numeric(), estimated_r_value = numeric(), est_lower = numeric(), est_upper = numeric() ) # For each true r_value, generate data and fit the model for (r_val in r_values_to_test) { # Generate synthetic data synth_data &lt;- generate_prototype_data(r_val) # Prepare for Stan stan_data &lt;- prepare_prototype_stan_data(synth_data) # Fit model (assuming the Stan model is already compiled) fit &lt;- prototype_single_stan$sample( data = stan_data, seed = 123, chains = 2, parallel_chains = 2, threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 1000, refresh = 0 ) # Extract posterior for r_value draws &lt;- as_draws_df(fit$draws(&quot;r_value&quot;)) estimate &lt;- mean(draws$r_value) ci &lt;- quantile(draws$r_value, c(0.025, 0.975)) # Store results recovery_results &lt;- recovery_results %&gt;% add_row( true_r_value = r_val, estimated_r_value = estimate, est_lower = ci[1], est_upper = ci[2] ) } # Save model fits write_csv(recovery_results, &quot;simdata/W12_prototype_recovery.csv&quot;) cat(&quot;Models fitted and saved.\\n&quot;) } else { # Load existing model fits recovery_results &lt;- read_csv(&quot;simdata/W12_prototype_recovery.csv&quot;) cat(&quot;Loaded existing model fits.\\n&quot;) } ## Loaded existing model fits. # Visualize parameter recovery ggplot(recovery_results, aes(x = true_r_value, y = estimated_r_value)) + geom_point(size = 3) + geom_errorbar(aes(ymin = est_lower, ymax = est_upper), width = 0.1) + geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;) + labs( title = &quot;Parameter Recovery for Prototype Model&quot;, subtitle = &quot;Error bars show 95% credible intervals&quot;, x = &quot;True r-value&quot;, y = &quot;Estimated r-value&quot; ) + theme_minimal() Good parameter recovery would show estimated values close to the true values (points near the diagonal line) with reasonable uncertainty (error bars that include the true value). Poor recovery would indicate potential issues with the model’s identifiability or implementation. 13.10 Cognitive Insights from the Prototype Model 13.10.1 1. Incremental Learning The Kalman filter model captures the dynamic, incremental nature of human learning (n.b. not all prototype models do that, some compute a static average based on some training data and never update it). People don’t wait to see all examples before forming a category representation—they update their understanding with each new example. 13.10.2 2. Uncertainty-Driven Learning The Kalman gain modulates learning based on uncertainty, mirroring how humans learn faster when their knowledge is uncertain and more slowly as they become confident. This creates a natural “fast-then-slow” learning curve similar to what we observe in human behavior. 13.10.3 3. Selective Attention Emerges Naturally Though we didn’t implement it explicitly, the Kalman filter can naturally develop different levels of certainty along different feature dimensions (as the uncertainty can vary between dimensions). This creates an emergent form of selective attention without requiring explicit attention parameters like in the GCM. These uncertainty estimates can be extracted and used to better understand the data on which the model is fitted. 13.10.4 4. Memory Efficiency Prototype models provide a computationally efficient account of categorization, storing only summary statistics rather than individual exemplars. This aligns with the fact that humans can effectively categorize even when their memory for specific examples is poor. 13.11 Contrasting with Exemplar Models Memory Requirements: The prototype model stores only means and covariance matrices—a fixed memory footprint regardless of category size. The exemplar model’s memory requirements grow linearly with the number of examples. Abstraction: The prototype model abstracts away individual examples, focusing on the central tendency. The exemplar model preserves the details of each individual example. Decision Boundaries: Prototype models typically produce smoother, more regular decision boundaries based on distance from category centers. Exemplar models can produce more complex boundaries shaped by the specific distribution of examples. Behavioral Predictions: Prototype models predict that the most typical (central) members will be categorized most easily (e.g. more accurately, but perhaps also faster) Exemplar models predict advantages for distinctive or isolated exemplars Prototype models predict poorer memory for specific examples if we encounter them again Forgetting: The prototype model naturally accommodates forgetting of specific examples, while the exemplar model would need an explicit forgetting mechanism. 13.12 Strengths of the Prototype Approach The prototype approach has several strengths as a model of human categorization: Cognitive Efficiency: Prototypes provide an efficient summary of category information, requiring minimal memory resources. Handling Noise: By averaging across examples, prototype models naturally handle noisy or variable data. Graceful Degradation: Prototype representations remain robust even when specific exemplars are forgotten. Explanation of Typicality Effects: Prototype models naturally explain why typical category members are processed more fluently. Good Fit for Natural Categories: Many natural categories have a graded, prototype structure (e.g., birds, furniture) that prototype models capture well. 13.13 Limitations of the Prototype Approach Despite its strengths, the prototype approach also has important limitations: Difficulty with Complex Categories: Prototype models struggle with categories that have complex internal structure, such as those defined by rules or relations. Limited Use of Distributional Information: By focusing on central tendency, traditional prototype models ignore useful information about the distribution of features. Insensitivity to Specific Examples: Prototype models can’t easily account for cases where specific examples strongly influence categorization decisions. Challenge of Disjunctive Categories: Categories with multiple distinct clusters (e.g., the category of “games”) are difficult for single-prototype models to handle. 13.14 Extensions to the Basic Model Several extensions can address some of these limitations: Multiple Prototypes per Category: Allow categories to be represented by multiple prototypes, better handling disjunctive categories (e.g. via k-means). Feature Correlations: Explicitly model correlations between features in the prototype representation. Hierarchical Structure: Implement hierarchical prototype models to capture taxonomic category structures. Mixture of Prototypes and Exemplars: Combine elements of both approaches, using prototypes for more similar exemplars and remember the exemplars that are further from the prototype. 13.15 Conclusion: The Prototype and Exemplar Debate The debate between prototype and exemplar theories of categorization has been one of the most productive in cognitive psychology, leading to refined theories and empirical tests that have deepened our understanding of human categorization. Current evidence suggests that neither approach alone fully accounts for human categorization behavior: Humans show prototype effects, categorizing items more quickly and accurately when they’re close to the category center. Humans also show exemplar effects, being influenced by specific, distinctive examples and showing correlations between recognition and categorization performance. Many researchers now favor hybrid or multiple-system accounts, where prototype-based and exemplar-based processes coexist and potentially interact. The Kalman filter implementation of prototype learning provides a dynamic, uncertainty-sensitive approach that addresses some criticisms of traditional prototype models while maintaining their cognitive efficiency. By integrating ideas from Bayesian learning theory with classic prototype models, this approach offers a sophisticated account of category learning that can be directly compared with exemplar models like the GCM. The Prototype-Exemplar Spectrum and Multiple-Prototype Models It’s important to recognize that prototype and exemplar models are not strictly distinct approaches, but rather represent two ends of a theoretical spectrum. The core difference lies in the granularity of representation: exemplar models store all individual instances, while traditional prototype models store a single central tendency. However, we can imagine a continuum of intermediate approaches: Pure exemplar models: Store all instances with no abstraction Clustered exemplar models: Store instances but group similar ones Multiple-prototype models: Store several prototypes per category Single-prototype models: Store one prototype per category Multiple-prototype models offer an appealing middle ground that maintains much of the computational efficiency of prototype models while capturing more complex category structures. There are several ways to extend our Kalman filter approach to implement multiple centroids per category: K-means clustering: First cluster the exemplars of each category into k subclusters, then apply the Kalman filter separately to each cluster Mixture of Gaussians: Represent each category as a mixture of Gaussian distributions, with each component tracking a different subcategory prototype Adaptive resonance theory: Dynamically create new prototypes when an observation is too dissimilar from existing prototypes Splitting criteria: Monitor the variance of exemplars around each prototype and split the prototype when variance exceeds a threshold In the next section, we’ll explore the third major approach to categorization: rule-based models, which represent a fundamentally different perspective on how humans organize the world into categories. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
