[["index.html", "08-MixtureModels Chapter 1 Advanced Cognitive Modeling 1.1 Course Philosophy and Approach 1.2 Course Structure and Learning Path 1.3 Prerequisites and Preparation 1.4 Course Resources 1.5 About These Notes", " 08-MixtureModels Riccardo Fusaroli 2025-03-07 Chapter 1 Advanced Cognitive Modeling These course notes support the Advanced Cognitive Modeling course taught in the Master’s program in Cognitive Science at Aarhus University. The course represents a journey into how we can understand cognitive processes through the formalization and implementation of hypothesized mechanisms, their testing and validation. 1.1 Course Philosophy and Approach Advanced cognitive modeling focuses on three interrelated objectives that shape how we approach the modeling of cognitive processes: The first objective centers on understanding the thought process behind model development. Rather than simply providing a toolbox of existing scripts, we explore how cognitive models are conceptualized and constructed from the ground up. This approach ensures you develop the skills to create novel models for unique research questions. The second objective emphasizes mastering the Bayesian workflow essential for robust model development. This workflow encompasses simulation design, prior assessment, parameter recovery testing, and thorough model fit evaluation. These skills ensure your models are not just theoretically sound but also practically reliable and generalize way beyond cognitive modeling. The third objective focuses on developing advanced probabilistic modeling capabilities. Through hands-on experience with Stan, you will learn to implement increasingly sophisticated models while maintaining scientific rigor. 1.2 Course Structure and Learning Path The course follows a carefully structured progression that builds your modeling capabilities step by step: After a deepdive into the physics of pizza ovens, we begin with simple scenarios that introduce fundamental modeling concepts. Each subsequent chapter introduces new modeling techniques while building upon previous knowledge. This cumulative approach ensures you develop a deep understanding of both basic principles and advanced applications. The chapters include theoretical discussions paired with practical coding exercises. During practical sessions, we work with real datasets, design models collaboratively, and implement them using modern statistical tools. This hands-on approach provides ample opportunity for questions and exploration. The course schedule maintains flexibility to adapt to the collective learning pace of each cohort. While we have clear learning objectives, we ensure everyone develops a solid foundation before moving to more advanced topics. 1.3 Prerequisites and Preparation To make the most of this course, students should prepare their technical environment and review fundamental concepts: Software Requirements: - R (version 4.4 or above) - RStudio (version 2024.12.0 or above) - brms package with proper configuration - cmdstanr package with complete installation Technical Prerequisites: - Working knowledge of R programming - Basic understanding of Bayesian statistics - Familiarity with cognitive science fundamentals Additional Resources: - Introduction to R and tidyverse: https://r4ds.had.co.nz/ - A condensed Bayesian statistics primer (by Chris Cox and me): https://4ccoxau.github.io/PriorsWorkshop/ 1.4 Course Resources The course materials include: - Lecture notes and presentations - Practical exercise guides - Example code and solutions - Additional readings and references For comprehensive information: - Course syllabus: [TBA] - Lecture videos: [TBA] 1.5 About These Notes These notes represent an evolving resource that builds upon previous iterations of the course while incorporating new developments in the field. They are designed to serve both as a learning guide during the course and as a reference for your future research endeavors. knitr::opts_chunk$set( warning = FALSE, # Suppress warnings message = FALSE, # Suppress package loading messages echo = TRUE, # Show R code fig.width = 8, # Set default figure width fig.height = 5, # Set default figure height fig.align = &#39;center&#39;, # Center figures out.width = &quot;80%&quot;, # Make figures 80% of text width dpi = 300 # Set high resolution for figures ) "],["foundations.html", "Chapter 2 Foundations 2.1 From Pizza to Cognitive Models: An Introduction 2.2 Why Start with Pizza? 2.3 Learning Objectives 2.4 Part 1: Exploring the Pizza Stone Temperature Data 2.5 Part 2: Initial Statistical Modeling 2.6 Part 3: Understanding the Physics Model 2.7 Part 4: Implementing the Physics-Based Model 2.8 Part 5: Model Analysis and Practical Applications 2.9 Conclusion: From Pizza to Principles", " Chapter 2 Foundations 2.1 From Pizza to Cognitive Models: An Introduction This chapter introduces core modeling concepts through an unexpected lens: the physics of pizza stone heating. While this might seem far removed from cognitive science, it provides an insightful introduction to the challenges and methodologies of modeling complex phenomena. 2.2 Why Start with Pizza? Do I even need to answer that question? Because pizza, obviously. In any case, understanding how humans think and make decisions is arguably one of the most complex challenges in science. Rather than diving directly into this complexity, we begin with a more tractable problem: modeling how a pizza stone heats up in an oven. This seemingly simple process introduces us to key modeling concepts: The importance of selecting appropriate levels of analysis The role of prior knowledge in model development The challenge of balancing model complexity with practical utility The necessity of rigorous validation approaches Through this concrete example, we can focus on understanding modeling principles without the added complexity of cognitive theory. 2.3 Learning Objectives This first chpater is a bit odd, in that it pushes you straight into the deep waters of a complex example. I don’t expect you to understand all the technicalities. But, by completing this tutorial, you will be able to better grasp the importance of generative modeling, that is, of modeling that is focused on the underlying mechanisms producing the data. On the side you might learn something about how to * Implement physics-based thermal modeling using R and Stan * Apply Bayesian inference to real-world temperature data * Compare different statistical models using posterior predictions * Create professional visualizations of temperature evolution * Make practical predictions about heating times under various conditions Oh, and you’ll probably get hungry as well! Required Packages required_packages &lt;- c( &quot;tidyverse&quot;, # For data manipulation and visualization &quot;brms&quot;, # For Bayesian regression modeling &quot;bayesplot&quot;, # For visualization of Bayesian models &quot;tidybayes&quot;, # For working with Bayesian samples &quot;cmdstanr&quot; # For Stan implementation ) # Install and load packages for (pkg in required_packages) { if (!require(pkg, character.only = TRUE)) { install.packages(pkg) library(pkg, character.only = TRUE) } } 2.4 Part 1: Exploring the Pizza Stone Temperature Data In this study, we collected temperature measurements from a pizza stone in a gas-fired oven using an infrared temperature gun. Three different raters (N, TR, and R) took measurements over time to track how the stone heated up. Understanding how pizza stones heat up is crucial for achieving the perfect pizza crust, as consistent and sufficient stone temperature is essential for proper baking. The measurements were taken as follows: # Load and examine the data data &lt;- tibble( Order = rep(0:18, 3), Seconds = rep(c(0, 175, 278, 333, 443, 568, 731, 773, 851, 912, 980, 1040, 1074, 1124, 1175, 1237, 1298, 1359, 1394), 3), Temperature = c(15.1, 233, 244, 280, 289, 304, 343, NA, 333, 341, 320, 370, 325, 362, 363, 357, 380, 376, 380, 14.5, 139.9, 153, 36.1, 254, 459, 263, 369, rep(NA, 11), 12.9, 149.5, 159, 179.4, 191.7, 201, 210, NA, 256, 257, 281, 293, 297, 309, 318, 321, rep(NA, 3)), Rater = rep(c(&quot;N&quot;, &quot;TR&quot;, &quot;R&quot;), each = 19) ) # Create summary statistics summary_stats &lt;- data %&gt;% group_by(Rater) %&gt;% summarize( n_measurements = sum(!is.na(Temperature)), mean_temp = mean(Temperature, na.rm = TRUE), sd_temp = sd(Temperature, na.rm = TRUE), min_temp = min(Temperature, na.rm = TRUE), max_temp = max(Temperature, na.rm = TRUE) ) # Display summary statistics knitr::kable(summary_stats, digits = 1) Rater n_measurements mean_temp sd_temp min_temp max_temp N 18 312.0 86.4 15.1 380 R 15 229.0 83.9 12.9 321 TR 8 211.1 155.2 14.5 459 2.4.1 Initial Data Visualization Let’s visualize how the temperature evolves over time for each rater: ggplot(data, aes(x = Seconds/60, y = Temperature, color = Rater)) + geom_point(size = 3, alpha = 0.7) + geom_line(alpha = 0.5) + labs( title = &quot;Pizza Stone Temperature Evolution&quot;, subtitle = &quot;Measurements by three different raters&quot;, x = &quot;Time (minutes)&quot;, y = &quot;Temperature (°C)&quot;, color = &quot;Rater&quot; ) + theme_bw() + scale_color_brewer(palette = &quot;Set1&quot;) 2.4.2 Key Observations Several interesting patterns emerge from our data: Heating Patterns: The temperature generally increases over time, but not uniformly. We observe some fluctuations that might be due to: Variation in gas flame intensity Different measurement locations on the stone Measurement technique differences between raters Measurement Patterns by Rater Rater N maintained consistent measurements throughout the experiment Rater TR shows more variability and fewer total measurements Rater R shows a more gradual temperature increase pattern Missing Data: Some measurements are missing (NA values), particularly in the later time points for Rater TR. This is common in real-world data collection and needs to be considered in our analysis. Let’s examine the rate of temperature change: # Calculate temperature change rate data_with_rate &lt;- data %&gt;% group_by(Rater) %&gt;% arrange(Seconds) %&gt;% mutate( temp_change = (Temperature - lag(Temperature)) / (Seconds - lag(Seconds)) * 60, minutes = Seconds/60 ) %&gt;% filter(!is.na(temp_change)) # Visualize temperature change rate ggplot(data_with_rate, aes(x = minutes, y = temp_change, color = Rater)) + geom_point() + geom_smooth(se = FALSE, span = 0.75) + labs( title = &quot;Rate of Temperature Change Over Time&quot;, subtitle = &quot;Degrees Celsius per minute&quot;, x = &quot;Time (minutes)&quot;, y = &quot;Temperature Change Rate (°C/min)&quot;, color = &quot;Rater&quot; ) + theme_bw() + scale_color_brewer(palette = &quot;Set1&quot;) This visualization reveals that the heating rate is highest in the first few minutes and gradually decreases as the stone temperature approaches the oven temperature. This aligns with Newton’s Law of Cooling/Heating, which we will explore in the next section. 2.5 Part 2: Initial Statistical Modeling Before developing our physics-based model, let’s explore how standard statistical approaches perform in modeling our temperature data. We’ll implement two types of models using the brms package: a linear mixed-effects model and a lognormal mixed-effects model. Both models will account for variations between raters. 2.5.1 Model Setup and Priors First, let’s ensure we have a directory for our models and set up our computational parameters: # Create models directory if it doesn&#39;t exist dir.create(&quot;models&quot;, showWarnings = FALSE) # Define computational parameters mc_settings &lt;- list( chains = 2, iter = 6000, seed = 123, backend = &quot;cmdstanr&quot; ) 2.5.2 Linear Mixed-Effects Model We begin with a linear mixed-effects model, which assumes that temperature increases linearly with time but allows for different patterns across raters. This model includes both fixed effects (overall time trend) and random effects (rater-specific variations). # Define priors for linear model linear_priors &lt;- c( prior(normal(15, 20), class = &quot;Intercept&quot;), # Centered around room temperature prior(normal(0, 1), class = &quot;b&quot;), # Expected temperature change per second prior(normal(0, 100), class = &quot;sigma&quot;), # Residual variation prior(normal(0, 100), class = &quot;sd&quot;), # Random effects variation prior(lkj(3), class = &quot;cor&quot;) # Random effects correlation ) # Fit linear mixed-effects model linear_model &lt;- brm( Temperature ~ Seconds + (1 + Seconds | Rater), data = data, family = gaussian, prior = linear_priors, chains = mc_settings$chains, iter = mc_settings$iter, seed = mc_settings$seed, backend = mc_settings$backend, file = &quot;models/01_pizza_linear_model&quot;, cores = 2, adapt_delta = 0.99, max_treedepth = 20 ) # Display model summary summary(linear_model) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Temperature ~ Seconds + (1 + Seconds | Rater) ## Data: data (Number of observations: 41) ## Draws: 2 chains, each with iter = 6000; warmup = 3000; thin = 1; ## total post-warmup draws = 6000 ## ## Multilevel Hyperparameters: ## ~Rater (Number of levels: 3) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 88.18 49.29 14.36 210.34 1.00 1804 1750 ## sd(Seconds) 0.70 0.57 0.15 2.34 1.00 1226 1398 ## cor(Intercept,Seconds) -0.03 0.38 -0.71 0.67 1.00 2106 3251 ## ## Regression Coefficients: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 75.42 71.38 -110.73 194.77 1.00 1461 1136 ## Seconds -0.08 0.11 -0.25 0.19 1.00 1406 1133 ## ## Further Distributional Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 58.36 7.51 46.01 75.32 1.00 3735 3662 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). # Generate predictions linear_preds &lt;- fitted( linear_model, newdata = data, probs = c(0.025, 0.975) ) %&gt;% as_tibble() %&gt;% bind_cols(data) 2.5.3 Lognormal Mixed-Effects Model The lognormal model accounts for the fact that temperature changes might be proportional rather than additive, and ensures predictions cannot go below zero (I don’t bring my oven out in the freezing cold!). # Define priors for lognormal model lognormal_priors &lt;- c( prior(normal(2.7, 1), class = &quot;Intercept&quot;), # Log scale for room temperature prior(normal(0, 0.01), class = &quot;b&quot;), # Expected log-scale change per second prior(normal(0, 1), class = &quot;sigma&quot;), # Log-scale residual variation prior(normal(0, 1), class = &quot;sd&quot;), # Random effects variation prior(lkj(3), class = &quot;cor&quot;) # Random effects correlation ) # Fit lognormal mixed-effects model lognormal_model &lt;- brm( Temperature ~ Seconds + (1 + Seconds | Rater), data = data, family = lognormal, prior = lognormal_priors, chains = mc_settings$chains, cores = 2, adapt_delta = 0.99, max_treedepth = 20, iter = mc_settings$iter, seed = mc_settings$seed, backend = mc_settings$backend, file = &quot;models/01_pizza_lognormal_model&quot; ) # Generate predictions lognormal_preds &lt;- fitted( lognormal_model, newdata = data, probs = c(0.025, 0.975) ) %&gt;% as_tibble() %&gt;% bind_cols(data) 2.5.4 Model Comparison and Visualization Let’s compare how these models fit our data: # Compare models using LOO model_comparison &lt;- loo_compare( loo(linear_model), loo(lognormal_model) ) # Create comparison plot ggplot() + # Raw data points geom_point(data = data, aes(x = Seconds/60, y = Temperature, color = Rater), alpha = 0.5) + # Linear model predictions geom_line(data = linear_preds, aes(x = Seconds/60, y = Estimate, linetype = &quot;Linear&quot;), color = &quot;blue&quot;) + geom_ribbon(data = linear_preds, aes(x = Seconds/60, ymin = Q2.5, ymax = Q97.5), fill = &quot;blue&quot;, alpha = 0.1) + # Lognormal model predictions geom_line(data = lognormal_preds, aes(x = Seconds/60, y = Estimate, linetype = &quot;Lognormal&quot;), color = &quot;red&quot;) + geom_ribbon(data = lognormal_preds, aes(x = Seconds/60, ymin = Q2.5, ymax = Q97.5), fill = &quot;red&quot;, alpha = 0.1) + # Formatting facet_wrap(~Rater) + labs( title = &quot;Comparison of Statistical Models&quot;, subtitle = &quot;Linear vs Lognormal Mixed-Effects Models&quot;, x = &quot;Time (minutes)&quot;, y = &quot;Temperature (°C)&quot;, linetype = &quot;Model Type&quot; ) + theme_bw() # Create comparison plot but capping the y axis ggplot() + # Raw data points geom_point(data = data, aes(x = Seconds/60, y = Temperature, color = Rater), alpha = 0.5) + # Linear model predictions geom_line(data = linear_preds, aes(x = Seconds/60, y = Estimate, linetype = &quot;Linear&quot;), color = &quot;blue&quot;) + geom_ribbon(data = linear_preds, aes(x = Seconds/60, ymin = Q2.5, ymax = Q97.5), fill = &quot;blue&quot;, alpha = 0.1) + # Lognormal model predictions geom_line(data = lognormal_preds, aes(x = Seconds/60, y = Estimate, linetype = &quot;Lognormal&quot;), color = &quot;red&quot;) + geom_ribbon(data = lognormal_preds, aes(x = Seconds/60, ymin = Q2.5, ymax = Q97.5), fill = &quot;red&quot;, alpha = 0.1) + ylim(0, 1000) + # Formatting facet_wrap(~Rater) + labs( title = &quot;Comparison of Statistical Models&quot;, subtitle = &quot;Linear vs Lognormal Mixed-Effects Models&quot;, x = &quot;Time (minutes)&quot;, y = &quot;Temperature (°C)&quot;, linetype = &quot;Model Type&quot; ) + theme_bw() 2.5.5 Model Assessment I have seen worse models in my time, but they do seem to have important issues: The linear mixed-effects model assumes a constant rate of temperature change, which we can see is not at all accurate. The actual temperature increase is fast at the beginning and appears to slow down over time, particularly at higher temperatures. While this model has the advantage of simplicity, it is not likely to produce accurate predictions as it seem to fail to capture the underlying physics of heat transfer. The lognormal mixed-effects model is completely off. Further, the models produce some divergences, which is often a sign that they are not well suited to the data. I suggest that the issue is that neither model incorporates our knowledge of heat transfer physics, which suggests an exponential approach to equilibrium temperature. This limitation motivates our next section, where we’ll develop a physics-based model. 2.6 Part 3: Understanding the Physics Model Temperature evolution in a pizza stone follows Newton’s Law of Cooling/Heating. We’ll start by exploring this physical model before applying it to real data. 2.6.1 The Basic Temperature Evolution Equation The temperature evolution of a pizza stone in a gas-fired oven is governed by the heat diffusion equation, which describes how heat flows through solid materials: \\[\\rho c_p \\frac{\\partial T}{\\partial t} = k\\nabla^2T + Q\\] where: \\(\\rho\\) represents the stone’s density (kg/m³) \\(c_p\\) denotes specific heat capacity (J/kg·K) \\(T\\) is temperature (K) \\(t\\) represents time (s) \\(k\\) is thermal conductivity (W/m·K) \\(\\nabla^2\\) is the Laplacian operator \\(Q\\) represents heat input from the oven (W/m³) While this equation provides a complete description of heat flow, we can significantly simplify our analysis by applying the lumped capacitance model. This simplification assumes that the temperature throughout the pizza stone remains uniform at any given time - not perfect, but a reasonable assumption given the stone’s relatively thin profile and good thermal conductivity. This approach reduces our model to: \\[\\frac{dT}{dt} = \\frac{hA}{mc_p}(T_{\\infty} - T)\\] where: \\(h\\) is the heat transfer coefficient (W/m²·K) \\(A\\) is the surface area exposed to heat (m²) \\(m\\) is the stone’s mass (kg) \\(T_{\\infty}\\) is the oven temperature (K) This simplified equation relates the rate of temperature change to the difference between the current stone temperature T and the flame temperature T∞. The coefficient h represents the heat transfer coefficient between the flame and stone, A is the stone’s surface area exposed to heat, m is its mass, and cp remains the specific heat capacity. To solve this differential equation, we begin by separating variables: \\[\\frac{dT}{T_{\\infty} - T} = \\left(\\frac{hA}{mc_p}\\right)dt\\] Integration of both sides yields: \\[-\\ln|T_{\\infty} - T| = \\left(\\frac{hA}{mc_p}\\right)t + C\\] where C is an integration constant. Using the initial condition \\(T = T_i\\) at \\(t = 0\\), we can determine the integration constant: \\[C = -\\ln|T_{\\infty} - T_i|\\] Substituting this back and solving for temperature gives us: \\[T = T_{\\infty} + (T_i - T_{\\infty})\\exp\\left(-\\frac{hA}{mc_p}t\\right)\\] For practical reasons, we combine physical parameters into a single coefficient \\(\\theta\\): \\[HOT = \\frac{hA}{mc_p}\\] Giving our working equation: \\[T = T_{\\infty} + (T_i - T_{\\infty})\\exp(-HOT * t)\\] This equation retains the essential physics while providing a practical model for analyzing our experimental data. The HOT coefficient encapsulates the combined effects of heat transfer efficiency, stone geometry, and material properties into a single parameter that determines how quickly the stone approaches the flame temperature. 2.7 Part 4: Implementing the Physics-Based Model Having established the theoretical foundation for our heat transfer model, we now move to its practical implementation. We will use Stan to create a Bayesian implementation of our physics-based model, allowing us to account for measurement uncertainty and variation between raters. First, we prepare our data for the Stan model. Our model requires initial temperatures, time measurements, and observed temperatures from each rater: # Create data structure for Stan stan_data &lt;- list( N = nrow(data %&gt;% filter(!is.na(Temperature))), time = data %&gt;% filter(!is.na(Temperature)) %&gt;% pull(Seconds), temp = data %&gt;% filter(!is.na(Temperature)) %&gt;% pull(Temperature), n_raters = 3, rater = as.numeric(factor(data %&gt;% filter(!is.na(Temperature)) %&gt;% pull(Rater))), Ti = c(100, 100, 100), # Initial temperature estimates Tinf = 450 # Flame temperature estimate ) Next, we implement our physics-based model in Stan. The model incorporates our derived equation while allowing for rater-specific heating coefficients: stan_code &lt;- &quot; data { int&lt;lower=0&gt; N; // Number of observations vector[N] time; // Time points vector[N] temp; // Observed temperatures int&lt;lower=0&gt; n_raters; // Number of raters array[N] int&lt;lower=1,upper=n_raters&gt; rater; // Rater indices vector[n_raters] Ti; // Initial temperatures real Tinf; // Flame temperature } parameters { vector&lt;lower=0&gt;[n_raters] HOT; // Heating coefficients vector&lt;lower=0&gt;[n_raters] sigma; // Measurement error } model { vector[N] mu; // Physics-based temperature prediction for (i in 1:N) { mu[i] = Tinf + (Ti[rater[i]] - Tinf) * exp(-HOT[rater[i]] * time[i]); } // Prior distributions target += normal_lpdf(HOT | 0.005, 0.005); // Prior for heating rate target += exponential_lpdf(sigma | 1); // Prior for measurement error // Likelihood target += normal_lpdf(temp | mu, sigma[rater]); } &quot; # Save the model writeLines(stan_code, &quot;models/pizza_physics_model.stan&quot;) # Compile and fit the model mod &lt;- cmdstan_model(&quot;models/pizza_physics_model.stan&quot;) fit &lt;- mod$sample( data = stan_data, seed = 123, chains = 2, parallel_chains = 2 ) ## Running MCMC with 2 parallel chains... ## ## Chain 1 Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1 Iteration: 100 / 2000 [ 5%] (Warmup) ## Chain 1 Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1 Iteration: 300 / 2000 [ 15%] (Warmup) ## Chain 1 Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1 Iteration: 500 / 2000 [ 25%] (Warmup) ## Chain 1 Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1 Iteration: 700 / 2000 [ 35%] (Warmup) ## Chain 1 Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1 Iteration: 900 / 2000 [ 45%] (Warmup) ## Chain 1 Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1 Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1 Iteration: 1100 / 2000 [ 55%] (Sampling) ## Chain 1 Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1 Iteration: 1300 / 2000 [ 65%] (Sampling) ## Chain 1 Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1 Iteration: 1500 / 2000 [ 75%] (Sampling) ## Chain 1 Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1 Iteration: 1700 / 2000 [ 85%] (Sampling) ## Chain 1 Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1 Iteration: 1900 / 2000 [ 95%] (Sampling) ## Chain 1 Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2 Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2 Iteration: 100 / 2000 [ 5%] (Warmup) ## Chain 2 Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2 Iteration: 300 / 2000 [ 15%] (Warmup) ## Chain 2 Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2 Iteration: 500 / 2000 [ 25%] (Warmup) ## Chain 2 Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2 Iteration: 700 / 2000 [ 35%] (Warmup) ## Chain 2 Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2 Iteration: 900 / 2000 [ 45%] (Warmup) ## Chain 2 Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2 Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2 Iteration: 1100 / 2000 [ 55%] (Sampling) ## Chain 2 Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2 Iteration: 1300 / 2000 [ 65%] (Sampling) ## Chain 2 Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2 Iteration: 1500 / 2000 [ 75%] (Sampling) ## Chain 2 Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2 Iteration: 1700 / 2000 [ 85%] (Sampling) ## Chain 2 Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2 Iteration: 1900 / 2000 [ 95%] (Sampling) ## Chain 2 Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1 finished in 0.1 seconds. ## Chain 2 finished in 0.1 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.1 seconds. ## Total execution time: 0.6 seconds. The Stan implementation translates our mathematical model into a computational framework. We assign informative priors to our parameters based on physical understanding: the heating coefficient (HOT) is expected to be small but positive, while measurement error (sigma) follows an exponential distribution to ensure positivity while allowing for varying levels of uncertainty between raters. To visualize our model’s predictions and assess its performance, we extract posterior samples and generate predictions across our time range: # Extract draws post &lt;- as_draws_df(fit$draws()) %&gt;% dplyr::select(starts_with(&quot;HOT&quot;), starts_with(&quot;sigma&quot;)) %&gt;% slice_sample(n = 100) # Create prediction grid pred_data &lt;- crossing( time = seq(0, max(stan_data$time), length.out = 100), rater = 1:stan_data$n_raters ) %&gt;% mutate( Ti = stan_data$Ti[rater], Tinf = stan_data$Tinf ) # Generate predictions pred_matrix &lt;- matrix(NA, nrow = nrow(pred_data), ncol = 100) for (i in 1:nrow(pred_data)) { pred_matrix[i,] &lt;- with(pred_data[i,], Tinf + (Ti - Tinf) * exp(-as.matrix(post)[,rater] * time)) } # Summarize predictions predictions &lt;- pred_data %&gt;% mutate( mean = rowMeans(pred_matrix), lower = apply(pred_matrix, 1, quantile, 0.025), upper = apply(pred_matrix, 1, quantile, 0.975) ) # Create visualization ggplot(predictions, aes(x = time/60)) + geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) + geom_line(aes(y = mean)) + geom_point( data = data %&gt;% filter(!is.na(Temperature)) %&gt;% mutate(rater = case_when( Rater == &quot;N&quot; ~ 1, Rater == &quot;TR&quot; ~ 2, Rater == &quot;R&quot; ~ 3 )), aes(x = Seconds/60, y = Temperature) ) + facet_wrap(~rater, labeller = labeller(rater = c( &quot;1&quot; = &quot;Rater N&quot;, &quot;2&quot; = &quot;Rater TR&quot;, &quot;3&quot; = &quot;Rater R&quot; ))) + labs( title = &quot;Physics-Based Model Predictions&quot;, x = &quot;Time (minutes)&quot;, y = &quot;Temperature (°C)&quot; ) + theme_bw() Our implementation combines the theoretical understanding developed in Part 3 with practical considerations for real-world data analysis. The model accounts for measurement uncertainty while maintaining the fundamental physics of heat transfer, providing a robust framework for understanding pizza stone temperature evolution. 2.8 Part 5: Model Analysis and Practical Applications Having implemented our physics-based model, we can now analyze its predictions and develop practical insights for pizza stone temperature management. A key question for pizza making is how long it takes to reach optimal cooking temperatures under different conditions. We begin by creating a function that calculates the time needed to reach a target temperature: time_to_temp &lt;- function(target_temp, HOT, Ti, Tinf) { # Solve: target = Tinf + (Ti - Tinf) * exp(-HOT * t) # for t t = -1/HOT * log((target_temp - Tinf)/(Ti - Tinf)) return(t/60) # Convert seconds to minutes } To understand heating times across different oven conditions, we examine how varying flame temperatures affect the time needed to reach pizza-making temperatures. We extract the heating coefficients from our fitted model and analyze temperature scenarios: # Extract HOT samples from our posterior hot_samples &lt;- as_draws_df(fit$draws()) %&gt;% dplyr::select(starts_with(&quot;HOT&quot;)) # Create prediction grid for different flame temperatures pred_data &lt;- crossing( Tinf = seq(450, 1200, by = 50), # Range of flame temperatures rater = 1:3 ) %&gt;% mutate( Ti = stan_data$Ti[rater], target_temp = 400 # Target temperature for pizza cooking ) # Calculate heating times across conditions n_samples &lt;- 100 time_preds &lt;- map_dfr(1:nrow(pred_data), function(i) { times &lt;- sapply(1:n_samples, function(j) { hot &lt;- hot_samples[j, paste0(&quot;HOT[&quot;, pred_data$rater[i], &quot;]&quot;)][[1]] time_to_temp( pred_data$target_temp[i], hot, pred_data$Ti[i], pred_data$Tinf[i] ) }) data.frame( rater = pred_data$rater[i], Tinf = pred_data$Tinf[i], mean_time = mean(times), lower = quantile(times, 0.025), upper = quantile(times, 0.975) ) }) # Visualize heating time predictions ggplot(time_preds, aes(x = Tinf)) + geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) + geom_line(aes(y = mean_time)) + facet_wrap(~rater, labeller = labeller(rater = c( &quot;1&quot; = &quot;Rater N&quot;, &quot;2&quot; = &quot;Rater TR&quot;, &quot;3&quot; = &quot;Rater R&quot; ))) + labs( title = &quot;Time Required to Reach Pizza-Making Temperature&quot;, subtitle = &quot;Target temperature: 400°C&quot;, x = &quot;Flame Temperature (°C)&quot;, y = &quot;Minutes to reach target&quot; ) + theme_bw() Our analysis reveals several important insights for practical pizza making. First, the heating time decreases nonlinearly with flame temperature, showing diminishing returns at very high temperatures. We can also observe differences between raters in their measured heating times. These variations likely stem from differences in measurement technique and location on the stone, highlighting the importance of consistent temperature monitoring practices. For practical application, we can provide specific heating guidelines based on our model. At a typical flame temperature of 800°C, the model predicts it will take approximately 20-30 minutes to reach optimal pizza-making temperature, assuming room temperature start. However, this time can vary significantly based on: Initial stone temperature Flame temperature and consistency Environmental conditions. Can we really wait that long? 2.9 Conclusion: From Pizza to Principles The journey from modeling a heating pizza stone to understanding cognitive processes might seem unusual, but it illustrates fundamental principles that will guide us throughout this course. Through this seemingly simple physics problem, we have encountered the core challenges that cognitive scientists face daily. Just relying on standard statistical models is not enough. We need to understand the underlying generative processes. We discovered how choosing the right level of analysis shapes our understanding - just as we simplified complex heat equations into workable models, cognitive scientists must decide which aspects of the mental processes to model explicitly and which to abstract. We learned that even well-understood physical processes require careful statistical treatment, foreshadowing the challenges we will face with more complex cognitive phenomena. The pizza stone experiment also demonstrated the importance of rigorous methodology. We saw how multiple measurements from different raters revealed variability in our data, leading us to consider measurement error and individual differences - themes that will become crucial when studying human behavior. Our exploration of different statistical approaches, from simple linear models to more sophisticated Bayesian frameworks, established a foundation for the modeling techniques we will develop throughout this course. Perhaps most importantly, this chapter starts showing that successful modeling requires balancing competing demands. We must weigh theoretical complexity against practical utility, statistical sophistication against interpretability, and mathematical elegance against real-world applicability. These trade-offs will become even more prominent as we move into modeling cognitive processes. As we progress through this course, we will encounter increasingly complex cognitive phenomena. The principles we learned here - careful data collection, thoughtful model specification, rigorous validation, and balanced interpretation - will serve as our guide. While human cognition presents challenges far beyond those of heating pizza stones, the fundamental approach remains the same: start with clear observations, build theoretically motivated models, and test them systematically against data. In the next chapter, we will begin applying these principles directly to cognitive processes, starting with simple decision-making tasks. The mathematical tools and statistical frameworks introduced here will provide the foundation for understanding how humans process information and make choices. Finally, I hope you are hungry now. I know I am. Let’s go and make some pizza! "],["building-models-of-strategic-decision-making.html", "Chapter 3 Building Models of Strategic Decision-Making 3.1 Learning goals 3.2 Introduction 3.3 The Matching Pennies Game 3.4 Game Structure 3.5 Empirical Investigation 3.6 Empirical explorations 3.7 Notes from previous years 3.8 Building Formal Models 3.9 Cognitive constraints 3.10 Continuity between models 3.11 Mixture of strategies 3.12 Differences from more traditional (general linear model-based) approaches", " Chapter 3 Building Models of Strategic Decision-Making 3.1 Learning goals Becoming more aware of the issue involved in theory building (and assessment); Identifying a small set of verbal models that we can then formalize in mathematical cognitive models and algorithms for simulations and model fitting. 3.2 Introduction In order to do computational models we need a phenomenon to study (and ideally some data), throughout the course you will be asked undergo several experiments, which provides specific behaviors to model. The matching pennies game provides a fun starting point for exploring cognitive modeling. This simple game allows us to examine how humans make decisions in strategic situations, while introducing fundamental concepts in model development and validation. Through this chapter, we will progress from observing actual gameplay behavior to developing formal models that capture decision-making processes. 3.3 The Matching Pennies Game In the matching pennies game, two players engage in a series of choices. One player attempts to match the other’s choice, while the other player aims to achieve a mismatch, and they repeatedly play with each other. This is a prototypical example of interacting behaviors that are usually tackled by game theory, and bring up issues of theory of mind and recursivity. For an introduction see the paper: Waade, Peter T., et al. “Introducing tomsup: Theory of mind simulations using Python.” Behavior Research Methods 55.5 (2023): 2197-2231. 3.4 Game Structure The game proceeds as follows: Two players sit facing each other Each round, both players choose either “left” or “right” to indicate where they believe a penny is hidden The matcher wins by choosing the same hand as their opponent The hider wins by choosing the opposite hand Points are awarded: +1 for winning, -1 for losing Repeat This simple structure creates a rich environment for studying decision-making strategies, learning, and adaptation. 3.5 Empirical Investigation 3.5.1 Data Collection Protocol If you are attending my class you have been (or will be) asked to participate in a matching pennies game. This game provides the foundation for our modeling efforts. By observing gameplay and collecting data, we can develop models that capture the cognitive processes underlying decision-making in strategic situations. Participants play 30 rounds as the matcher and 30 rounds as the hider, allowing us to observe behavior in both roles. While playing, participants track their scores, which can provide quantitative data for later analysis. Participants are also asked to reflect on their strategies and the strategies they believe their opponents are using, as that provides valuable materials to build models on. 3.5.2 Initial Observations Through the careful observation and discussion of gameplay we do in class, several patterns typically emerge. For instance, players often demonstrate strategic adaptation, adjusting their choices based on their opponent’s previous moves. They may attempt to identify patterns in their opponent’s behavior while trying to make their own choices less predictable. The tension between exploitation of perceived patterns and maintenance of unpredictability creates fascinating dynamics for modeling. 3.6 Empirical explorations Below you can observe how a previous year of CogSci did against bots (computational agents) playing according to different strategies. Look at the plots below, where the x axes indicate trial, the y axes how many points the CogSci’ers scored (0 being chance, negative means being completely owned by the bots, positive owning the bot) and the different colors indicate different strategies employed by the bots. Strategy “-2” was a Win-Stay-Lose-Shift bot: when it got a +1, it repeated its previous move (e.g. right if it had just played right), otherwise it would perform the opposite move (e.g. left if it had just played right). Strategy “-1” was a biased Nash both, playing “right” 80% of the time. Strategy “0” indicates a reinforcement learning bot; “1” a bot assuming you were playing according to a reinforcement learning strategy and trying to infer your learning and temperature parameters; “2” a bot assuming you were following strategy “1” and trying to accordingly infer your parameters. library(tidyverse) d &lt;- read_csv(&quot;data/MP_MSc_CogSci22.csv&quot;) %&gt;% mutate(BotStrategy = as.factor(BotStrategy)) d$Role &lt;- ifelse(d$Role == 0, &quot;Matcher&quot;, &quot;Hider&quot;) ggplot(d, aes(Trial, Payoff, group = BotStrategy, color = BotStrategy)) + geom_smooth(se = F) + theme_classic() + facet_wrap(.~Role) That doesn’t look too good, ah? What about individual variability? In the plot below we indicate the score of each of the former students, against the different bots. d1 &lt;- d %&gt;% group_by(ID, BotStrategy) %&gt;% dplyr::summarize(Score = sum(Payoff)) ggplot(d1, aes(BotStrategy, Score, label = ID)) + geom_point(aes(color = ID)) + geom_boxplot(alpha = 0.3) + theme_classic() Now, let’s take a bit of group discussion. Get together in groups, and discuss which strategies and cognitive processes might underlie your and the agents’ behaviors in the game. One thing to keep in mind is what a model is: a simplification that can help us make sense of the world. In other words, any behavior is incredibly complex and involves many complex cognitive mechanisms. So start simple, and if you think it’s too simple, progressively add simple components. Once your study group has discussed a few (during the PE), let’s discuss them. 3.7 Notes from previous years 3.7.1 From Observation to Theory The transition from observing gameplay to building formal models requires careful consideration of multiple factors. We must identify which aspects of behavior to model explicitly while deciding which details can be abstracted away. 3.7.2 Core Modeling Considerations When developing models of matching pennies behavior, we must address several key questions: What information do players use to make decisions? How do players integrate past experiences with current choices? What role does randomness play in decision-making? How do players adapt their strategies over time? Are there notions and models from previous cognitive science courses that can help us understand the behavior? These questions guide our model development process, helping us move from verbal theories to mathematical formulations. 3.7.3 The distinction between participant and researcher perspectives As participants we might not be aware of the strategy we use, or we might believe something erroneous. The exercise here is to act as researchers: what are the principles underlying the participants’ behaviors, no matter what the participants know or believe? Note that talking to participants and being participants helps developing ideas, but it’s not the end point of the process. Also note that as cognitive scientists we can rely on what we have learned about cognitive processes (e.g. memory). Another important component of the distinction is that participants leave in a rich world: they rely on facial expressions and bodily posture, the switch strategies, etc. On the other hand, the researcher is trying to identify one or few at most “simple” strategies. Rich bodily interactions and mixtures or sequences of multiple strategies are not a good place to start modeling. These aspects are a poor starting point for building your first model, and are often pretty difficult to fit to empirical data. Nevertheless, they are important intuitions that the researcher should (eventually?) accommodate. 3.8 Building Formal Models Based on observed behavior patterns and theoretical considerations, we can develop several candidate models of decision-making in the matching pennies game. 3.8.1 Random Choice Model The simplest model assumes players make choices randomly, independent of history or context. Players might simply be randomly choosing “head” or “tail” independently on the opponent’s choices and of how well they are doing. Choices could be fully at random (50% “head”, 50% “tail”) or biased (e.g. 60% “head”, 40% tail). While this may seem overly simplistic, it provides an important baseline for comparison and introduces key concepts in model specification. 3.8.2 Immediate reaction (Win-Stay-Lose-Shift) Another simple strategy is simply to follow the previous choice: if it was successful keep it, if not change it. This strategy is also called Win-Stay-Lose-Shift (WSLS). The model can be formalized as: \\[P(a_t = a_{t-1}) = \\begin{cases} p_w &amp; \\text{if win at } t-1 \\ 1 - p_l &amp; \\text{if loss at } t-1 \\end{cases}\\] where \\(a_t\\) represents the action at time \\(t\\), and \\(p_w\\) and \\(p_l\\) are the probabilities of staying after wins and losses respectively. Alternatively, one could do the opposite: Win-Shift-Lose-Stay. 3.8.3 Keep track of the bias (perfect memory) A more sophisticated approach considers how players track and respond to their opponent’s choice patterns. This model maintains a running estimate of the opponent’s choice probabilities and updates these estimates based on observed choices. 3.8.4 Keep track of the bias (imperfect memory) A player could not be able to keep in mind all previous trials, or decide to forget old trials, in case the biase shifts over time. So we could use only the last n trials, or do a weighted mean with weigths proportional to temporal closeness (the more recent, the higher the weight). 3.8.5 Reinforcement learning Since there is a lot of leeway in how much memory we should keep of previous trials, we could also use a model that explicitly estimates how much players are learning on a trial by trial basis (high learning, low memory; low learning, high memory). This is the model of reinforcement learning, which we will deal with in future chapters. Shortly described, reinforcement learning assumes that each choice has a possible reward (probability of winning) and at every trial given the feedback received updates the expected value of the choice taken. The update depends on the prediction error (difference between expected and actual reward) and the learning rate. 3.8.6 k-ToM Reinforcement learning is a neat model, but can be problematic when playing against other agents: what the game is really about is not assessing the probability of the opponent choosing “head” generalizing from their past choices, but predicting what they will do. This requires making an explicit model of how the opponent chooses. k-ToM models will be dealt with in future chapters, but can be here anticipated as models assuming that the opponent follows a random bias (0-ToM), or models us as following a random bias (1-ToM), or models us modeling them as following a random bias (2-ToM), etc. 3.8.7 Other possible strategies Many additional strategies can be generated by combining former strategies. Generating random output is hard, so if we want to confuse the opponent, we could act first choosing tail 8 times, and then switching to a WSLS strategy for 4 trials, and then choosing head 4 times. Or implementing any of the previous strategies and doing the opposite “to mess with the opponent”. 3.9 Cognitive constraints As we discuss strategies, we can also identify several cognitive constraints that we know from former studies: in particular, memory, perseveration, and errors. 3.9.1 Memory Humans have limited memory and a tendency to forget that is roughly exponential. Models assuming perfect memory for longer stretches of trials are unrealistic. We could for instance use the exponential decay of memory to create weights following the same curve in the “keeping track of bias” models. Roughly, this is what reinforcement learning is doing via the learning rate parameter. 3.9.2 Perseveration Winning choice is not changed. People tend to have a tendency to perseverate with “good” choices independently of which other strategy they might be using. 3.9.3 Errors Humans make mistakes, get distracted, push the wrong button, forget to check whether they won or lost before. So a realistic model of what happens in these games should contain a certain chance of making a mistake. E.g. a 10% chance that any choice will be perfectly random instead of following the strategy. Such random deviations from the strategy might also be conceptualized as explorations: keeping the door open to the strategy not being optimal and therefore testing other choices. For instance, one could have an imperfect WSLS where the probability of staying if winning (or shifting if losing) is only 80% and not 100%. Further, these deviations could be asymmetric, with the probability of staying if winning is 80% and of shifting if losing is 100%; for instance if negative and positive feedback are perceived asymmetrically. 3.10 Continuity between models Many of these models are simply extreme cases of others. For instance, WSLS is a reinforcement learning model with an extreme learning rate (reward replaces the formerly expected value without any moderation), which is also a memory model with a memory of 1 previous trial. k-ToM builds on reinforcement learning: at level 1 assumes the other is a RL agent. 3.11 Mixture of strategies We discussed that there are techniques to consider the data generated by a mixture of models: estimating the probability that they are generated by model 1 or 2 or n. This probability can then be conditioned, according to our research question, to group (are people w schizophrenia more likely to employ model 1) or ID (are different participants using different models), or condition, or… We discussed that we often need lots of data to disambiguate between models, so conditioning e.g. on trial would in practice almost (?) never work. 3.12 Differences from more traditional (general linear model-based) approaches In a more traditional approach we would carefully set up the experiment to discriminate between hypotheses. For instance, if the hypothesis is that humans deploy ToM only when playing against intentional agents, we can set agents with increasing levels of k-ToM against humans, set up two framings (this is a human playing hide and seek, this is a slot machine), and assess whether humans perform differently. E.g. whether they perform better when thinking it’s a human. We analyze performance e.g. as binary outcome on a trial by trial base and condition its rate on framing and complexity. If framing makes a difference in the expected direction, we are good. If we do this properly, thanks to the clever experimental designs we set up, we can discriminate between hypotheses. And that is good. However, cognitive modeling opens additional opportunities. For instance, we can actually reconstruct which level of recursion the participants are enacting and if it changes over time. This might be very useful in the experimental setup, and crucial in more observational setups. Cognitive modeling also allows us to discriminate between different cognitive components more difficult to assess by looking at performance only. For instance, why are participants performing less optimally when facing a supposedly non-intentional agent? Is their learning rate different? Is their estimate of volatility different? In other setups, e.g. a gambling context, we might observe that some participants (e.g. parkinson’s patients) are gambling away much. Is this due to changes in their risk-seeking propensities, loss aversion, or changes in the ability to actually learn the reward structure? Experimental setups help, but cognitive modeling can provide more nuanced and direct evidence. "],["from-verbal-descriptions-to-formal-models.html", "Chapter 4 From verbal descriptions to formal models 4.1 Learning Goals 4.2 The Value of Formalization 4.3 Defining general conditions 4.4 Implementing a random agent 4.5 Implementing a Win-Stay-Lose-Shift agent 4.6 Now we scale it up 4.7 Conclusion", " Chapter 4 From verbal descriptions to formal models This chapter bridges the gap between verbal theories and computational implementations of cognitive models. Building on our observations of the matching pennies game, we now develop precise mathematical formulations that can generate testable predictions. 4.1 Learning Goals After completing this chapter, you will be able to: Transform verbal descriptions of decision-making strategies into precise mathematical formulations, which implications can be more easily explored and that can be empirically tested Create computational implementations of these mathematical models as agent-based models in R Generate and analyze simulated data to understand model behavior under different conditions 4.2 The Value of Formalization Moving from verbal to formal models represents a crucial step in cognitive science. When we describe behavior in words, ambiguities often remain hidden. For instance, a verbal description might state that players “tend to repeat successful choices.” But what exactly constitutes “tend to”? How strongly should past successes influence future choices? Mathematical formalization forces us to be precise about these specifications. By computationally implementing the our models, we are forced to make them very explicit in their assumptions; we become able to simulate the models in a variety of different situations and therefore better understand their implications So, what we’ll do throughout the chapter is to: choose two of the models and formalize them, that is, produce an algorithm that enacts the strategy, so we can simulate them. implement the algorithms as functions: getting an input and producing an output, so we can more easily implement them across various contexts (e.g. varying amount of trials, input, etc). See R4DataScience, if you need a refresher: https://r4ds.had.co.nz/functions.html implement a Random Bias agent (choosing “head” 70% of the times) and get your agents to play against it for 120 trials (and save the data) implement a Win-Stay-Lose-Shift agent (keeping the same choice if it won, changing it if it lost) and do the same. scale up the simulation: have 100 agents for each of your strategy playing against both Random Bias and Win-Stay-Lose-Shift and save their data. figure out a good way to visualize the data to assess which strategy performs better, whether that changes over time and generally explore what the agents are doing. 4.3 Defining general conditions pacman::p_load(tidyverse, patchwork) # Number of trials per simulation trials &lt;- 120 # Number of agents to simulate agents &lt;- 100 # Optional: Set random seed for reproducibility # set.seed(123) 4.4 Implementing a random agent Remember a random agent is an agent that picks at random between “right” and “left” independently on what the opponent is doing. A random agent might be perfectly random (50% chance of choosing “right”, same for “left”) or biased. The variable “rate” determines the rate of choosing “right”. rate &lt;- 0.5 RandomAgent &lt;- rbinom(trials, 1, rate) # we simply sample randomly from a binomial # Now let&#39;s plot how it&#39;s choosing d1 &lt;- tibble(trial = seq(trials), choice = RandomAgent) p1 &lt;- ggplot(d1, aes(trial, choice)) + geom_line() + labs( title = &quot;Random Agent Behavior (rate 0.5)&quot;, x = &quot;Trial Number&quot;, y = &quot;Choice (0/1)&quot; ) + theme_classic() p1 # What if we were to compare it to an agent being biased? rate &lt;- 0.8 RandomAgent &lt;- rbinom(trials, 1, rate) # we simply sample randomly from a binomial # Now let&#39;s plot how it&#39;s choosing d2 &lt;- tibble(trial = seq(trials), choice = RandomAgent) p2 &lt;- ggplot(d2, aes(trial, choice)) + geom_line() + labs( title = &quot;Biased Random Agent Behavior&quot;, x = &quot;Trial Number&quot;, y = &quot;Choice (0/1)&quot; ) + theme_classic() p1 + p2 print(&quot;This first visualization shows the behavior of a purely random agent - one that chooses between options with equal probability (rate = 0.5). Looking at the jagged line jumping between 0 and 1, we can see that the agent&#39;s choices appear truly random, with no discernible pattern. This represents what we might expect from a player who is deliberately trying to be unpredictable in the matching pennies game. However, this raw choice plot can be hard to interpret. A more informative way to look at the agent&#39;s behavior is to examine how its average rate of choosing option 1 evolves over time:&quot;) ## [1] &quot;This first visualization shows the behavior of a purely random agent - one that chooses between options with equal probability (rate = 0.5). Looking at the jagged line jumping between 0 and 1, we can see that the agent&#39;s choices appear truly random, with no discernible pattern. This represents what we might expect from a player who is deliberately trying to be unpredictable in the matching pennies game.\\nHowever, this raw choice plot can be hard to interpret. A more informative way to look at the agent&#39;s behavior is to examine how its average rate of choosing option 1 evolves over time:&quot; # Tricky to see, let&#39;s try writing the cumulative rate: d1$cumulativerate &lt;- cumsum(d1$choice) / seq_along(d1$choice) d2$cumulativerate &lt;- cumsum(d2$choice) / seq_along(d2$choice) p3 &lt;- ggplot(d1, aes(trial, cumulativerate)) + geom_line() + ylim(0,1) + labs( title = &quot;Random Agent Behavior&quot;, x = &quot;Trial Number&quot;, y = &quot;Cumulative probability of choosing 1 (0-1)&quot; ) + theme_classic() p4 &lt;- ggplot(d2, aes(trial, cumulativerate)) + geom_line() + labs( title = &quot;Random Agent Behavior&quot;, x = &quot;Trial Number&quot;, y = &quot;Cumulative probability of choosing 1 (0-1)&quot; ) + ylim(0,1) + theme_classic() p3 + p4 print(&quot;This cumulative rate plot helps us better understand the agent&#39;s overall tendencies. For a truly random agent, we expect this line to converge toward 0.5 as the number of trials increases. Early fluctuations away from 0.5 are possible due to random chance, but with more trials, these fluctuations tend to even out. When we compare agents with different underlying biases (rate = 0.5 vs rate = 0.8):&quot;) ## [1] &quot;This cumulative rate plot helps us better understand the agent&#39;s overall tendencies. For a truly random agent, we expect this line to converge toward 0.5 as the number of trials increases. Early fluctuations away from 0.5 are possible due to random chance, but with more trials, these fluctuations tend to even out.\\nWhen we compare agents with different underlying biases (rate = 0.5 vs rate = 0.8):&quot; ## Now in the same plot d1$rate &lt;- 0.5 d2$rate &lt;- 0.8 d &lt;- rbind(d1,d2) %&gt;% mutate(rate = as.factor(rate)) p5 &lt;- ggplot(d, aes(trial, cumulativerate, color = rate, group = rate)) + geom_line() + labs( title = &quot;Random Agents Behavior&quot;, x = &quot;Trial Number&quot;, y = &quot;Cumulative probability of choosing 1 (0-1)&quot; ) + ylim(0,1) + theme_classic() p5 print(&quot;We can clearly see how bias affects choice behavior. The unbiased agent (rate = 0.5) stabilizes around choosing each option equally often, while the biased agent (rate = 0.8) shows a strong preference for option 1, choosing it approximately 80% of the time. This comparison helps us understand how we might detect biases in real players&#39; behavior - consistent deviation from 50-50 choice proportions could indicate an underlying preference or strategy.&quot;) ## [1] &quot;We can clearly see how bias affects choice behavior. The unbiased agent (rate = 0.5) stabilizes around choosing each option equally often, while the biased agent (rate = 0.8) shows a strong preference for option 1, choosing it approximately 80% of the time. This comparison helps us understand how we might detect biases in real players&#39; behavior - consistent deviation from 50-50 choice proportions could indicate an underlying preference or strategy.&quot; # Now as a function #&#39; Create a random decision-making agent #&#39; @param input Vector of previous choices (not used but included for API consistency) #&#39; @param rate Probability of choosing option 1 (default: 0.5 for unbiased) #&#39; @return Vector of binary choices #&#39; @examples #&#39; # Create unbiased random agent for 10 trials #&#39; choices &lt;- RandomAgent_f(rep(1,10), 0.5) RandomAgent_f &lt;- function(input, rate = 0.5) { # Input validation if (!is.numeric(rate) || rate &lt; 0 || rate &gt; 1) { stop(&quot;Rate must be a probability between 0 and 1&quot;) } n &lt;- length(input) choice &lt;- rbinom(n, 1, rate) return(choice) } input &lt;- rep(1,trials) # it doesn&#39;t matter, it&#39;s not taken into account choice &lt;- RandomAgent_f(input, rate) d3 &lt;- tibble(trial = seq(trials), choice) ggplot(d3, aes(trial, choice)) + geom_line() + labs( title = &quot;Random Agent Behavior&quot;, x = &quot;Trial Number&quot;, y = &quot;Cumulative probability of choosing 1 (0-1)&quot; ) + theme_classic() ## What if there&#39;s noise? RandomAgentNoise_f &lt;- function(input, rate, noise){ n &lt;- length(input) choice &lt;- rbinom(n, 1, rate) if (rbinom(1, 1, noise) == 1) {choice = rbinom(1,1,0.5)} return(choice) } 4.5 Implementing a Win-Stay-Lose-Shift agent #&#39; Create a Win-Stay-Lose-Shift decision-making agent #&#39; @param prevChoice Previous choice made by the agent (0 or 1) #&#39; @param feedback Success of previous choice (1 for win, 0 for loss) #&#39; @param noise Optional probability of random choice (default: 0) #&#39; @return Next choice (0 or 1) #&#39; @examples #&#39; # Basic WSLS decision after a win #&#39; next_choice &lt;- WSLSAgent_f(prevChoice = 1, feedback = 1) WSLSAgent_f &lt;- function(prevChoice, feedback, noise = 0) { # Input validation if (!is.numeric(prevChoice) || !prevChoice %in% c(0,1)) { stop(&quot;Previous choice must be 0 or 1&quot;) } if (!is.numeric(feedback) || !feedback %in% c(0,1)) { stop(&quot;Feedback must be 0 or 1&quot;) } if (!is.numeric(noise) || noise &lt; 0 || noise &gt; 1) { stop(&quot;Noise must be a probability between 0 and 1&quot;) } # Core WSLS logic choice &lt;- if (feedback == 1) { prevChoice # Stay with previous choice if won } else { 1 - prevChoice # Switch to opposite choice if lost } # Apply noise if specified if (noise &gt; 0 &amp;&amp; runif(1) &lt; noise) { choice &lt;- sample(c(0,1), 1) } return(choice) } WSLSAgentNoise_f &lt;- function(prevChoice, Feedback, noise){ if (Feedback == 1) { choice = prevChoice } else if (Feedback == 0) { choice = 1 - prevChoice } if (rbinom(1, 1, noise) == 1) {choice &lt;- rbinom(1, 1, .5)} return(choice) } WSLSAgent &lt;- WSLSAgent_f(1, 0) # Against a random agent Self &lt;- rep(NA, trials) Other &lt;- rep(NA, trials) Self[1] &lt;- RandomAgent_f(1, 0.5) Other &lt;- RandomAgent_f(seq(trials), rate) for (i in 2:trials) { if (Self[i - 1] == Other[i - 1]) { Feedback = 1 } else {Feedback = 0} Self[i] &lt;- WSLSAgent_f(Self[i - 1], Feedback) } sum(Self == Other) ## [1] 88 df &lt;- tibble(Self, Other, trial = seq(trials), Feedback = as.numeric(Self == Other)) ggplot(df) + theme_classic() + geom_line(color = &quot;red&quot;, aes(trial, Self)) + geom_line(color = &quot;blue&quot;, aes(trial, Other)) + labs( title = &quot;WSLS Agent (red) vs Biased Random Opponent (blue)&quot;, x = &quot;Trial Number&quot;, y = &quot;Choice (0/1)&quot;, color = &quot;Agent Type&quot; ) ggplot(df) + theme_classic() + geom_line(color = &quot;red&quot;, aes(trial, Feedback)) + geom_line(color = &quot;blue&quot;, aes(trial, 1 - Feedback)) + labs( title = &quot;WSLS Agent (red) vs Biased Random Opponent (blue)&quot;, x = &quot;Trial Number&quot;, y = &quot;Feedback received (0/1)&quot;, color = &quot;Agent Type&quot; ) print(&quot;These plots compare how a Win-Stay-Lose-Shift (WSLS) agent performs against different opponents. The red line shows the WSLS agent&#39;s choices, while the blue line shows the opponent&#39;s choices. When playing against a biased random opponent, we can see clearer patterns in the WSLS agent&#39;s behavior as it responds to wins and losses. Against another WSLS agent, the interaction becomes more complex, as each agent is trying to adapt to the other&#39;s adaptations. This kind of visualization helps us understand how different strategies might interact in actual gameplay.&quot;) ## [1] &quot;These plots compare how a Win-Stay-Lose-Shift (WSLS) agent performs against different opponents. The red line shows the WSLS agent&#39;s choices, while the blue line shows the opponent&#39;s choices. When playing against a biased random opponent, we can see clearer patterns in the WSLS agent&#39;s behavior as it responds to wins and losses. Against another WSLS agent, the interaction becomes more complex, as each agent is trying to adapt to the other&#39;s adaptations. This kind of visualization helps us understand how different strategies might interact in actual gameplay.&quot; df$cumulativerateSelf &lt;- cumsum(df$Feedback) / seq_along(df$Feedback) df$cumulativerateOther &lt;- cumsum(1 - df$Feedback) / seq_along(df$Feedback) ggplot(df) + theme_classic() + geom_line(color = &quot;red&quot;, aes(trial, cumulativerateSelf)) + geom_line(color = &quot;blue&quot;, aes(trial, cumulativerateOther)) + labs( title = &quot;WSLS Agent (red) vs Biased Random Opponent (blue)&quot;, x = &quot;Trial Number&quot;, y = &quot;Cumulative probability of choosing 1 (0-1)&quot;, color = &quot;Agent Type&quot; ) # Against a Win-Stay-Lose Shift Self &lt;- rep(NA, trials) Other &lt;- rep(NA, trials) Self[1] &lt;- RandomAgent_f(1, 0.5) Other[1] &lt;- RandomAgent_f(1, 0.5) for (i in 2:trials) { if (Self[i - 1] == Other[i - 1]) { Feedback = 1 } else {Feedback = 0} Self[i] &lt;- WSLSAgent_f(Self[i - 1], Feedback) Other[i] &lt;- WSLSAgent_f(Other[i - 1], 1 - Feedback) } sum(Self == Other) ## [1] 60 df &lt;- tibble(Self, Other, trial = seq(trials), Feedback = as.numeric(Self == Other)) ggplot(df) + theme_classic() + geom_line(color = &quot;red&quot;, aes(trial, Self)) + geom_line(color = &quot;blue&quot;, aes(trial, Other)) ggplot(df) + theme_classic() + geom_line(color = &quot;red&quot;, aes(trial, Feedback)) + geom_line(color = &quot;blue&quot;, aes(trial, 1 - Feedback)) df$cumulativerateSelf &lt;- cumsum(df$Feedback) / seq_along(df$Feedback) df$cumulativerateOther &lt;- cumsum(1 - df$Feedback) / seq_along(df$Feedback) ggplot(df) + theme_classic() + geom_line(color = &quot;red&quot;, aes(trial, cumulativerateSelf)) + geom_line(color = &quot;blue&quot;, aes(trial, cumulativerateOther)) print(&quot;This cumulative performance plot reveals the overall effectiveness of the WSLS strategy. By tracking the running average of successes, we can see whether the strategy leads to above-chance performance in the long run. When playing against a biased random opponent, the WSLS agent can potentially exploit the opponent&#39;s predictable tendencies, though success depends on how strong and consistent the opponent&#39;s bias is. When we pit the WSLS agent against another WSLS agent, the dynamics become more complex. Both agents are now trying to adapt to each other&#39;s adaptations, creating a more sophisticated strategic interaction. The resulting behavior often shows interesting patterns of mutual adaptation, where each agent&#39;s attempts to exploit the other&#39;s strategy leads to evolving patterns of play.&quot;) ## [1] &quot;This cumulative performance plot reveals the overall effectiveness of the WSLS strategy. By tracking the running average of successes, we can see whether the strategy leads to above-chance performance in the long run. When playing against a biased random opponent, the WSLS agent can potentially exploit the opponent&#39;s predictable tendencies, though success depends on how strong and consistent the opponent&#39;s bias is.\\nWhen we pit the WSLS agent against another WSLS agent, the dynamics become more complex. Both agents are now trying to adapt to each other&#39;s adaptations, creating a more sophisticated strategic interaction. The resulting behavior often shows interesting patterns of mutual adaptation, where each agent&#39;s attempts to exploit the other&#39;s strategy leads to evolving patterns of play.&quot; 4.6 Now we scale it up trials = 120 agents = 100 # WSLS vs agents with varying rates for (rate in seq(from = 0.5, to = 1, by = 0.05)) { for (agent in seq(agents)) { Self &lt;- rep(NA, trials) Other &lt;- rep(NA, trials) Self[1] &lt;- RandomAgent_f(1, 0.5) Other &lt;- RandomAgent_f(seq(trials), rate) for (i in 2:trials) { if (Self[i - 1] == Other[i - 1]) { Feedback = 1 } else {Feedback = 0} Self[i] &lt;- WSLSAgent_f(Self[i - 1], Feedback) } temp &lt;- tibble(Self, Other, trial = seq(trials), Feedback = as.numeric(Self == Other), agent, rate) if (agent == 1 &amp; rate == 0.5) {df &lt;- temp} else {df &lt;- bind_rows(df, temp)} } } ## WSLS with another WSLS for (agent in seq(agents)) { Self &lt;- rep(NA, trials) Other &lt;- rep(NA, trials) Self[1] &lt;- RandomAgent_f(1, 0.5) Other[1] &lt;- RandomAgent_f(1, 0.5) for (i in 2:trials) { if (Self[i - 1] == Other[i - 1]) { Feedback = 1 } else {Feedback = 0} Self[i] &lt;- WSLSAgent_f(Self[i - 1], Feedback) Other[i] &lt;- WSLSAgent_f(Other[i - 1], 1 - Feedback) } temp &lt;- tibble(Self, Other, trial = seq(trials), Feedback = as.numeric(Self == Other), agent, rate) if (agent == 1 ) {df1 &lt;- temp} else {df1 &lt;- bind_rows(df1, temp)} } 4.6.1 And we visualize it ggplot(df, aes(trial, Feedback, group = rate, color = rate)) + geom_smooth(se = F) + theme_classic() We can see that the bigger the bias in the random agent, the bigger the performance in the WSLS (the higher the chances the random agent picks the same hand more than once in a row). Now it’s your turn to follow a similar process for your 2 chosen strategies. 4.7 Conclusion Moving from verbal descriptions to formal computational models represents a crucial step in cognitive science. Through our work with the matching pennies game, we have seen how this transformation process requires careful consideration of theoretical assumptions, mathematical precision, and practical implementation details. The development of formal models forces us to be explicit about mechanisms that might remain ambiguous in verbal descriptions. When we state that an agent “learns from experience” or “responds to patterns,” we must specify exactly how these processes work. This precision not only clarifies our theoretical understanding but also enables rigorous empirical testing. Our implementation of different agent types - from simple random choice to more sophisticated strategies - demonstrates how computational modeling can reveal surprising implications of seemingly straightforward theories. Through simulation, we discovered that even basic strategies can produce complex patterns of behavior, especially when agents interact with each other over multiple trials. Perhaps most importantly, this chapter has established a foundational workflow for cognitive modeling: begin with careful observation, think carefully and develop precise mathematical formulations, implement these as computational models, and validate predictions against data. Don’t be afraid to make mistakes, or rethink your strategy and iterate the modeling process. This systematic approach will serve as our template as we progress to more complex cognitive phenomena in subsequent chapters. While our matching pennies models may seem simple compared to the rich complexity of human cognition, they exemplify the essential principles of good modeling practice: clarity of assumptions, precision in implementation, and rigorous validation against empirical data. These principles will guide our exploration of more sophisticated cognitive models throughout this course. For more advanced examples of models that can underly behavior in the Matching Pennies game check: Chapter 12 on reinforcement learning. the paper by Waade et al mentioned at the beginning of the chapter. "],["from-simulation-to-model-fitting.html", "Chapter 5 From simulation to model fitting 5.1 Learning Goals 5.2 The Challenge of Model Fitting 5.3 Simulating data 5.4 Building our basic model in Stan 5.5 Parameter recovery 5.6 The memory model: conditioning theta 5.7 Memory agent with internal parameter 5.8 Relationship to Rescorla-Wagner 5.9 Bayesian memory agent 5.10 Conclusion: From Simple Models to Complex Cognitive Processes", " Chapter 5 From simulation to model fitting This chapter introduces essential techniques for moving from theoretical models to empirical validation. Building on our implementation of decision-making agents, we now tackle the challenge of determining whether these models accurately describe observed behavior. 5.1 Learning Goals After completing this chapter, you will be able to: Design and implement Bayesian parameter estimation for cognitive models using Stan Create and interpret prior and posterior predictive checks to validate model behavior Evaluate model quality through systematic parameter recovery studies 5.2 The Challenge of Model Fitting Understanding human behavior requires more than just implementing plausible models - we must determine whether these models actually capture meaningful empirical patterns. Consider our biased agent model that tends to favor one choice over another. While we can specify different levels of bias in our simulations, real-world application requires determining what bias values best explain observed behavior, and for instance whether a pharmacological manipulation can affect the bias. Bayesian inference provides a powerful framework for this challenge. It allows us to: Express our prior beliefs about reasonable parameter values Update these beliefs based on observed data Quantify uncertainty in our parameter estimates Generate predictions that account for parameter uncertainty 5.3 Simulating data As usual we start with simulated data, where we know the underlying mechanisms and parameter values. Simulated data are rarely enough (empirical data often offer unexpected challenges), but they are a great starting point to stress test your model: does the model reconstruct the right parameter values? Does it reproduce the overall patterns in the data? Here we build a new simulation of random agents with bias and noise. The code and visualization is really nothing different from last chapter. # Set this to TRUE when you want to regenerate all simulation results # Otherwise, existing results will be loaded regenerate_simulations &lt;- TRUE pacman::p_load(tidyverse, here, posterior, cmdstanr, brms, tidybayes) trials &lt;- 120 RandomAgentNoise_f &lt;- function(rate, noise) { choice &lt;- rbinom(1, 1, rate) # generating noiseless choices if (rbinom(1, 1, noise) == 1) { choice = rbinom(1, 1, 0.5) # introducing noise } return(choice) } # Check if the simulation data file exists already sim_data_file &lt;- &quot;simdata/W3_randomnoise.csv&quot; if (regenerate_simulations || !file.exists(sim_data_file)) { # Generate new simulation data d &lt;- NULL for (noise in seq(0, 0.5, 0.1)) { # looping through noise levels for (rate in seq(0, 1, 0.1)) { # looping through rate levels randomChoice &lt;- rep(NA, trials) for (t in seq(trials)) { # looping through trials (to make it homologous to more reactive models) randomChoice[t] &lt;- RandomAgentNoise_f(rate, noise) } temp &lt;- tibble(trial = seq(trials), choice = randomChoice, rate, noise) temp$cumulativerate &lt;- cumsum(temp$choice) / seq_along(temp$choice) if (exists(&quot;d&quot;)) { d &lt;- rbind(d, temp) } else{ d &lt;- temp } } } # Save the simulation data write_csv(d, sim_data_file) cat(&quot;Generated new simulation data and saved to&quot;, sim_data_file, &quot;\\n&quot;) } else { # Load existing simulation data d &lt;- read_csv(sim_data_file) cat(&quot;Loaded existing simulation data from&quot;, sim_data_file, &quot;\\n&quot;) } ## Generated new simulation data and saved to simdata/W3_randomnoise.csv # Now we visualize it p1 &lt;- ggplot(d, aes(trial, cumulativerate, group = rate, color = rate)) + geom_line() + geom_hline(yintercept = 0.5, linetype = &quot;dashed&quot;) + ylim(0,1) + facet_wrap(.~noise) + theme_classic() p1 5.4 Building our basic model in Stan N.B. Refer to the video and slides for the step by step build-up of the Stan code. Now we subset to a simple case, no noise and rate of 0.8, to focus on the Stan model. We make it into the right format for Stan, build the Stan model, and fit it. 5.4.1 Data Here we define the data and format it for Stan. Stan likes data as a list. Why a list? Well, dataframes (now tibbles) are amazing. But they have a big drawback: they require each variable to have the same length. Lists do not have that limitation, they are more flexible. So, lists. We’ll have to learn how to live with them. d1 &lt;- d %&gt;% subset(noise == 0 &amp; rate == 0.8) ## Create the data. N.B. note the two variables have different lengths: 1 for n, n for h. data_biased &lt;- list( n = 120, # n of trials h = d1$choice # sequence of choices (h stands for hand) ) 5.4.2 Model We write the stan code within the R code (so I can show it to you more easily), then we save it as a stan file, which can be loaded at a later stage in order to compile it. [Missing: more info on compiling etc.] Remember that the minimal Stan model requires 3 chunks, one specifying the data it will need as input; one specifying the parameters to be estimated; one specifying the model within which the parameters appear, and the priors for those parameters. stan_model &lt;- &quot; // This model infers a random bias from a sequences of 1s and 0s (right and left hand choices) // The input (data) for the model. n of trials and the sequence of choices (right as 1, left as 0) data { int&lt;lower=1&gt; n; // n of trials array[n] int h; // sequence of choices (right as 1, left as 0) as long as n } // The parameters that the model needs to estimate (theta) parameters { real&lt;lower=0, upper=1&gt; theta; // rate or theta is a probability and therefore bound between 0 and 1 } // The model to be estimated (a bernoulli, parameter theta, prior on the theta) model { // The prior for theta is a beta distribution alpha of 1, beta of 1, equivalent to a uniform between 0 and 1 target += beta_lpdf(theta | 1, 1); // N.B. you could also define the parameters of the priors as variables to be found in the data // target += beta_lpdf(theta | beta_alpha, beta_beta); BUT remember to add beta_alpha and beta_beta to the data list // The model consists of a bernoulli distribution (binomial w 1 trial only) with a rate theta target += bernoulli_lpmf(h | theta); } &quot; write_stan_file( stan_model, dir = &quot;stan/&quot;, basename = &quot;W3_SimpleBernoulli.stan&quot;) 5.4.3 Compiling and fitting the model ## Specify where the model is file &lt;- file.path(&quot;stan/W3_SimpleBernoulli.stan&quot;) # File path for saved model model_file &lt;- &quot;simmodels/W3_SimpleBernoulli.rds&quot; # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(model_file)) { # Compile the model mod &lt;- cmdstan_model(file, # this specifies we can parallelize the gradient estimations on multiple cores cpp_options = list(stan_threads = TRUE), # this is a trick to make it faster stanc_options = list(&quot;O1&quot;)) # The following command calls Stan with specific options. samples_biased &lt;- mod$sample( data = data_biased, # the data :-) seed = 123, # a seed, so I always get the same results chains = 2, # how many chains should I fit (to check whether they give the same results) parallel_chains = 2, # how many of the chains can be run in parallel? threads_per_chain = 2, # distribute gradient estimations within chain across multiple cores iter_warmup = 1000, # warmup iterations through which hyperparameters (steps and step length) are adjusted iter_sampling = 2000, # total number of iterations refresh = 0, # how often to show that iterations have been run max_treedepth = 20, # how many steps in the future to check to avoid u-turns adapt_delta = 0.99, # how high a learning rate to adjust hyperparameters during warmup ) # Save the fitted model samples_biased$save_object(file = model_file) cat(&quot;Generated new model fit and saved to&quot;, model_file, &quot;\\n&quot;) } else { # Load existing results cat(&quot;Loading biased model samples...\\n&quot;) samples_biased &lt;- readRDS(model_file) cat(&quot;Available parameters:&quot;, paste(colnames(as_draws_df(samples_biased$draws())), collapse=&quot;, &quot;), &quot;\\n&quot;) cat(&quot;Loaded existing model fit from&quot;, model_file, &quot;\\n&quot;) } samples_biased$summary() # summarize the model 5.4.4 Assessing model quality Then we need to look more in the details at the quality of the estimation: * the markov chains * how the prior and the posterior estimates relate to each other (whether the prior is constraining the posterior estimate) # Check if samples_biased exists if (!exists(&quot;samples_biased&quot;)) { model_file &lt;- &quot;simmodels/W3_SimpleBernoulli.rds&quot; if (file.exists(model_file)) { cat(&quot;Loading biased model samples...\\n&quot;) samples_biased &lt;- readRDS(model_file) cat(&quot;Available parameters:&quot;, paste(colnames(as_draws_df(samples_biased$draws())), collapse=&quot;, &quot;), &quot;\\n&quot;) } else { cat(&quot;Model file not found. Set regenerate_simulations=TRUE to create it.\\n&quot;) # Provide dummy data or skip the remaining code knitr::knit_exit() } } # Extract posterior samples and include sampling of the prior: draws_df_biased &lt;- as_draws_df(samples_biased$draws()) # Explicitly extract parameters theta_param &lt;- draws_df_biased$theta cat(&quot;Successfully extracted theta parameter with&quot;, length(theta_param), &quot;values\\n&quot;) ## Successfully extracted theta parameter with 4000 values # Checking the model&#39;s chains ggplot(draws_df_biased, aes(.iteration, theta, group = .chain, color = .chain)) + geom_line() + theme_classic() # add a prior for theta (ugly, but we&#39;ll do better soon) draws_df_biased &lt;- draws_df_biased %&gt;% mutate( theta_prior = rbeta(nrow(draws_df_biased), 1, 1) ) # Now let&#39;s plot the density for theta (prior and posterior) ggplot(draws_df_biased) + geom_density(aes(theta), fill = &quot;blue&quot;, alpha = 0.3) + geom_density(aes(theta_prior), fill = &quot;red&quot;, alpha = 0.3) + geom_vline(xintercept = 0.8, linetype = &quot;dashed&quot;, color = &quot;black&quot;, linewidth = 1.5) + xlab(&quot;Rate&quot;) + ylab(&quot;Posterior Density&quot;) + theme_classic() As we can see from the posterior estimates and the prior posterior update check, our model is doing a decent job. It doesn’t exactly reconstruct the rate of 0.8, but 0.755 is pretty close and 0.8 is included within the credible interval. Now we build the same model, but using the log odds scale for the theta parameter, which will become useful later when we condition theta on variables and build multilevel models (as we can do what we want in a log odds space and it will always be bound between 0 and 1). stan_model &lt;- &quot; // This model infers a random bias from a sequences of 1s and 0s (right and left hand choices) // The input (data) for the model. n of trials and the sequence of choices (right as 1, left as 0) data { int&lt;lower=1&gt; n; // n of trials array[n] int h; // sequence of choices (right as 1, left as 0) as long as n } // The parameters that the model needs to estimate (theta) parameters { real theta; // note it is unbounded as we now work on log odds } // The model to be estimated (a bernoulli, parameter theta, prior on the theta) model { // The prior for theta on a log odds scale is a normal distribution with a mean of 0 and a sd of 1. // This covers most of the probability space between 0 and 1, after being converted to probability. target += normal_lpdf(theta | 0, 1); // as before the parameters of the prior could be fed as variables // target += normal_lpdf(theta | normal_mu, normal_sigma); // The model consists of a bernoulli distribution (binomial w 1 trial only) with a rate theta, // note we specify it uses a logit link (theta is in logodds) target += bernoulli_logit_lpmf(h | theta); } &quot; write_stan_file( stan_model, dir = &quot;stan/&quot;, basename = &quot;W3_SimpleBernoulli_logodds.stan&quot;) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W3_SimpleBernoulli_logodds.stan&quot; ## With the logit format ## Specify where the model is file &lt;- file.path(&quot;stan/W3_SimpleBernoulli_logodds.stan&quot;) # File path for saved model model_file &lt;- &quot;simmodels/W3_SimpleBernoulli_logodds.rds&quot; # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(model_file)) { # Compile the model mod &lt;- cmdstan_model(file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) # The following command calls Stan with specific options. samples_biased_logodds &lt;- mod$sample( data = data_biased, seed = 123, chains = 2, parallel_chains = 2, threads_per_chain = 2, iter_warmup = 1000, iter_sampling = 2000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99, ) # Save the fitted model samples_biased_logodds$save_object(file = model_file) cat(&quot;Generated new model fit and saved to&quot;, model_file, &quot;\\n&quot;) } else { # Load existing results cat(&quot;Loading biased model (log-odds) samples...\\n&quot;) samples_biased_logodds &lt;- readRDS(model_file) cat(&quot;Available parameters:&quot;, paste(colnames(as_draws_df(samples_biased_logodds$draws())), collapse = &quot;, &quot;), &quot;\\n&quot;) cat(&quot;Loaded existing model fit from&quot;, model_file, &quot;\\n&quot;) } ## Running MCMC with 2 parallel chains, with 2 thread(s) per chain... ## ## Chain 1 finished in 0.0 seconds. ## Chain 2 finished in 0.1 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.1 seconds. ## Total execution time: 0.5 seconds. ## ## Generated new model fit and saved to simmodels/W3_SimpleBernoulli_logodds.rds 5.4.5 Summarizing the results if (!exists(&quot;samples_biased_logodds&quot;)) { cat(&quot;Loading biased model (log-odds) samples...\\n&quot;) samples_biased_logodds &lt;- readRDS(&quot;simmodels/W3_SimpleBernoulli_logodds.rds&quot;) cat(&quot;Available parameters:&quot;, paste(colnames(as_draws_df(samples_biased_logodds$draws())), collapse = &quot;, &quot;), &quot;\\n&quot;) } # Extract posterior samples and include sampling of the prior: draws_df_biased_logodds &lt;- as_draws_df(samples_biased_logodds$draws()) # Explicitly extract theta parameter theta_param_logodds &lt;- draws_df_biased_logodds$theta cat(&quot;Successfully extracted theta parameter with&quot;, length(theta_param_logodds), &quot;values\\n&quot;) ## Successfully extracted theta parameter with 4000 values ggplot(draws_df_biased_logodds, aes(.iteration, theta, group = .chain, color = .chain)) + geom_line() + theme_classic() # add a prior for theta (ugly, but we&#39;ll do better soon) draws_df_biased_logodds &lt;- draws_df_biased_logodds %&gt;% mutate( theta_prior = rnorm(nrow(draws_df_biased_logodds), 0, 1) ) # Now let&#39;s plot the density for theta (prior and posterior) ggplot(draws_df_biased_logodds) + geom_density(aes(theta), fill = &quot;blue&quot;, alpha = 0.3) + geom_density(aes(theta_prior), fill = &quot;red&quot;, alpha = 0.3) + geom_vline(xintercept = 1.38, linetype = &quot;dashed&quot;, color = &quot;black&quot;, size = 1.5) + xlab(&quot;Rate&quot;) + ylab(&quot;Posterior Density&quot;) + theme_classic() # Summary samples_biased_logodds$summary() ## # A tibble: 2 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lp__ -55.2 -54.9 0.758 0.325 -56.7 -54.7 1.00 838. 1001. ## 2 theta 1.60 1.60 0.244 0.241 1.20 1.99 1.00 985. 925. We can see that the results are very similar. 5.5 Parameter recovery Now that we see that the model works in one case, we can run it through all possible rate and noise levels in the simulation. Here we’ll implement a better approach using parallelization, which is more efficient for complex models. To parallelize, we rely on furrr, a neat R package that distributes parallel operations across cores. This approach becomes crucial with more complex models. First we need to define the function that will define the operations to be run on each core separately, here we simulate the data according to a seed, a n of trials, a rate and a noise, and then we fit the model to them. Second, we need to create a tibble of the seeds, n of trials, rate and noise values that should be simulated. Third, we use future_pmap_dfr to run the function on each row of the tibble above separately on a different core. Note that I set the system to split across 4 parallel cores (to work on my computer without clogging it). Do change it according to the system you are using. Note that if you have 40 “jobs” (rows of the tibble, sets of parameter values to run), using e.g. 32 cores will not substantially speed things more than using 20. # File path for saved recovery results recovery_file &lt;- &quot;simdata/W3_recoverydf_parallel.csv&quot; # Check if we need to run the parameter recovery if (regenerate_simulations || !file.exists(recovery_file)) { # Load necessary packages for parallelization pacman::p_load(future, purrr, furrr) # Set up parallel processing (adjust workers based on your system) plan(multisession, workers = 4) # Define the function that will be run on each core separately sim_d_and_fit &lt;- function(seed, trials, rateLvl, noiseLvl) { # Generate random choices randomChoice &lt;- rep(NA, trials) for (t in seq(trials)) { randomChoice[t] &lt;- RandomAgentNoise_f(rateLvl, noiseLvl) } # Create data for Stan data &lt;- list( n = trials, h = randomChoice ) # Compile the model file &lt;- file.path(&quot;stan/W3_SimpleBernoulli_logodds.stan&quot;) mod &lt;- cmdstan_model(file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) # Fit the model samples_recovery &lt;- mod$sample( data = data, seed = 1000, chains = 1, parallel_chains = 1, threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 2000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99, ) # Extract results draws_df_recovery &lt;- as_draws_df(samples_recovery$draws()) # Check for parameter name cat(&quot;Parameters available:&quot;, paste(colnames(draws_df_recovery), collapse=&quot;, &quot;), &quot;\\n&quot;) # Extract theta parameter theta_param &lt;- draws_df_recovery$theta temp &lt;- tibble(biasEst = inv_logit_scaled(theta_param), biasTrue = rateLvl, noise = noiseLvl) return(temp) } # Create a tibble with all parameter combinations param_combinations &lt;- tibble( rateLvl = rep(seq(0, 1, 0.1), each = 6), noiseLvl = rep(seq(0, 0.5, 0.1), 11), seed = 1000, trials = 120 ) # Run the function on each parameter combination in parallel recovery_df &lt;- future_pmap_dfr( param_combinations, sim_d_and_fit, .options = furrr_options(seed = TRUE) ) # Save results for future use write_csv(recovery_df, recovery_file) cat(&quot;Generated new parameter recovery results and saved to&quot;, recovery_file, &quot;\\n&quot;) } else { # Load existing results recovery_df &lt;- read_csv(recovery_file) cat(&quot;Loaded existing parameter recovery results from&quot;, recovery_file, &quot;\\n&quot;) } # Visualize the parameter recovery results ggplot(recovery_df, aes(biasTrue, biasEst)) + geom_point(alpha = 0.1) + geom_smooth() + facet_wrap(.~noise) + theme_classic() There’s much to be said about the final plot, but for now let’s just say that it looks good. We can reconstruct in a nice ordered way true rate values. However, our ability to do so decreases with the increase in noise. So far no surprises. Wait, you say, shouldn’t we actually model the generative process, that is, include noise in the Stan model? Gold star, there! But let’s wait a bit before we get there, we’ll need mixture models. 5.6 The memory model: conditioning theta Now that we fitted the base model, we can move onto more complex models. For instance a memory model (including all previous trials). Here we rely on a generalized linear model kind of thinking: the theta is the expression of a linear model (bias + b1 * PreviousRate). To make the variable more intuitive we code previous rate - which is bound to a probability 0-1 space - into log-odds via a logit link/transformation. In this way a previous rate with more left than right choices will result in a negative value, thereby decreasing our propensity to choose right; and one with more right than left choices will result in a positive value, thereby increasing our propensity to choose right. # We subset to only include no noise and a specific rate d1 &lt;- d %&gt;% subset(noise == 0 &amp; rate == 0.8) %&gt;% rename(Other = choice) %&gt;% mutate(cumulativerate = lag(cumulativerate, 1)) d1$cumulativerate[1] &lt;- 0.5 # no prior info at first trial d1$cumulativerate[d1$cumulativerate == 0] &lt;- 0.01 d1$cumulativerate[d1$cumulativerate == 1] &lt;- 0.99 # Now we create the memory agent with a coefficient of 1 (in log odds) MemoryAgent_f &lt;- function(bias, beta, cumulativerate){ choice = rbinom(1, 1, inv_logit_scaled(bias + beta * logit_scaled(cumulativerate))) return(choice) } d1$Self[1] &lt;- RandomAgentNoise_f(0.5, 0) for (i in 2:trials) { d1$Self[i] &lt;- MemoryAgent_f(bias = 0, beta = 1, d1$cumulativerate[i]) } ## Create the data data_memory &lt;- list( n = 120, h = d1$Self, memory = d1$cumulativerate # this creates the new parameter: the rate of right hands so far in log-odds ) stan_model &lt;- &quot; // The input (data) for the model. n of trials and h for (right and left) hand data { int&lt;lower=1&gt; n; array[n] int h; vector[n] memory; // here we add the new variable between 0.01 and .99 } // The parameters accepted by the model. parameters { real bias; // how likely is the agent to pick right when the previous rate has no information (50-50)? real beta; // how strongly is previous rate impacting the decision? } // The model to be estimated. model { // priors target += normal_lpdf(bias | 0, .3); target += normal_lpdf(beta | 0, .5); // model target += bernoulli_logit_lpmf(h | bias + beta * logit(memory)); } &quot; write_stan_file( stan_model, dir = &quot;stan/&quot;, basename = &quot;W3_MemoryBernoulli.stan&quot;) ## Specify where the model is file &lt;- file.path(&quot;stan/W3_MemoryBernoulli.stan&quot;) # File path for saved model model_file_memory &lt;- &quot;simmodels/W3_MemoryBernoulli.rds&quot; # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(model_file_memory)) { # Compile the model mod_memory &lt;- cmdstan_model(file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) # The following command calls Stan with specific options. samples_memory &lt;- mod_memory$sample( data = data_memory, seed = 123, chains = 2, parallel_chains = 2, threads_per_chain = 2, iter_warmup = 1000, iter_sampling = 1000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99, ) # Save the fitted model samples_memory$save_object(file = model_file_memory) cat(&quot;Generated new model fit and saved to&quot;, model_file_memory, &quot;\\n&quot;) } else { # Load existing results cat(&quot;Loading memory model samples...\\n&quot;) samples_memory &lt;- readRDS(model_file_memory) cat(&quot;Available parameters:&quot;, paste(colnames(as_draws_df(samples_memory$draws())), collapse=&quot;, &quot;), &quot;\\n&quot;) cat(&quot;Loaded existing model fit from&quot;, model_file_memory, &quot;\\n&quot;) } 5.6.1 Summarizing the results # Check if samples_memory exists if (!exists(&quot;samples_memory&quot;)) { cat(&quot;Loading memory model samples...\\n&quot;) samples_memory &lt;- readRDS(&quot;simmodels/W3_MemoryBernoulli.rds&quot;) cat(&quot;Available parameters:&quot;, paste(colnames(as_draws_df(samples_memory$draws())), collapse = &quot;, &quot;), &quot;\\n&quot;) } # Extract posterior samples and include sampling of the prior: draws_df_memory &lt;- as_draws_df(samples_memory$draws()) # Explicitly extract parameters bias_param &lt;- draws_df_memory$bias beta_param &lt;- draws_df_memory$beta cat(&quot;Successfully extracted&quot;, length(bias_param), &quot;values for bias parameter\\n&quot;) ## Successfully extracted 2000 values for bias parameter cat(&quot;Successfully extracted&quot;, length(beta_param), &quot;values for beta parameter\\n&quot;) ## Successfully extracted 2000 values for beta parameter # Trace plot for bias ggplot(draws_df_memory, aes(.iteration, bias, group = .chain, color = .chain)) + geom_line() + labs(title = &quot;Trace plot for bias parameter&quot;) + theme_classic() # Trace plot for beta ggplot(draws_df_memory, aes(.iteration, beta, group = .chain, color = .chain)) + geom_line() + labs(title = &quot;Trace plot for beta parameter&quot;) + theme_classic() # add prior distributions draws_df_memory &lt;- draws_df_memory %&gt;% mutate( bias_prior = rnorm(nrow(draws_df_memory), 0, .3), beta_prior = rnorm(nrow(draws_df_memory), 0, .5) ) # Now let&#39;s plot the density for bias (prior and posterior) ggplot(draws_df_memory) + geom_density(aes(bias), fill = &quot;blue&quot;, alpha = 0.3) + geom_density(aes(bias_prior), fill = &quot;red&quot;, alpha = 0.3) + geom_vline(xintercept = 0, linetype = &quot;dashed&quot;, color = &quot;black&quot;, size = 1.5) + labs(title = &quot;Prior-Posterior Update for Bias Parameter&quot;, subtitle = &quot;Blue: posterior, Red: prior, Dashed: true value&quot;) + xlab(&quot;Bias&quot;) + ylab(&quot;Posterior Density&quot;) + theme_classic() # Now let&#39;s plot the density for beta (prior and posterior) ggplot(draws_df_memory) + geom_density(aes(beta), fill = &quot;blue&quot;, alpha = 0.3) + geom_density(aes(beta_prior), fill = &quot;red&quot;, alpha = 0.3) + geom_vline(xintercept = 1, linetype = &quot;dashed&quot;, color = &quot;black&quot;, size = 1.5) + labs(title = &quot;Prior-Posterior Update for Beta Parameter&quot;, subtitle = &quot;Blue: posterior, Red: prior, Dashed: true value&quot;) + xlab(&quot;Beta&quot;) + ylab(&quot;Posterior Density&quot;) + theme_classic() # Print summary samples_memory$summary() ## # A tibble: 3 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lp__ -69.8 -69.5 1.10 0.777 -72.1 -68.8 1.00 621. 673. ## 2 bias 0.184 0.184 0.261 0.258 -0.260 0.611 1.01 478. 654. ## 3 beta 0.690 0.678 0.258 0.265 0.278 1.13 1.01 511. 708. We can see that the model has now estimated both the bias and the role of previous memory. Bias should reflect the bias in the setup (0.5 which in log odds is 0), and the beta coefficient for memory (roughly 1). More on the quality checks of the models in the next chapter. 5.7 Memory agent with internal parameter So far we behaved like in GLM: we keep feeding to the model an external variable of memory, but what if we coded memory as an internal parameter? This opens up to further possibilities to model how long memory is kept and weighted by distance from the current moment, etc. ## Create the data data &lt;- list( n = 120, h = d1$Self, other = d1$Other ) stan_model &lt;- &quot; // Memory-based choice model with prior and posterior predictions data { int&lt;lower=1&gt; n; array[n] int h; array[n] int other; } parameters { real bias; real beta; } transformed parameters { vector[n] memory; for (trial in 1:n) { if (trial == 1) { memory[trial] = 0.5; } if (trial &lt; n) { memory[trial + 1] = memory[trial] + ((other[trial] - memory[trial]) / (trial + 1)); if (memory[trial + 1] == 0) { memory[trial + 1] = 0.01; } if (memory[trial + 1] == 1) { memory[trial + 1] = 0.99; } } } } model { // Priors target += normal_lpdf(bias | 0, .3); target += normal_lpdf(beta | 0, .5); // Likelihood for (trial in 1:n) { target += bernoulli_logit_lpmf(h[trial] | bias + beta * logit(memory[trial])); } } generated quantities { // Generate prior samples real bias_prior = normal_rng(0, .3); real beta_prior = normal_rng(0, .5); // Variables for predictions array[n] int prior_preds; array[n] int posterior_preds; vector[n] memory_prior; vector[n] log_lik; // Generate predictions at different memory levels array[3] real memory_levels = {0.2, 0.5, 0.8}; // Low, neutral, and high memory array[3] int prior_preds_memory; array[3] int posterior_preds_memory; // Generate predictions from prior for each memory level for (i in 1:3) { real logit_memory = logit(memory_levels[i]); prior_preds_memory[i] = bernoulli_logit_rng(bias_prior + beta_prior * logit_memory); posterior_preds_memory[i] = bernoulli_logit_rng(bias + beta * logit_memory); } // Generate predictions from prior memory_prior[1] = 0.5; for (trial in 1:n) { if (trial == 1) { prior_preds[trial] = bernoulli_logit_rng(bias_prior + beta_prior * logit(memory_prior[trial])); } else { memory_prior[trial] = memory_prior[trial-1] + ((other[trial-1] - memory_prior[trial-1]) / trial); if (memory_prior[trial] == 0) { memory_prior[trial] = 0.01; } if (memory_prior[trial] == 1) { memory_prior[trial] = 0.99; } prior_preds[trial] = bernoulli_logit_rng(bias_prior + beta_prior * logit(memory_prior[trial])); } } // Generate predictions from posterior for (trial in 1:n) { posterior_preds[trial] = bernoulli_logit_rng(bias + beta * logit(memory[trial])); log_lik[trial] = bernoulli_logit_lpmf(h[trial] | bias + beta * logit(memory[trial])); } } &quot; write_stan_file( stan_model, dir = &quot;stan/&quot;, basename = &quot;W3_InternalMemory.stan&quot;) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W3_InternalMemory.stan&quot; ## Specify where the model is file &lt;- file.path(&quot;stan/W3_InternalMemory.stan&quot;) # File path for saved model model_file &lt;- &quot;simmodels/W3_InternalMemory.rds&quot; # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(model_file)) { # Compile the model mod &lt;- cmdstan_model(file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) # The following command calls Stan with specific options. samples_memory_internal &lt;- mod$sample( data = data, seed = 123, chains = 1, parallel_chains = 2, threads_per_chain = 2, iter_warmup = 1000, iter_sampling = 1000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99, ) # Save the fitted model samples_memory_internal$save_object(file = model_file) cat(&quot;Generated new model fit and saved to&quot;, model_file, &quot;\\n&quot;) } else { # Load existing results samples_memory_internal &lt;- readRDS(model_file) cat(&quot;Loaded existing model fit from&quot;, model_file, &quot;\\n&quot;) } ## Running MCMC with 1 chain, with 2 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Generated new model fit and saved to simmodels/W3_InternalMemory.rds draws_df &lt;- as_draws_df(samples_memory_internal$draws()) # 1. Check chain convergence # Plot traces for main parameters mcmc_trace(draws_df, pars = c(&quot;bias&quot;, &quot;beta&quot;)) + theme_minimal() + ggtitle(&quot;Parameter Traces Across Chains&quot;) # Plot rank histograms to check mixing mcmc_rank_hist(draws_df, pars = c(&quot;bias&quot;, &quot;beta&quot;)) # 2. Prior-Posterior Update Check p1 &lt;- ggplot() + geom_density(data = draws_df, aes(bias, fill = &quot;Posterior&quot;), alpha = 0.5) + geom_density(data = draws_df, aes(bias_prior, fill = &quot;Prior&quot;), alpha = 0.5) + geom_vline(xintercept = 0, linetype = &quot;dashed&quot;) + scale_fill_manual(values = c(&quot;Prior&quot; = &quot;red&quot;, &quot;Posterior&quot; = &quot;blue&quot;)) + theme_minimal() + ggtitle(&quot;Prior-Posterior Update: Bias Parameter&quot;) p2 &lt;- ggplot() + geom_density(data = draws_df, aes(beta, fill = &quot;Posterior&quot;), alpha = 0.5) + geom_density(data = draws_df, aes(beta_prior, fill = &quot;Prior&quot;), alpha = 0.5) + geom_vline(xintercept = 1, linetype = &quot;dashed&quot;) + scale_fill_manual(values = c(&quot;Prior&quot; = &quot;red&quot;, &quot;Posterior&quot; = &quot;blue&quot;)) + theme_minimal() + ggtitle(&quot;Prior-Posterior Update: Beta Parameter&quot;) p3 &lt;- ggplot() + geom_point(data = draws_df, aes(bias, beta), alpha = 0.5) + theme_minimal() + ggtitle(&quot;Correlation&quot;) p1 + p2 + p3 # First let&#39;s properly extract and organize our posterior predictions posterior_predictions &lt;- draws_df %&gt;% dplyr::select(starts_with(&quot;posterior_preds[&quot;)) %&gt;% # Select all posterior prediction columns pivot_longer(everything(), names_to = &quot;trial&quot;, values_to = &quot;prediction&quot;) %&gt;% # Clean up the trial number from the Stan array notation mutate(trial = as.numeric(str_extract(trial, &quot;\\\\d+&quot;))) # Calculate summary statistics for posterior predictions posterior_summary &lt;- posterior_predictions %&gt;% group_by(trial) %&gt;% summarise( mean = mean(prediction), lower = quantile(prediction, 0.025), upper = quantile(prediction, 0.975) ) # Do the same for prior predictions prior_predictions &lt;- draws_df %&gt;% dplyr::select(starts_with(&quot;prior_preds[&quot;)) %&gt;% pivot_longer(everything(), names_to = &quot;trial&quot;, values_to = &quot;prediction&quot;) %&gt;% mutate(trial = as.numeric(str_extract(trial, &quot;\\\\d+&quot;))) prior_summary &lt;- prior_predictions %&gt;% group_by(trial) %&gt;% summarise( mean = mean(prediction), lower = quantile(prediction, 0.025), upper = quantile(prediction, 0.975) ) # Now let&#39;s create our visualization # First the prior predictive check p4 &lt;- ggplot() + # Add prior prediction interval geom_ribbon(data = prior_summary, aes(x = trial, ymin = lower, ymax = upper), alpha = 0.2, fill = &quot;red&quot;) + # Add mean prior prediction geom_line(data = prior_summary, aes(x = trial, y = mean), color = &quot;red&quot;) + # Add actual data points geom_point(data = tibble(trial = 1:length(data$h), choice = data$h), aes(x = trial, y = choice), alpha = 0.5) + labs(title = &quot;Prior Predictive Check&quot;, x = &quot;Trial&quot;, y = &quot;Choice (0/1)&quot;) + theme_minimal() # Then the posterior predictive check p5 &lt;- ggplot() + # Add posterior prediction interval geom_ribbon(data = posterior_summary, aes(x = trial, ymin = lower, ymax = upper), alpha = 0.2, fill = &quot;blue&quot;) + # Add mean posterior prediction geom_line(data = posterior_summary, aes(x = trial, y = mean), color = &quot;blue&quot;) + # Add actual data points geom_point(data = tibble(trial = 1:length(data$h), choice = data$h), aes(x = trial, y = choice), alpha = 0.5) + labs(title = &quot;Posterior Predictive Check&quot;, x = &quot;Trial&quot;, y = &quot;Choice (0/1)&quot;) + theme_minimal() # Display plots side by side p4 + p5 # First, let&#39;s calculate the total number of 1s predicted in each posterior sample posterior_totals &lt;- draws_df %&gt;% dplyr::select(starts_with(&quot;posterior_preds[&quot;)) %&gt;% # Sum across rows to get total 1s per sample mutate(total_ones = rowSums(.)) # Do the same for prior predictions prior_totals &lt;- draws_df %&gt;% dplyr::select(starts_with(&quot;prior_preds[&quot;)) %&gt;% mutate(total_ones = rowSums(.)) # Calculate actual number of 1s in the data actual_ones &lt;- sum(data$h) # Create visualization comparing distributions ggplot() + # Prior predictive distribution geom_histogram(data = prior_totals, aes(x = total_ones, fill = &quot;Prior&quot;), alpha = 0.3) + # Posterior predictive distribution geom_histogram(data = posterior_totals, aes(x = total_ones, fill = &quot;Posterior&quot;), alpha = 0.3) + # Vertical line for actual data geom_vline(xintercept = actual_ones, linetype = &quot;dashed&quot;, color = &quot;black&quot;, size = 1) + # Aesthetics scale_fill_manual(values = c(&quot;Prior&quot; = &quot;red&quot;, &quot;Posterior&quot; = &quot;blue&quot;), name = &quot;Distribution&quot;) + labs(title = &quot;Distribution of Predicted Successes (1s) out of 120 Trials&quot;, subtitle = &quot;Comparing Prior, Posterior and Actual Data&quot;, x = &quot;Number of 1s&quot;, y = &quot;Density&quot;) + theme_minimal() + # Add annotation for actual value annotate(&quot;text&quot;, x = actual_ones, y = 0, label = paste(&quot;Actual:&quot;, actual_ones), vjust = -0.5) # Let&#39;s also print summary statistics prior_summary &lt;- prior_totals %&gt;% summarise( mean = mean(total_ones), sd = sd(total_ones), q025 = quantile(total_ones, 0.025), q975 = quantile(total_ones, 0.975) ) posterior_summary &lt;- posterior_totals %&gt;% summarise( mean = mean(total_ones), sd = sd(total_ones), q025 = quantile(total_ones, 0.025), q975 = quantile(total_ones, 0.975) ) print(&quot;Prior predictive summary:&quot;) ## [1] &quot;Prior predictive summary:&quot; print(prior_summary) ## # A tibble: 1 × 4 ## mean sd q025 q975 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 60.3 27.0 12 108. print(&quot;Posterior predictive summary:&quot;) ## [1] &quot;Posterior predictive summary:&quot; print(posterior_summary) ## # A tibble: 1 × 4 ## mean sd q025 q975 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 108. 4.52 98 115 # First let&#39;s calculate predicted probabilities for each draw and memory level predicted_probs &lt;- draws_df %&gt;% mutate( # Calculate probability of choosing right for each memory level # using the logistic function on our parameter estimates prob_low = inv_logit_scaled(bias + beta * logit_scaled(0.2)), prob_mid = inv_logit_scaled(bias + beta * logit_scaled(0.5)), prob_high = inv_logit_scaled(bias + beta * logit_scaled(0.8)) ) %&gt;% # Reshape to long format for easier plotting pivot_longer( cols = starts_with(&quot;prob_&quot;), names_to = &quot;memory_level&quot;, values_to = &quot;probability&quot; ) %&gt;% mutate( memory_value = case_when( memory_level == &quot;prob_low&quot; ~ 0.2, memory_level == &quot;prob_mid&quot; ~ 0.5, memory_level == &quot;prob_high&quot; ~ 0.8 ) ) # Do the same for prior predictions prior_probs &lt;- draws_df %&gt;% mutate( prob_low = inv_logit_scaled(bias_prior + beta_prior * logit_scaled(0.2)), prob_mid = inv_logit_scaled(bias_prior + beta_prior * logit_scaled(0.5)), prob_high = inv_logit_scaled(bias_prior + beta_prior * logit_scaled(0.8)) ) %&gt;% pivot_longer( cols = starts_with(&quot;prob_&quot;), names_to = &quot;memory_level&quot;, values_to = &quot;probability&quot; ) %&gt;% mutate( memory_value = case_when( memory_level == &quot;prob_low&quot; ~ 0.2, memory_level == &quot;prob_mid&quot; ~ 0.5, memory_level == &quot;prob_high&quot; ~ 0.8 ) ) # Create visualization with density plots p1 &lt;- ggplot() + # Add prior distributions geom_density(data = prior_probs, aes(x = probability, fill = &quot;Prior&quot;), alpha = 0.3) + # Add posterior distributions geom_density(data = predicted_probs, aes(x = probability, fill = &quot;Posterior&quot;), alpha = 0.3) + # Separate by memory level facet_wrap(~memory_value, labeller = labeller(memory_value = c( &quot;0.2&quot; = &quot;Low Memory (20% Right)&quot;, &quot;0.5&quot; = &quot;Neutral Memory (50% Right)&quot;, &quot;0.8&quot; = &quot;High Memory (80% Right)&quot; ))) + # Aesthetics scale_fill_manual(values = c(&quot;Prior&quot; = &quot;red&quot;, &quot;Posterior&quot; = &quot;blue&quot;), name = &quot;Distribution&quot;) + labs(title = &quot;Distribution of Predicted Probabilities at Different Memory Levels&quot;, x = &quot;Probability of Choosing Right&quot;, y = &quot;Density&quot;) + theme_minimal() # Alternative visualization using violin plots p2 &lt;- ggplot() + # Add prior distributions geom_violin(data = prior_probs, aes(x = factor(memory_value), y = probability, fill = &quot;Prior&quot;), alpha = 0.3, position = position_dodge(width = 0.5)) + # Add posterior distributions geom_violin(data = predicted_probs, aes(x = factor(memory_value), y = probability, fill = &quot;Posterior&quot;), alpha = 0.3, position = position_dodge(width = 0.5)) + # Aesthetics scale_fill_manual(values = c(&quot;Prior&quot; = &quot;red&quot;, &quot;Posterior&quot; = &quot;blue&quot;), name = &quot;Distribution&quot;) + scale_x_discrete(labels = c(&quot;Low\\n(20% Right)&quot;, &quot;Neutral\\n(50% Right)&quot;, &quot;High\\n(80% Right)&quot;)) + labs(title = &quot;Distribution of Predicted Probabilities by Memory Level&quot;, x = &quot;Memory Level&quot;, y = &quot;Probability of Choosing Right&quot;) + theme_minimal() # Display both visualizations p1 / p2 # 4. Check for divergences # Extract divergent transitions n_div &lt;- sum(draws_df$.divergent) print(paste(&quot;Number of divergent transitions:&quot;, n_div)) ## [1] &quot;Number of divergent transitions: 0&quot; Now that we know how to model memory as an internal state, we can play with making the update discount the past, setting a parameter that indicates after how many trials memory is lost, etc. 5.7.1 Trying out a more complex memory model, with a rate of forgetting that exponentially discounts the past stan_model &lt;- &quot; // The input (data) for the model. n of trials and h for (right and left) hand data { int&lt;lower=1&gt; n; array[n] int h; array[n] int other; } // The parameters accepted by the model. parameters { real bias; // how likely is the agent to pick right when the previous rate has no information (50-50)? real beta; // how strongly is previous rate impacting the decision? real&lt;lower=0, upper=1&gt; forgetting; } // The model to be estimated. model { vector[n] memory; // Priors target += beta_lpdf(forgetting | 1, 1); target += normal_lpdf(bias | 0, .3); target += normal_lpdf(beta | 0, .5); // Model, looping to keep track of memory for (trial in 1:n) { if (trial == 1) { memory[trial] = 0.5; } target += bernoulli_logit_lpmf(h[trial] | bias + beta * logit(memory[trial])); if (trial &lt; n){ memory[trial + 1] = (1 - forgetting) * memory[trial] + forgetting * other[trial]; if (memory[trial + 1] == 0){memory[trial + 1] = 0.01;} if (memory[trial + 1] == 1){memory[trial + 1] = 0.99;} } } } &quot; write_stan_file( stan_model, dir = &quot;stan/&quot;, basename = &quot;W3_InternalMemory2.stan&quot;) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W3_InternalMemory2.stan&quot; ## Specify where the model is file &lt;- file.path(&quot;stan/W3_InternalMemory2.stan&quot;) # File path for saved model model_file &lt;- &quot;simmodels/W3_InternalMemory2.rds&quot; # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(model_file)) { # Compile the model mod &lt;- cmdstan_model(file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) # The following command calls Stan with specific options. samples_memory_forgetting &lt;- mod$sample( data = data, seed = 123, chains = 1, parallel_chains = 2, threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 1000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99, ) # Save the fitted model samples_memory_forgetting$save_object(file = model_file) cat(&quot;Generated new model fit and saved to&quot;, model_file, &quot;\\n&quot;) } else { # Load existing results samples_memory_forgetting &lt;- readRDS(model_file) cat(&quot;Loaded existing model fit from&quot;, model_file, &quot;\\n&quot;) } ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 0.7 seconds. ## Generated new model fit and saved to simmodels/W3_InternalMemory2.rds samples_memory_forgetting$summary() ## # A tibble: 4 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lp__ -45.0 -44.7 1.22 1.01 -47.4 -43.7 1.01 395. 496. ## 2 bias 0.447 0.442 0.267 0.266 0.00516 0.898 1.01 327. 297. ## 3 beta 0.888 0.881 0.229 0.231 0.536 1.29 1.01 399. 382. ## 4 forgetting 0.105 0.0982 0.0399 0.0343 0.0494 0.176 1.00 433. 495. The memory model we’ve implemented can be seen as part of a broader family of models that track and update beliefs based on incoming evidence. Let’s explore how it relates to some key frameworks. 5.7.2 Connection to Kalman Filters Our memory model updates beliefs about the probability of right-hand choices using a weighted average of past observations. This is conceptually similar to how a Kalman filter works, though simpler: Kalman filters maintain both an estimate and uncertainty about that estimate They optimally weight new evidence based on relative uncertainty Our model uses a fixed weighting scheme (1/trial or the forgetting parameter) The key difference is that Kalman filters dynamically adjust how much they learn from new evidence based on uncertainty, while our model uses a fixed learning scheme. 5.8 Relationship to Rescorla-Wagner The Rescorla-Wagner model of learning follows the form: V(t+1) = V(t) + α(λ - V(t)) where: V(t) is the current estimate α is the learning rate λ is the observed outcome (λ - V(t)) is the prediction error Our memory model with forgetting parameter follows a very similar structure: memory(t+1) = (1-forgetting) * memory(t) + forgetting * outcome(t) This can be rewritten as: memory(t+1) = memory(t) + forgetting * (outcome(t) - memory(t)) Making the parallel clear: our forgetting parameter acts as the learning rate α in Rescorla-Wagner. 5.8.1 Connection to Hierarchical Gaussian Filter (HGF) The HGF extends these ideas by: Tracking beliefs at multiple levels Allowing learning rates to vary over time Explicitly modeling environmental volatility Our model could be seen as the simplest case of an HGF where: We only track one level (probability of right-hand choice) Have a fixed learning rate (forgetting parameter) Don’t explicitly model environmental volatility 5.8.2 Implications for Model Development Understanding these relationships helps us think about how models relate to each other and to extend our model: We could add uncertainty estimates to get Kalman-like behavior We could make the forgetting parameter dynamic to capture changing learning rates We could add multiple levels to track both immediate probabilities and longer-term trends Each extension would make the model more flexible but also more complex to fit to data. The choice depends on our specific research questions and available data. 5.9 Bayesian memory agent We can also model the memory agent in a Bayesian framework. This allows us to model the agent as (optimally) estimating a possible distribution of rates from the other’s behavior and keep all the uncertainty. stan_model &lt;- &quot; data { int&lt;lower=1&gt; n; // number of trials array[n] int h; // agent&#39;s choices (0 or 1) array[n] int other; // other player&#39;s choices (0 or 1) } parameters { real&lt;lower=0&gt; alpha_prior; // Prior alpha parameter real&lt;lower=0&gt; beta_prior; // Prior beta parameter } transformed parameters { vector[n] alpha; // Alpha parameter at each trial vector[n] beta; // Beta parameter at each trial vector[n] rate; // Expected rate at each trial // Initialize with prior alpha[1] = alpha_prior; beta[1] = beta_prior; rate[1] = alpha[1] / (alpha[1] + beta[1]); // Sequential updating of Beta distribution for(t in 2:n) { // Update Beta parameters based on previous observation alpha[t] = alpha[t-1] + other[t-1]; beta[t] = beta[t-1] + (1 - other[t-1]); // Calculate expected rate rate[t] = alpha[t] / (alpha[t] + beta[t]); } } model { // Priors on hyperparameters target += gamma_lpdf(alpha_prior | 2, 1); target += gamma_lpdf(beta_prior | 2, 1); // Agent&#39;s choices follow current rate estimates for(t in 1:n) { target += bernoulli_lpmf(h[t] | rate[t]); } } generated quantities { array[n] int prior_preds; array[n] int posterior_preds; real initial_rate = alpha_prior / (alpha_prior + beta_prior); // Prior predictions use initial rate for(t in 1:n) { prior_preds[t] = bernoulli_rng(initial_rate); } // Posterior predictions use sequentially updated rates for(t in 1:n) { posterior_preds[t] = bernoulli_rng(rate[t]); } } &quot; write_stan_file( stan_model, dir = &quot;stan/&quot;, basename = &quot;W3_BayesianMemory.stan&quot;) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W3_BayesianMemory.stan&quot; ## Specify where the model is file &lt;- file.path(&quot;stan/W3_BayesianMemory.stan&quot;) # File path for saved model model_file &lt;- &quot;simmodels/W3_BayesianMemory.rds&quot; # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(model_file)) { # Compile the model mod &lt;- cmdstan_model(file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) # The following command calls Stan with specific options. samples_memory_bayes &lt;- mod$sample( data = data, seed = 123, chains = 1, parallel_chains = 2, threads_per_chain = 2, iter_warmup = 1000, iter_sampling = 1000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99, ) # Save the fitted model samples_memory_bayes$save_object(file = model_file) cat(&quot;Generated new model fit and saved to&quot;, model_file, &quot;\\n&quot;) } else { # Load existing results samples_memory_bayes &lt;- readRDS(model_file) cat(&quot;Loaded existing model fit from&quot;, model_file, &quot;\\n&quot;) } ## Running MCMC with 1 chain, with 2 thread(s) per chain... ## ## Chain 1 finished in 0.2 seconds. ## Generated new model fit and saved to simmodels/W3_BayesianMemory.rds samples_memory_bayes$summary() ## # A tibble: 604 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lp__ -39.0 -38.7 1.05 0.774 -41.1 -38.0 1.00 263. 361. ## 2 alpha_prior 1.89 1.53 1.44 1.27 0.288 4.69 1.01 335. 410. ## 3 beta_prior 0.841 0.741 0.511 0.446 0.202 1.77 1.00 370. 442. ## 4 alpha[1] 1.89 1.53 1.44 1.27 0.288 4.69 1.01 335. 410. ## 5 alpha[2] 2.89 2.53 1.44 1.27 1.29 5.69 1.01 335. 410. ## 6 alpha[3] 3.89 3.53 1.44 1.27 2.29 6.69 1.01 335. 410. ## 7 alpha[4] 4.89 4.53 1.44 1.27 3.29 7.69 1.01 335. 410. ## 8 alpha[5] 5.89 5.53 1.44 1.27 4.29 8.69 1.01 335. 410. ## 9 alpha[6] 6.89 6.53 1.44 1.27 5.29 9.69 1.01 335. 410. ## 10 alpha[7] 7.89 7.53 1.44 1.27 6.29 10.7 1.01 335. 410. ## # ℹ 594 more rows # Extract draws draws_df &lt;- as_draws_df(samples_memory_bayes$draws()) # First let&#39;s look at the priors ggplot(draws_df) + geom_density(aes(alpha_prior), fill = &quot;blue&quot;, alpha = 0.3) + geom_density(aes(beta_prior), fill = &quot;red&quot;, alpha = 0.3) + theme_classic() + labs(title = &quot;Prior Distributions&quot;, x = &quot;Parameter Value&quot;, y = &quot;Density&quot;) # Now let&#39;s look at how the rate evolves over trials # First melt the rate values across trials into long format rate_df &lt;- draws_df %&gt;% dplyr::select(starts_with(&quot;rate[&quot;)) %&gt;% pivot_longer(everything(), names_to = &quot;trial&quot;, values_to = &quot;rate&quot;, names_pattern = &quot;rate\\\\[(\\\\d+)\\\\]&quot;) %&gt;% mutate(trial = as.numeric(trial)) # Calculate summary statistics for each trial rate_summary &lt;- rate_df %&gt;% group_by(trial) %&gt;% summarise( mean_rate = mean(rate), lower = quantile(rate, 0.025), upper = quantile(rate, 0.975) ) plot_data &lt;- tibble(trial = seq(120), choices = data$other) # Plot the evolution of rate estimates ggplot(rate_summary, aes(x = trial)) + geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) + geom_line(aes(y = mean_rate), color = &quot;blue&quot;) + # Add true data points geom_line(data = plot_data, aes(x = trial, y = choices), color = &quot;orange&quot;, alpha = 0.5) + theme_classic() + labs(title = &quot;Evolution of Rate Estimates&quot;, x = &quot;Trial&quot;, y = &quot;Rate&quot;, subtitle = &quot;Blue line: posterior mean, Gray band: 95% CI&quot;) + ylim(0, 1) # Let&#39;s also look at the correlation between alpha and beta parameters ggplot(draws_df) + geom_point(aes(alpha_prior, beta_prior), alpha = 0.1) + theme_classic() + labs(title = &quot;Correlation between Alpha and Beta Parameters&quot;, x = &quot;Alpha&quot;, y = &quot;Beta&quot;) 5.10 Conclusion: From Simple Models to Complex Cognitive Processes Throughout this chapter, we’ve progressed from basic parameter estimation to increasingly sophisticated models of decision-making. We began with a simple biased agent model, demonstrating how Bayesian inference allows us to recover underlying parameters from observed behavior. We saw how we can transform parameters from one scale to another - here from probability-scale to log-odds parameterizations -, thus gaining flexibility that will prove valuable for more complex models. The transition to memory-based models illustrated how we can incorporate psychological theory into our statistical framework. We explored different approaches to modeling memory - from treating it as an external predictor to implementing it as an internal state variable that evolves over time. The final exploration of exponential forgetting demonstrated how we can capture more nuanced cognitive processes while maintaining mathematical tractability. This progression sets the stage for Chapter 12, where we’ll explore how these memory updating mechanisms relate to reinforcement learning models. The exponential discounting of past events we implemented here represents a simplified version of the learning mechanisms we’ll encounter in reinforcement learning. Several key principles emerged that will guide our future modeling work: The importance of systematic model validation through parameter recovery studies and prior-posterior checks. These techniques help ensure our models can meaningfully capture the processes we aim to study. The value of starting simple and gradually adding complexity. Each model we implemented built upon previous ones, allowing us to understand the impact of new components while maintaining a solid foundation. This principle will become particularly important when we tackle reinforcement learning models, where multiple parameters interact in complex ways to produce learning behavior. The relationship between mathematical convenience and psychological reality. The log-odds transformation, for instance, provides both computational benefits and psychological insights about how humans might represent probabilities. Similarly, the memory updating rules we explored here foreshadow the prediction error calculations central to reinforcement learning and relates very tightly to other popular models like the Kalman filter and the Hierarchical Gaussian Filter. In the next chapters, we will build upon these foundations to tackle even more sophisticated cognitive models. Chapter 5 will introduce multilevel modeling, allowing us to capture individual differences while maintaining population-level insights. This will set the stage for exploring how different individuals might employ different strategies or show varying levels of memory decay in their decision-making processes. These individual differences become again relevant in future models where parameters like learning rate, or bias for social information can vary substantially across individuals. "],["model-quality-assessment.html", "Chapter 6 Model Quality Assessment 6.1 Introduction 6.2 Generating and plotting additional variables 6.3 Assessing priors 6.4 Prior Predictive Checks 6.5 Posterior Predictive Checks 6.6 Prior sensitivity analysis 6.7 The memory model 6.8 Prior sensitivity check for the memory model 6.9 Conclusion", " Chapter 6 Model Quality Assessment 6.1 Introduction Building computational models is only the first step in understanding cognitive processes. We must rigorously evaluate whether our models actually capture meaningful patterns in behavior and provide reliable insights. This chapter introduces systematic approaches for assessing model quality, focusing on techniques that help us understand both the strengths and limitations of our cognitive models. This document covers: - generating and plotting priors (against posteriors) - generating and plotting predictive checks (prior and posterior ones) - prior sensitivity checks [I SHOULD RESTRUCTURE THE DOCUMENT SO THAT PRIOR PREDICTIVE CHECKS COME BEFORE PRIOR / POSTERIOR UPDATE CHECKS] 6.2 Generating and plotting additional variables As we try to understand our model, we might want to plot how the prior relates to the posterior, or - in other words, what has the model learned from looking at the data? We can do so by overlaying the prior and the posterior distributions, what is also called a “prior - posterior update check”. Stan does not automatically save the prior distribution, so we need to tell it to generate and save prior distributions in a convenient place so we can easily plot or use them at will from R. Luckily, Stan gives us a dedicated code chunk to do that: the generated quantities chunk. As before, we need to define the kind of variable we want to save, and then how to generate it. If we take the example of the random agent (with a bias), we have one parameter: theta. We can then generate theta according to the prior in generated quantities. While we are at this, we can also generate a nicer version of the posterior estimate for the theta parameter, now in probability scale (instead of log odds). However, prior and posterior estimates are not always the most immediate thing to understand. For instance, we might have trouble having a good grasp for how the uncertainty in the estimate will play out on 120 trials, or 6 trials, or however many trials we are planning for our experiment. Luckily, we can ask Stan to run predictions from either priors or posteriors, or both: given the priors how many trials will have “right hand” choice? and given the posterior estimates? As we use complex models, the relation between prior/posterior estimates and predictions becomes less and less intuitive. Simulating their implications for the outcomes - also called prior/posterior predictive checks - becomes a very useful tool to adjust our priors and their uncertainty so that they reflect what we know of the outcome scale; as well as to assess whether the model (and its posterior estimates) can appropriately describe the data we observe, or there’s some bias there. More discussion of this can be found at https://4ccoxau.github.io/PriorsWorkshop/. pacman::p_load(tidyverse, here, posterior, cmdstanr, brms, tidybayes) d &lt;- read_csv(&quot;simdata/W3_randomnoise.csv&quot;) stan_model &lt;- &quot; // This model infers a random bias from a sequences of 1s and 0s (right and left hand choices) // The input (data) for the model. n of trials and the sequence of choices (right as 1, left as 0) data { int&lt;lower=1&gt; n; // n of trials array[n] int h; // sequence of choices (right as 1, left as 0) as long as n } // The parameters that the model needs to estimate (theta) parameters { real theta; // note it is unbounded as we now work on log odds } // The model to be estimated (a bernoulli, parameter theta, prior on the theta) model { // The prior for theta on a log odds scale is a normal distribution with a mean of 0 and a sd of 1. // This covers most of the probability space between 0 and 1, after being converted to probability. target += normal_lpdf(theta | 0, 1); // The model consists of a bernoulli distribution (binomial w 1 trial only) with a rate theta, // note we specify it uses a logit link (theta is in logodds) target += bernoulli_logit_lpmf(h | theta); } generated quantities{ real&lt;lower=0, upper=1&gt; theta_prior; // theta prior parameter, on a prob scale (0-1) real&lt;lower=0, upper=1&gt; theta_posterior; // theta posterior parameter, on a prob scale (0-1) int&lt;lower=0, upper=n&gt; prior_preds; // distribution of right hand choices according to the prior int&lt;lower=0, upper=n&gt; posterior_preds; // distribution of right hand choices according to the posterior theta_prior = inv_logit(normal_rng(0,1)); // generating the prior on a log odds scale and converting theta_posterior = inv_logit(theta); // converting the posterior estimate from log odds to prob. prior_preds = binomial_rng(n, theta_prior); posterior_preds = binomial_rng(n, inv_logit(theta)); } &quot; write_stan_file( stan_model, dir = &quot;stan/&quot;, basename = &quot;W5_SimpleBernoulli_logodds.stan&quot;) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W5_SimpleBernoulli_logodds.stan&quot; ## With the logit format ## Specify where the model is file &lt;- file.path(&quot;stan/W5_SimpleBernoulli_logodds.stan&quot;) mod &lt;- cmdstan_model(file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) d1 &lt;- d %&gt;% subset(noise == 0 &amp; rate == 0.8) ## Create the data. N.B. note the two variables have different lengths: 1 for n, n for h. data &lt;- list( n = 120, # n of trials h = d1$choice # sequence of choices (h stands for hand) ) # The following command calls Stan with specific options. samples &lt;- mod$sample( data = data, seed = 123, chains = 2, parallel_chains = 2, threads_per_chain = 2, iter_warmup = 1000, iter_sampling = 2000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99, ) ## Running MCMC with 2 parallel chains, with 2 thread(s) per chain... ## ## Chain 1 finished in 0.1 seconds. ## Chain 2 finished in 0.1 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.1 seconds. ## Total execution time: 0.5 seconds. draws_df &lt;- as_draws_df(samples$draws()) 6.3 Assessing priors # Now let&#39;s plot the density for theta (prior and posterior) ggplot(draws_df) + geom_histogram(aes(theta_posterior), fill = &quot;blue&quot;, alpha = 0.3) + geom_histogram(aes(theta_prior), fill = &quot;red&quot;, alpha = 0.3) + geom_vline(xintercept = 0.8, linetype = &quot;dashed&quot;, color = &quot;black&quot;, size = 1.5) + xlab(&quot;Rate&quot;) + ylab(&quot;Estimate Densities&quot;) + theme_classic() 6.4 Prior Predictive Checks Prior predictive checks involve simulating data from our model using only the prior distributions, before seeing any actual data. This helps us understand what kinds of patterns our model assumes are possible before we begin fitting to real observations. These predictions should be assessed for: Plausible ranges of behavior Appropriate levels of uncertainty Preservation of known constraints Coverage of theoretically important patterns 6.5 Posterior Predictive Checks After fitting our models, posterior predictive checks help us determine whether the fitted model can reproduce key patterns in our observed data. We generate new data using parameters sampled from the posterior distribution and compare these simulations to our actual observations. For decision-making models, important patterns to check include: Overall choice proportions Sequential dependencies in choices Learning curves Response to feedback Individual differences in strategies ggplot(draws_df) + geom_histogram(aes(prior_preds), color = &quot;darkblue&quot;, fill = &quot;blue&quot;, alpha = 0.3) + xlab(&quot;Predicted heads out of 120 trials&quot;) + ylab(&quot;Posterior Density&quot;) + theme_classic() ggplot(draws_df) + geom_histogram(aes(posterior_preds), color = &quot;darkblue&quot;, fill = &quot;blue&quot;, alpha = 0.3, bins = 90) + geom_point(x = sum(data$h), y = 0, color = &quot;red&quot;, shape = 17, size = 5) + xlab(&quot;Predicted heads out of 120 trials&quot;) + ylab(&quot;Posterior Density&quot;) + theme_classic() ggplot(draws_df) + geom_histogram(aes(prior_preds), color = &quot;lightblue&quot;, fill = &quot;blue&quot;, alpha = 0.3, bins = 90) + geom_histogram(aes(posterior_preds), color = &quot;darkblue&quot;, fill = &quot;blue&quot;, alpha = 0.3, bins = 90) + geom_point(x = sum(data$h), y = 0, color = &quot;red&quot;, shape = 17, size = 5) + xlab(&quot;Predicted heads out of 120 trials&quot;) + ylab(&quot;Posterior Density&quot;) + theme_classic() 6.6 Prior sensitivity analysis ## Now we adding different priors for theta prior_mean &lt;- seq(-3, 3, .5) prior_sd &lt;- seq(0.1, 1, 0.1) priors &lt;- expand.grid(prior_mean, prior_sd) priors &lt;- tibble(prior_mean = priors$Var1, prior_sd = priors$Var2) stan_model &lt;- &quot; // The input (data) for the model data { int&lt;lower=1&gt; n; array[n] int h; real prior_mean; real&lt;lower=0&gt; prior_sd; } // The parameters accepted by the model. parameters { real theta; } // The model to be estimated. model { // Prior target += normal_lpdf(theta | prior_mean, prior_sd); // Model target += bernoulli_logit_lpmf(h | theta); } generated quantities{ real&lt;lower=0, upper=1&gt; theta_prior; real&lt;lower=0, upper=1&gt; theta_posterior; int&lt;lower=0, upper=n&gt; prior_preds; int&lt;lower=0, upper=n&gt; posterior_preds; theta_prior = inv_logit(normal_rng(0,1)); theta_posterior = inv_logit(theta); prior_preds = binomial_rng(n, theta_prior); posterior_preds = binomial_rng(n, inv_logit(theta)); } &quot; write_stan_file( stan_model, dir = &quot;stan/&quot;, basename = &quot;W4_PriorBernoulli.stan&quot;) file &lt;- file.path(&quot;stan/W4_PriorBernoulli.stan&quot;) mod &lt;- cmdstan_model(file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) dd &lt;- d %&gt;% subset(noise == 0.1 &amp; rate == 0.8) pacman::p_load(future, purrr, furrr) plan(multisession, workers = 4) sim_d_and_fit &lt;- function(prior_mean, prior_sd) { data &lt;- list( n = nrow(dd), h = dd$choice, prior_mean = prior_mean, prior_sd = prior_sd ) samples &lt;- mod$sample( data = data, seed = 1000, chains = 1, parallel_chains = 1, threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 2000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99, ) draws_df &lt;- as_draws_df(samples$draws()) temp &lt;- tibble(theta_prior = draws_df$theta_prior, theta_posterior = draws_df$theta_posterior, prior_preds = draws_df$prior_preds, posterior_preds = draws_df$posterior_preds, prior_mean = prior_mean, prior_sd = prior_sd) return(temp) } # Commenting this out to ensure faster compiling time for the book. Uncomment to run the code recovery_df &lt;- future_pmap_dfr(priors, sim_d_and_fit, .options = furrr_options(seed = TRUE)) write_csv(recovery_df, &quot;simdata/W5_priorSensitivityRecovery.csv&quot;) Now we load the data and plot it recovery_df &lt;- read_csv(&quot;simdata/W5_priorSensitivityRecovery.csv&quot;) ggplot(recovery_df, aes(prior_mean, theta_posterior)) + geom_point(alpha = 0.1) + geom_hline(yintercept = 0.8, color = &quot;red&quot;) + geom_smooth() + facet_wrap(.~prior_sd) + theme_classic() 6.7 The memory model We can do the same for the memory model: generate prior distributions to overlay to the posteriors (prior-posterior update checks), generate predicted outcomes based on the priors (prior predictive checks) and on the posteriors (posterior predictive checks). N.B. prior and posterior predictions now depend on the value on memory. I identified 3 meaningful values for the memory value (e.g. 0.5, 0.7, 0.9) and used those to generate 3 prior and posterior predictive checks. # We subset to only include no noise and a specific rate d1 &lt;- d %&gt;% subset(noise == 0 &amp; rate == 0.8) %&gt;% rename(Other = choice) %&gt;% mutate(cumulativerate = lag(cumulativerate, 1)) d1$cumulativerate[1] &lt;- 0.5 # no prior info at first trial d1$cumulativerate[d1$cumulativerate == 0] &lt;- 0.01 d1$cumulativerate[d1$cumulativerate == 1] &lt;- 0.99 # Now we create the memory agent with a coefficient of 0.9 bias = 0 beta = 0.9 MemoryAgent_f &lt;- function(bias, beta, cumulativerate){ choice = rbinom(1, 1, inv_logit_scaled(bias + beta * logit_scaled(cumulativerate))) return(choice) } d1$Self[1] &lt;- RandomAgentNoise_f(0.5, 0) for (i in 1:trials) { d1$Self[i] &lt;- MemoryAgent_f(bias, beta, d1$cumulativerate[i]) } ## Create the data. data &lt;- list( n = 120, h = d1$Self, other = d1$Other ) stan_model &lt;- &quot; // The input (data) for the model. n of trials and h for (right and left) hand data { int&lt;lower=1&gt; n; array[n] int h; array[n] int other; } // The parameters accepted by the model. parameters { real bias; // how likely is the agent to pick right when the previous rate has no information (50-50)? real beta; // how strongly is previous rate impacting the decision? } transformed parameters{ vector[n] memory; for (trial in 1:n){ if (trial == 1) { memory[trial] = 0.5; } if (trial &lt; n){ memory[trial + 1] = memory[trial] + ((other[trial] - memory[trial]) / trial); if (memory[trial + 1] == 0){memory[trial + 1] = 0.01;} if (memory[trial + 1] == 1){memory[trial + 1] = 0.99;} } } } // The model to be estimated. model { // Priors target += normal_lpdf(bias | 0, .3); target += normal_lpdf(beta | 0, .5); // Model, looping to keep track of memory for (trial in 1:n) { target += bernoulli_logit_lpmf(h[trial] | bias + beta * logit(memory[trial])); } } generated quantities{ real bias_prior; real beta_prior; int&lt;lower=0, upper=n&gt; prior_preds5; int&lt;lower=0, upper=n&gt; post_preds5; int&lt;lower=0, upper=n&gt; prior_preds7; int&lt;lower=0, upper=n&gt; post_preds7; int&lt;lower=0, upper=n&gt; prior_preds9; int&lt;lower=0, upper=n&gt; post_preds9; bias_prior = normal_rng(0, 0.3); beta_prior = normal_rng(0, 0.5); prior_preds5 = binomial_rng(n, inv_logit(bias_prior + beta_prior * logit(0.5))); prior_preds7 = binomial_rng(n, inv_logit(bias_prior + beta_prior * logit(0.7))); prior_preds9 = binomial_rng(n, inv_logit(bias_prior + beta_prior * logit(0.9))); post_preds5 = binomial_rng(n, inv_logit(bias + beta * logit(0.5))); post_preds7 = binomial_rng(n, inv_logit(bias + beta * logit(0.7))); post_preds9 = binomial_rng(n, inv_logit(bias + beta * logit(0.9))); } &quot; write_stan_file( stan_model, dir = &quot;stan/&quot;, basename = &quot;W5_MemoryBernoulli.stan&quot;) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W5_MemoryBernoulli.stan&quot; ## Specify where the model is file &lt;- file.path(&quot;stan/W5_MemoryBernoulli.stan&quot;) mod &lt;- cmdstan_model(file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) # The following command calls Stan with specific options. samples &lt;- mod$sample( data = data, seed = 123, chains = 1, parallel_chains = 2, threads_per_chain = 2, iter_warmup = 1000, iter_sampling = 1000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99, ) ## Running MCMC with 1 chain, with 2 thread(s) per chain... ## ## Chain 1 finished in 0.5 seconds. samples$summary() ## # A tibble: 131 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lp__ -35.8 -35.4 1.05 0.737 -38.0 -34.8 1.00 312. 487. ## 2 bias 0.0484 0.0551 0.300 0.295 -0.463 0.527 0.999 360. 424. ## 3 beta 1.01 1.00 0.201 0.203 0.691 1.36 1.00 313. 416. ## 4 memory[1] 0.5 0.5 0 0 0.5 0.5 NA NA NA ## 5 memory[2] 0.99 0.99 0 0 0.99 0.99 NA NA NA ## 6 memory[3] 0.995 0.995 0 0 0.995 0.995 NA NA NA ## 7 memory[4] 0.997 0.997 0 0 0.997 0.997 NA NA NA ## 8 memory[5] 0.998 0.998 0 0 0.998 0.998 NA NA NA ## 9 memory[6] 0.998 0.998 0 0 0.998 0.998 NA NA NA ## 10 memory[7] 0.998 0.998 0 0 0.998 0.998 NA NA NA ## # ℹ 121 more rows # Extract posterior samples and include sampling of the prior: draws_df &lt;- as_draws_df(samples$draws()) # Now let&#39;s plot the density for bias (prior and posterior) ggplot(draws_df) + geom_density(aes(bias), fill = &quot;blue&quot;, alpha = 0.3) + geom_density(aes(bias_prior), fill = &quot;red&quot;, alpha = 0.3) + geom_vline(xintercept = 0, size = 2) + xlab(&quot;Bias&quot;) + ylab(&quot;Posterior Density&quot;) + theme_classic() ggplot(draws_df) + geom_density(aes(beta), fill = &quot;blue&quot;, alpha = 0.3) + geom_density(aes(beta_prior), fill = &quot;red&quot;, alpha = 0.3) + geom_vline(xintercept = 0.9, size = 2) + xlab(&quot;MemoryBeta&quot;) + ylab(&quot;Posterior Density&quot;) + theme_classic() ggplot(draws_df) + geom_histogram(aes(`prior_preds5`), color = &quot;yellow&quot;, fill = &quot;lightyellow&quot;, alpha = 0.2) + geom_histogram(aes(`prior_preds7`), color = &quot;green&quot;, fill = &quot;lightgreen&quot;, alpha = 0.2) + geom_histogram(aes(`prior_preds9`), color = &quot;blue&quot;, fill = &quot;lightblue&quot;, alpha = 0.2) + xlab(&quot;Predicted heads out of 120 trials&quot;) + ylab(&quot;Posterior Density&quot;) + theme_classic() ggplot(draws_df) + geom_histogram(aes(`post_preds5`), color = &quot;yellow&quot;, fill = &quot;lightyellow&quot;, alpha = 0.3, bins = 90) + geom_histogram(aes(`post_preds7`), color = &quot;green&quot;, fill = &quot;lightgreen&quot;, alpha = 0.3, bins = 90) + geom_histogram(aes(`post_preds9`), color = &quot;blue&quot;, fill = &quot;lightblue&quot;, alpha = 0.3, bins = 90) + #geom_point(x = sum(data$h), y = 0, color = &quot;red&quot;, shape = 17, size = 5) + xlab(&quot;Predicted heads out of 120 trials&quot;) + ylab(&quot;Posterior Density&quot;) + theme_classic() ggplot(draws_df) + geom_histogram(aes(`prior_preds5`), color = &quot;lightblue&quot;, fill = &quot;blue&quot;, alpha = 0.3, bins = 90) + geom_histogram(aes(`post_preds5`), color = &quot;darkblue&quot;, fill = &quot;blue&quot;, alpha = 0.3, bins = 90) + xlab(&quot;Predicted heads out of 120 trials&quot;) + ylab(&quot;Posterior Density&quot;) + theme_classic() 6.8 Prior sensitivity check for the memory model ## Now we adding different priors for theta prior_mean_bias &lt;- 0 prior_sd_bias &lt;- seq(0.1, 0.5, 0.1) prior_mean_beta &lt;- 0 prior_sd_beta &lt;- seq(0.1, 0.5, 0.1) priors &lt;- tibble(expand.grid(tibble(prior_mean_bias, prior_sd_bias, prior_mean_beta, prior_sd_beta))) stan_model &lt;- &quot; // The input (data) for the model data { int&lt;lower=1&gt; n; array[n] int h; array[n] int other; real prior_mean_bias; real&lt;lower=0&gt; prior_sd_bias; real prior_mean_beta; real&lt;lower=0&gt; prior_sd_beta; } // The parameters accepted by the model. parameters { real bias; // how likely is the agent to pick right when the previous rate has no information (50-50)? real beta; // how strongly is previous rate impacting the decision? } transformed parameters{ vector[n] memory; for (trial in 1:n){ if (trial == 1) { memory[trial] = 0.5; } if (trial &lt; n){ memory[trial + 1] = memory[trial] + ((other[trial] - memory[trial]) / trial); if (memory[trial + 1] == 0){memory[trial + 1] = 0.01;} if (memory[trial + 1] == 1){memory[trial + 1] = 0.99;} } } } // The model to be estimated. model { // The priors target += normal_lpdf(bias | prior_mean_bias, prior_sd_bias); target += normal_lpdf(beta | prior_mean_beta, prior_sd_beta); // The model target += bernoulli_logit_lpmf(h | bias + beta * logit(memory)); } generated quantities{ real bias_prior; real beta_prior; int&lt;lower=0, upper=n&gt; prior_preds5; int&lt;lower=0, upper=n&gt; post_preds5; int&lt;lower=0, upper=n&gt; prior_preds7; int&lt;lower=0, upper=n&gt; post_preds7; int&lt;lower=0, upper=n&gt; prior_preds9; int&lt;lower=0, upper=n&gt; post_preds9; bias_prior = normal_rng(prior_mean_bias, prior_sd_bias); beta_prior = normal_rng(prior_mean_beta, prior_sd_beta); prior_preds5 = binomial_rng(n, inv_logit(bias_prior + beta_prior * logit(0.5))); prior_preds7 = binomial_rng(n, inv_logit(bias_prior + beta_prior * logit(0.7))); prior_preds9 = binomial_rng(n, inv_logit(bias_prior + beta_prior * logit(0.9))); post_preds5 = binomial_rng(n, inv_logit(bias + beta * logit(0.5))); post_preds7 = binomial_rng(n, inv_logit(bias + beta * logit(0.7))); post_preds9 = binomial_rng(n, inv_logit(bias + beta * logit(0.9))); } &quot; write_stan_file( stan_model, dir = &quot;stan/&quot;, basename = &quot;W5_PriorMemory.stan&quot;) file &lt;- file.path(&quot;stan/W5_PriorMemory.stan&quot;) mod &lt;- cmdstan_model(file, cpp_options = list(stan_threads = TRUE)) dd &lt;- d %&gt;% subset(noise == 0.1 &amp; rate == 0.8) %&gt;% mutate(memory = lag(cumulativerate, 1)) dd$memory[1] &lt;- 0.5 pacman::p_load(future, purrr, furrr) plan(multisession, workers = 4) sim_d_and_fit &lt;- function(prior_mean_bias, prior_sd_bias, prior_mean_beta, prior_sd_beta) { data &lt;- list( n = nrow(d1), h = d1$Self, other = d1$Other, prior_mean_bias = prior_mean_bias, prior_sd_bias = prior_sd_bias, prior_mean_beta = prior_mean_beta, prior_sd_beta = prior_sd_beta ) samples &lt;- mod$sample( data = data, seed = 1000, chains = 1, parallel_chains = 1, threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 2000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99, ) draws_df &lt;- as_draws_df(samples$draws()) temp &lt;- tibble(bias_prior = draws_df$bias_prior, beta_prior = draws_df$beta_prior, bias_posterior = draws_df$bias, beta_posterior = draws_df$beta, prior_preds5 = draws_df$prior_preds5, prior_preds7 = draws_df$prior_preds7, prior_preds9 = draws_df$prior_preds9, posterior_preds5 = draws_df$post_preds5, posterior_preds7 = draws_df$post_preds7, posterior_preds9 = draws_df$post_preds9, prior_mean_bias = prior_mean_bias, prior_sd_bias = prior_sd_bias, prior_mean_beta = prior_mean_beta, prior_sd_beta = prior_sd_beta) return(temp) } # Commenting this out to ensure the book compiles faster. Uncomment to run the code. recovery_df &lt;- future_pmap_dfr(priors, sim_d_and_fit, .options = furrr_options(seed = TRUE)) write_csv(recovery_df, &quot;simdata/W5_MemoryPriorSensitivity.csv&quot;) ggplot(recovery_df, aes(prior_sd_beta, beta_posterior)) + geom_point(alpha = 0.1) + geom_hline(yintercept = 0.8, color = &quot;red&quot;) + geom_smooth(method = lm) + facet_wrap(.~prior_sd_bias) + theme_classic() 6.9 Conclusion Rigorous model assessment is essential for developing reliable insights into cognitive processes. The techniques covered in this chapter provide a systematic framework for validating our models and understanding their limitations. As we move forward to more complex models incorporating individual differences and learning mechanisms, these quality checks become increasingly important for ensuring our conclusions are well-supported by the evidence. In the next chapter, we’ll build on these foundations as we explore multilevel modeling approaches that can capture individual differences while maintaining population-level insights. "],["individual-differences-in-cognitive-strategies-multilevel-modeling.html", "Chapter 7 Individual Differences in Cognitive Strategies (Multilevel modeling) 7.1 Introduction 7.2 Learning Objectives 7.3 The Value of Multilevel Modeling 7.4 Graphical Model Visualization 7.5 Generating the agents 7.6 Plotting the agents 7.7 Coding the multilevel agents 7.8 Multilevel Random Agent Model 7.9 Let’s look at individuals 7.10 Prior sensitivity checks 7.11 Parameter recovery 7.12 Multilevel Memory Agent Model 7.13 Comparing Pooling Approaches 7.14 Comparing Pooling Approaches 7.15 Multilevel Modeling Cheatsheet 7.16 Conclusion: The Power and Challenges of Multilevel Modeling 7.17 Exercises (just some ideas)", " Chapter 7 Individual Differences in Cognitive Strategies (Multilevel modeling) 7.1 Introduction Our exploration of decision-making models has so far focused on single agents or averaged behavior across many agents. However, cognitive science consistentlyreveals that individuals differ systematically in how they approach tasks and process information. Some people may be more risk-averse, have better memory, learn faster, or employ entirely different strategies than others. This chapter introduces multilevel modeling as a powerful framework for capturing these individual differences while still identifying population-level patterns. Multilevel modeling (also called hierarchical modeling) provides a powerful framework for addressing this challenge. It allows us to simultaneously: Capture individual differences across participants Identify population-level patterns that generalize across individuals Improve estimates for individuals with limited data by leveraging information from the group Consider our matching pennies game: different players might vary in their strategic sophistication, memory capacity, or learning rates. Some may show strong biases toward particular choices while others adapt more flexibly to their opponents. Multilevel modeling allows us to capture these variations while still understanding what patterns hold across the population. Consider our matching pennies game: players might vary in their strategic sophistication, memory capacity, or learning rates. Some may show strong biases toward particular choices while others adapt more flexibly to their opponents. Multilevel modeling allows us to quantify these variations while still understanding what patterns hold across the population. 7.2 Learning Objectives After completing this chapter, you will be able to: Understand how multilevel modeling balances individual and group-level information Distinguish between complete pooling, no pooling, and partial pooling approaches to modeling group and individual variation Use different parameterizations to improve model efficiency Evaluate model quality through systematic parameter recovery studies Apply multilevel modeling techniques to cognitive science questions 7.3 The Value of Multilevel Modeling Traditional approaches to handling individual differences often force a choice between two extremes: 7.3.1 Complete Pooling Treats all participants as identical by averaging or combining their data Estimates a single set of parameters for the entire group Ignores individual differences entirely Example: Fitting a single model to all participants’ data combined 7.3.2 No Pooling Analyzes each participant completely separately Estimates separate parameters for each individual Fails to leverage information shared across participants and can lead to unstable estimates Example: Fitting separate models to each participant’s data Multilevel modeling offers a middle ground through partial pooling. Individual estimates are informed by both individual-level data and the overall population distribution. 7.3.3 Partial Pooling (Multilevel Modeling) Individual parameters are treated as coming from a group-level distribution Estimates are informed by both individual data and the population distribution Creates a balance between individual and group information Example: Hierarchical Bayesian model with parameters at both individual and group levels This partial pooling approach is particularly valuable when: Data per individual is limited (e.g., few trials per participant) Individual differences are meaningful but not completely independent We want to make predictions about new individuals from the same population 7.4 Graphical Model Visualization Before diving into code, let’s understand the structure of our multilevel models using graphical model notation. These diagrams help visualize how parameters relate to each other and to the observed data. 7.4.1 Biased Agent Model In this model, each agent has an individual bias parameter (θ) that determines their probability of choosing “right” (1) versus “left” (0). We are now conceptualizing our agents as being part of (sampled from) a more general population. This general population is characterized by a population level average parameter value (e.g. a general bias of 0.8 as we all like right hands more) and a certain variation in the population (e.g. a standard deviation of 0.1, as we are all a bit different from each other). Each biased agent’s bias is then sampled from that distribution. The key elements are: Population parameters: μ_θ (mean bias) and σ_θ (standard deviation of bias) Individual parameters: θ_i (bias for agent i) Observed data: y_it (choice for agent i on trial t) 7.4.2 Memory Agent Model This model is more complex, with each agent having two parameters: a baseline bias (α) and a memory sensitivity parameter (β). The key elements are: Population parameters: μ_α, σ_α, μ_β, σ_β (means and standard deviations) Individual parameters: α_i (bias for agent i), β_i (memory sensitivity for agent i) Transformed variables: m_it (memory state for agent i on trial t) Observed data: y_it (choice for agent i on trial t) These graphical models help us understand how information flows in our models and guide our implementation in Stan. Again, it’s practical to work in log odds. Why? Well, it’s not unconceivable that an agent would be 3 sd from the mean. So a biased agent could have a rate of 0.8 + 3 * 0.1, which gives a rate of 1.1. It’s kinda impossible to choose 110% of the time the right hand. We want an easy way to avoid these situations without too carefully tweaking our parameters, or including exception statements (e.g. if rate &gt; 1, then rate = 1). Conversion to log odds is again a wonderful way to work in a boundless space, and in the last step shrinking everything back to 0-1 probability space. N.B. we model all agents with some added noise as we assume it cannot be eliminated from empirical studies. pacman::p_load(tidyverse, here, posterior, cmdstanr, brms, tidybayes, patchwork, bayesplot, furrr, LaplacesDemon) # Population-level parameters agents &lt;- 100 # Number of agents to simulate trials &lt;- 120 # Number of trials per agent noise &lt;- 0 # Base noise level (probability of random choice) # Biased agent population parameters rateM &lt;- 1.386 # Population mean of bias (log-odds scale, ~0.8 in probability) rateSD &lt;- 0.65 # Population SD of bias (log-odds scale, ~0.1 in probability) # Memory agent population parameters biasM &lt;- 0 # Population mean of baseline bias (log-odds scale) biasSD &lt;- 0.1 # Population SD of baseline bias (log-odds scale) betaM &lt;- 1.5 # Population mean of memory sensitivity (log-odds scale) betaSD &lt;- 0.3 # Population SD of memory sensitivity (log-odds scale) # For reference, convert log-odds parameters to probability scale cat(&quot;Biased agent population mean (probability scale):&quot;, round(plogis(rateM), 2), &quot;\\n&quot;) ## Biased agent population mean (probability scale): 0.8 cat(&quot;Approximate biased agent population SD (probability scale):&quot;, round(0.1, 2), &quot;\\n&quot;) ## Approximate biased agent population SD (probability scale): 0.1 # Random agent function: makes choices based on bias parameter # Parameters: # rate: log-odds of choosing option 1 (&quot;right&quot;) # noise: probability of making a random choice # Returns: binary choice (0 or 1) RandomAgentNoise_f &lt;- function(rate, noise) { # Generate choice based on agent&#39;s bias parameter (on log-odds scale) choice &lt;- rbinom(1, 1, plogis(rate)) # With probability &#39;noise&#39;, override choice with random 50/50 selection if (rbinom(1, 1, noise) == 1) { choice = rbinom(1, 1, 0.5) } return(choice) } # Memory agent function: makes choices based on opponent&#39;s historical choices # Parameters: # bias: baseline tendency to choose option 1 (log-odds scale) # beta: sensitivity to memory (how strongly past choices affect decisions) # otherRate: opponent&#39;s observed rate of choosing option 1 (probability scale) # noise: probability of making a random choice # Returns: binary choice (0 or 1) MemoryAgentNoise_f &lt;- function(bias, beta, otherRate, noise) { # Calculate choice probability based on memory of opponent&#39;s choices # Higher beta means agent responds more strongly to opponent&#39;s pattern choice_prob &lt;- inv_logit_scaled(bias + beta * logit_scaled(otherRate)) # Generate choice choice &lt;- rbinom(1, 1, choice_prob) # With probability &#39;noise&#39;, override choice with random 50/50 selection if (rbinom(1, 1, noise) == 1) { choice = rbinom(1, 1, 0.5) } return(choice) } 7.5 Generating the agents [MISSING: PARALLELIZE] # Function to simulate one agent&#39;s behavior simulate_agent &lt;- function(agent_id, population_params, n_trials, noise_level) { # Sample agent-specific parameters from population distributions rate &lt;- rnorm(1, population_params$rateM, population_params$rateSD) bias &lt;- rnorm(1, population_params$biasM, population_params$biasSD) beta &lt;- rnorm(1, population_params$betaM, population_params$betaSD) # Initialize choice vectors randomChoice &lt;- rep(NA, n_trials) memoryChoice &lt;- rep(NA, n_trials) # Generate choices for each trial for (trial in 1:n_trials) { # Random agent makes choice based on bias parameter randomChoice[trial] &lt;- RandomAgentNoise_f(rate, noise_level) # Memory agent uses history of random agent&#39;s choices if (trial == 1) { # First trial: no history, so use 50/50 chance memoryChoice[trial] &lt;- rbinom(1, 1, 0.5) } else { # Later trials: use memory of previous random agent choices memoryChoice[trial] &lt;- MemoryAgentNoise_f( bias, beta, mean(randomChoice[1:trial], na.rm = TRUE), # Current memory noise_level ) } } # Create tibble with all agent data return(tibble( agent = agent_id, trial = seq(n_trials), randomChoice, trueRate = rate, # Store true parameter values for later validation memoryChoice, noise = noise_level, rateM = population_params$rateM, rateSD = population_params$rateSD, bias = bias, beta = beta, biasM = population_params$biasM, biasSD = population_params$biasSD, betaM = population_params$betaM, betaSD = population_params$betaSD )) } # Population parameters bundled in a list population_params &lt;- list( rateM = rateM, rateSD = rateSD, biasM = biasM, biasSD = biasSD, betaM = betaM, betaSD = betaSD ) # Simulate all agents (in a real application, consider using purrr::map functions) d &lt;- NULL for (agent_id in 1:agents) { agent_data &lt;- simulate_agent(agent_id, population_params, trials, noise) if (agent_id == 1) { d &lt;- agent_data } else { d &lt;- rbind(d, agent_data) } } # Calculate running statistics for each agent d &lt;- d %&gt;% group_by(agent) %&gt;% mutate( # Cumulative proportions of choices (shows learning/strategy over time) randomRate = cumsum(randomChoice) / seq_along(randomChoice), memoryRate = cumsum(memoryChoice) / seq_along(memoryChoice) ) # Display information about the simulated dataset cat(&quot;Generated data for&quot;, agents, &quot;agents with&quot;, trials, &quot;trials each\\n&quot;) ## Generated data for 100 agents with 120 trials each cat(&quot;Total observations:&quot;, nrow(d), &quot;\\n&quot;) ## Total observations: 12000 # Show a small sample of the data head(d, 5) ## # A tibble: 5 × 16 ## # Groups: agent [1] ## agent trial randomChoice trueRate memoryChoice noise rateM rateSD bias beta biasM biasSD ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 0.999 0 0 1.39 0.65 0.00321 1.34 0 0.1 ## 2 1 2 1 0.999 1 0 1.39 0.65 0.00321 1.34 0 0.1 ## 3 1 3 1 0.999 1 0 1.39 0.65 0.00321 1.34 0 0.1 ## 4 1 4 0 0.999 1 0 1.39 0.65 0.00321 1.34 0 0.1 ## 5 1 5 0 0.999 1 0 1.39 0.65 0.00321 1.34 0 0.1 ## # ℹ 4 more variables: betaM &lt;dbl&gt;, betaSD &lt;dbl&gt;, randomRate &lt;dbl&gt;, memoryRate &lt;dbl&gt; 7.6 Plotting the agents # Create plot themes that we&#39;ll reuse custom_theme &lt;- theme_classic() + theme( legend.position = &quot;top&quot;, plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5, size = 9) ) # Plot 1: Trajectories of randomRate for all agents p1 &lt;- ggplot(d, aes(x = trial, y = randomRate)) + geom_line(aes(group = agent, color = &quot;Individual Agents&quot;), alpha = 0.25) + # Individual agents geom_smooth(aes(color = &quot;Average&quot;), se = TRUE, size = 1.2) + # Group average geom_hline(yintercept = 0.5, linetype = &quot;dashed&quot;) + ylim(0, 1) + labs( title = &quot;Random Agent Behavior&quot;, subtitle = &quot;Cumulative proportion of &#39;right&#39; choices over trials&quot;, x = &quot;Trial Number&quot;, y = &quot;Proportion of Right Choices&quot;, color = NULL ) + scale_color_manual(values = c(&quot;Individual Agents&quot; = &quot;gray50&quot;, &quot;Average&quot; = &quot;blue&quot;)) + custom_theme # Plot 2: Trajectories of memoryRate for all agents p2 &lt;- ggplot(d, aes(x = trial, y = memoryRate)) + geom_line(alpha = 0.15, aes(color = &quot;Individual Agents&quot;, group = agent)) + # Individual agents geom_smooth(aes(color = &quot;Average&quot;), se = TRUE, size = 1.2) + # Group average geom_hline(yintercept = 0.5, linetype = &quot;dashed&quot;) + ylim(0, 1) + labs( title = &quot;Memory Agent Behavior&quot;, subtitle = &quot;Cumulative proportion of &#39;right&#39; choices over trials&quot;, x = &quot;Trial Number&quot;, y = &quot;Proportion of Right Choices&quot;, color = NULL ) + scale_color_manual(values = c(&quot;Individual Agents&quot; = &quot;gray50&quot;, &quot;Average&quot; = &quot;darkred&quot;)) + custom_theme # Display plots side by side p1 + p2 # Plot 3-5: Correlation between random and memory agent behavior at different timepoints # These show how well memory agents track random agents&#39; behavior over time p3 &lt;- d %&gt;% filter(trial == 10) %&gt;% ggplot(aes(randomRate, memoryRate)) + geom_point(alpha = 0.6) + geom_smooth(method = &quot;lm&quot;, color = &quot;blue&quot;) + geom_abline(intercept = 0, slope = 1, color = &quot;red&quot;, linetype = &quot;dashed&quot;) + coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) + labs( title = &quot;After 10 Trials&quot;, x = &quot;Random Agent Rate&quot;, y = &quot;Memory Agent Rate&quot; ) + custom_theme p4 &lt;- d %&gt;% filter(trial == 60) %&gt;% ggplot(aes(randomRate, memoryRate)) + geom_point(alpha = 0.6) + geom_smooth(method = &quot;lm&quot;, color = &quot;blue&quot;) + geom_abline(intercept = 0, slope = 1, color = &quot;red&quot;, linetype = &quot;dashed&quot;) + coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) + labs( title = &quot;After 60 Trials&quot;, x = &quot;Random Agent Rate&quot;, y = &quot;Memory Agent Rate&quot; ) + custom_theme p5 &lt;- d %&gt;% filter(trial == 120) %&gt;% ggplot(aes(randomRate, memoryRate)) + geom_point(alpha = 0.6) + geom_smooth(method = &quot;lm&quot;, color = &quot;blue&quot;) + geom_abline(intercept = 0, slope = 1, color = &quot;red&quot;, linetype = &quot;dashed&quot;) + coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) + labs( title = &quot;After 120 Trials&quot;, x = &quot;Random Agent Rate&quot;, y = &quot;Memory Agent Rate&quot; ) + custom_theme # Display plots in a single row p3 + p4 + p5 + plot_layout(guides = &quot;collect&quot;) + plot_annotation( title = &quot;Memory Agents&#39; Adaptation to Random Agents Over Time&quot;, subtitle = &quot;Red line: perfect tracking; Blue line: actual relationship&quot;, theme = theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) ) # Plot 6-8: Correlation between true rate parameter and observed rate # These show how well we can recover the underlying rate parameter p6 &lt;- d %&gt;% filter(trial == 10) %&gt;% ggplot(aes(inv_logit_scaled(trueRate), randomRate)) + geom_point(alpha = 0.6) + geom_smooth(method = &quot;lm&quot;, color = &quot;blue&quot;) + geom_abline(intercept = 0, slope = 1, color = &quot;red&quot;, linetype = &quot;dashed&quot;) + coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) + labs( title = &quot;After 10 Trials&quot;, x = &quot;True Rate Parameter&quot;, y = &quot;Observed Rate&quot; ) + custom_theme p7 &lt;- d %&gt;% filter(trial == 60) %&gt;% ggplot(aes(inv_logit_scaled(trueRate), randomRate)) + geom_point(alpha = 0.6) + geom_smooth(method = &quot;lm&quot;, color = &quot;blue&quot;) + geom_abline(intercept = 0, slope = 1, color = &quot;red&quot;, linetype = &quot;dashed&quot;) + coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) + labs( title = &quot;After 60 Trials&quot;, x = &quot;True Rate Parameter&quot;, y = &quot;Observed Rate&quot; ) + custom_theme p8 &lt;- d %&gt;% filter(trial == 120) %&gt;% ggplot(aes(inv_logit_scaled(trueRate), randomRate)) + geom_point(alpha = 0.6) + geom_smooth(method = &quot;lm&quot;, color = &quot;blue&quot;) + geom_abline(intercept = 0, slope = 1, color = &quot;red&quot;, linetype = &quot;dashed&quot;) + coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) + labs( title = &quot;After 120 Trials&quot;, x = &quot;True Rate Parameter&quot;, y = &quot;Observed Rate&quot; ) + custom_theme # Display plots in a single row p6 + p7 + p8 + plot_layout(guides = &quot;collect&quot;) + plot_annotation( title = &quot;Parameter Recovery: True vs. Observed Rates Over Time&quot;, subtitle = &quot;Red line: perfect recovery; Blue line: actual relationship&quot;, theme = theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) ) Note that as the n of trials increases, the memory model matches the random model better and better 7.7 Coding the multilevel agents 7.7.1 Multilevel random Remember that the simulated parameters are: * biasM &lt;- 0 * biasSD &lt;- 0.1 * betaM &lt;- 1.5 * betaSD &lt;- 0.3 Prep the data # For multilevel models, we need to reshape our data into matrices # where rows are trials and columns are agents # Function to create matrices from our long-format data create_stan_data &lt;- function(data, agent_type) { # Select relevant choice column based on agent type choice_col &lt;- ifelse(agent_type == &quot;random&quot;, &quot;randomChoice&quot;, &quot;memoryChoice&quot;) other_col &lt;- ifelse(agent_type == &quot;random&quot;, &quot;memoryChoice&quot;, &quot;randomChoice&quot;) # Create choice matrix choice_data &lt;- data %&gt;% dplyr::select(agent, trial, all_of(choice_col)) %&gt;% pivot_wider( names_from = agent, values_from = all_of(choice_col), names_prefix = &quot;agent_&quot; ) %&gt;% dplyr::select(-trial) %&gt;% as.matrix() # Create other-choice matrix (used for memory agent) other_data &lt;- data %&gt;% dplyr::select(agent, trial, all_of(other_col)) %&gt;% pivot_wider( names_from = agent, values_from = all_of(other_col), names_prefix = &quot;agent_&quot; ) %&gt;% dplyr::select(-trial) %&gt;% as.matrix() # Return data as a list ready for Stan return(list( trials = trials, agents = agents, h = choice_data, other = other_data )) } # Create data for random agent model data_random &lt;- create_stan_data(d, &quot;random&quot;) # Create data for memory agent model data_memory &lt;- create_stan_data(d, &quot;memory&quot;) # Display dimensions of our data matrices cat(&quot;Random agent matrix dimensions:&quot;, dim(data_random$h), &quot;\\n&quot;) ## Random agent matrix dimensions: 120 100 cat(&quot;Memory agent matrix dimensions:&quot;, dim(data_memory$h), &quot;\\n&quot;) ## Memory agent matrix dimensions: 120 100 7.8 Multilevel Random Agent Model Our first multilevel model focuses on the biased random agent. For each agent, we’ll estimate an individual bias parameter (theta) that determines their probability of choosing “right” versus “left”. These individual parameters will be modeled as coming from a population distribution with mean thetaM and standard deviation thetaSD. This approach balances two sources of information: 1. The agent’s individual choice patterns 2. The overall population distribution of bias parameters The model implements the following hierarchical structure: Population level: θᵐ ~ Normal(0, 1), θˢᵈ ~ Normal⁺(0, 0.3) Individual level: θᵢ ~ Normal(θᵐ, θˢᵈ) Data level: yᵢₜ ~ Bernoulli(logit⁻¹(θᵢ)) Let’s implement this in Stan: # Stan model for multilevel random agent stan_model &lt;- &quot; /* Multilevel Bernoulli Model * This model infers agent-specific choice biases from sequences of binary choices (0/1) * The model assumes each agent has their own bias (theta) drawn from a population distribution */ functions { // Generate random numbers from truncated normal distribution real normal_lb_rng(real mu, real sigma, real lb) { real p = normal_cdf(lb | mu, sigma); // cdf for bounds real u = uniform_rng(p, 1); return (sigma * inv_Phi(u)) + mu; // inverse cdf for value } } data { int&lt;lower=1&gt; trials; // Number of trials per agent int&lt;lower=1&gt; agents; // Number of agents array[trials, agents] int&lt;lower=0, upper=1&gt; h; // Choice data: 0 or 1 for each trial/agent } parameters { real thetaM; // Population-level mean bias (log-odds scale) real&lt;lower=0&gt; thetaSD; // Population-level SD of bias array[agents] real theta; // Agent-specific biases (log-odds scale) } model { // Population-level priors target += normal_lpdf(thetaM | 0, 1); // Prior for population mean target += normal_lpdf(thetaSD | 0, 0.3) // Half-normal prior for population SD - normal_lccdf(0 | 0, 0.3); // Adjustment for truncation at 0 // Agent-level model target += normal_lpdf(theta | thetaM, thetaSD); // Agent biases drawn from population // Likelihood for observed choices for (i in 1:agents) { target += bernoulli_logit_lpmf(h[,i] | theta[i]); // Choice likelihood } } generated quantities { // Prior predictive samples real thetaM_prior = normal_rng(0, 1); real&lt;lower=0&gt; thetaSD_prior = normal_lb_rng(0, 0.3, 0); real&lt;lower=0, upper=1&gt; theta_prior = inv_logit(normal_rng(thetaM_prior, thetaSD_prior)); // Posterior predictive samples real&lt;lower=0, upper=1&gt; theta_posterior = inv_logit(normal_rng(thetaM, thetaSD)); // Predictive simulations int&lt;lower=0, upper=trials&gt; prior_preds = binomial_rng(trials, inv_logit(thetaM_prior)); int&lt;lower=0, upper=trials&gt; posterior_preds = binomial_rng(trials, inv_logit(thetaM)); // Convert parameters to probability scale for easier interpretation real&lt;lower=0, upper=1&gt; thetaM_prob = inv_logit(thetaM); } &quot; write_stan_file( stan_model, dir = &quot;stan/&quot;, basename = &quot;W6_MultilevelBias.stan&quot;) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W6_MultilevelBias.stan&quot; # File path for saved model model_file &lt;- &quot;simmodels/W6_MultilevelBias.RDS&quot; # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(model_file)) { file &lt;- file.path(&quot;stan/W6_MultilevelBias.stan&quot;) mod &lt;- cmdstan_model(file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) # Check if we need to rerun the simulation samples &lt;- mod$sample( data = data_random, seed = 123, chains = 2, parallel_chains = 2, threads_per_chain = 2, iter_warmup = 2000, iter_sampling = 2000, refresh = 500, max_treedepth = 20, adapt_delta = 0.99, ) samples$save_object(file = model_file) cat(&quot;Generated new model fit and saved to&quot;, model_file, &quot;\\n&quot;) } else { # Load existing results samples &lt;- readRDS(model_file) cat(&quot;Loaded existing model fit from&quot;, model_file, &quot;\\n&quot;) } ## Loaded existing model fit from simmodels/W6_MultilevelBias.RDS 7.8.1 Assessing multilevel random agents Besides the usual prior predictive checks, prior posterior update checks, posterior predictive checks, based on the population level estimates; we also want to plot at least a few of the single agents to assess how well the model is doing for them. [MISSING: PLOT MODEL ESTIMATES AGAINST N OF HEADS BY PARTICIPANT] # Load the model results samples &lt;- readRDS(&quot;simmodels/W6_MultilevelBias.RDS&quot;) # Display summary statistics for key parameters samples$summary(c(&quot;thetaM&quot;, &quot;thetaSD&quot;, &quot;thetaM_prob&quot;)) ## # A tibble: 3 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 thetaM 1.35 1.35 0.0677 0.0673 1.24 1.46 1.00 5172. 3042. ## 2 thetaSD 0.638 0.635 0.0516 0.0510 0.559 0.727 1.00 3636. 2796. ## 3 thetaM_prob 0.793 0.794 0.0111 0.0110 0.775 0.811 1.00 5172. 3042. # Extract posterior draws for analysis draws_df &lt;- as_draws_df(samples$draws()) # Create a function for standard diagnostic plots plot_diagnostics &lt;- function(parameter_name, true_value = NULL, prior_name = paste0(parameter_name, &quot;_prior&quot;)) { # Trace plot to check mixing and convergence p1 &lt;- ggplot(draws_df, aes(.iteration, .data[[parameter_name]], group = .chain, color = as.factor(.chain))) + geom_line(alpha = 0.5) + labs(title = paste(&quot;Trace Plot for&quot;, parameter_name), x = &quot;Iteration&quot;, y = parameter_name, color = &quot;Chain&quot;) + theme_classic() # Prior-posterior update plot p2 &lt;- ggplot(draws_df) + geom_histogram(aes(.data[[parameter_name]]), fill = &quot;blue&quot;, alpha = 0.3) + geom_histogram(aes(.data[[prior_name]]), fill = &quot;red&quot;, alpha = 0.3) # Add true value if provided if (!is.null(true_value)) { p2 &lt;- p2 + geom_vline(xintercept = true_value, linetype = &quot;dashed&quot;, size = 1) } p2 &lt;- p2 + labs(title = paste(&quot;Prior-Posterior Update for&quot;, parameter_name), subtitle = &quot;Blue: posterior, Red: prior, Dashed: true value&quot;, x = parameter_name, y = &quot;Density&quot;) + theme_classic() # Return both plots return(p1 + p2) } # Plot diagnostics for population mean pop_mean_plots &lt;- plot_diagnostics(&quot;thetaM&quot;, rateM) # Plot diagnostics for population SD pop_sd_plots &lt;- plot_diagnostics(&quot;thetaSD&quot;, rateSD) # Display diagnostic plots pop_mean_plots pop_sd_plots # Create predictive check plots # Prior predictive check p1 &lt;- ggplot(draws_df) + geom_histogram(aes(prior_preds), bins = 30, fill = &quot;blue&quot;, alpha = 0.3, color = &quot;darkblue&quot;) + labs(title = &quot;Prior Predictive Check&quot;, subtitle = &quot;Distribution of predicted &#39;right&#39; choices out of 120 trials&quot;, x = &quot;Number of Right Choices&quot;, y = &quot;Count&quot;) + theme_classic() # Posterior predictive check p2 &lt;- ggplot(draws_df) + geom_histogram(aes(posterior_preds), bins = 30, fill = &quot;green&quot;, alpha = 0.3, color = &quot;darkgreen&quot;) + geom_histogram(aes(prior_preds), bins = 30, fill = &quot;blue&quot;, alpha = 0.1, color = &quot;darkblue&quot;) + labs(title = &quot;Prior vs Posterior Predictive Check&quot;, subtitle = &quot;Green: posterior predictions, Blue: prior predictions&quot;, x = &quot;Number of Right Choices&quot;, y = &quot;Count&quot;) + theme_classic() # Average observed choices per agent agent_means &lt;- colMeans(data_random$h) observed_counts &lt;- agent_means * trials # Add observed counts to posterior predictive plot p3 &lt;- ggplot(draws_df) + geom_histogram(aes(posterior_preds), bins = 30, fill = &quot;green&quot;, alpha = 0.3, color = &quot;darkgreen&quot;) + geom_histogram(data = tibble(observed = observed_counts), aes(observed), bins = 30, fill = &quot;red&quot;, alpha = 0.3, color = &quot;darkred&quot;) + labs(title = &quot;Posterior Predictions vs Observed Data&quot;, subtitle = &quot;Green: posterior predictions, Red: actual observed counts&quot;, x = &quot;Number of Right Choices&quot;, y = &quot;Count&quot;) + theme_classic() # Display predictive check plots p1 + p2 + p3 7.9 Let’s look at individuals # Extract individual parameter estimates # Sample 10 random agents to examine more closely sample_agents &lt;- sample(1:agents, 10) # Extract posterior samples for each agent&#39;s theta parameter theta_samples &lt;- matrix(NA, nrow = nrow(draws_df), ncol = agents) for (a in 1:agents) { # Convert from log-odds to probability scale for ease of interpretation theta_samples[, a] &lt;- inv_logit_scaled(draws_df[[paste0(&quot;theta[&quot;, a, &quot;]&quot;)]]) } # Calculate true rates dddd &lt;- unique(d[,c(&quot;agent&quot;, &quot;trueRate&quot;)]) %&gt;% mutate(trueRate = inv_logit_scaled(trueRate)) # Create a dataframe with the true rates and empirical rates agent_comparison &lt;- tibble( agent = 1:agents, true_rate = dddd$trueRate, # True rate used in simulation empirical_rate = colMeans(data_random$h) # Observed proportion of right choices ) # Calculate summary statistics for posterior estimates theta_summaries &lt;- tibble( agent = 1:agents, mean = colMeans(theta_samples), lower_95 = apply(theta_samples, 2, quantile, 0.025), upper_95 = apply(theta_samples, 2, quantile, 0.975) ) %&gt;% # Join with true values for comparison left_join(agent_comparison, by = &quot;agent&quot;) %&gt;% # Calculate error metrics mutate( abs_error = abs(mean - true_rate), in_interval = true_rate &gt;= lower_95 &amp; true_rate &lt;= upper_95, rel_error = abs_error / true_rate ) # Plot 1: Posterior distributions for sample agents posterior_samples &lt;- tibble( agent = rep(sample_agents, each = nrow(draws_df)), sample_idx = rep(1:nrow(draws_df), times = length(sample_agents)), estimated_rate = as.vector(theta_samples[, sample_agents]) ) p1 &lt;- ggplot() + # Add density plot for posterior distribution of rates geom_density(data = posterior_samples, aes(x = estimated_rate, group = agent), fill = &quot;skyblue&quot;, alpha = 0.5) + # Add vertical line for true rate geom_vline(data = agent_comparison %&gt;% filter(agent %in% sample_agents), aes(xintercept = true_rate), color = &quot;red&quot;, size = 1) + # Add vertical line for empirical rate geom_vline(data = agent_comparison %&gt;% filter(agent %in% sample_agents), aes(xintercept = empirical_rate), color = &quot;green4&quot;, size = 1, linetype = &quot;dashed&quot;) + # Facet by agent facet_wrap(~agent, scales = &quot;free_y&quot;) + # Add formatting labs(title = &quot;Individual Agent Parameter Estimation&quot;, subtitle = &quot;Blue density: Posterior distribution\\nRed line: True rate\\nGreen dashed line: Empirical rate&quot;, x = &quot;Rate Parameter (Probability Scale)&quot;, y = &quot;Density&quot;) + theme_classic() + theme(strip.background = element_rect(fill = &quot;white&quot;), strip.text = element_text(face = &quot;bold&quot;)) + xlim(0, 1) # Plot 2: Parameter recovery for all agents p2 &lt;- ggplot(theta_summaries, aes(x = true_rate, y = mean)) + geom_point(aes(color = in_interval), size = 3, alpha = 0.7) + geom_errorbar(aes(ymin = lower_95, ymax = upper_95, color = in_interval), width = 0.01, alpha = 0.3) + geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;, color = &quot;black&quot;) + geom_smooth(method = &quot;lm&quot;, color = &quot;blue&quot;, se = FALSE, linetype = &quot;dotted&quot;) + scale_color_manual(values = c(&quot;TRUE&quot; = &quot;darkgreen&quot;, &quot;FALSE&quot; = &quot;red&quot;)) + labs(title = &quot;Parameter Recovery Performance&quot;, subtitle = &quot;Each point represents one agent; error bars show 95% credible intervals&quot;, x = &quot;True Rate&quot;, y = &quot;Estimated Rate&quot;, color = &quot;True Value in\\n95% Interval&quot;) + theme_classic() + xlim(0, 1) + ylim(0, 1) # Calculate parameter estimation metrics error_metrics &lt;- tibble( agent = 1:agents, true_rate = dddd$trueRate, empirical_rate = colMeans(data_random$h), estimated_rate = colMeans(theta_samples), lower_95 = apply(theta_samples, 2, quantile, 0.025), upper_95 = apply(theta_samples, 2, quantile, 0.975), absolute_error = abs(estimated_rate - true_rate), relative_error = absolute_error / true_rate, in_interval = true_rate &gt;= lower_95 &amp; true_rate &lt;= upper_95 ) # Create direct comparison plot p3 &lt;- ggplot(error_metrics, aes(x = agent)) + geom_errorbar(aes(ymin = lower_95, ymax = upper_95), width = 0.2, color = &quot;blue&quot;, alpha = 0.5) + geom_point(aes(y = estimated_rate), color = &quot;blue&quot;, size = 3) + geom_point(aes(y = true_rate), color = &quot;red&quot;, shape = 4, size = 3, stroke = 2, alpha = 0.3) + geom_point(aes(y = empirical_rate), color = &quot;green4&quot;, shape = 1, size = 3, stroke = 2, alpha = 0.3) + labs(title = &quot;Parameter Estimation Accuracy by Agent&quot;, subtitle = &quot;Blue points and bars: Estimated rate with 95% credible interval\\nRed X: True rate used in simulation\\nGreen circle: Empirical rate from observed data&quot;, x = &quot;Agent ID&quot;, y = &quot;Rate&quot;) + theme_classic() # Plot 4: Shrinkage visualization # Calculate population mean estimate pop_mean &lt;- mean(draws_df$thetaM_prob) # Add shrinkage information to the data shrinkage_data &lt;- theta_summaries %&gt;% mutate( # Distance from empirical to population mean (shows shrinkage direction) empirical_to_pop = empirical_rate - pop_mean, # Distance from estimate to empirical (shows amount of shrinkage) estimate_to_empirical = mean - empirical_rate, # Ratio of distances (shrinkage proportion) shrinkage_ratio = 1 - (abs(mean - pop_mean) / abs(empirical_rate - pop_mean)), # Define number of trials for sizing points n_trials = trials ) p4 &lt;- ggplot(shrinkage_data, aes(x = empirical_rate, y = mean)) + # Reference line for no shrinkage geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;, color = &quot;gray50&quot;) + # Horizontal line for population mean geom_hline(yintercept = pop_mean, linetype = &quot;dotted&quot;, color = &quot;blue&quot;) + # Points for each agent geom_point(aes(size = n_trials, color = abs(shrinkage_ratio)), alpha = 0.7) + # Connect points to their empirical values to show shrinkage geom_segment(aes(xend = empirical_rate, yend = empirical_rate, color = abs(shrinkage_ratio)), alpha = 0.3) + scale_color_gradient(low = &quot;yellow&quot;, high = &quot;red&quot;) + labs(title = &quot;Shrinkage Effects in Multilevel Modeling&quot;, subtitle = &quot;Points above diagonal shrink down, points below shrink up;\\nBlue dotted line: population mean estimate&quot;, x = &quot;Empirical Rate (Observed Proportion)&quot;, y = &quot;Posterior Mean Estimate&quot;, color = &quot;Shrinkage\\nMagnitude&quot;, size = &quot;Number of\\nTrials&quot;) + theme_classic() + xlim(0, 1) + ylim(0, 1) # Display plots p1 p2 + p3 p4 7.10 Prior sensitivity checks # File path for saved sensitivity results sensitivity_results_file &lt;- &quot;simdata/W6_sensitivity_results.csv&quot; # Check if we need to rerun the analysis if (regenerate_simulations || !file.exists(sensitivity_results_file)) { # Define a range of prior specifications to test prior_settings &lt;- expand_grid( prior_mean_theta = c(-1, 0, 1), # Different prior means for population rate prior_sd_theta = c(0.5, 1, 2), # Different prior SDs for population rate prior_scale_theta_sd = c(0.1, 0.3, 0.5, 1) # Different scales for the SD hyperprior ) # First, create a single Stan model that accepts prior hyperparameters as data stan_code &lt;- &quot; functions { real normal_lb_rng(real mu, real sigma, real lb) { real p = normal_cdf(lb | mu, sigma); real u = uniform_rng(p, 1); return (sigma * inv_Phi(u)) + mu; } } data { int&lt;lower=1&gt; trials; int&lt;lower=1&gt; agents; array[trials, agents] int&lt;lower=0, upper=1&gt; h; real prior_mean_theta; real&lt;lower=0&gt; prior_sd_theta; real&lt;lower=0&gt; prior_scale_theta_sd; } parameters { real thetaM; real&lt;lower=0&gt; thetaSD; array[agents] real theta; } model { // Population-level priors with specified hyperparameters target += normal_lpdf(thetaM | prior_mean_theta, prior_sd_theta); target += normal_lpdf(thetaSD | 0, prior_scale_theta_sd) - normal_lccdf(0 | 0, prior_scale_theta_sd); // Agent-level model target += normal_lpdf(theta | thetaM, thetaSD); // Likelihood for (i in 1:agents) { target += bernoulli_logit_lpmf(h[,i] | theta[i]); } } generated quantities { real thetaM_prob = inv_logit(thetaM); } &quot; # Write model to file writeLines(stan_code, &quot;stan/sensitivity_model.stan&quot;) # Compile the model once mod_sensitivity &lt;- cmdstan_model(&quot;stan/sensitivity_model.stan&quot;, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) # Function to fit model with specified priors fit_with_priors &lt;- function(prior_mean_theta, prior_sd_theta, prior_scale_theta_sd) { # Create data with prior specifications data_with_priors &lt;- c(data_random, list( prior_mean_theta = prior_mean_theta, prior_sd_theta = prior_sd_theta, prior_scale_theta_sd = prior_scale_theta_sd )) # Fit model fit &lt;- mod_sensitivity$sample( data = data_with_priors, seed = 123, chains = 1, parallel_chains = 1, threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 1000, refresh = 0 ) # Extract posterior summaries summary_df &lt;- fit$summary(c(&quot;thetaM&quot;, &quot;thetaSD&quot;, &quot;thetaM_prob&quot;)) # Return results return(tibble( prior_mean_theta = prior_mean_theta, prior_sd_theta = prior_sd_theta, prior_scale_theta_sd = prior_scale_theta_sd, est_thetaM = summary_df$mean[summary_df$variable == &quot;thetaM&quot;], est_thetaSD = summary_df$mean[summary_df$variable == &quot;thetaSD&quot;], est_thetaM_prob = summary_df$mean[summary_df$variable == &quot;thetaM_prob&quot;] )) } # Run parallel analysis library(furrr) plan(multisession, workers = 4) # Using fewer workers to reduce resource use sensitivity_results &lt;- future_pmap_dfr( prior_settings, function(prior_mean_theta, prior_sd_theta, prior_scale_theta_sd) { fit_with_priors(prior_mean_theta, prior_sd_theta, prior_scale_theta_sd) }, .options = furrr_options(seed = TRUE) ) # Save results for future use write_csv(sensitivity_results, sensitivity_results_file) cat(&quot;Generated new sensitivity analysis results and saved to&quot;, sensitivity_results_file, &quot;\\n&quot;) } else { # Load existing results sensitivity_results &lt;- read_csv(sensitivity_results_file) cat(&quot;Loaded existing sensitivity analysis results from&quot;, sensitivity_results_file, &quot;\\n&quot;) } ## Loaded existing sensitivity analysis results from simdata/W6_sensitivity_results.csv # Plot for population mean estimate p1 &lt;- ggplot(sensitivity_results, aes(x = prior_mean_theta, y = est_thetaM_prob, color = factor(prior_scale_theta_sd))) + geom_point(size = 3) + geom_hline(yintercept = inv_logit_scaled(d$rateM), linetype = &quot;dashed&quot;) + labs(title = &quot;Sensitivity of Population Mean Parameter&quot;, subtitle = &quot;Dashed line shows average empirical rate across participants&quot;, x = &quot;Prior Mean, SD for Population Parameter&quot;, y = &quot;Estimated Population Mean (probability scale)&quot;, color = &quot;Prior Scale for\\nPopulation SD&quot;) + theme_classic() + ylim(0.7, 0.9) + facet_wrap(.~prior_sd_theta) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Plot for population SD parameter p2 &lt;- ggplot(sensitivity_results, aes(x = prior_mean_theta, y = est_thetaSD, color = factor(prior_scale_theta_sd))) + geom_point(size = 3) + geom_hline(yintercept = d$rateSD, linetype = &quot;dashed&quot;) + labs(title = &quot;Sensitivity of Population Variance Parameter&quot;, x = &quot;Prior Mean, SD for Population Parameter&quot;, y = &quot;Estimated Population SD&quot;, color = &quot;Prior Scale for\\nPopulation SD&quot;) + theme_classic() + ylim(0.5, 0.7) + facet_wrap(.~prior_sd_theta) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Display plots p1 / p2 7.11 Parameter recovery # File path for saved recovery results recovery_results_file &lt;- &quot;simdata/W6_recovery_results.csv&quot; # Check if we need to rerun the analysis if (regenerate_simulations || !file.exists(recovery_results_file)) { # Function to simulate data and recover parameters recover_parameters &lt;- function(true_thetaM, true_thetaSD, n_agents, n_trials) { # Generate agent-specific true rates agent_thetas &lt;- rnorm(n_agents, true_thetaM, true_thetaSD) # Generate choice data sim_data &lt;- matrix(NA, nrow = n_trials, ncol = n_agents) for (a in 1:n_agents) { sim_data[,a] &lt;- rbinom(n_trials, 1, inv_logit_scaled(agent_thetas[a])) } # Prepare data for Stan stan_data &lt;- list( trials = n_trials, agents = n_agents, h = sim_data, prior_mean_theta = 0, # Using neutral priors prior_sd_theta = 1, prior_scale_theta_sd = 0.3 ) # Compile the model once mod_sensitivity &lt;- cmdstan_model(&quot;stan/sensitivity_model.stan&quot;, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) # Fit model fit &lt;- mod_sensitivity$sample( data = stan_data, seed = 123, chains = 1, parallel_chains = 1, threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 1000, refresh = 0 ) # Extract estimates summary_df &lt;- fit$summary(c(&quot;thetaM&quot;, &quot;thetaSD&quot;)) # Return comparison of true vs. estimated parameters return(tibble( true_thetaM = true_thetaM, true_thetaSD = true_thetaSD, n_agents = n_agents, n_trials = n_trials, est_thetaM = summary_df$mean[summary_df$variable == &quot;thetaM&quot;], est_thetaSD = summary_df$mean[summary_df$variable == &quot;thetaSD&quot;] )) } # Create parameter grid for recovery study recovery_settings &lt;- expand_grid( true_thetaM = c(-1, 0, 1), true_thetaSD = c(0.1, 0.3, 0.5, 0.7), n_agents = c(20, 50), n_trials = c(60, 120) ) # Run parameter recovery study (this can be very time-consuming) recovery_results &lt;- pmap_dfr( recovery_settings, function(true_thetaM, true_thetaSD, n_agents, n_trials) { recover_parameters(true_thetaM, true_thetaSD, n_agents, n_trials) } ) # Save results for future use write_csv(recovery_results, recovery_results_file) cat(&quot;Generated new parameter recovery results and saved to&quot;, recovery_results_file, &quot;\\n&quot;) } else { # Load existing results recovery_results &lt;- read_csv(recovery_results_file) cat(&quot;Loaded existing parameter recovery results from&quot;, recovery_results_file, &quot;\\n&quot;) } ## Loaded existing parameter recovery results from simdata/W6_recovery_results.csv # Visualize parameter recovery results p1 &lt;- ggplot(recovery_results, aes(x = true_thetaM, y = est_thetaM, color = factor(true_thetaSD))) + geom_point(size = 3) + geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;) + facet_grid(n_agents ~ n_trials, labeller = labeller( n_agents = function(x) paste0(&quot;Agents: &quot;, x), n_trials = function(x) paste0(&quot;Trials: &quot;, x) )) + labs(title = &quot;Recovery of Population Mean Parameter&quot;, x = &quot;True Value&quot;, y = &quot;Estimated Value&quot;, color = &quot;True Population SD&quot;) + theme_classic() p2 &lt;- ggplot(recovery_results, aes(x = true_thetaSD, y = est_thetaSD, color = factor(true_thetaM))) + geom_point(size = 3) + geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;) + facet_grid(n_agents ~ n_trials, labeller = labeller( n_agents = function(x) paste0(&quot;Agents: &quot;, x), n_trials = function(x) paste0(&quot;Trials: &quot;, x) )) + labs(title = &quot;Recovery of Population SD Parameter&quot;, x = &quot;True Value&quot;, y = &quot;Estimated Value&quot;, color = &quot;True Population Mean&quot;) + theme_classic() # Display parameter recovery plots p1 / p2 7.12 Multilevel Memory Agent Model Now we’ll implement a more complex model for the memory agent. This model has two parameters per agent: bias: baseline tendency to choose “right” (log-odds scale) beta: sensitivity to the memory of opponent’s past choices Like the random agent model, we’ll use a multilevel structure where individual parameters come from population distributions. However, this model presents some additional challenges: We need to handle two parameters per agent We need to track and update memory states during the model The hierarchical structure is more complex The hierarchical structure for this model is: Population level: μ_bias ~ Normal(0, 1), σ_bias ~ Normal⁺(0, 0.3) μ_beta ~ Normal(0, 0.3), σ_beta ~ Normal⁺(0, 0.3) Individual level: bias_i ~ Normal(μ_bias, σ_bias) beta_i ~ Normal(μ_beta, σ_beta) Transformed variables: memory_it = updated based on opponent’s choices Data level: y_it ~ Bernoulli(logit⁻¹(bias_i + beta_i * logit(memory_it))) Let’s implement this model. [MISSING: DAGS] Code, compile and fit the model # Stan model for multilevel memory agent with centered parameterization stan_model &lt;- &quot; // Multilevel Memory Agent Model (Centered Parameterization) // functions{ real normal_lb_rng(real mu, real sigma, real lb) { real p = normal_cdf(lb | mu, sigma); // cdf for bounds real u = uniform_rng(p, 1); return (sigma * inv_Phi(u)) + mu; // inverse cdf for value } } // The input data for the model data { int&lt;lower = 1&gt; trials; // Number of trials per agent int&lt;lower = 1&gt; agents; // Number of agents array[trials, agents] int h; // Memory agent choices array[trials, agents] int other; // Opponent (random agent) choices } // Parameters to be estimated parameters { // Population-level parameters real biasM; // Mean of baseline bias real&lt;lower = 0&gt; biasSD; // SD of baseline bias real betaM; // Mean of memory sensitivity real&lt;lower = 0&gt; betaSD; // SD of memory sensitivity // Individual-level parameters array[agents] real bias; // Individual baseline bias parameters array[agents] real beta; // Individual memory sensitivity parameters } // Transformed parameters (derived quantities) transformed parameters { // Memory state for each agent and trial array[trials, agents] real memory; // Calculate memory states based on opponent&#39;s choices for (agent in 1:agents){ // Initial memory state (no prior information) memory[1, agent] = 0.5; for (trial in 1:trials){ // Update memory based on opponent&#39;s choices if (trial &lt; trials){ // Simple averaging memory update memory[trial + 1, agent] = memory[trial, agent] + ((other[trial, agent] - memory[trial, agent]) / trial); // Handle edge cases to avoid numerical issues if (memory[trial + 1, agent] == 0){memory[trial + 1, agent] = 0.01;} if (memory[trial + 1, agent] == 1){memory[trial + 1, agent] = 0.99;} } } } } // Model definition model { // Population-level priors target += normal_lpdf(biasM | 0, 1); target += normal_lpdf(biasSD | 0, .3) - normal_lccdf(0 | 0, .3); // Half-normal target += normal_lpdf(betaM | 0, .3); target += normal_lpdf(betaSD | 0, .3) - normal_lccdf(0 | 0, .3); // Half-normal // Individual-level priors target += normal_lpdf(bias | biasM, biasSD); target += normal_lpdf(beta | betaM, betaSD); // Likelihood for (agent in 1:agents) { for (trial in 1:trials) { target += bernoulli_logit_lpmf(h[trial,agent] | bias[agent] + logit(memory[trial, agent]) * beta[agent]); } } } // Generated quantities for model checking and predictions generated quantities{ // Prior samples for checking real biasM_prior; real&lt;lower=0&gt; biasSD_prior; real betaM_prior; real&lt;lower=0&gt; betaSD_prior; real bias_prior; real beta_prior; // Predictive simulations with different memory values int&lt;lower=0, upper = trials&gt; prior_preds0; // No memory effect (memory=0) int&lt;lower=0, upper = trials&gt; prior_preds1; // Neutral memory (memory=0.5) int&lt;lower=0, upper = trials&gt; prior_preds2; // Strong memory (memory=1) int&lt;lower=0, upper = trials&gt; posterior_preds0; int&lt;lower=0, upper = trials&gt; posterior_preds1; int&lt;lower=0, upper = trials&gt; posterior_preds2; // Individual-level predictions (for each agent) array[agents] int&lt;lower=0, upper = trials&gt; posterior_predsID0; array[agents] int&lt;lower=0, upper = trials&gt; posterior_predsID1; array[agents] int&lt;lower=0, upper = trials&gt; posterior_predsID2; // Generate prior samples biasM_prior = normal_rng(0,1); biasSD_prior = normal_lb_rng(0,0.3,0); betaM_prior = normal_rng(0,1); betaSD_prior = normal_lb_rng(0,0.3,0); bias_prior = normal_rng(biasM_prior, biasSD_prior); beta_prior = normal_rng(betaM_prior, betaSD_prior); // Prior predictive checks with different memory values prior_preds0 = binomial_rng(trials, inv_logit(bias_prior + 0 * beta_prior)); prior_preds1 = binomial_rng(trials, inv_logit(bias_prior + 1 * beta_prior)); prior_preds2 = binomial_rng(trials, inv_logit(bias_prior + 2 * beta_prior)); // Posterior predictive checks with different memory values posterior_preds0 = binomial_rng(trials, inv_logit(biasM + 0 * betaM)); posterior_preds1 = binomial_rng(trials, inv_logit(biasM + 1 * betaM)); posterior_preds2 = binomial_rng(trials, inv_logit(biasM + 2 * betaM)); // Individual-level predictions for (agent in 1:agents){ posterior_predsID0[agent] = binomial_rng(trials, inv_logit(bias[agent] + 0 * beta[agent])); posterior_predsID1[agent] = binomial_rng(trials, inv_logit(bias[agent] + 1 * beta[agent])); posterior_predsID2[agent] = binomial_rng(trials, inv_logit(bias[agent] + 2 * beta[agent])); } } &quot; write_stan_file( stan_model, dir = &quot;stan/&quot;, basename = &quot;W6_MultilevelMemory.stan&quot;) # File path for saved model model_file &lt;- &quot;simmodels/W6_MultilevelMemory_centered.RDS&quot; # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(model_file)) { file &lt;- file.path(&quot;stan/W6_MultilevelMemory.stan&quot;) mod &lt;- cmdstan_model(file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) # Sample from the posterior distribution samples_mlvl &lt;- mod$sample( data = data_memory, seed = 123, chains = 2, parallel_chains = 2, threads_per_chain = 1, iter_warmup = 2000, iter_sampling = 2000, refresh = 500, max_treedepth = 20, adapt_delta = 0.99 ) # Save the model results samples_mlvl$save_object(file = model_file) cat(&quot;Generated new model fit and saved to&quot;, model_file, &quot;\\n&quot;) } else { # Load existing results samples_mlvl &lt;- readRDS(model_file) cat(&quot;Loaded existing model fit from&quot;, model_file, &quot;\\n&quot;) } 7.12.1 Assessing multilevel memory # Check if samples_biased exists if (!exists(&quot;samples_mlvl&quot;)) { cat(&quot;Loading multilevel model samples...\\n&quot;) samples_mlvl &lt;- readRDS(&quot;simmodels/W6_MultilevelMemory_centered.RDS&quot;) cat(&quot;Available parameters:&quot;, paste(colnames(as_draws_df(samples_mlvl$draws())), collapse = &quot;, &quot;), &quot;\\n&quot;) } # Show summary statistics for key parameters print(samples_mlvl$summary(c(&quot;biasM&quot;, &quot;betaM&quot;, &quot;biasSD&quot;, &quot;betaSD&quot;))) ## # A tibble: 4 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 biasM 0.416 0.414 0.0808 0.0800 0.285 0.552 1.01 241. 484. ## 2 betaM 1.16 1.16 0.0711 0.0727 1.04 1.27 1.00 347. 833. ## 3 biasSD 0.241 0.240 0.0611 0.0599 0.141 0.343 1.00 205. 242. ## 4 betaSD 0.381 0.379 0.0465 0.0457 0.308 0.461 1.00 1155. 1764. # Extract posterior draws for analysis draws_df &lt;- as_draws_df(samples_mlvl$draws()) # Create trace plots to check convergence p1 &lt;- mcmc_trace(draws_df, pars = c(&quot;biasM&quot;, &quot;biasSD&quot;, &quot;betaM&quot;, &quot;betaSD&quot;)) + theme_classic() + ggtitle(&quot;Trace Plots for Population Parameters&quot;) # Show trace plots p1 # Create prior-posterior update plots create_density_plot &lt;- function(param, true_value, title) { prior_name &lt;- paste0(param, &quot;_prior&quot;) ggplot(draws_df) + geom_histogram(aes(get(param)), fill = &quot;blue&quot;, alpha = 0.3) + geom_histogram(aes(get(prior_name)), fill = &quot;red&quot;, alpha = 0.3) + geom_vline(xintercept = true_value, linetype = &quot;dashed&quot;) + labs(title = title, subtitle = &quot;Blue: posterior, Red: prior, Dashed: true value&quot;, x = param, y = &quot;Density&quot;) + theme_classic() } # Create individual plots p_biasM &lt;- create_density_plot(&quot;biasM&quot;, biasM, &quot;Population Mean Bias&quot;) p_biasSD &lt;- create_density_plot(&quot;biasSD&quot;, biasSD, &quot;Population SD of Bias&quot;) p_betaM &lt;- create_density_plot(&quot;betaM&quot;, betaM, &quot;Population Mean Beta&quot;) p_betaSD &lt;- create_density_plot(&quot;betaSD&quot;, betaSD, &quot;Population SD of Beta&quot;) # Show them in a grid (p_biasM + p_biasSD) / (p_betaM + p_betaSD) # Show correlations p1 &lt;- ggplot(draws_df, aes(biasM, biasSD, group = .chain, color = .chain)) + geom_point(alpha = 0.1) + theme_classic() p2 &lt;- ggplot(draws_df, aes(betaM, betaSD, group = .chain, color = .chain)) + geom_point(alpha = 0.1) + theme_classic() p3 &lt;- ggplot(draws_df, aes(biasM, betaM, group = .chain, color = .chain)) + geom_point(alpha = 0.1) + theme_classic() p4 &lt;- ggplot(draws_df, aes(biasSD, betaSD, group = .chain, color = .chain)) + geom_point(alpha = 0.1) + theme_classic() p1 + p2 + p3 + p4 # Create posterior predictive check plots p1 &lt;- ggplot(draws_df) + geom_histogram(aes(prior_preds0), fill = &quot;red&quot;, alpha = 0.3, bins = 30) + geom_histogram(aes(posterior_preds0), fill = &quot;blue&quot;, alpha = 0.3, bins = 30) + geom_histogram(aes(posterior_preds1), fill = &quot;green&quot;, alpha = 0.3, bins = 30) + geom_histogram(aes(posterior_preds2), fill = &quot;purple&quot;, alpha = 0.3, bins = 30) + labs(title = &quot;Prior and Posterior Predictive Distributions&quot;, subtitle = &quot;Red: prior, Blue: no memory effect, Green: neutral memory, Purple: strong memory&quot;, x = &quot;Predicted Right Choices (out of 120)&quot;, y = &quot;Count&quot;) + theme_classic() # Display plots p1 # Individual-level parameter recovery # Extract individual parameters for a sample of agents sample_agents &lt;- sample(1:agents, 100) sample_data &lt;- d %&gt;% filter(agent %in% sample_agents, trial == 1) %&gt;% dplyr::select(agent, bias, beta) # Extract posterior means for individual agents bias_means &lt;- c() beta_means &lt;- c() for (i in sample_agents) { bias_means[i] &lt;- mean(draws_df[[paste0(&quot;bias[&quot;, i, &quot;]&quot;)]]) beta_means[i] &lt;- mean(draws_df[[paste0(&quot;beta[&quot;, i, &quot;]&quot;)]]) } # Create comparison data comparison_data &lt;- tibble( agent = sample_agents, true_bias = sample_data$bias, est_bias = bias_means[sample_agents], true_beta = sample_data$beta, est_beta = beta_means[sample_agents] ) # Plot comparison p1 &lt;- ggplot(comparison_data, aes(true_bias, est_bias)) + geom_point(size = 3) + geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;) + geom_smooth(method = lm) + ylim(-0.2, 1.1) + xlim(-0.2, 1.1) + labs(title = &quot;Bias Parameter Recovery&quot;, x = &quot;True Bias&quot;, y = &quot;Estimated Bias&quot;) + theme_classic() p2 &lt;- ggplot(comparison_data, aes(true_beta, est_beta)) + geom_point(size = 3) + geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;) + ylim(0.6, 2.4) + xlim(0.6, 2.4) + labs(title = &quot;Beta Parameter Recovery&quot;, x = &quot;True Beta&quot;, y = &quot;Estimated Beta&quot;) + theme_classic() # Display parameter recovery plots p1 + p2 7.12.2 Diagnosing the issue with bias and beta # First, let&#39;s examine the individual bias parameters distribution bias_params &lt;- draws_df %&gt;% dplyr::select(starts_with(&quot;bias[&quot;)) %&gt;% pivot_longer(everything(), names_to = &quot;parameter&quot;, values_to = &quot;value&quot;) # Plot distribution of all individual bias parameters ggplot(bias_params, aes(x = value)) + geom_histogram(bins = 30) + labs(title = &quot;Distribution of Individual Bias Parameters&quot;, x = &quot;Bias Value (log-odds scale)&quot;, y = &quot;Count&quot;) + theme_classic() # Check for correlation between bias and beta parameters # For a few sample agents cors &lt;- NULL for (i in sample(1:agents)) { cors[i] &lt;- (cor(draws_df[[paste0(&quot;bias[&quot;, i, &quot;]&quot;)]], draws_df[[paste0(&quot;beta[&quot;, i, &quot;]&quot;)]])) } corr_data &lt;- tibble( agent = seq_along(cors), correlation = cors ) %&gt;% filter(!is.na(correlation)) # Calculate summary statistics mean_cor &lt;- mean(corr_data$correlation) median_cor &lt;- median(corr_data$correlation) min_cor &lt;- min(corr_data$correlation) max_cor &lt;- max(corr_data$correlation) # Create correlation strength categories corr_data &lt;- corr_data %&gt;% mutate( strength = case_when( correlation &lt;= -0.7 ~ &quot;Strong negative&quot;, correlation &lt;= -0.5 ~ &quot;Moderate negative&quot;, correlation &lt;= -0.3 ~ &quot;Weak negative&quot;, correlation &lt; 0 ~ &quot;Negligible negative&quot;, correlation &gt;= 0 &amp; correlation &lt; 0.3 ~ &quot;Negligible positive&quot;, correlation &gt;= 0.3 &amp; correlation &lt; 0.5 ~ &quot;Weak positive&quot;, correlation &gt;= 0.5 &amp; correlation &lt; 0.7 ~ &quot;Moderate positive&quot;, TRUE ~ &quot;Strong positive&quot; ), strength = factor(strength, levels = c( &quot;Strong negative&quot;, &quot;Moderate negative&quot;, &quot;Weak negative&quot;, &quot;Negligible negative&quot;, &quot;Negligible positive&quot;, &quot;Weak positive&quot;, &quot;Moderate positive&quot;, &quot;Strong positive&quot; )) ) # Create the plot ggplot(corr_data, aes(x = correlation)) + # Histogram colored by correlation strength geom_histogram(aes(fill = strength), bins = 30, alpha = 0.8, color = &quot;white&quot;) + # Density curve geom_density(color = &quot;black&quot;, linewidth = 1, alpha = 0.3) + # Reference lines geom_vline(xintercept = median_cor, linewidth = 1, linetype = &quot;dotted&quot;, color = &quot;black&quot;) + geom_vline(xintercept = 0, linewidth = 0.5, color = &quot;gray50&quot;) + # Annotations annotate(&quot;label&quot;, x = median_cor, y = Inf, label = paste(&quot;Median:&quot;, round(median_cor, 2)), vjust = 3, size = 4, fill = &quot;lightblue&quot;, alpha = 0.8) + # Colors for correlation strength scale_fill_manual(values = c( &quot;Strong negative&quot; = &quot;#d73027&quot;, &quot;Moderate negative&quot; = &quot;#fc8d59&quot;, &quot;Weak negative&quot; = &quot;#fee090&quot;, &quot;Negligible negative&quot; = &quot;#ffffbf&quot;, &quot;Negligible positive&quot; = &quot;#e0f3f8&quot;, &quot;Weak positive&quot; = &quot;#91bfdb&quot;, &quot;Moderate positive&quot; = &quot;#4575b4&quot;, &quot;Strong positive&quot; = &quot;#313695&quot; )) + # Titles and labels labs( title = &quot;Distribution of Bias-Beta Parameter Correlations&quot;, subtitle = paste0( &quot;Analysis of &quot;, nrow(corr_data), &quot; agents shows consistent negative correlation\\n&quot;, &quot;Range: [&quot;, round(min_cor, 2), &quot;, &quot;, round(max_cor, 2), &quot;]&quot; ), x = &quot;Correlation Coefficient&quot;, y = &quot;Count&quot;, fill = &quot;Correlation Strength&quot; ) + # Set axis limits and theme xlim(-1, 1) + theme_minimal() 7.12.3 Multilevel memory with non centered parameterization When implementing multilevel models, we sometimes encounter sampling efficiency issues, especially when group-level variance parameters are small or data is limited. This creates a “funnel” in the posterior distribution that’s difficult for the sampler to navigate efficiently. Non-centered parameterization addresses this by reparameterizing individual parameters as standardized deviations from the group mean: Instead of: θᵢ ~ Normal(μ, σ) We use: θᵢ = μ + σ · zᵢ, where zᵢ ~ Normal(0, 1) This is conceptually similar to when we z-score variables in regression models. This approach separates the sampling of the standardized individual parameters (zᵢ) from the group-level parameters (μ and σ), improving sampling efficiency. The transformation between these parameterizations is invertible, so the models are equivalent, but the non-centered version often performs better computationally. In our code, we implement this by: Sampling standardized individual parameters (biasID_z, betaID_z) Multiplying by group SD and adding group mean to get individual parameters # Stan model for multilevel memory agent with non-centered parameterization stan_model_nc &lt;- &quot; // Multilevel Memory Agent Model (Non-Centered Parameterization) // functions{ real normal_lb_rng(real mu, real sigma, real lb) { real p = normal_cdf(lb | mu, sigma); // cdf for bounds real u = uniform_rng(p, 1); return (sigma * inv_Phi(u)) + mu; // inverse cdf for value } } // The input data for the model data { int&lt;lower = 1&gt; trials; // Number of trials per agent int&lt;lower = 1&gt; agents; // Number of agents array[trials, agents] int h; // Memory agent choices array[trials, agents] int other; // Opponent (random agent) choices } // Parameters to be estimated parameters { // Population-level parameters real biasM; // Mean of baseline bias real&lt;lower = 0&gt; biasSD; // SD of baseline bias real betaM; // Mean of memory sensitivity real&lt;lower = 0&gt; betaSD; // SD of memory sensitivity // Standardized individual parameters (non-centered parameterization) vector[agents] biasID_z; // Standardized individual bias parameters vector[agents] betaID_z; // Standardized individual beta parameters } // Transformed parameters (derived quantities) transformed parameters { // Memory state for each agent and trial array[trials, agents] real memory; // Individual parameters (constructed from non-centered parameterization) vector[agents] biasID; vector[agents] betaID; // Calculate memory states based on opponent&#39;s choices for (agent in 1:agents){ for (trial in 1:trials){ // Initial memory state (no prior information) if (trial == 1) { memory[trial, agent] = 0.5; } // Update memory based on opponent&#39;s choices if (trial &lt; trials){ // Simple averaging memory update memory[trial + 1, agent] = memory[trial, agent] + ((other[trial, agent] - memory[trial, agent]) / trial); // Handle edge cases to avoid numerical issues if (memory[trial + 1, agent] == 0){memory[trial + 1, agent] = 0.01;} if (memory[trial + 1, agent] == 1){memory[trial + 1, agent] = 0.99;} } } } // Construct individual parameters from non-centered parameterization biasID = biasM + biasID_z * biasSD; betaID = betaM + betaID_z * betaSD; } // Model definition model { // Population-level priors target += normal_lpdf(biasM | 0, 1); target += normal_lpdf(biasSD | 0, .3) - normal_lccdf(0 | 0, .3); // Half-normal target += normal_lpdf(betaM | 0, .3); target += normal_lpdf(betaSD | 0, .3) - normal_lccdf(0 | 0, .3); // Half-normal // Standardized individual parameters (non-centered parameterization) target += std_normal_lpdf(biasID_z); // Standard normal prior for z-scores target += std_normal_lpdf(betaID_z); // Standard normal prior for z-scores // Likelihood for (agent in 1:agents){ for (trial in 1:trials){ target += bernoulli_logit_lpmf(h[trial,agent] | biasID[agent] + logit(memory[trial, agent]) * betaID[agent]); } } } // Generated quantities for model checking and predictions generated quantities{ // Prior samples for checking real biasM_prior; real&lt;lower=0&gt; biasSD_prior; real betaM_prior; real&lt;lower=0&gt; betaSD_prior; real bias_prior; real beta_prior; // Predictive simulations with different memory values (individual level) array[agents] int&lt;lower=0, upper = trials&gt; prior_preds0; // No memory effect (memory=0) array[agents] int&lt;lower=0, upper = trials&gt; prior_preds1; // Neutral memory (memory=0.5) array[agents] int&lt;lower=0, upper = trials&gt; prior_preds2; // Strong memory (memory=1) array[agents] int&lt;lower=0, upper = trials&gt; posterior_preds0; array[agents] int&lt;lower=0, upper = trials&gt; posterior_preds1; array[agents] int&lt;lower=0, upper = trials&gt; posterior_preds2; // Generate prior samples biasM_prior = normal_rng(0,1); biasSD_prior = normal_lb_rng(0,0.3,0); betaM_prior = normal_rng(0,1); betaSD_prior = normal_lb_rng(0,0.3,0); bias_prior = normal_rng(biasM_prior, biasSD_prior); beta_prior = normal_rng(betaM_prior, betaSD_prior); // Generate predictions for each agent for (agent in 1:agents){ // Prior predictive checks prior_preds0[agent] = binomial_rng(trials, inv_logit(bias_prior + 0 * beta_prior)); prior_preds1[agent] = binomial_rng(trials, inv_logit(bias_prior + 1 * beta_prior)); prior_preds2[agent] = binomial_rng(trials, inv_logit(bias_prior + 2 * beta_prior)); // Posterior predictive checks posterior_preds0[agent] = binomial_rng(trials, inv_logit(biasM + biasID[agent] + 0 * (betaM + betaID[agent]))); posterior_preds1[agent] = binomial_rng(trials, inv_logit(biasM + biasID[agent] + 1 * (betaM + betaID[agent]))); posterior_preds2[agent] = binomial_rng(trials, inv_logit(biasM + biasID[agent] + 2 * (betaM + betaID[agent]))); } } &quot; # Write the Stan model to a file write_stan_file( stan_model_nc, dir = &quot;stan/&quot;, basename = &quot;W6_MultilevelMemory_nc.stan&quot; ) # File path for saved model model_file &lt;- &quot;simmodels/W6_MultilevelMemory_noncentered.RDS&quot; # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(model_file)) { # Compile the model file &lt;- file.path(&quot;stan/W6_MultilevelMemory_nc.stan&quot;) mod_nc &lt;- cmdstan_model(file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) # Sample from the posterior distribution samples_mlvl_nc &lt;- mod_nc$sample( data = data_memory, seed = 123, chains = 2, parallel_chains = 2, threads_per_chain = 1, iter_warmup = 2000, iter_sampling = 2000, refresh = 500, max_treedepth = 20, adapt_delta = 0.99 ) # Save the model results samples_mlvl_nc$save_object(file = model_file) cat(&quot;Generated new model fit and saved to&quot;, model_file, &quot;\\n&quot;) } else { # Load existing results samples_mlvl_nc &lt;- readRDS(model_file) cat(&quot;Loaded existing model fit from&quot;, model_file, &quot;\\n&quot;) } 7.12.4 Assessing multilevel memory # Check if samples_biased exists if (!exists(&quot;samples_mlvl_nc&quot;)) { cat(&quot;Loading multilevel non centered model samples...\\n&quot;) samples_mlvl_nc &lt;- readRDS(&quot;simmodels/W6_MultilevelMemory_noncentered.RDS&quot;) cat(&quot;Available parameters:&quot;, paste(colnames(as_draws_df(samples_mlvl_nc$draws())), collapse=&quot;, &quot;), &quot;\\n&quot;) } # Show summary statistics for key parameters print(samples_mlvl_nc$summary(c(&quot;biasM&quot;, &quot;betaM&quot;, &quot;biasSD&quot;, &quot;betaSD&quot;))) ## # A tibble: 4 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 biasM 0.407 0.403 0.0800 0.0802 0.280 0.542 1.00 1343. 1892. ## 2 betaM 1.16 1.17 0.0695 0.0694 1.05 1.28 1.00 1431. 2057. ## 3 biasSD 0.233 0.233 0.0660 0.0646 0.127 0.338 1.00 723. 830. ## 4 betaSD 0.379 0.378 0.0468 0.0457 0.306 0.460 1.00 1798. 2762. # Extract posterior draws for analysis draws_df &lt;- as_draws_df(samples_mlvl_nc$draws()) # Create trace plots to check convergence p1 &lt;- mcmc_trace(draws_df, pars = c(&quot;biasM&quot;, &quot;biasSD&quot;, &quot;betaM&quot;, &quot;betaSD&quot;)) + theme_classic() + ggtitle(&quot;Trace Plots for Population Parameters&quot;) # Show trace plots p1 # Create prior-posterior update plots create_density_plot &lt;- function(param, true_value, title) { prior_name &lt;- paste0(param, &quot;_prior&quot;) ggplot(draws_df) + geom_histogram(aes(get(param)), fill = &quot;blue&quot;, alpha = 0.3) + geom_histogram(aes(get(prior_name)), fill = &quot;red&quot;, alpha = 0.3) + geom_vline(xintercept = true_value, linetype = &quot;dashed&quot;) + labs(title = title, subtitle = &quot;Blue: posterior, Red: prior, Dashed: true value&quot;, x = param, y = &quot;Density&quot;) + theme_classic() } # Create individual plots p_biasM &lt;- create_density_plot(&quot;biasM&quot;, biasM, &quot;Population Mean Bias&quot;) p_biasSD &lt;- create_density_plot(&quot;biasSD&quot;, biasSD, &quot;Population SD of Bias&quot;) p_betaM &lt;- create_density_plot(&quot;betaM&quot;, betaM, &quot;Population Mean Beta&quot;) p_betaSD &lt;- create_density_plot(&quot;betaSD&quot;, betaSD, &quot;Population SD of Beta&quot;) # Show them in a grid (p_biasM + p_biasSD) / (p_betaM + p_betaSD) # Show correlations p1 &lt;- ggplot(draws_df, aes(biasM, biasSD, group = .chain, color = .chain)) + geom_point(alpha = 0.1) + theme_classic() p2 &lt;- ggplot(draws_df, aes(betaM, betaSD, group = .chain, color = .chain)) + geom_point(alpha = 0.1) + theme_classic() p3 &lt;- ggplot(draws_df, aes(biasM, betaM, group = .chain, color = .chain)) + geom_point(alpha = 0.1) + theme_classic() p4 &lt;- ggplot(draws_df, aes(biasSD, betaSD, group = .chain, color = .chain)) + geom_point(alpha = 0.1) + theme_classic() p1 + p2 + p3 + p4 # Create posterior predictive check plots p1 &lt;- ggplot(draws_df) + geom_histogram(aes(`prior_preds0[1]`), fill = &quot;red&quot;, alpha = 0.3, bins = 30) + geom_histogram(aes(`posterior_preds0[1]`), fill = &quot;blue&quot;, alpha = 0.3, bins = 30) + geom_histogram(aes(`posterior_preds1[1]`), fill = &quot;green&quot;, alpha = 0.3, bins = 30) + geom_histogram(aes(`posterior_preds2[1]`), fill = &quot;purple&quot;, alpha = 0.3, bins = 30) + labs(title = &quot;Prior and Posterior Predictive Distributions&quot;, subtitle = &quot;Red: prior, Blue: no memory effect, Green: neutral memory, Purple: strong memory&quot;, x = &quot;Predicted Right Choices (out of 120)&quot;, y = &quot;Count&quot;) + theme_classic() # Display plots p1 # Individual-level parameter recovery # Extract individual parameters for a sample of agents sample_agents &lt;- sample(1:agents, 100) sample_data &lt;- d %&gt;% filter(agent %in% sample_agents, trial == 1) %&gt;% dplyr::select(agent, bias, beta) # Extract posterior means for individual agents bias_means &lt;- c() beta_means &lt;- c() for (i in sample_agents) { bias_means[i] &lt;- mean(draws_df[[paste0(&quot;biasID[&quot;, i, &quot;]&quot;)]]) beta_means[i] &lt;- mean(draws_df[[paste0(&quot;betaID[&quot;, i, &quot;]&quot;)]]) } # Create comparison data comparison_data &lt;- tibble( agent = sample_agents, true_bias = sample_data$bias, est_bias = bias_means[sample_agents], true_beta = sample_data$beta, est_beta = beta_means[sample_agents] ) # Plot comparison p1 &lt;- ggplot(comparison_data, aes(true_bias, est_bias)) + geom_point(size = 3) + geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;) + geom_smooth(method = lm) + labs(title = &quot;Bias Parameter Recovery&quot;, x = &quot;True Bias&quot;, y = &quot;Estimated Bias&quot;) + theme_classic() p2 &lt;- ggplot(comparison_data, aes(true_beta, est_beta)) + geom_point(size = 3) + geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;) + geom_smooth(method = lm) + labs(title = &quot;Beta Parameter Recovery&quot;, x = &quot;True Beta&quot;, y = &quot;Estimated Beta&quot;) + theme_classic() # Display parameter recovery plots p1 + p2 7.12.5 Multilevel memory with correlation between parameters stan_model_nc_cor &lt;- &quot; // // This STAN model infers a random bias from a sequences of 1s and 0s (heads and tails) // functions{ real normal_lb_rng(real mu, real sigma, real lb) { real p = normal_cdf(lb | mu, sigma); // cdf for bounds real u = uniform_rng(p, 1); return (sigma * inv_Phi(u)) + mu; // inverse cdf for value } } // The input (data) for the model. data { int&lt;lower = 1&gt; trials; int&lt;lower = 1&gt; agents; array[trials, agents] int h; array[trials, agents] int other; } // The parameters accepted by the model. parameters { real biasM; real betaM; vector&lt;lower = 0&gt;[2] tau; matrix[2, agents] z_IDs; cholesky_factor_corr[2] L_u; } transformed parameters { array[trials, agents] real memory; matrix[agents,2] IDs; IDs = (diag_pre_multiply(tau, L_u) * z_IDs)&#39;; for (agent in 1:agents){ for (trial in 1:trials){ if (trial == 1) { memory[trial, agent] = 0.5; } if (trial &lt; trials){ memory[trial + 1, agent] = memory[trial, agent] + ((other[trial, agent] - memory[trial, agent]) / trial); if (memory[trial + 1, agent] == 0){memory[trial + 1, agent] = 0.01;} if (memory[trial + 1, agent] == 1){memory[trial + 1, agent] = 0.99;} } } } } // The model to be estimated. model { target += normal_lpdf(biasM | 0, 1); target += normal_lpdf(tau[1] | 0, .3) - normal_lccdf(0 | 0, .3); target += normal_lpdf(betaM | 0, .3); target += normal_lpdf(tau[2] | 0, .3) - normal_lccdf(0 | 0, .3); target += lkj_corr_cholesky_lpdf(L_u | 2); target += std_normal_lpdf(to_vector(z_IDs)); for (agent in 1:agents){ for (trial in 1:trials){ target += bernoulli_logit_lpmf(h[trial, agent] | biasM + IDs[agent, 1] + memory[trial, agent] * (betaM + IDs[agent, 2])); } } } generated quantities{ real biasM_prior; real&lt;lower=0&gt; biasSD_prior; real betaM_prior; real&lt;lower=0&gt; betaSD_prior; real bias_prior; real beta_prior; array[agents] int&lt;lower=0, upper = trials&gt; prior_preds0; array[agents] int&lt;lower=0, upper = trials&gt; prior_preds1; array[agents] int&lt;lower=0, upper = trials&gt; prior_preds2; array[agents] int&lt;lower=0, upper = trials&gt; posterior_preds0; array[agents] int&lt;lower=0, upper = trials&gt; posterior_preds1; array[agents] int&lt;lower=0, upper = trials&gt; posterior_preds2; biasM_prior = normal_rng(0,1); biasSD_prior = normal_lb_rng(0,0.3,0); betaM_prior = normal_rng(0,1); betaSD_prior = normal_lb_rng(0,0.3,0); bias_prior = normal_rng(biasM_prior, biasSD_prior); beta_prior = normal_rng(betaM_prior, betaSD_prior); for (i in 1:agents){ prior_preds0[i] = binomial_rng(trials, inv_logit(bias_prior + 0 * beta_prior)); prior_preds1[i] = binomial_rng(trials, inv_logit(bias_prior + 1 * beta_prior)); prior_preds2[i] = binomial_rng(trials, inv_logit(bias_prior + 2 * beta_prior)); posterior_preds0[i] = binomial_rng(trials, inv_logit(biasM + IDs[i,1] + 0 * (betaM + IDs[i,2]))); posterior_preds1[i] = binomial_rng(trials, inv_logit(biasM + IDs[i,1] + 1 * (betaM + IDs[i,2]))); posterior_preds2[i] = binomial_rng(trials, inv_logit(biasM + IDs[i,1] + 2 * (betaM + IDs[i,2]))); } } &quot; write_stan_file( stan_model_nc_cor, dir = &quot;stan/&quot;, basename = &quot;W6_MultilevelMemory_nc_cor.stan&quot;) model_file &lt;- &quot;simmodels/W6_MultilevelMemory_noncentered_cor.RDS&quot; # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(model_file)) { # Compile the model file &lt;- file.path(&quot;stan/W6_MultilevelMemory_nc_cor.stan&quot;) mod &lt;- cmdstan_model(file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) # Sample from the posterior distribution samples_mlvl_nc_cor &lt;- mod$sample( data = data_memory, seed = 123, chains = 2, parallel_chains = 2, threads_per_chain = 1, iter_warmup = 2000, iter_sampling = 2000, refresh = 500, max_treedepth = 20, adapt_delta = 0.99 ) # Save the model results samples_mlvl_nc_cor$save_object(file = model_file) cat(&quot;Generated new model fit and saved to&quot;, model_file, &quot;\\n&quot;) } else { # Load existing results samples_mlvl_nc_cor &lt;- readRDS(model_file) cat(&quot;Loaded existing model fit from&quot;, model_file, &quot;\\n&quot;) } 7.12.6 Assessing multilevel memory # Check if samples_biased exists if (!exists(&quot;samples_mlvl_nc_cor&quot;)) { cat(&quot;Loading multilevel non centered correlated model samples...\\n&quot;) samples_mlvl_nc_cor &lt;- readRDS(&quot;simmodels/W6_MultilevelMemory_noncentered_cor.RDS&quot;) cat(&quot;Available parameters:&quot;, paste(colnames(as_draws_df(samples_mlvl_nc_cor$draws())), collapse = &quot;, &quot;), &quot;\\n&quot;) } # Show summary statistics for key parameters print(samples_mlvl_nc_cor$summary(c(&quot;biasM&quot;, &quot;betaM&quot;, &quot;tau[1]&quot;, &quot;tau[2]&quot;, &quot;L_u[2,2]&quot;))) ## # A tibble: 5 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 biasM -0.536 -0.538 0.217 0.224 -0.885 -0.175 1.00 1812. 2564. ## 2 betaM 3.11 3.11 0.265 0.268 2.66 3.54 1.00 1522. 2250. ## 3 tau[1] 0.850 0.852 0.191 0.194 0.536 1.17 1.01 408. 950. ## 4 tau[2] 1.17 1.22 0.361 0.342 0.503 1.70 1.03 81.0 173. ## 5 L_u[2,2] 0.709 0.693 0.155 0.167 0.476 0.987 1.02 164. 511. # Extract posterior draws for analysis draws_df &lt;- as_draws_df(samples_mlvl_nc_cor$draws()) # Create trace plots to check convergence p1 &lt;- mcmc_trace(draws_df, pars = c(&quot;biasM&quot;, &quot;betaM&quot;, &quot;tau[1]&quot;, &quot;tau[2]&quot;, &quot;L_u[2,2]&quot;)) + theme_classic() + ggtitle(&quot;Trace Plots for Population Parameters&quot;) # Show trace plots p1 # Create prior-posterior update plots create_density_plot &lt;- function(param, true_value, title) { prior_name &lt;- paste0(param, &quot;_prior&quot;) param &lt;- case_when( param == &quot;biasSD&quot; ~ &quot;tau[1]&quot;, param == &quot;betaSD&quot; ~ &quot;tau[2]&quot;, TRUE ~ param ) ggplot(draws_df) + geom_histogram(aes(get(param)), fill = &quot;blue&quot;, alpha = 0.3) + geom_histogram(aes(get(prior_name)), fill = &quot;red&quot;, alpha = 0.3) + geom_vline(xintercept = true_value, linetype = &quot;dashed&quot;) + labs(title = title, subtitle = &quot;Blue: posterior, Red: prior, Dashed: true value&quot;, x = param, y = &quot;Density&quot;) + theme_classic() } # Create individual plots p_biasM &lt;- create_density_plot(&quot;biasM&quot;, biasM, &quot;Population Mean Bias&quot;) p_biasSD &lt;- create_density_plot(&quot;biasSD&quot;, biasSD, &quot;Population SD of Bias&quot;) p_betaM &lt;- create_density_plot(&quot;betaM&quot;, betaM, &quot;Population Mean Beta&quot;) p_betaSD &lt;- create_density_plot(&quot;betaSD&quot;, betaSD, &quot;Population SD of Beta&quot;) # Show them in a grid (p_biasM + p_biasSD) / (p_betaM + p_betaSD) # Show correlations between pop level parameters p1 &lt;- ggplot(draws_df, aes(biasM, biasSD, group = .chain, color = .chain)) + geom_point(alpha = 0.1) + theme_classic() p2 &lt;- ggplot(draws_df, aes(betaM, betaSD, group = .chain, color = .chain)) + geom_point(alpha = 0.1) + theme_classic() p3 &lt;- ggplot(draws_df, aes(biasM, betaM, group = .chain, color = .chain)) + geom_point(alpha = 0.1) + theme_classic() p4 &lt;- ggplot(draws_df, aes(biasSD, betaSD, group = .chain, color = .chain)) + geom_point(alpha = 0.1) + theme_classic() p1 + p2 + p3 + p4 # Show correlation between individual level parameters # Function to convert Cholesky factor to correlation matrix chol_to_corr &lt;- function(L) { # L is lower triangular cholesky factor # For 2x2 matrix, correlation is L[2,1] # We assume the input is a 2x2 cholesky factor where L[1,1] and L[2,2] are ignored L_full &lt;- matrix(0, 2, 2) L_full[1,1] &lt;- 1 L_full[2,1] &lt;- L[1] L_full[2,2] &lt;- sqrt(1 - L[1]^2) # Correlation = L * L^T corr &lt;- L_full %*% t(L_full) return(corr[1,2]) # Return correlation between dimension 1 and 2 } # Extract the Cholesky factor from posterior samples posterior_L &lt;- draws_df %&gt;% dplyr::select(starts_with(&quot;L_u[2,1]&quot;)) # This is the cholesky factor element for correlation # Convert to correlation values posterior_corr &lt;- posterior_L %&gt;% mutate(correlation = `L_u[2,1]`) # For 2×2 case, directly using the parameter works # Generate prior samples from LKJ distribution (approximated via a beta) n_prior_samples &lt;- nrow(posterior_corr) prior_corr &lt;- tibble( correlation = 2 * rbeta(n_prior_samples, 2, 2) - 1 # Scale beta to [-1,1] ) # Combine for plotting plot_data &lt;- bind_rows( mutate(posterior_corr, type = &quot;Posterior&quot;), mutate(prior_corr, type = &quot;Prior&quot;) ) # Create the visualization ggplot(plot_data, aes(x = correlation, fill = type)) + geom_density(alpha = 0.5) + geom_vline(xintercept = 0, linetype = &quot;dashed&quot;, color = &quot;gray50&quot;) + scale_fill_manual(values = c(&quot;Prior&quot; = &quot;red&quot;, &quot;Posterior&quot; = &quot;blue&quot;)) + labs( title = &quot;Prior vs Posterior: Correlation Between Bias and Beta Parameters&quot;, subtitle = &quot;LKJ(2) prior vs posterior correlation distribution&quot;, x = &quot;Correlation Coefficient&quot;, y = &quot;Density&quot;, fill = &quot;Distribution&quot; ) + coord_cartesian(xlim = c(-1, 1)) + theme_minimal() + annotate(&quot;text&quot;, x = 0.2, y = Inf, label = &quot;Negative correlation suggests\\ntradeoff between bias and\\nmemory sensitivity parameters&quot;, vjust = 2, hjust = 0, size = 3.5) # Create posterior predictive check plots p1 &lt;- ggplot(draws_df) + geom_histogram(aes(`prior_preds0[1]`), fill = &quot;red&quot;, alpha = 0.3, bins = 30) + geom_histogram(aes(`posterior_preds0[1]`), fill = &quot;blue&quot;, alpha = 0.3, bins = 30) + geom_histogram(aes(`posterior_preds1[1]`), fill = &quot;green&quot;, alpha = 0.3, bins = 30) + geom_histogram(aes(`posterior_preds2[1]`), fill = &quot;purple&quot;, alpha = 0.3, bins = 30) + labs(title = &quot;Prior and Posterior Predictive Distributions&quot;, subtitle = &quot;Red: prior, Blue: no memory effect, Green: neutral memory, Purple: strong memory&quot;, x = &quot;Predicted Right Choices (out of 120)&quot;, y = &quot;Count&quot;) + theme_classic() # Display plots p1 # Individual-level parameter recovery # Extract individual parameters for a sample of agents sample_agents &lt;- sample(1:agents, 100) sample_data &lt;- d %&gt;% filter(agent %in% sample_agents, trial == 1) %&gt;% dplyr::select(agent, bias, beta) # Extract posterior means for individual agents bias_means &lt;- c() beta_means &lt;- c() for (i in sample_agents) { bias_means[i] &lt;- mean(draws_df[[paste0(&quot;z_IDs[1,&quot;, i, &quot;]&quot;)]]) beta_means[i] &lt;- mean(draws_df[[paste0(&quot;z_IDs[2,&quot;, i, &quot;]&quot;)]]) } # Create comparison data comparison_data &lt;- tibble( agent = sample_agents, true_bias = scale(sample_data$bias), est_bias = scale(bias_means[sample_agents]), true_beta = scale(sample_data$beta), est_beta = scale(beta_means[sample_agents]) ) # Plot comparison p1 &lt;- ggplot(comparison_data, aes(true_bias, est_bias)) + geom_point(size = 3) + geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;) + geom_smooth(method = lm) + labs(title = &quot;Bias Parameter Recovery&quot;, x = &quot;Standardized True Bias&quot;, y = &quot;Standardized Estimated Bias&quot;) + theme_classic() p2 &lt;- ggplot(comparison_data, aes(true_beta, est_beta)) + geom_point(size = 3) + geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;) + geom_smooth(method = lm) + labs(title = &quot;Beta Parameter Recovery&quot;, x = &quot;Standardized True Beta&quot;, y = &quot;Standardized Estimated Beta&quot;) + theme_classic() # Display parameter recovery plots p1 + p2 7.12.7 # Load both models centered_model &lt;- readRDS(&quot;simmodels/W6_MultilevelMemory_centered.RDS&quot;) noncentered_model &lt;- readRDS(&quot;simmodels/W6_MultilevelMemory_noncentered.RDS&quot;) # Function to extract divergences and tree depths extract_diagnostics &lt;- function(model_fit) { draws &lt;- as_draws_df(model_fit$draws()) # Extract diagnostic information diagnostics &lt;- tibble( model = model_fit$metadata()$id, divergent = sum(draws$.divergent), max_treedepth = sum(draws$.treedepth &gt;= 10), n_draws = nrow(draws) ) return(diagnostics) } # Get diagnostics for both models centered_diag &lt;- extract_diagnostics(centered_model) noncentered_diag &lt;- extract_diagnostics(noncentered_model) # Combine diagnostics diagnostics &lt;- bind_rows(centered_diag, noncentered_diag) # Create diagnostic summary table diagnostics_table &lt;- diagnostics %&gt;% mutate( divergent_pct = round(divergent / n_draws * 100, 2), max_treedepth_pct = round(max_treedepth / n_draws * 100, 2) ) %&gt;% dplyr::select(model, divergent, divergent_pct, max_treedepth, max_treedepth_pct) # Display diagnostics table knitr::kable(diagnostics_table, caption = &quot;Sampling Diagnostics Comparison: Centered vs. Non-Centered&quot;, col.names = c(&quot;Model&quot;, &quot;Divergent Transitions&quot;, &quot;% Divergent&quot;, &quot;Max Tree Depth&quot;, &quot;% Max Tree&quot;)) (#tab:compare_parameterizations)Sampling Diagnostics Comparison: Centered vs. Non-Centered Model Divergent Transitions % Divergent Max Tree Depth % Max Tree 1 0 0 0 0 2 0 0 0 0 1 0 0 0 0 2 0 0 0 0 # Extract summary statistics for key parameters from both models centered_summary &lt;- centered_model$summary(c(&quot;biasM&quot;, &quot;biasSD&quot;, &quot;betaM&quot;, &quot;betaSD&quot;)) noncentered_summary &lt;- noncentered_model$summary(c(&quot;biasM&quot;, &quot;biasSD&quot;, &quot;betaM&quot;, &quot;betaSD&quot;)) # Combine and format for comparison parameter_comparison &lt;- bind_rows( mutate(centered_summary, model = &quot;Centered&quot;), mutate(noncentered_summary, model = &quot;Non-Centered&quot;) ) # Display parameter comparison knitr::kable(parameter_comparison %&gt;% dplyr::select(model, variable, mean, q5, q95), caption = &quot;Parameter Estimates: Centered vs. Non-Centered&quot;, col.names = c(&quot;Model&quot;, &quot;Parameter&quot;, &quot;Mean&quot;, &quot;5% Quantile&quot;, &quot;95% Quantile&quot;)) (#tab:compare_parameterizations)Parameter Estimates: Centered vs. Non-Centered Model Parameter Mean 5% Quantile 95% Quantile Centered biasM 0.4157257 0.2854175 0.5518449 Centered biasSD 0.2409784 0.1410021 0.3429320 Centered betaM 1.1583858 1.0421215 1.2717900 Centered betaSD 0.3811980 0.3079295 0.4605237 Non-Centered biasM 0.4071065 0.2802553 0.5417424 Non-Centered biasSD 0.2331416 0.1272396 0.3384460 Non-Centered betaM 1.1646613 1.0494895 1.2778800 Non-Centered betaSD 0.3793339 0.3056419 0.4599512 # Visual comparison of posterior distributions # Extract draws from both models centered_draws &lt;- as_draws_df(centered_model$draws()) %&gt;% dplyr::select(biasM, biasSD, betaM, betaSD) %&gt;% mutate(model = &quot;Centered&quot;) noncentered_draws &lt;- as_draws_df(noncentered_model$draws()) %&gt;% dplyr::select(biasM, biasSD, betaM, betaSD) %&gt;% mutate(model = &quot;Non-Centered&quot;) # Combine draws combined_draws &lt;- bind_rows(centered_draws, noncentered_draws) # Create comparison plots compare_density &lt;- function(param, true_value) { ggplot(combined_draws, aes(x = .data[[param]], fill = model)) + geom_histogram(alpha = 0.5) + geom_vline(xintercept = true_value, linetype = &quot;dashed&quot;) + labs( title = paste(&quot;Posterior Distribution Comparison:&quot;, param), subtitle = &quot;Centered vs. Non-Centered Parameterization&quot;, x = param, y = &quot;Density&quot; ) + theme_classic() } # Create comparison plots for each parameter p1 &lt;- compare_density(&quot;biasM&quot;, biasM) p2 &lt;- compare_density(&quot;biasSD&quot;, biasSD) p3 &lt;- compare_density(&quot;betaM&quot;, betaM) p4 &lt;- compare_density(&quot;betaSD&quot;, betaSD) # Display comparison plots p1 + p2 p3 + p4 7.13 Comparing Pooling Approaches To better understand the trade-offs between different modeling approaches, let’s implement and compare three ways of handling individual differences: No Pooling: Separate models for each agent with no sharing of information Complete Pooling: A single model with identical parameters for all agents Partial Pooling: Our multilevel approach that balances individual and group information Each approach has advantages and disadvantages: Approach Advantages Disadvantages No Pooling Captures all individual differences Unstable for agents with little data; Can’t generalize Complete Pooling Stable estimates; Simple Ignores individual differences Partial Pooling Balances individual vs. group data; Better for small samples More complex; Requires careful implementation Let’s compare how these approaches perform with our memory agent data # First we&#39;ll implement the no-pooling model stan_model_nopooling &lt;- &quot; // Memory Agent Model - No Pooling Approach // (Separate parameters for each agent, no sharing of information) data { int&lt;lower = 1&gt; trials; // Number of trials per agent int&lt;lower = 1&gt; agents; // Number of agents array[trials, agents] int h; // Memory agent choices array[trials, agents] int other; // Opponent (random agent) choices } parameters { // Individual parameters for each agent (no population structure) array[agents] real bias; // Individual bias parameters array[agents] real beta; // Individual beta parameters } transformed parameters { // Memory state for each agent and trial array[trials, agents] real memory; // Calculate memory states for (agent in 1:agents){ for (trial in 1:trials){ if (trial == 1) { memory[trial, agent] = 0.5; } if (trial &lt; trials){ memory[trial + 1, agent] = memory[trial, agent] + ((other[trial, agent] - memory[trial, agent]) / trial); if (memory[trial + 1, agent] == 0){memory[trial + 1, agent] = 0.01;} if (memory[trial + 1, agent] == 1){memory[trial + 1, agent] = 0.99;} } } } } model { // Separate priors for each agent (no pooling) for (agent in 1:agents) { target += normal_lpdf(bias[agent] | 0, 1); target += normal_lpdf(beta[agent] | 0, 1); } // Likelihood for (agent in 1:agents){ for (trial in 1:trials){ target += bernoulli_logit_lpmf(h[trial, agent] | bias[agent] + memory[trial, agent] * beta[agent]); } } } generated quantities{ // Predictions with different memory values array[agents] int&lt;lower=0, upper = trials&gt; posterior_preds0; array[agents] int&lt;lower=0, upper = trials&gt; posterior_preds1; array[agents] int&lt;lower=0, upper = trials&gt; posterior_preds2; // Generate predictions for (agent in 1:agents){ posterior_preds0[agent] = binomial_rng(trials, inv_logit(bias[agent] + 0 * beta[agent])); posterior_preds1[agent] = binomial_rng(trials, inv_logit(bias[agent] + 1 * beta[agent])); posterior_preds2[agent] = binomial_rng(trials, inv_logit(bias[agent] + 2 * beta[agent])); } } &quot; # Now implement the complete pooling model stan_model_fullpooling &lt;- &quot; // Memory Agent Model - Complete Pooling Approach // (Single set of parameters shared by all agents) data { int&lt;lower = 1&gt; trials; // Number of trials per agent int&lt;lower = 1&gt; agents; // Number of agents array[trials, agents] int h; // Memory agent choices array[trials, agents] int other; // Opponent (random agent) choices } parameters { // Single set of parameters shared by all agents real bias; // Shared bias parameter real beta; // Shared beta parameter } transformed parameters { // Memory state for each agent and trial array[trials, agents] real memory; // Calculate memory states for (agent in 1:agents){ for (trial in 1:trials){ if (trial == 1) { memory[trial, agent] = 0.5; } if (trial &lt; trials){ memory[trial + 1, agent] = memory[trial, agent] + ((other[trial, agent] - memory[trial, agent]) / trial); if (memory[trial + 1, agent] == 0){memory[trial + 1, agent] = 0.01;} if (memory[trial + 1, agent] == 1){memory[trial + 1, agent] = 0.99;} } } } } model { // Priors for shared parameters target += normal_lpdf(bias | 0, 1); target += normal_lpdf(beta | 0, 1); // Likelihood (same parameters for all agents) for (agent in 1:agents){ for (trial in 1:trials){ target += bernoulli_logit_lpmf(h[trial, agent] | bias + memory[trial, agent] * beta); } } } generated quantities{ // Single set of predictions for all agents int&lt;lower=0, upper = trials&gt; posterior_preds0; int&lt;lower=0, upper = trials&gt; posterior_preds1; int&lt;lower=0, upper = trials&gt; posterior_preds2; // Generate predictions posterior_preds0 = binomial_rng(trials, inv_logit(bias + 0 * beta)); posterior_preds1 = binomial_rng(trials, inv_logit(bias + 1 * beta)); posterior_preds2 = binomial_rng(trials, inv_logit(bias + 2 * beta)); } &quot; # Write the models to files write_stan_file(stan_model_nopooling, dir = &quot;stan/&quot;, basename = &quot;W5_MultilevelMemory_nopooling.stan&quot;) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W5_MultilevelMemory_nopooling.stan&quot; write_stan_file(stan_model_fullpooling, dir = &quot;stan/&quot;, basename = &quot;W5_MultilevelMemory_fullpooling.stan&quot;) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W5_MultilevelMemory_fullpooling.stan&quot; # Define file paths for saved models file_nopooling_results &lt;- &quot;simmodels/W5_MultilevelMemory_nopooling.RDS&quot; file_fullpooling_results &lt;- &quot;simmodels/W5_MultilevelMemory_fullpooling.RDS&quot; # Fit no pooling model if needed if (regenerate_simulations || !file.exists(file_nopooling_results)) { # Compile the models file_nopooling &lt;- file.path(&quot;stan/W5_MultilevelMemory_nopooling.stan&quot;) mod_nopooling &lt;- cmdstan_model(file_nopooling, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) samples_nopooling &lt;- mod_nopooling$sample( data = data_memory, seed = 123, chains = 2, parallel_chains = 2, threads_per_chain = 2, iter_warmup = 2000, iter_sampling = 2000, refresh = 500, max_treedepth = 20, adapt_delta = 0.99 ) samples_nopooling$save_object(file = file_nopooling_results) cat(&quot;Generated new no-pooling model fit\\n&quot;) } else { cat(&quot;Loading existing no-pooling model fit\\n&quot;) } ## Loading existing no-pooling model fit # Fit full pooling model if needed if (regenerate_simulations || !file.exists(file_fullpooling_results)) { file_fullpooling &lt;- file.path(&quot;stan/W5_MultilevelMemory_fullpooling.stan&quot;) mod_fullpooling &lt;- cmdstan_model(file_fullpooling, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) samples_fullpooling &lt;- mod_fullpooling$sample( data = data_memory, seed = 123, chains = 2, parallel_chains = 2, threads_per_chain = 2, iter_warmup = 2000, iter_sampling = 2000, refresh = 500, max_treedepth = 20, adapt_delta = 0.99 ) samples_fullpooling$save_object(file = file_fullpooling_results) cat(&quot;Generated new full-pooling model fit\\n&quot;) } else { cat(&quot;Loading existing full-pooling model fit\\n&quot;) } ## Loading existing full-pooling model fit 7.14 Comparing Pooling Approaches # Load required packages # Function to simulate and visualize shrinkage from different pooling approaches visualize_pooling_approaches &lt;- function() { # First, simulate some data for demonstration set.seed(42) n_groups &lt;- 20 n_per_group &lt;- c(5, 10, 20, 50) # Different group sizes # True group means (population distribution) true_pop_mean &lt;- 0 true_pop_sd &lt;- 1 true_group_means &lt;- rnorm(n_groups, true_pop_mean, true_pop_sd) # Function to simulate data and estimates for one scenario simulate_one_scenario &lt;- function(n_per_group) { # Create data frame results &lt;- tibble( group_id = factor(1:n_groups), true_mean = true_group_means, n_obs = n_per_group ) # Simulate observed data observed_data &lt;- map2_dfr(1:n_groups, n_per_group, function(group, n) { tibble( group_id = factor(group), value = rnorm(n, true_group_means[group], 1) # Within-group SD = 1 ) }) # Calculate no-pooling estimates (just the group means) no_pooling &lt;- observed_data %&gt;% group_by(group_id) %&gt;% summarize(estimate = mean(value)) %&gt;% pull(estimate) # Calculate full-pooling estimate (grand mean) full_pooling &lt;- mean(observed_data$value) # Calculate partial-pooling estimates (empirical Bayes approach) # This is a simplified version of what happens in a multilevel model grand_mean &lt;- mean(observed_data$value) group_means &lt;- observed_data %&gt;% group_by(group_id) %&gt;% summarize(mean = mean(value), n = n()) # Calculate group variances and total variance components group_var &lt;- var(group_means$mean) within_var &lt;- mean((observed_data %&gt;% group_by(group_id) %&gt;% summarize(var = var(value)) %&gt;% pull(var))) # Calculate shrinkage factor for each group partial_pooling &lt;- map_dbl(1:n_groups, function(i) { group_mean &lt;- group_means$mean[i] group_size &lt;- group_means$n[i] # Optimal shrinkage factor lambda &lt;- within_var / (within_var + group_var * group_size) # Shrunk estimate lambda * grand_mean + (1 - lambda) * group_mean }) # Add estimates to results results &lt;- results %&gt;% mutate( no_pooling = no_pooling, full_pooling = full_pooling, partial_pooling = partial_pooling, # Calculate absolute errors no_pooling_error = abs(no_pooling - true_mean), full_pooling_error = abs(full_pooling - true_mean), partial_pooling_error = abs(partial_pooling - true_mean), scenario = paste(n_per_group, &quot;observations per group&quot;) ) return(results) } # Simulate all scenarios all_results &lt;- map_dfr(n_per_group, simulate_one_scenario) # Convert to long format for plotting results_long &lt;- all_results %&gt;% pivot_longer( cols = c(no_pooling, full_pooling, partial_pooling), names_to = &quot;method&quot;, values_to = &quot;estimate&quot; ) %&gt;% mutate( method = factor(method, levels = c(&quot;no_pooling&quot;, &quot;partial_pooling&quot;, &quot;full_pooling&quot;), labels = c(&quot;No Pooling&quot;, &quot;Partial Pooling&quot;, &quot;Full Pooling&quot;)) ) # Plot 1: Shrinkage visualization p1 &lt;- ggplot(results_long, aes(x = true_mean, y = estimate, color = method)) + geom_point(alpha = 0.7) + geom_abline(slope = 1, intercept = 0, linetype = &quot;dashed&quot;) + geom_hline(yintercept = true_pop_mean, linetype = &quot;dotted&quot;) + facet_wrap(~scenario) + scale_color_manual(values = c(&quot;No Pooling&quot; = &quot;red&quot;, &quot;Partial Pooling&quot; = &quot;green&quot;, &quot;Full Pooling&quot; = &quot;blue&quot;)) + labs( title = &quot;Shrinkage Effects in Different Pooling Approaches&quot;, subtitle = &quot;Dashed line: perfect recovery; Dotted line: population mean&quot;, x = &quot;True Group Mean&quot;, y = &quot;Estimated Mean&quot;, color = &quot;Method&quot; ) + theme_minimal() # Calculate error metrics for each scenario and method error_summary &lt;- all_results %&gt;% group_by(scenario) %&gt;% summarize( No_Pooling_MSE = mean(no_pooling_error^2), Full_Pooling_MSE = mean(full_pooling_error^2), Partial_Pooling_MSE = mean(partial_pooling_error^2) ) %&gt;% pivot_longer( cols = contains(&quot;_MSE&quot;), names_to = &quot;method&quot;, values_to = &quot;mse&quot; ) %&gt;% mutate( method = gsub(&quot;_MSE&quot;, &quot;&quot;, method), method = gsub(&quot;_&quot;, &quot; &quot;, method) ) # Plot 2: Error comparison p2 &lt;- ggplot(error_summary, aes(x = scenario, y = mse, fill = method)) + geom_col(position = &quot;dodge&quot;) + scale_fill_manual(values = c(&quot;No Pooling&quot; = &quot;red&quot;, &quot;Partial Pooling&quot; = &quot;green&quot;, &quot;Full Pooling&quot; = &quot;blue&quot;)) + labs( title = &quot;Mean Squared Error by Pooling Approach&quot;, subtitle = &quot;Lower values indicate better parameter recovery&quot;, x = &quot;Scenario&quot;, y = &quot;Mean Squared Error&quot;, fill = &quot;Method&quot; ) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Plot 3: Shrinkage as a function of group size and distance from mean # Calculate shrinkage ratio shrinkage_data &lt;- all_results %&gt;% mutate( dist_from_mean = true_mean - true_pop_mean, # Shrinkage ratio: how much of the distance from population mean is preserved # 1 = no shrinkage, 0 = complete shrinkage to mean no_pool_shrinkage = (no_pooling - true_pop_mean) / dist_from_mean, full_pool_shrinkage = (full_pooling - true_pop_mean) / dist_from_mean, partial_pool_shrinkage = (partial_pooling - true_pop_mean) / dist_from_mean ) %&gt;% # Filter out cases where dist_from_mean is too close to zero filter(abs(dist_from_mean) &gt; 0.1) # Convert to long format shrinkage_long &lt;- shrinkage_data %&gt;% dplyr::select(group_id, scenario, n_obs, dist_from_mean, contains(&quot;_shrinkage&quot;)) %&gt;% pivot_longer( cols = contains(&quot;_shrinkage&quot;), names_to = &quot;method&quot;, values_to = &quot;shrinkage_ratio&quot; ) %&gt;% mutate( method = gsub(&quot;_shrinkage&quot;, &quot;&quot;, method), method = gsub(&quot;_&quot;, &quot; &quot;, method), method = factor(method, levels = c(&quot;no pool&quot;, &quot;partial pool&quot;, &quot;full pool&quot;), labels = c(&quot;No Pooling&quot;, &quot;Partial Pooling&quot;, &quot;Full Pooling&quot;)), # Clip extreme values for visualization shrinkage_ratio = pmin(pmax(shrinkage_ratio, -0.5), 1.5) ) # Plot shrinkage ratio p3 &lt;- ggplot(shrinkage_long, aes(x = abs(dist_from_mean), y = shrinkage_ratio, color = method)) + geom_point(alpha = 0.7) + geom_smooth(method = &quot;loess&quot;, se = FALSE) + geom_hline(yintercept = 1, linetype = &quot;dashed&quot;) + geom_hline(yintercept = 0, linetype = &quot;dotted&quot;) + facet_wrap(~scenario) + scale_color_manual(values = c(&quot;No Pooling&quot; = &quot;red&quot;, &quot;Partial Pooling&quot; = &quot;green&quot;, &quot;Full Pooling&quot; = &quot;blue&quot;)) + labs( title = &quot;Shrinkage Ratio by Distance from Population Mean&quot;, subtitle = &quot;1.0 = No shrinkage; 0.0 = Complete shrinkage to population mean&quot;, x = &quot;Distance from Population Mean&quot;, y = &quot;Shrinkage Ratio&quot;, color = &quot;Method&quot; ) + theme_minimal() # Return all plots together return(list( main_plot = p1, error_plot = p2, shrinkage_plot = p3 )) } # Generate the visualizations plots &lt;- visualize_pooling_approaches() # Display the plots plots$main_plot plots$error_plot plots$shrinkage_plot # Create combined visualization for conceptual understanding set.seed(123) # Generate data for a single example n_groups &lt;- 8 true_pop_mean &lt;- 0 true_pop_sd &lt;- 1 true_means &lt;- rnorm(n_groups, true_pop_mean, true_pop_sd) group_sizes &lt;- sample(c(3, 5, 10, 20, 30), n_groups, replace = TRUE) # Generate observations generate_group_data &lt;- function(group_id, true_mean, n_obs) { tibble( group = factor(group_id), true_mean = true_mean, n_obs = n_obs, value = rnorm(n_obs, true_mean, 1) ) } sim_data &lt;- map_dfr(1:n_groups, ~generate_group_data( ., true_means[.], group_sizes[.] )) # Calculate estimates estimates &lt;- sim_data %&gt;% group_by(group) %&gt;% summarize( n = n(), true_mean = first(true_mean), no_pooling = mean(value), full_pooling = mean(sim_data$value) ) %&gt;% mutate( # Simplified partial pooling calculation reliability = n / (n + 10), # 10 is arbitrary scaling factor for demonstration partial_pooling = reliability * no_pooling + (1 - reliability) * full_pooling ) # Convert to long format for visualization est_long &lt;- estimates %&gt;% pivot_longer( cols = c(no_pooling, partial_pooling, full_pooling), names_to = &quot;method&quot;, values_to = &quot;estimate&quot; ) %&gt;% mutate( method = factor(method, levels = c(&quot;no_pooling&quot;, &quot;partial_pooling&quot;, &quot;full_pooling&quot;), labels = c(&quot;No Pooling&quot;, &quot;Partial Pooling&quot;, &quot;Full Pooling&quot;)) ) # Create conceptual visualization conceptual_plot &lt;- ggplot(est_long, aes(x = reorder(group, true_mean), y = estimate, color = method)) + # Draw vertical lines showing shrinkage #geom_segment(data = est_long %&gt;% filter(method == &quot;No Pooling&quot;), # aes(xend = group, y = estimate, yend = full_pooling), # color = &quot;gray&quot;, linetype = &quot;dotted&quot;) + # Draw points for estimates geom_point(aes(size = n), alpha = 0.8) + # Draw true means geom_point(aes(y = true_mean), shape = 4, size = 3, color = &quot;black&quot;) + # Draw horizontal line for population mean geom_hline(yintercept = mean(sim_data$value), linetype = &quot;dashed&quot;, color = &quot;gray&quot;) + # Formatting scale_color_manual(values = c(&quot;No Pooling&quot; = &quot;red&quot;, &quot;Partial Pooling&quot; = &quot;green&quot;, &quot;Full Pooling&quot; = &quot;blue&quot;)) + labs( title = &quot;Conceptual Visualization of Shrinkage in Multilevel Modeling&quot;, subtitle = &quot;X = True group mean; Dotted lines show shrinkage; Point size = group sample size&quot;, x = &quot;Group&quot;, y = &quot;Estimate&quot;, color = &quot;Pooling Method&quot;, size = &quot;Group Size&quot; ) + theme_minimal() # Display conceptual plot conceptual_plot [MISSING: PARAMETER RECOVERY IN A MULTILEVEL FRAMEWORK (IND VS POP)] 7.15 Multilevel Modeling Cheatsheet Multilevel Model Visualization 7.15.1 When to Use Each Pooling Approach: Approach When to Use Advantages Disadvantages No Pooling Many observations per group; groups truly independent • Simple to implement• Captures all individual differences • Unstable with small sample sizes• Can’t predict for new individuals Full Pooling Few observations per group; minimal individual differences • Stable estimates• Simple model • Ignores individual differences• Can lead to poor predictions for outliers Partial Pooling Moderate observations per group; meaningful individual differences • Balances individual and group data• More accurate for small groups• Can predict for new individuals • More complex model• Requires careful implementation 7.15.2 Parameter Recovery Rules of Thumb: Group-level means (e.g., biasM) typically require fewer observations for good recovery than group-level variances (e.g., biasSD) Individual parameters undergo more shrinkage when: Group-level variance is small Individual data is limited Individual estimates are far from the group mean 7.16 Conclusion: The Power and Challenges of Multilevel Modeling In this chapter, we’ve explored how multilevel modeling provides a principled approach to analyzing data with hierarchical structure. By implementing models for both biased agents and memory agents, we’ve seen how to: Represent population-level distributions of parameters Allow for individual variations while maintaining population constraints Implement different parameterizations to improve sampling efficiency Assess model quality through various diagnostic techniques Multilevel modeling offers several key advantages for cognitive modeling (only some of which have been exemplified here): - Improved parameter estimation for individuals with limited data - Detection of population-level patterns while respecting individual differences - More efficient use of data through partial pooling of information - Capacity to model correlations between different cognitive parameters The practical implementation challenges we’ve encountered—such as sampling difficulties with correlated parameters and the need for non-centered parameterization—are common in cognitive modeling applications. Developing familiarity with these techniques prepares you for implementing more complex models in your own research. 7.17 Exercises (just some ideas) Parameter Recovery Analysis Simulate data with different levels of individual variability (try biasSD values of 0.05, 0.3, and 0.8). Fit the multilevel model to each dataset and assess how well individual and population parameters are recovered. How does the amount of individual variability affect the benefits of partial pooling? Model Comparison Challenge For the memory agent model, compare the predictive performance of: Full pooling (single parameters for all agents) No pooling (separate parameters for each agent) Partial pooling (hierarchical model) Use different amounts of data (60, 120 and 500 trials) to determine when each approach works best. Create a plot showing the relative advantage of each approach as data quantity changes. Extend the Model Modify the memory agent model to include a “forgetting rate” parameter that weights recent observations more heavily. Implement this as a multilevel parameter (varying across individuals). Does this additional parameter improve model fit? How does it correlate with the other parameters? Applied Modeling [MISSING] The file cognitive_data.csv contains real data from a sequential decision-making experiment. Apply the multilevel memory model to this dataset. Interpret the population-level parameters and identify any interesting individual differences. Create visualizations that communicate your findings effectively. Debugging Challenge [MISSING] The file problematic_model.stan contains a multilevel model with several implementation issues. Identify and fix the problems to get the model running efficiently. Common issues include poor parameterization, inefficient computation, or misspecified priors. "],["model-comparison-in-cognitive-science.html", "Chapter 8 Model Comparison in Cognitive Science 8.1 Learning Objectives 8.2 Why Compare Models? 8.3 Cross-Validation: The Foundation of Model Comparison 8.4 PSIS-LOO: An Efficient Approximation to LOO-CV 8.5 Simulation-Based Model Comparison 8.6 Implementing Models for Comparison 8.7 Fitting Models and Calculating Expected Log Predictive Density 8.8 Fitting the models to the data 8.9 Cross-Validation for Model Comparison 8.10 Formal Model Comparison 8.11 Limitations of Model Comparison Approaches 8.12 Exercises", " Chapter 8 Model Comparison in Cognitive Science Cognitive science aims to understand the processes that give rise to human thought and behavior. To do this effectively, we often create formal models that represent our hypotheses about these underlying processes. However, human cognition is complex, and multiple theoretical accounts might plausibly explain the same observed behaviors. This is where model comparison becomes essential. Model comparison is the principled evaluation of competing models to determine which best explains observed data. Model comparison techniques as described here balance a model’s ability to fit existing data against its ability to generalize to new observations, helping us avoid the trap of overfitting. But remember, model comparison is not a fail-safe procedure to determine which model embodies the truth, as always we need to be careful, tentative and open about the probabilistic and fallible nature of our inference. 8.1 Learning Objectives After completing this chapter, you will be able to: Implement cross-validation techniques for comparing cognitive models using Stan Calculate and interpret expected log predictive density (ELPD) scores Assess model predictions through posterior and prior predictive checks Understand the strengths and limitations of different model comparison approaches Apply these techniques to compare competing cognitive models using real data 8.2 Why Compare Models? Model comparison serves multiple purposes in cognitive science: Theory Testing: Different models often represent competing theoretical accounts of cognitive processes. Comparing their fit to data helps evaluate these theories. Parsimony: When multiple models can explain the data, more complex models should only be preferred if they are justified by better predictive performance. Generalization: By assessing how well different models predict new data, we can evaluate their ability to capture general patterns rather than just fitting to specific samples. Individual Differences: Model comparison can reveal whether different individuals or groups are better described by different cognitive strategies. This chapter demonstrates these principles using our matching pennies models as concrete examples. We’ll compare simple random choice models against more sophisticated memory-based approaches, showing how to rigorously evaluate which better explains observed behavior. 8.2.1 The Challenge of Model Selection Imagine having several models of what might be going on and wanting to know which is the best explanation of the data. For example: Are people more likely to use a memory strategy or a win-stay-lose-shift strategy? Are we justified in assuming that people react differently to losses than to wins? Would we be justified in assuming that capuchin monkeys and cognitive science students use the same model? Model comparison defines a broad range of practices aimed at identifying the best model for a given dataset. What “best” means is, however, a non-trivial question. Ideally, “best” would mean the model describing the mechanism that actually generated the data. However, knowing the truth is a tricky proposition and we need to use proxies. There are many of such proxies in the literature, for instance Bayes Factors (see Nicenboim et al 2023, https://vasishth.github.io/bayescogsci/book/ch-comparison.html). In this course, we rely on predictive performance - this helps combat overfitting, but has limitations we’ll discuss at the end. In other words, this chapter will assess models in terms of their (estimated) ability to predict new (test) data. Remember that predictive performance is a very useful tool, but not a magical solution. It allows us to combat overfitting to the training sample (your model snuggling to your data so much that it fits both signal and noise), but it has key limitations, which we will discuss at the end of the chapter. To learn how to make model comparison, in this chapter, we rely on our usual simulation based approach to ensure that the method is doing what we want. We simulate the behavior of biased agents playing against the memory agents. This provides us with data generated according to two different mechanisms: biased agents and memory agents. We can fit both models separately on each of the two sets of agents, so we can compare the relative performance of the two models: can we identify the true model generating the data (in a setup where truth is known)? This is what is usually called “model recovery” and complements nicely “parameter recovery”. In model recovery we assess whether we can identify the correct model, in parameter recovery we assess whether - once we know the correct model - we can identify the correct parameter values. Let’s get going. # Flag to control whether to regenerate simulations # Set this to TRUE when you need to rerun the models regenerate_simulations &lt;- TRUE # Load required packages pacman::p_load( tidyverse, # For data manipulation and visualization here, # For file path management posterior, # For working with posterior samples cmdstanr, # For interfacing with Stan brms, # For Bayesian regression models tidybayes, # For working with Bayesian samples loo, # For leave-one-out cross-validation patchwork, # For combining plots bayesplot, # For Bayesian visualization MASS ) # Set seed for reproducibility set.seed(123) # Generate demonstration data trials &lt;- 40 # Model 1: Random biased agent (80% right choice) biased_agent &lt;- rbinom(trials, 1, 0.8) cumulative_biased &lt;- cumsum(biased_agent) / seq_along(biased_agent) # Model 2: Memory-based agent (adjusts based on past patterns) memory_agent &lt;- rep(NA, trials) memory_agent[1] &lt;- rbinom(1, 1, 0.5) # First choice is random for (i in 2:trials) { # Agent uses memory of past choices with some randomness memory_agent[i] &lt;- rbinom(1, 1, 0.3 + 0.4 * mean(biased_agent[1:(i - 1)])) } cumulative_memory &lt;- cumsum(memory_agent) / seq_along(memory_agent) # Create plotting data model_data &lt;- tibble( trial = rep(1:trials, 2), choice = c(biased_agent, memory_agent), cumulative = c(cumulative_biased, cumulative_memory), model = rep(c(&quot;Biased Agent&quot;, &quot;Memory Agent&quot;), each = trials) ) # Generate simulated data that could come from either model observed_data &lt;- if (runif(1) &gt; 0.5) biased_agent else memory_agent observed_cumulative &lt;- cumsum(observed_data) / seq_along(observed_data) observed_df &lt;- tibble( trial = 1:trials, choice = observed_data, cumulative = observed_cumulative ) # Create the plot p1 &lt;- ggplot() + geom_jitter(data = observed_df %&gt;% filter(trial&gt;2), aes(x = trial, y = choice), height = 0.05, alpha = 0.7, color = &quot;black&quot;) + geom_line(data = model_data %&gt;% filter(trial&gt;2), aes(x = trial, y = cumulative, color = model, linetype = model)) + scale_color_brewer(palette = &quot;Set1&quot;) + labs( title = &quot;Two Competing Models of Decision Making&quot;, subtitle = &quot;The biased agent makes choices with fixed 80% bias toward right option, regardless of context \\nThe memory agent adjusts choices based on memory of opponent&#39;s previous moves \\nJust looking at the data (sampled at random from one of the two models) it&#39;s hard to identify the model generating it&quot;, x = &quot;Trial Number&quot;, y = &quot;Choice (0 = Left, 1 = Right)&quot;, color = &quot;Model&quot; ) + theme_minimal() + theme(legend.position = &quot;top&quot;) p1 In this example, we have: Model 1 (Biased Agent): Makes choices with a consistent 80% bias toward the right option Model 2 (Memory Agent): Adjusts choices based on memory of previous patterns The critical insight is that both models can produce similar-looking data, making it difficult to determine which cognitive process generated the observed behavior by simple visual inspection. Formal model comparison techniques give us principled ways to evaluate which model better explains the data while accounting for model complexity and generalization ability. 8.3 Cross-Validation: The Foundation of Model Comparison Cross-validation is a fundamental technique for comparing models based on their predictive performance. The core idea is simple: a good model should not only fit the observed data but also generalize well to new, unseen data. 8.3.1 The Problem of Overfitting When we fit a model to data, there’s always a risk of overfitting - capturing noise or idiosyncrasies in the particular sample rather than the underlying pattern we care about. [MISSING: A QUICK ILLUSTRATION OF AN EXAMPLE] Cross-validation helps us find the optimal balance between fitting the training data and generalizing to new data. When the datasets are small, as it is often the case in cognitive science, keeping a substantial portion of the data out - substantial enough to be representative of a more general population - is problematic as it risks starving the model of data: there might not be enough data for reliable estimation of the parameter values. This is where the notion of cross-validation comes in: we can split the dataset in k folds, let’s say k = 10. Then each fold is in turn kept aside as validation set, the model is fitted on the other folds, and its predictive performance tested on the validation set. Repeat this operation of each of the folds. This operation ensures that all the data can be used for training as well as for validation, and is in its own terms quite genial. However, this does not mean it is free of shortcomings. First, small validation folds might not be representative of the diversity of true out-of-sample populations - and there is a tendency to set k equal to the number of datapoints (leave-one-out cross validation). Second, there are many ways in which information could leak or contaminate across folds if the pipeline is not very careful (e.g. via data preprocessing scaling the full dataset, or hyper-parameter estimation). Third, and crucial for our case here, cross validation implies refitting the model k times, which for Bayesian models might be very cumbersome (I once had a model that took 6 weeks to run). 8.3.2 How Cross-Validation Works The basic idea of cross-validation is to: Split your data into training and test sets Fit your model to the training data Evaluate the model’s performance on the test data (which it hasn’t seen during training) Repeat with different training/test splits and average the results There are several variations of cross-validation: 8.3.2.1 K-Fold Cross-Validation In k-fold cross-validation, we: 1. Divide the data into k equally sized subsets (folds) 2. Use k-1 folds for training and the remaining fold for testing 3. Repeat k times, each time using a different fold as the test set 4. Average the k test performance metrics This visualization shows how 5-fold cross-validation works: # Visualization of k-fold cross-validation set.seed(123) n_data &lt;- 20 # K-Fold CV (k=5) cv_data &lt;- tibble( index = 1:n_data, value = rnorm(n_data) ) set.seed(456) cv_data$fold &lt;- sample(rep(1:5, length.out = n_data)) # Create visualization data for k-fold CV cv_viz_data &lt;- tibble( iteration = rep(1:5, each = n_data), data_point = rep(1:n_data, 5), role = ifelse(cv_data$fold[data_point] == iteration, &quot;test&quot;, &quot;train&quot;), approach = &quot;5-Fold CV&quot; ) # Define a consistent color scheme role_colors &lt;- c(&quot;train&quot; = &quot;steelblue&quot;, &quot;test&quot; = &quot;firebrick&quot;) # Create the plot ggplot(cv_viz_data, aes(x = data_point, y = iteration, fill = role)) + geom_tile(color = &quot;white&quot;, size = 0.5) + scale_fill_manual(values = role_colors, name = &quot;Data Role&quot;) + labs( title = &quot;K-Fold Cross-Validation (k=5)&quot;, subtitle = &quot;Each row shows one iteration, columns represent data points&quot;, x = &quot;Data Point&quot;, y = &quot;Iteration&quot;, fill = &quot;Data Usage&quot; ) + theme_minimal() + theme( panel.grid = element_blank(), legend.position = &quot;bottom&quot; ) 8.3.2.2 Leave-One-Out Cross-Validation (LOO-CV) Leave-one-out is a special case of k-fold cross-validation where k equals the number of data points. In each iteration, we: 1. Hold out a single observation for testing 2. Train on all other observations 3. Repeat for every observation 4. Average the performance metrics This approach can be very computationally intensive for large datasets or complex models. # Create visualization data for LOO CV loo_viz_data &lt;- tibble( iteration = rep(1:n_data, each = n_data), data_point = rep(1:n_data, n_data), role = ifelse(iteration == data_point, &quot;test&quot;, &quot;train&quot;), approach = &quot;LOO CV&quot; ) # Create the plot ggplot(loo_viz_data, aes(x = data_point, y = iteration, fill = role)) + geom_tile(color = &quot;white&quot;, size = 0.5) + scale_fill_manual(values = role_colors, name = &quot;Data Role&quot;) + labs( title = &quot;Leave-One-Out Cross-Validation&quot;, subtitle = &quot;Each row shows one iteration, columns represent data points&quot;, x = &quot;Data Point&quot;, y = &quot;Iteration&quot;, fill = &quot;Data Usage&quot; ) + theme_minimal() + theme( panel.grid = element_blank(), legend.position = &quot;bottom&quot; ) 8.3.3 Cross-Validation in Bayesian Models Cross-validation is especially important in Bayesian modeling for several reasons: Bayesian models can be highly parameterized and prone to overfitting Prior choices can significantly influence model performance The complexity of hierarchical structures needs careful validation We often need to compare competing theoretical accounts However, cross-validation for Bayesian models presents two key challenges: Computational cost: Bayesian models fitted with MCMC can take hours or days to run, making it impractical to refit them k times for cross-validation Proper scoring: We need appropriate metrics for evaluating predictive performance in a Bayesian framework Next, we’ll see how these challenges are addressed. 8.3.4 Expected Log Predictive Density (ELPD) When comparing Bayesian models, we use the expected log predictive density (ELPD) as our metric. This measures how well the model predicts new data points on the log scale. For a single observation, the log predictive density is: \\[\\log p(y_i | y_{-i})\\] where \\(y_i\\) is the observation we’re trying to predict, and \\(y_{-i}\\) represents all other observations that were used for training. For the entire dataset, we sum across all observations: \\[\\text{ELPD} = \\sum_{i=1}^{n} \\log p(y_i | y_{-i})\\] The ELPD has several desirable properties: It accounts for the full predictive distribution, not just point estimates It automatically penalizes overconfident models Higher values indicate better predictive performance Computing the ELPD exactly requires fitting the model n times (for n data points), which brings us back to the computational challenge. 8.4 PSIS-LOO: An Efficient Approximation to LOO-CV For complex Bayesian models, true leave-one-out cross-validation (LOO-CV) is often computationally infeasible. Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO) provides an elegant solution by approximating LOO-CV using just a single model fit. 8.4.1 How Importance Sampling Works When we fit a Bayesian model, we obtain samples from the posterior distribution \\(p(\\theta|y_1,...,y_n)\\) - the distribution of model parameters given all observations. For LOO-CV, we need \\(p(\\theta|y_1,...,y_{i-1},y_{i+1},...,y_n)\\) - the distribution without the i-th observation. Importance sampling bridges this gap by reweighting the full posterior samples to approximate the LOO posterior. The importance weights for the i-th observation are: \\[w_i(\\theta) = \\frac{p(\\theta|y_1,...,y_{i-1},y_{i+1},...,y_n)}{p(\\theta|y_1,...,y_n)} \\propto \\frac{1}{p(y_i|\\theta)}\\] By Bayes’ theorem, this simplifies to: w_i(θ) ∝ 1 / p(yᵢ|θ) These weights effectively “undo” the influence of the i-th observation on the posterior. However, standard importance sampling can be unstable when: * The full posterior and LOO posterior differ substantially * Some importance weights become extremely large * The variance of the weights is high 8.4.2 Pareto Smoothing for Stability Pareto smoothing improves the reliability of importance sampling: Fit a generalized Pareto distribution to the largest importance weights Replace the largest weights with values from this smoothed distribution Use the modified weights for more stable LOO estimation The diagnostic parameter k from the Pareto fit helps assess reliability: * k &lt; 0.5: Reliable estimation * 0.5 &lt; k &lt; 0.7: Somewhat reliable, proceed with caution * k &gt; 0.7: Unreliable, consider using other methods for this observation These diagnostics help identify problematic observations that might require more attention or alternative methods. 8.4.3 The Complete PSIS-LOO Process The full PSIS-LOO method follows these steps: Fit the Bayesian model once to all available data For each observation i: Calculate raw importance weights using the log-likelihood: w_i(θ) ∝ 1/p(yᵢ|θ) Apply Pareto smoothing to stabilize the largest weights Normalize the smoothed weights Use the weights to compute the expected log predictive density (ELPD) Sum the individual ELPD contributions to get the overall PSIS-LOO estimate [I NEED TO FIND A BETTER WAY TO EXPLAIN AND VISUALIZE!] # Create improved visualizations of PSIS-LOO set.seed(789) # Generate some sample data n &lt;- 50 x &lt;- seq(-3, 3, length.out = n) y_true &lt;- 2 + 1.5 * x + 0.5 * x^2 y &lt;- y_true + rnorm(n, 0, 2) data &lt;- data.frame(x = x, y = y) # Fit a simple model to all data (this will be our &quot;Bayesian model&quot;) full_model &lt;- lm(y ~ x + I(x^2)) full_predictions &lt;- predict(full_model) # Generate posterior samples (simplified for illustration) n_samples &lt;- 1000 beta_samples &lt;- mvrnorm(n_samples, coef(full_model), vcov(full_model)) sigma_samples &lt;- rep(sigma(full_model), n_samples) # Function to compute log-likelihood for each observation given parameters log_lik &lt;- function(beta, sigma, x, y) { mu &lt;- beta[1] + beta[2] * x + beta[3] * x^2 dnorm(y, mean = mu, sd = sigma, log = TRUE) } # Compute log-likelihood matrix (n_samples × n_observations) log_lik_matrix &lt;- matrix(NA, nrow = n_samples, ncol = n) for (i in 1:n_samples) { for (j in 1:n) { log_lik_matrix[i, j] &lt;- log_lik(beta_samples[i,], sigma_samples[i], x[j], y[j]) } } # Select a point for demonstration loo_idx &lt;- 25 # This will be our &quot;left out&quot; point # Calculate raw importance weights for the selected point log_weights &lt;- -log_lik_matrix[, loo_idx] # Negative log-likelihood weights &lt;- exp(log_weights - max(log_weights)) # Stabilize by subtracting max # Function to simulate Pareto smoothing (simplified) pareto_smooth &lt;- function(weights) { # Sort weights sorted_weights &lt;- sort(weights, decreasing = TRUE) n_tail &lt;- min(500, length(weights) / 5) # Top 20% or 500, whichever is smaller # Fit generalized Pareto to the tail (simplified) # In practice, actual fitting would be used k &lt;- 0.4 # Simulated Pareto shape parameter sigma &lt;- mean(sorted_weights[1:n_tail]) * 0.8 # Smooth the tail weights for (i in 1:n_tail) { q &lt;- (i - 0.5) / n_tail sorted_weights[i] &lt;- sigma/k * ((1 - q)^(-k) - 1) } # Rearrange to original order smoothed_weights &lt;- weights smoothed_weights[order(weights, decreasing = TRUE)[1:n_tail]] &lt;- sorted_weights[1:n_tail] return(smoothed_weights) } # Apply simulated Pareto smoothing smoothed_weights &lt;- pareto_smooth(weights) # Normalize weights raw_norm_weights &lt;- weights / sum(weights) smoothed_norm_weights &lt;- smoothed_weights / sum(smoothed_weights) # Calculate true LOO prediction for comparison loo_model &lt;- lm(y[-loo_idx] ~ x[-loo_idx] + I(x[-loo_idx]^2)) loo_prediction &lt;- predict(loo_model, newdata = data.frame(x = x[loo_idx])) # Calculate PSIS-LOO prediction for the point psis_prediction &lt;- 0 for (i in 1:n_samples) { beta &lt;- beta_samples[i,] psis_prediction &lt;- psis_prediction + smoothed_norm_weights[i] * (beta[1] + beta[2] * x[loo_idx] + beta[3] * x[loo_idx]^2) } # Create visualization data viz_data &lt;- data.frame( x = x, y = y, full_fit = full_predictions, highlighted = ifelse(seq_along(x) == loo_idx, &quot;Point Left Out&quot;, &quot;Other Points&quot;) ) # Sample subset for posterior draws sample_indices &lt;- sample(1:n_samples, 50) posterior_lines &lt;- data.frame( x = rep(x, length(sample_indices)), sample = rep(1:length(sample_indices), each = n), y_pred = NA ) for (i in 1:length(sample_indices)) { s &lt;- sample_indices[i] beta &lt;- beta_samples[s,] posterior_lines$y_pred[posterior_lines$sample == i] &lt;- beta[1] + beta[2] * x + beta[3] * x^2 } # Create data for weight visualization weight_data &lt;- data.frame( weight_idx = 1:100, # Show only first 100 weights for clarity raw_weight = raw_norm_weights[1:100], smoothed_weight = smoothed_norm_weights[1:100] ) # PLOT 1: Data and model fit p1 &lt;- ggplot(viz_data, aes(x = x, y = y)) + # Add posterior draws geom_line(data = posterior_lines, aes(y = y_pred, group = sample), alpha = 0.1, color = &quot;blue&quot;) + # Add full model fit geom_line(aes(y = full_fit), color = &quot;blue&quot;, size = 1) + # Add data points geom_point(aes(color = highlighted, size = highlighted), alpha = 0.7) + # Add predictions for the left-out point geom_segment( aes(x = x[loo_idx], y = y[loo_idx], xend = x[loo_idx], yend = loo_prediction), arrow = arrow(length = unit(0.3, &quot;cm&quot;)), color = &quot;red&quot;, linetype = &quot;dashed&quot; ) + # Add the true LOO prediction point geom_point( aes(x = x[loo_idx], y = loo_prediction), color = &quot;red&quot;, size = 4, shape = 18 ) + geom_segment( aes(x = x[loo_idx] + 0.1, y = y[loo_idx], xend = x[loo_idx] + 0.1, yend = psis_prediction), arrow = arrow(length = unit(0.3, &quot;cm&quot;)), color = &quot;purple&quot;, linetype = &quot;dashed&quot; ) + # Add the PSIS-LOO prediction point geom_point( aes(x = x[loo_idx] + 0.1, y = psis_prediction), color = &quot;purple&quot;, size = 4, shape = 18 ) + # Add legend annotations annotate(&quot;text&quot;, x = x[loo_idx] - 0.3, y = (y[loo_idx] + loo_prediction)/2, label = &quot;True LOO&quot;, color = &quot;red&quot;, hjust = 1) + annotate(&quot;text&quot;, x = x[loo_idx] + 0.4, y = (y[loo_idx] + psis_prediction)/2, label = &quot;PSIS-LOO&quot;, color = &quot;purple&quot;, hjust = 0) + # Styling scale_color_manual(values = c(&quot;Other Points&quot; = &quot;black&quot;, &quot;Point Left Out&quot; = &quot;red&quot;)) + scale_size_manual(values = c(&quot;Other Points&quot; = 2, &quot;Point Left Out&quot; = 3)) + labs( title = &quot;PSIS-LOO Approximation of Leave-One-Out Cross-Validation&quot;, subtitle = &quot;Comparing true LOO (red) vs. PSIS-LOO approximation (purple)&quot;, x = &quot;x&quot;, y = &quot;y&quot;, color = NULL, size = NULL ) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) # PLOT 2: Importance weights before and after Pareto smoothing p2 &lt;- ggplot(weight_data, aes(x = weight_idx)) + # Raw weights geom_segment(aes(xend = weight_idx, y = 0, yend = raw_weight), color = &quot;gray50&quot;, alpha = 0.7) + geom_point(aes(y = raw_weight), color = &quot;blue&quot;, size = 2, alpha = 0.7) + # Smoothed weights geom_segment(aes(xend = weight_idx, y = 0, yend = smoothed_weight), color = &quot;gray50&quot;, alpha = 0.7) + geom_point(aes(y = smoothed_weight), color = &quot;purple&quot;, size = 2, alpha = 0.7) + # Connect raw and smoothed weights for visual clarity geom_segment(aes(xend = weight_idx, y = raw_weight, yend = smoothed_weight), arrow = arrow(length = unit(0.1, &quot;cm&quot;)), color = &quot;purple&quot;, alpha = 0.3) + # Styling labs( title = &quot;Importance Sampling Weights for PSIS-LOO&quot;, subtitle = &quot;Pareto smoothing stabilizes extreme weights (arrows show the adjustment)&quot;, x = &quot;Sample Index (ordered by weight)&quot;, y = &quot;Normalized Weight&quot; ) + scale_x_continuous(breaks = seq(0, 100, by = 20)) + theme_minimal() + annotate(&quot;text&quot;, x = 20, y = max(weight_data$raw_weight) * 0.9, label = &quot;Before smoothing&quot;, color = &quot;blue&quot;, hjust = 0) + annotate(&quot;text&quot;, x = 60, y = max(weight_data$smoothed_weight) * 0.9, label = &quot;After Pareto smoothing&quot;, color = &quot;purple&quot;, hjust = 0) # PLOT 3: Conceptual diagram of PSIS-LOO process p3 &lt;- ggplot() + # Add process steps annotate(&quot;rect&quot;, xmin = 0.5, xmax = 3.5, ymin = 4, ymax = 5, fill = &quot;lightblue&quot;, alpha = 0.7, color = &quot;black&quot;) + annotate(&quot;text&quot;, x = 2, y = 4.5, label = &quot;1. Fit Model to All Data\\nObtain posterior samples p(θ|y₁,...,yₙ)&quot;) + annotate(&quot;rect&quot;, xmin = 0.5, xmax = 3.5, ymin = 2.5, ymax = 3.5, fill = &quot;lightgreen&quot;, alpha = 0.7, color = &quot;black&quot;) + annotate(&quot;text&quot;, x = 2, y = 3, label = &quot;2. Calculate Log-Likelihood for Each Observation\\nℓᵢ(θ) = log p(yᵢ|θ)&quot;) + annotate(&quot;rect&quot;, xmin = 0.5, xmax = 3.5, ymin = 1, ymax = 2, fill = &quot;lightyellow&quot;, alpha = 0.7, color = &quot;black&quot;) + annotate(&quot;text&quot;, x = 2, y = 1.5, label = &quot;3. Compute Importance Weights\\nwᵢ(θ) ∝ 1/p(yᵢ|θ)&quot;) + annotate(&quot;rect&quot;, xmin = 0.5, xmax = 3.5, ymin = -0.5, ymax = 0.5, fill = &quot;mistyrose&quot;, alpha = 0.7, color = &quot;black&quot;) + annotate(&quot;text&quot;, x = 2, y = 0, label = &quot;4. Apply Pareto Smoothing\\nStabilize extreme weights&quot;) + annotate(&quot;rect&quot;, xmin = 0.5, xmax = 3.5, ymin = -2, ymax = -1, fill = &quot;lavender&quot;, alpha = 0.7, color = &quot;black&quot;) + annotate(&quot;text&quot;, x = 2, y = -1.5, label = &quot;5. Compute ELPD Using Smoothed Weights\\nApproximate leave-one-out prediction&quot;) + # Add arrows annotate(&quot;segment&quot;, x = 2, y = 4, xend = 2, yend = 3.5, arrow = arrow(length = unit(0.2, &quot;cm&quot;))) + annotate(&quot;segment&quot;, x = 2, y = 2.5, xend = 2, yend = 2, arrow = arrow(length = unit(0.2, &quot;cm&quot;))) + annotate(&quot;segment&quot;, x = 2, y = 1, xend = 2, yend = 0.5, arrow = arrow(length = unit(0.2, &quot;cm&quot;))) + annotate(&quot;segment&quot;, x = 2, y = -0.5, xend = 2, yend = -1, arrow = arrow(length = unit(0.2, &quot;cm&quot;))) + # Explain the advantage annotate(&quot;text&quot;, x = 2, y = -2.7, label = &quot;PSIS-LOO provides accurate LOO approximation\\nfrom a single model fit&quot;, fontface = &quot;italic&quot;) + # Add Pareto k value diagnostic information annotate(&quot;rect&quot;, xmin = 3.7, xmax = 5.7, ymin = -0.2, ymax = 1.5, fill = &quot;white&quot;, alpha = 0.8, color = &quot;black&quot;, linetype = &quot;dashed&quot;) + annotate(&quot;text&quot;, x = 4.7, y = 1.2, label = &quot;Pareto k Diagnostics&quot;, fontface = &quot;bold&quot;) + annotate(&quot;text&quot;, x = 4.7, y = 0.8, label = &quot;k &lt; 0.5: Reliable&quot;, color = &quot;darkgreen&quot;) + annotate(&quot;text&quot;, x = 4.7, y = 0.4, label = &quot;0.5 &lt; k &lt; 0.7: Use caution&quot;, color = &quot;darkorange&quot;) + annotate(&quot;text&quot;, x = 4.7, y = 0, label = &quot;k &gt; 0.7: Unreliable&quot;, color = &quot;darkred&quot;) + # Styling theme_void() + labs(title = &quot;PSIS-LOO Process Flow&quot;) # Arrange the plots in a grid p1 / p2 p3 The red circle represents our “left-out” data point, while the blue line shows the model fit using all data points (including that red circle). The red diamond shows the prediction we get when we actually refit the model without the red circle. When we fit a model (the blue line), each data point “pulls” the model fit toward itself to some degree. The red circle data point influenced the original model to bend slightly closer to it. This is why the red circle appears relatively close to the blue line—it helped shape that line! When we perform true leave-one-out cross-validation, we remove that red circle point completely and refit the model using only the remaining data. Without the “pull” from that point, the model (which we don’t directly show) follows a slightly different path determined solely by the other points. The prediction from this new model (the red diamond) naturally lands in a different position. This difference between the original model prediction and the leave-one-out prediction is exactly what makes cross-validation valuable: It reveals how much individual data points influence your model It gives a more honest assessment of how your model will perform on truly unseen data Large differences can help identify influential or outlier points The purple diamond (PSIS-LOO prediction) attempts to approximate where that red diamond would be without actually refitting the model, by mathematically down-weighting the influence of the left-out point—which is why it’s positioned close to the red diamond if the approximation is working well. 8.5 Simulation-Based Model Comparison Now that we understand the principles, let’s apply these techniques to compare cognitive models using a simulation-based approach. This approach has two key advantages: We know the “ground truth” (which model generated each dataset) We can systematically test if our comparison methods work We’ll simulate data from two different model types: Biased agents: Make choices based on a fixed bias parameter Memory agents: Make choices influenced by the history of their opponent’s actions By fitting both models to data generated from each type of agent, we can evaluate whether our model comparison techniques correctly identify the true generating model. 8.5.1 Define Simulation Parameters # Shared parameters agents &lt;- 100 # Number of agents to simulate trials &lt;- 120 # Number of trials per agent noise &lt;- 0 # Base noise level (probability of random choice) # Biased agents parameters (on log-odds scale) rateM &lt;- 1.386 # Population mean of bias (~0.8 in probability space) rateSD &lt;- 0.65 # Population SD of bias (~0.1 in probability space) # Memory agents parameters biasM &lt;- 0 # Population mean of baseline bias biasSD &lt;- 0.1 # Population SD of baseline bias betaM &lt;- 1.5 # Population mean of memory sensitivity betaSD &lt;- 0.3 # Population SD of memory sensitivity # Print the parameters in probability space for easier interpretation cat(&quot;Biased agents have mean choice probability of:&quot;, round(plogis(rateM), 2), &quot;\\n&quot;) ## Biased agents have mean choice probability of: 0.8 cat(&quot;Memory agents have mean baseline bias of:&quot;, round(plogis(biasM), 2), &quot;\\n&quot;) ## Memory agents have mean baseline bias of: 0.5 cat(&quot;Memory agents have mean sensitivity to opponent&#39;s history of:&quot;, betaM, &quot;\\n&quot;) ## Memory agents have mean sensitivity to opponent&#39;s history of: 1.5 8.5.2 Define Agent Functions # Random agent function: makes choices based on bias parameter RandomAgentNoise_f &lt;- function(rate, noise) { # Generate choice based on agent&#39;s bias parameter (on log-odds scale) choice &lt;- rbinom(1, 1, plogis(rate)) # With probability &#39;noise&#39;, override choice with random 50/50 selection if (rbinom(1, 1, noise) == 1) { choice = rbinom(1, 1, 0.5) } return(choice) } # Memory agent function: makes choices based on opponent&#39;s historical choices MemoryAgentNoise_f &lt;- function(bias, beta, otherRate, noise) { # Calculate log-odds of choosing option 1, influenced by opponent&#39;s historical choice rate log_odds &lt;- bias + beta * qlogis(otherRate) # Convert to probability and generate choice choice &lt;- rbinom(1, 1, plogis(log_odds)) # With probability &#39;noise&#39;, override choice with random 50/50 selection if (rbinom(1, 1, noise) == 1) { choice = rbinom(1, 1, 0.5) } return(choice) } 8.5.3 Generate Simulation Data # Function to simulate one run of agents playing against each other simulate_agents &lt;- function() { # Create empty dataframe to store results d &lt;- NULL # Loop through all agents for (agent in 1:agents) { # Sample individual agent parameters from population distributions rate &lt;- rnorm(1, rateM, rateSD) # Individual bias for random agent bias &lt;- rnorm(1, biasM, biasSD) # Individual baseline bias for memory agent beta &lt;- rnorm(1, betaM, betaSD) # Individual memory sensitivity # Initialize choice vectors randomChoice &lt;- rep(NA, trials) memoryChoice &lt;- rep(NA, trials) memoryRate &lt;- rep(NA, trials) # Generate choices for each trial for (trial in 1:trials) { # Random (biased) agent makes choice randomChoice[trial] &lt;- RandomAgentNoise_f(rate, noise) # Memory agent responds (with no history for first trial) if (trial == 1) { memoryChoice[trial] &lt;- rbinom(1, 1, 0.5) # First choice is random } else { # Use mean of random agent&#39;s previous choices as &quot;memory&quot; memoryChoice[trial] &lt;- MemoryAgentNoise_f( bias, beta, mean(randomChoice[1:(trial - 1)], na.rm = TRUE), noise ) } } # Create data frame for this agent temp &lt;- tibble( agent = agent, trial = seq(trials), randomChoice = randomChoice, randomRate = rate, memoryChoice = memoryChoice, noise = noise, rateM = rateM, rateSD = rateSD, bias = bias, beta = beta, biasM = biasM, biasSD = biasSD, betaM = betaM, betaSD = betaSD ) # Append to main dataframe if (agent &gt; 1) { d &lt;- rbind(d, temp) } else { d &lt;- temp } } # Calculate cumulative choice rates d &lt;- d %&gt;% group_by(agent) %&gt;% mutate( randomRate_cumulative = cumsum(randomChoice) / seq_along(randomChoice), memoryRate_cumulative = cumsum(memoryChoice) / seq_along(memoryChoice) ) return(d) } # Generate the data d &lt;- simulate_agents() # Show a quick summary of the generated data summary_stats &lt;- d %&gt;% filter(trial == trials) %&gt;% # Get final trial stats summarize( mean_random_rate = mean(randomRate_cumulative), mean_memory_rate = mean(memoryRate_cumulative) ) print(summary_stats) ## # A tibble: 100 × 3 ## agent mean_random_rate mean_memory_rate ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.892 0.833 ## 2 2 0.775 0.883 ## 3 3 0.917 0.958 ## 4 4 0.883 0.992 ## 5 5 0.783 0.692 ## 6 6 0.825 0.867 ## 7 7 0.875 0.983 ## 8 8 0.842 0.925 ## 9 9 0.608 0.608 ## 10 10 0.875 0.975 ## # ℹ 90 more rows 8.5.4 Visualize the Simulation Data # Select a random sample of agents to visualize sample_agents &lt;- sample(unique(d$agent), 6) # Filter data for these agents sample_data &lt;- d %&gt;% filter(agent %in% sample_agents) # Create plot showing choice patterns p1 &lt;- ggplot(sample_data, aes(x = trial)) + geom_line(aes(y = randomRate_cumulative, color = &quot;Biased Agent&quot;), size = 1) + geom_line(aes(y = memoryRate_cumulative, color = &quot;Memory Agent&quot;), size = 1) + geom_hline(yintercept = 0.5, linetype = &quot;dashed&quot;, color = &quot;gray50&quot;) + facet_wrap(~agent, ncol = 3) + scale_color_brewer(palette = &quot;Set1&quot;) + labs( title = &quot;Choice Patterns of Biased vs. Memory Agents&quot;, subtitle = &quot;Lines show cumulative proportion of &#39;right&#39; choices&quot;, x = &quot;Trial&quot;, y = &quot;Cumulative Proportion Right Choices&quot;, color = &quot;Agent Type&quot; ) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) # Create density plot of final choice rates p2 &lt;- d %&gt;% filter(trial == trials) %&gt;% dplyr::select(agent, randomRate_cumulative, memoryRate_cumulative) %&gt;% pivot_longer(cols = c(randomRate_cumulative, memoryRate_cumulative), names_to = &quot;agent_type&quot;, values_to = &quot;rate&quot;) %&gt;% mutate(agent_type = ifelse(agent_type == &quot;randomRate_cumulative&quot;, &quot;Biased Agent&quot;, &quot;Memory Agent&quot;)) %&gt;% ggplot(aes(x = rate, fill = agent_type)) + geom_density(alpha = 0.5) + scale_fill_brewer(palette = &quot;Set1&quot;) + labs( title = &quot;Distribution of Final Choice Rates&quot;, subtitle = &quot;After 120 trials&quot;, x = &quot;Proportion of Right Choices&quot;, y = &quot;Density&quot;, fill = &quot;Agent Type&quot; ) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) # Combine the plots p1 / p2 + plot_layout(heights = c(2, 1)) 8.5.5 Prepare Data for Stan Models # Prepare the data for the biased agent model d_random &lt;- d %&gt;% dplyr::select(agent, randomChoice) %&gt;% mutate(row = row_number()) %&gt;% pivot_wider(names_from = agent, values_from = randomChoice) d_memory &lt;- d %&gt;% dplyr::select(agent, memoryChoice) %&gt;% mutate(row = row_number()) %&gt;% pivot_wider(names_from = agent, values_from = memoryChoice) # Create data lists for Stan data_biased &lt;- list( trials = trials, agents = agents, h = as.matrix(d_random[, 2:(agents + 1)]), # Matrix where columns are agents, rows are trials other = as.matrix(d_memory[, 2:(agents + 1)]) # Other agent&#39;s choices (for memory model) ) data_memory &lt;- list( trials = trials, agents = agents, h = as.matrix(d_memory[, 2:(agents + 1)]), # For memory agent model, we&#39;ll predict memory choices other = as.matrix(d_random[, 2:(agents + 1)]) # And use random agent choices as input ) # Show dimensions of the data matrices cat(&quot;Dimensions of data matrices for biased agent model:\\n&quot;) ## Dimensions of data matrices for biased agent model: cat(&quot;h matrix:&quot;, dim(data_biased$h), &quot;\\n&quot;) ## h matrix: 120 100 cat(&quot;other matrix:&quot;, dim(data_biased$other), &quot;\\n\\n&quot;) ## other matrix: 120 100 cat(&quot;Dimensions of data matrices for memory agent model:\\n&quot;) ## Dimensions of data matrices for memory agent model: cat(&quot;h matrix:&quot;, dim(data_memory$h), &quot;\\n&quot;) ## h matrix: 120 100 cat(&quot;other matrix:&quot;, dim(data_memory$other), &quot;\\n&quot;) ## other matrix: 120 100 8.6 Implementing Models for Comparison Now we need to implement our two competing models in Stan. Both will be multilevel (hierarchical) to account for individual differences among agents. The key feature for model comparison is that we’ll include a log_lik calculation in the generated quantities block of each model. 8.6.1 Understanding Log-Likelihood in Model Comparison When comparing models, we need a way to quantify how well each model explains the observed data. The log-likelihood represents the logarithm of the probability that a model would generate the observed data given specific parameter values. Given certain values for our parameters (let’s say a bias of 0 and beta for memory of 1) and for our variables (let’s say the vector of memory values estimated by the agent on a trial by trial basis), the model will predict a certain distribution of outcomes, that is, a certain distribution of choices (n times right, m times left hand). Comparing this to the actual data, we can identify how likely the model is to produce it. In other words, the probability that the model will actually generate the data we observed out of all its possible outcomes. Remember that we are doing Bayesian statistics, so this probability needs to be combined with the probability of the parameter values given the priors on those parameters. This would give us a posterior likelihood of the model’s parameter values given the data. The last step is that we need to work on a log scale. Working on a log scale is very useful because it avoids low probabilities (close to 0) being rounded down to exactly 0. By log-transforming the posterior likelihood, we now have the log-posterior likelihood. Now, remember that our agent’s memory varies on a trial by trial level. In other words, for each data point, for each agent we can calculate separate values of log-posterior likelihood for each of the possible values of the parameters. That is, we can have a distribution of log-posterior likelihood for each data point. Telling Stan to calculate these distributions is straightforward: we add to the generated quantities block the same log probability statements used in the model block, but save them to variables instead of adding them to the target. N.B. Some of you might be wandering: if Stan is already using the log-posterior probability in the sampling process, why do we need to tell it to calculate and save it? Fair enough point. But Stan does not save by default (to avoid clogging your computer with endless data) and we need the log posterior likelihood saved as “log_lik” in order to be able to use more automated functions later on. 8.6.2 Multilevel Biased Agent Model Here’s the Stan model for the biased agent (remember that we will add the log_lik part in the generated quantities block!). # Stan model for multilevel biased agent stan_biased_model &lt;- &quot; // Multilevel Biased Agent Model // // This model assumes each agent has a fixed bias (theta) that determines // their probability of choosing option 1 (&#39;right&#39;) functions{ // Helper function for generating truncated normal random numbers real normal_lb_rng(real mu, real sigma, real lb) { real p = normal_cdf(lb | mu, sigma); // CDF for bounds real u = uniform_rng(p, 1); return (sigma * inv_Phi(u)) + mu; // Inverse CDF for value } } // Input data data { int&lt;lower = 1&gt; trials; // Number of trials per agent int&lt;lower = 1&gt; agents; // Number of agents array[trials, agents] int h; // Choice data (0/1 for each trial and agent) } // Parameters to be estimated parameters { real thetaM; // Population mean of bias (log-odds scale) real&lt;lower = 0&gt; thetaSD; // Population SD of bias array[agents] real theta; // Individual agent biases (log-odds scale) } // Model definition model { // Population-level priors target += normal_lpdf(thetaM | 0, 1); // Prior for SD with lower bound at zero (half-normal) target += normal_lpdf(thetaSD | 0, .3) - normal_lccdf(0 | 0, .3); // Individual-level parameters drawn from population distribution target += normal_lpdf(theta | thetaM, thetaSD); // Likelihood: predict each agent&#39;s choices for (i in 1:agents) target += bernoulli_logit_lpmf(h[,i] | theta[i]); } // Additional quantities to calculate generated quantities{ // Prior predictive samples real thetaM_prior; real&lt;lower=0&gt; thetaSD_prior; real&lt;lower=0, upper=1&gt; theta_prior; real&lt;lower=0, upper=1&gt; theta_posterior; // Posterior predictive samples int&lt;lower=0, upper = trials&gt; prior_preds; int&lt;lower=0, upper = trials&gt; posterior_preds; // Log-likelihood for each observation (crucial for model comparison) array[trials, agents] real log_lik; // Generate prior samples thetaM_prior = normal_rng(0, 1); thetaSD_prior = normal_lb_rng(0, 0.3, 0); theta_prior = inv_logit(normal_rng(thetaM_prior, thetaSD_prior)); theta_posterior = inv_logit(normal_rng(thetaM, thetaSD)); // Generate predictions from prior and posterior prior_preds = binomial_rng(trials, inv_logit(thetaM_prior)); posterior_preds = binomial_rng(trials, inv_logit(thetaM)); // Calculate log-likelihood for each observation for (i in 1:agents){ for (t in 1:trials){ log_lik[t, i] = bernoulli_logit_lpmf(h[t, i] | theta[i]); } } } &quot; # Write the Stan model to a file write_stan_file( stan_biased_model, dir = &quot;stan/&quot;, basename = &quot;W7_MultilevelBias.stan&quot; ) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W7_MultilevelBias.stan&quot; # Compile the model file &lt;- file.path(&quot;stan/W7_MultilevelBias.stan&quot;) mod_biased &lt;- cmdstan_model( file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;) ) Let’s break down this Stan model: Data Block: Defines the input data - number of trials, number of agents, and the choice data matrix. Parameters Block: Specifies the parameters we want to estimate: thetaM: The population mean bias thetaSD: The population standard deviation of bias theta: Individual bias parameters for each agent Model Block: Defines the prior distributions and likelihood function: Priors for population parameters Individual parameters drawn from the population distribution Likelihood of observing the choice data given the parameters Generated Quantities Block: Calculates additional quantities of interest: Prior and posterior predictive samples Log-likelihood for each observation - this is crucial for model comparison The most important part for model comparison is the log_lik calculation in the generated quantities block. This computes the log probability of each observation given the model and its parameters, which we’ll use for comparing models. 8.6.3 Multilevel Memory Agent Model Now let’s implement our second model - the memory agent model: # Stan model for multilevel memory agent stan_memory_model &lt;- &quot; // Multilevel Memory Agent Model // // This model assumes agents make choices based on their memory of // the opponent&#39;s previous choices. functions{ // Helper function for generating truncated normal random numbers real normal_lb_rng(real mu, real sigma, real lb) { real p = normal_cdf(lb | mu, sigma); // CDF for bounds real u = uniform_rng(p, 1); return (sigma * inv_Phi(u)) + mu; // Inverse CDF for value } } // Input data data { int&lt;lower = 1&gt; trials; // Number of trials per agent int&lt;lower = 1&gt; agents; // Number of agents array[trials, agents] int h; // Choice data (0/1 for each trial and agent) array[trials, agents] int other;// Opponent&#39;s choices (input to memory) } // Parameters to be estimated parameters { real biasM; // Population mean baseline bias real betaM; // Population mean memory sensitivity vector&lt;lower = 0&gt;[2] tau; // Population SDs for bias and beta matrix[2, agents] z_IDs; // Standardized individual parameters (non-centered) cholesky_factor_corr[2] L_u; // Cholesky factor of correlation matrix } // Transformed parameters (derived quantities) transformed parameters { // Memory state for each agent and trial array[trials, agents] real memory; // Individual parameters (bias and beta for each agent) matrix[agents, 2] IDs; // Transform standardized parameters to actual parameters (non-centered parameterization) IDs = (diag_pre_multiply(tau, L_u) * z_IDs)&#39;; // Calculate memory states based on opponent&#39;s choices for (agent in 1:agents){ for (trial in 1:trials){ // Initialize first trial with neutral memory if (trial == 1) { memory[trial, agent] = 0.5; } // Update memory based on opponent&#39;s choices if (trial &lt; trials){ // Simple averaging memory update memory[trial + 1, agent] = memory[trial, agent] + ((other[trial, agent] - memory[trial, agent]) / trial); // Handle edge cases to avoid numerical issues if (memory[trial + 1, agent] == 0){memory[trial + 1, agent] = 0.01;} if (memory[trial + 1, agent] == 1){memory[trial + 1, agent] = 0.99;} } } } } // Model definition model { // Population-level priors target += normal_lpdf(biasM | 0, 1); target += normal_lpdf(tau[1] | 0, .3) - normal_lccdf(0 | 0, .3); // Half-normal for SD target += normal_lpdf(betaM | 0, .3); target += normal_lpdf(tau[2] | 0, .3) - normal_lccdf(0 | 0, .3); // Half-normal for SD // Prior for correlation matrix target += lkj_corr_cholesky_lpdf(L_u | 2); // Standardized individual parameters have standard normal prior target += std_normal_lpdf(to_vector(z_IDs)); // Likelihood: predict each agent&#39;s choices for (agent in 1:agents){ for (trial in 1:trials){ // choice ~ bias + memory_effect*beta target += bernoulli_logit_lpmf(h[trial, agent] | biasM + IDs[agent, 1] + memory[trial, agent] * (betaM + IDs[agent, 2])); } } } // Additional quantities to calculate generated quantities{ // Prior predictive samples real biasM_prior; real&lt;lower=0&gt; biasSD_prior; real betaM_prior; real&lt;lower=0&gt; betaSD_prior; real bias_prior; real beta_prior; // Posterior predictive samples for different memory conditions array[agents] int&lt;lower=0, upper = trials&gt; prior_preds0; array[agents] int&lt;lower=0, upper = trials&gt; prior_preds1; array[agents] int&lt;lower=0, upper = trials&gt; prior_preds2; array[agents] int&lt;lower=0, upper = trials&gt; posterior_preds0; array[agents] int&lt;lower=0, upper = trials&gt; posterior_preds1; array[agents] int&lt;lower=0, upper = trials&gt; posterior_preds2; // Log-likelihood for each observation (crucial for model comparison) array[trials, agents] real log_lik; // Generate prior samples biasM_prior = normal_rng(0,1); biasSD_prior = normal_lb_rng(0,0.3,0); betaM_prior = normal_rng(0,1); betaSD_prior = normal_lb_rng(0,0.3,0); bias_prior = normal_rng(biasM_prior, biasSD_prior); beta_prior = normal_rng(betaM_prior, betaSD_prior); // Generate predictions for different memory conditions for (i in 1:agents){ // Prior predictions for low, medium, high memory prior_preds0[i] = binomial_rng(trials, inv_logit(bias_prior + 0 * beta_prior)); prior_preds1[i] = binomial_rng(trials, inv_logit(bias_prior + 1 * beta_prior)); prior_preds2[i] = binomial_rng(trials, inv_logit(bias_prior + 2 * beta_prior)); // Posterior predictions for low, medium, high memory posterior_preds0[i] = binomial_rng(trials, inv_logit(biasM + IDs[i,1] + 0 * (betaM + IDs[i,2]))); posterior_preds1[i] = binomial_rng(trials, inv_logit(biasM + IDs[i,1] + 1 * (betaM + IDs[i,2]))); posterior_preds2[i] = binomial_rng(trials, inv_logit(biasM + IDs[i,1] + 2 * (betaM + IDs[i,2]))); // Calculate log-likelihood for each observation for (t in 1:trials){ log_lik[t,i] = bernoulli_logit_lpmf(h[t, i] | biasM + IDs[i, 1] + memory[t, i] * (betaM + IDs[i, 2])); } } } &quot; # Write the Stan model to a file write_stan_file( stan_memory_model, dir = &quot;stan/&quot;, basename = &quot;W7_MultilevelMemory.stan&quot; ) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W7_MultilevelMemory.stan&quot; # Compile the model file &lt;- file.path(&quot;stan/W7_MultilevelMemory.stan&quot;) mod_memory &lt;- cmdstan_model( file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;) ) The memory agent model is more complex, but follows a similar structure: Data Block: Includes the same data as the biased model, plus the opponent’s choices. Parameters Block: Includes parameters for the memory model: biasM: Population mean baseline bias betaM: Population mean memory sensitivity tau: Population standard deviations z_IDs: Standardized individual parameters L_u: Cholesky factor of correlation matrix Transformed Parameters Block: Calculates derived quantities: memory: The memory state for each agent and trial IDs: Individual parameters for each agent Model Block: Defines priors and likelihood: Priors for population parameters Memory-based choice likelihood Generated Quantities Block: Calculates additional quantities: Prior and posterior predictive samples Log-likelihood for each observation The use of a non-centered parameterization for the individual parameters (through z_IDs and transformation) is a technique to improve sampling efficiency in hierarchical models. This is important when estimating multilevel models with potentially correlated parameters. 8.7 Fitting Models and Calculating Expected Log Predictive Density Now that we’ve defined our models, we’ll fit them to our simulated data and perform model comparison. We’ll fit both models to both types of data: Biased agent model fitted to biased agent data Biased agent model fitted to memory agent data Memory agent model fitted to biased agent data Memory agent model fitted to memory agent data 8.8 Fitting the models to the data # File path for saved model model_file &lt;- &quot;simmodels/W7_fit_biased2biased.RDS&quot; # Check if we need to rerun the simulation or we can load a pre-run one (for computational efficiency) if (regenerate_simulations || !file.exists(model_file)) { # Fitting biased agent model to biased agent data fit_biased2biased &lt;- mod_biased$sample( data = data_biased, seed = 123, chains = 1, parallel_chains = 1, threads_per_chain = 1, iter_warmup = 2000, iter_sampling = 2000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99 ) samples$save_object(file = model_file) cat(&quot;Generated new model fit and saved to&quot;, model_file, &quot;\\n&quot;) } else { # Load existing results samples &lt;- readRDS(model_file) cat(&quot;Loaded existing model fit from&quot;, model_file, &quot;\\n&quot;) } ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## Chain 1 finished in 26.7 seconds. ## Generated new model fit and saved to simmodels/W7_fit_biased2biased.RDS # File path for saved model model_file &lt;- &quot;simmodels/W7_fit_biased2memory.RDS&quot; # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(model_file)) { # Fitting biased agent model to memory agent data fit_biased2memory &lt;- mod_biased$sample( data = data_memory, seed = 123, chains = 1, parallel_chains = 1, threads_per_chain = 1, iter_warmup = 2000, iter_sampling = 2000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99 ) samples$save_object(file = model_file) cat(&quot;Generated new model fit and saved to&quot;, model_file, &quot;\\n&quot;) } else { # Load existing results samples &lt;- readRDS(model_file) cat(&quot;Loaded existing model fit from&quot;, model_file, &quot;\\n&quot;) } ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## Chain 1 finished in 26.3 seconds. ## Generated new model fit and saved to simmodels/W7_fit_biased2memory.RDS # File path for saved model model_file &lt;- &quot;simmodels/W7_fit_memory2biased.RDS&quot; # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(model_file)) { # Fitting memory agent model to biased agent data fit_memory2biased &lt;- mod_memory$sample( data = data_biased, seed = 123, chains = 1, parallel_chains = 1, threads_per_chain = 4, iter_warmup = 2000, iter_sampling = 2000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99 ) samples$save_object(file = model_file) cat(&quot;Generated new model fit and saved to&quot;, model_file, &quot;\\n&quot;) } else { # Load existing results samples &lt;- readRDS(model_file) cat(&quot;Loaded existing model fit from&quot;, model_file, &quot;\\n&quot;) } ## Running MCMC with 1 chain, with 4 thread(s) per chain... ## Chain 1 finished in 1233.3 seconds. ## Generated new model fit and saved to simmodels/W7_fit_memory2biased.RDS # File path for saved model model_file &lt;- &quot;simmodels/W7_fit_memory2memory.RDS&quot; # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(model_file)) {# Fitting memory agent model to memory agent data fit_memory2memory &lt;- mod_memory$sample( data = data_memory, seed = 123, chains = 1, parallel_chains = 1, threads_per_chain = 4, iter_warmup = 2000, iter_sampling = 2000, refresh = 0, max_treedepth = 20, adapt_delta = 0.99 ) samples$save_object(file = model_file) cat(&quot;Generated new model fit and saved to&quot;, model_file, &quot;\\n&quot;) } else { # Load existing results samples &lt;- readRDS(model_file) cat(&quot;Loaded existing model fit from&quot;, model_file, &quot;\\n&quot;) } ## Running MCMC with 1 chain, with 4 thread(s) per chain... ## Chain 1 finished in 1670.9 seconds. ## Generated new model fit and saved to simmodels/W7_fit_memory2memory.RDS # Display basic summary of the models cat(&quot;Biased model fitted to biased data:\\n&quot;) ## Biased model fitted to biased data: print(fit_biased2biased$summary(c(&quot;thetaM&quot;, &quot;thetaSD&quot;))) ## # A tibble: 2 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 thetaM 1.35 1.35 0.0698 0.0696 1.23 1.46 1.00 3523. 1409. ## 2 thetaSD 0.638 0.636 0.0502 0.0475 0.560 0.725 1.00 3393. 1650. cat(&quot;\\nBiased model fitted to memory data:\\n&quot;) ## ## Biased model fitted to memory data: print(fit_biased2memory$summary(c(&quot;thetaM&quot;, &quot;thetaSD&quot;))) ## # A tibble: 2 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 thetaM 1.92 1.92 0.107 0.110 1.75 2.10 1.00 1831. 1463. ## 2 thetaSD 1.05 1.05 0.0775 0.0739 0.934 1.19 1.00 1510. 1246. cat(&quot;\\nMemory model fitted to biased data:\\n&quot;) ## ## Memory model fitted to biased data: print(fit_memory2biased$summary(c(&quot;biasM&quot;, &quot;betaM&quot;, &quot;tau[1]&quot;, &quot;tau[2]&quot;))) ## # A tibble: 4 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 biasM 1.14 1.13 0.169 0.172 0.865 1.42 1.00 777. 1330. ## 2 betaM 0.261 0.264 0.189 0.186 -0.0544 0.563 1.00 1119. 1464. ## 3 tau[1] 0.594 0.594 0.0787 0.0762 0.466 0.722 1.00 722. 1029. ## 4 tau[2] 0.165 0.145 0.122 0.126 0.0134 0.394 1.01 133. 257. cat(&quot;\\nMemory model fitted to memory data:\\n&quot;) ## ## Memory model fitted to memory data: print(fit_memory2memory$summary(c(&quot;biasM&quot;, &quot;betaM&quot;, &quot;tau[1]&quot;, &quot;tau[2]&quot;))) ## # A tibble: 4 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 biasM -0.878 -0.883 0.207 0.202 -1.21 -0.539 1.00 714. 1354. ## 2 betaM 3.56 3.56 0.226 0.220 3.18 3.91 1.00 1368. 1441. ## 3 tau[1] 0.650 0.655 0.155 0.142 0.382 0.902 1.00 982. 957. ## 4 tau[2] 0.514 0.499 0.293 0.315 0.0654 1.04 1.01 81.3 332. print(&quot;Let&#39;s visualize some key parameters to better understand what the models have learned:&quot;) ## [1] &quot;Let&#39;s visualize some key parameters to better understand what the models have learned:&quot; # Extract posterior samples for key parameters draws_biased2biased &lt;- as_draws_df(fit_biased2biased$draws()) draws_biased2memory &lt;- as_draws_df(fit_biased2memory$draws()) draws_memory2biased &lt;- as_draws_df(fit_memory2biased$draws()) draws_memory2memory &lt;- as_draws_df(fit_memory2memory$draws()) # Prepare data for plotting param_data &lt;- bind_rows( # Biased model parameters tibble( parameter = &quot;Bias (θ)&quot;, value = draws_biased2biased$thetaM, model = &quot;Biased Model&quot;, data = &quot;Biased Data&quot; ), tibble( parameter = &quot;Bias (θ)&quot;, value = draws_biased2memory$thetaM, model = &quot;Biased Model&quot;, data = &quot;Memory Data&quot; ), # Memory model parameters - converting to probability scale for bias tibble( parameter = &quot;Bias (θ)&quot;, value = draws_memory2biased$biasM, model = &quot;Memory Model&quot;, data = &quot;Biased Data&quot; ), tibble( parameter = &quot;Bias (θ)&quot;, value = draws_memory2memory$biasM, model = &quot;Memory Model&quot;, data = &quot;Memory Data&quot; ), # Memory sensitivity parameter tibble( parameter = &quot;Memory Sensitivity (β)&quot;, value = draws_memory2biased$betaM, model = &quot;Memory Model&quot;, data = &quot;Biased Data&quot; ), tibble( parameter = &quot;Memory Sensitivity (β)&quot;, value = draws_memory2memory$betaM, model = &quot;Memory Model&quot;, data = &quot;Memory Data&quot; ) ) # Create reference lines data frame for the true parameter values true_param_lines &lt;- bind_rows( tibble(parameter = &quot;Bias (θ)&quot;, model = &quot;Biased Model&quot;, true_value = rateM), tibble(parameter = &quot;Bias (θ)&quot;, model = &quot;Memory Model&quot;, true_value = biasM), tibble(parameter = &quot;Memory Sensitivity (β)&quot;, model = &quot;Memory Model&quot;, true_value = betaM) ) # Create visualization of posterior distributions ggplot(param_data, aes(x = value, fill = data)) + geom_density(alpha = 0.6) + # Add reference lines only to relevant panels geom_vline(data = true_param_lines, aes(xintercept = true_value), linetype = &quot;dashed&quot;, color = &quot;black&quot;) + facet_grid(model ~ parameter, scales = &quot;free&quot;) + scale_fill_brewer(palette = &quot;Set1&quot;) + labs( title = &quot;Posterior Distributions of Key Parameters&quot;, subtitle = &quot;Comparing model fits to different data types&quot;, x = &quot;Parameter Value&quot;, y = &quot;Density&quot;, fill = &quot;Data Type&quot; ) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) 8.9 Cross-Validation for Model Comparison Now that we’ve fit our models, we can use cross-validation techniques to compare them. We’ll start with PSIS-LOO since it’s computationally efficient, and then validate with true k-fold cross-validation. 8.9.1 PSIS-LOO Comparison # Calculate ELPD using loo Loo_biased2biased &lt;- fit_biased2biased$loo(save_psis = TRUE, cores = 4) Loo_biased2memory &lt;- fit_biased2memory$loo(save_psis = TRUE, cores = 4) Loo_memory2biased &lt;- fit_memory2biased$loo(save_psis = TRUE, cores = 4) Loo_memory2memory &lt;- fit_memory2memory$loo(save_psis = TRUE, cores = 4) # Diagnostic check for LOO reliability cat(&quot;Diagnostics for biased model on biased data:\\n&quot;) ## Diagnostics for biased model on biased data: print(sum(Loo_biased2biased$diagnostics$pareto_k &gt; 0.7)) ## [1] 0 cat(&quot;\\nDiagnostics for biased model on memory data:\\n&quot;) ## ## Diagnostics for biased model on memory data: print(sum(Loo_biased2memory$diagnostics$pareto_k &gt; 0.7)) ## [1] 0 cat(&quot;\\nDiagnostics for memory model on biased data:\\n&quot;) ## ## Diagnostics for memory model on biased data: print(sum(Loo_memory2biased$diagnostics$pareto_k &gt; 0.7)) ## [1] 0 cat(&quot;\\nDiagnostics for memory model on memory data:\\n&quot;) ## ## Diagnostics for memory model on memory data: print(sum(Loo_memory2memory$diagnostics$pareto_k &gt; 0.7)) ## [1] 0 # Extract ELPD values cat(&quot;\\nELPD for each model-data combination:\\n&quot;) ## ## ELPD for each model-data combination: cat(&quot;Biased model on biased data:&quot;, round(Loo_biased2biased$estimates[&quot;elpd_loo&quot;, &quot;Estimate&quot;], 2), &quot;\\n&quot;) ## Biased model on biased data: -6015.11 cat(&quot;Biased model on memory data:&quot;, round(Loo_biased2memory$estimates[&quot;elpd_loo&quot;, &quot;Estimate&quot;], 2), &quot;\\n&quot;) ## Biased model on memory data: -4696.54 cat(&quot;Memory model on biased data:&quot;, round(Loo_memory2biased$estimates[&quot;elpd_loo&quot;, &quot;Estimate&quot;], 2), &quot;\\n&quot;) ## Memory model on biased data: -6019.19 cat(&quot;Memory model on memory data:&quot;, round(Loo_memory2memory$estimates[&quot;elpd_loo&quot;, &quot;Estimate&quot;], 2), &quot;\\n&quot;) ## Memory model on memory data: -4526.47 8.9.2 Visualizing LOO-CV Results To better understand how our models perform, let’s visualize the pointwise differences in ELPD: # Create dataframe of pointwise ELPD differences elpd_diff &lt;- tibble( observation = seq(length(Loo_biased2biased$pointwise[, &quot;elpd_loo&quot;])), biased_data_diff = Loo_biased2biased$pointwise[, &quot;elpd_loo&quot;] - Loo_memory2biased$pointwise[, &quot;elpd_loo&quot;], memory_data_diff = Loo_memory2memory$pointwise[, &quot;elpd_loo&quot;] - Loo_biased2memory$pointwise[, &quot;elpd_loo&quot;] ) # Create long format for easier plotting elpd_long &lt;- elpd_diff %&gt;% pivot_longer( cols = c(biased_data_diff, memory_data_diff), names_to = &quot;comparison&quot;, values_to = &quot;elpd_diff&quot; ) %&gt;% mutate( comparison = case_when( comparison == &quot;biased_data_diff&quot; ~ &quot;Biased Model vs Memory Model\\non Biased Data&quot;, comparison == &quot;memory_data_diff&quot; ~ &quot;Memory Model vs Biased Model\\non Memory Data&quot; ), favors_true_model = case_when( comparison == &quot;Biased Model vs Memory Model\\non Biased Data&quot; &amp; elpd_diff &gt; 0 ~ TRUE, comparison == &quot;Memory Model vs Biased Model\\non Memory Data&quot; &amp; elpd_diff &gt; 0 ~ TRUE, TRUE ~ FALSE ) ) # Create visualization of ELPD differences p1 &lt;- ggplot(elpd_long, aes(x = observation, y = elpd_diff, color = favors_true_model)) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + geom_point(alpha = 0.3, size = 1) + facet_wrap(~comparison) + scale_color_manual(values = c(&quot;TRUE&quot; = &quot;green4&quot;, &quot;FALSE&quot; = &quot;red&quot;)) + labs( title = &quot;Pointwise ELPD Differences Between Models&quot;, subtitle = &quot;Positive values (represented in green) favor the true model (first model in comparison)&quot;, x = &quot;Observation&quot;, y = &quot;ELPD Difference&quot; ) + theme_minimal() + theme(legend.position = &quot;none&quot;) # Create density plots of ELPD differences p2 &lt;- ggplot(elpd_long, aes(x = elpd_diff, fill = comparison)) + geom_density(alpha = 0.5) + geom_vline(xintercept = 0, linetype = &quot;dashed&quot;) + scale_fill_brewer(palette = &quot;Set1&quot;) + labs( title = &quot;Distribution of ELPD Differences&quot;, subtitle = &quot;How consistently does the true model outperform the alternative?&quot;, x = &quot;ELPD Difference&quot;, y = &quot;Density&quot;, fill = &quot;Comparison&quot; ) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) # Combine plots p1 / p2 + plot_layout(heights = c(3, 2)) It’s quite clear that the bulk of the data a equally well explained by the models. And the differences for the biased data are tricky to see in the plot. Yet, we can see that in memory data there are a non trivial amount of data points better explained by the memory model (the true underlying data-generating mechanism). 8.10 Formal Model Comparison Now let’s perform formal model comparison using the loo_compare function, which computes the difference in ELPD between models and the standard error of this difference: # Compare models fitted to biased data comparison_biased &lt;- loo_compare(Loo_biased2biased, Loo_memory2biased) print(&quot;Model comparison for biased data:&quot;) ## [1] &quot;Model comparison for biased data:&quot; print(comparison_biased) ## elpd_diff se_diff ## model1 0.0 0.0 ## model2 -4.1 1.1 # Compare models fitted to memory data comparison_memory &lt;- loo_compare(Loo_memory2memory, Loo_biased2memory) print(&quot;\\nModel comparison for memory data:&quot;) ## [1] &quot;\\nModel comparison for memory data:&quot; print(comparison_memory) ## elpd_diff se_diff ## model1 0.0 0.0 ## model2 -170.1 13.0 # Calculate model weights weights_biased &lt;- loo_model_weights(list( &quot;Biased Model&quot; = Loo_biased2biased, &quot;Memory Model&quot; = Loo_memory2biased )) weights_memory &lt;- loo_model_weights(list( &quot;Memory Model&quot; = Loo_memory2memory, &quot;Biased Model&quot; = Loo_biased2memory )) # Create data for model weights visualization weights_data &lt;- tibble( model = c(&quot;Biased Model&quot;, &quot;Memory Model&quot;, &quot;Memory Model&quot;, &quot;Biased Model&quot;), data_type = c(&quot;Biased Data&quot;, &quot;Biased Data&quot;, &quot;Memory Data&quot;, &quot;Memory Data&quot;), weight = c(weights_biased[1], weights_biased[2], weights_memory[1], weights_memory[2]) ) # Visualize model weights ggplot(weights_data, aes(x = model, y = weight, fill = model)) + geom_col() + facet_wrap(~ data_type) + scale_fill_brewer(palette = &quot;Set1&quot;) + labs( title = &quot;Model Comparison via Stacking Weights&quot;, subtitle = &quot;Higher weights indicate better predictive performance&quot;, x = NULL, y = &quot;Model Weight&quot;, fill = &quot;Model Type&quot; ) + theme_minimal() + theme( legend.position = &quot;bottom&quot;, axis.text.x = element_text(angle = 45, hjust = 1) ) + geom_text(aes(label = scales::percent(weight, accuracy = 0.1)), vjust = -0.5) Here it is clear that formal model comparison can clearly pick the right model. Hurrah! 8.10.1 K-Fold Cross-Validation for Validation While PSIS-LOO is efficient, we need to check how it relates with true cross-validation. First we create new stan models, which separates data into training and test data and include the ability to calculate log-likelihood for test data. This is crucial for cross-validation, as we need to evaluate the model’s performance on unseen data. # Stan model for biased agent with cross-validation capabilities stan_biased_cv_model &lt;- &quot; // Multilevel Biased Agent Model with CV support // // This model has additional structures to handle test data for cross-validation functions{ real normal_lb_rng(real mu, real sigma, real lb) { real p = normal_cdf(lb | mu, sigma); // CDF for bounds real u = uniform_rng(p, 1); return (sigma * inv_Phi(u)) + mu; // Inverse CDF for value } } // Input data - now includes both training and test data data { int&lt;lower = 1&gt; trials; // Number of trials per agent int&lt;lower = 1&gt; agents; // Number of training agents array[trials, agents] int h; // Training choice data int&lt;lower = 1&gt; agents_test; // Number of test agents array[trials, agents_test] int h_test; // Test choice data } // Parameters to be estimated parameters { real thetaM; // Population mean of bias real&lt;lower = 0&gt; thetaSD; // Population SD of bias array[agents] real theta; // Individual agent biases } // Model definition - trained only on training data model { // Population-level priors target += normal_lpdf(thetaM | 0, 1); target += normal_lpdf(thetaSD | 0, .3) - normal_lccdf(0 | 0, .3); // Individual-level parameters target += normal_lpdf(theta | thetaM, thetaSD); // Likelihood for training data only for (i in 1:agents) target += bernoulli_logit_lpmf(h[,i] | theta[i]); } // Calculate log-likelihood for both training and test data generated quantities{ real thetaM_prior; real&lt;lower=0&gt; thetaSD_prior; real&lt;lower=0, upper=1&gt; theta_prior; real&lt;lower=0, upper=1&gt; theta_posterior; int&lt;lower=0, upper = trials&gt; prior_preds; int&lt;lower=0, upper = trials&gt; posterior_preds; // Log-likelihood for training data array[trials, agents] real log_lik; // Log-likelihood for test data - crucial for cross-validation array[trials, agents_test] real log_lik_test; // Generate prior and posterior samples thetaM_prior = normal_rng(0,1); thetaSD_prior = normal_lb_rng(0,0.3,0); theta_prior = inv_logit(normal_rng(thetaM_prior, thetaSD_prior)); theta_posterior = inv_logit(normal_rng(thetaM, thetaSD)); prior_preds = binomial_rng(trials, inv_logit(thetaM_prior)); posterior_preds = binomial_rng(trials, inv_logit(thetaM)); // Calculate log-likelihood for training data for (i in 1:agents){ for (t in 1:trials){ log_lik[t,i] = bernoulli_logit_lpmf(h[t,i] | theta[i]); } } // Calculate log-likelihood for test data // Note: We use population-level estimates for prediction for (i in 1:agents_test){ for (t in 1:trials){ log_lik_test[t,i] = bernoulli_logit_lpmf(h_test[t,i] | thetaM); } } } &quot; # Write the model to a file write_stan_file( stan_biased_cv_model, dir = &quot;stan/&quot;, basename = &quot;W7_MultilevelBias_cv.stan&quot; ) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W7_MultilevelBias_cv.stan&quot; # Compile the model file &lt;- file.path(&quot;stan/W7_MultilevelBias_cv.stan&quot;) mod_biased_cv &lt;- cmdstan_model( file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;) ) # Similarly, define the memory agent model with CV support stan_memory_cv_model &lt;- &quot; // Multilevel Memory Agent Model with CV support // // This model has additional structures to handle test data for cross-validation functions{ real normal_lb_rng(real mu, real sigma, real lb) { real p = normal_cdf(lb | mu, sigma); // CDF for bounds real u = uniform_rng(p, 1); return (sigma * inv_Phi(u)) + mu; // Inverse CDF for value } } // Input data - includes both training and test data data { int&lt;lower = 1&gt; trials; // Number of trials per agent int&lt;lower = 1&gt; agents; // Number of training agents array[trials, agents] int h; // Training choice data array[trials, agents] int other; // Opponent&#39;s choices for training agents int&lt;lower = 1&gt; agents_test; // Number of test agents array[trials, agents_test] int h_test; // Test choice data array[trials, agents_test] int other_test; // Opponent&#39;s choices for test agents } // Parameters to be estimated parameters { real biasM; // Population mean baseline bias real betaM; // Population mean memory sensitivity vector&lt;lower = 0&gt;[2] tau; // Population SDs matrix[2, agents] z_IDs; // Standardized individual parameters cholesky_factor_corr[2] L_u; // Cholesky factor of correlation matrix } // Transformed parameters transformed parameters { // Memory states for training data array[trials, agents] real memory; // Memory states for test data array[trials, agents_test] real memory_test; // Individual parameters matrix[agents,2] IDs; IDs = (diag_pre_multiply(tau, L_u) * z_IDs)&#39;; // Calculate memory states for training data for (agent in 1:agents){ for (trial in 1:trials){ if (trial == 1) { memory[trial, agent] = 0.5; } if (trial &lt; trials){ memory[trial + 1, agent] = memory[trial, agent] + ((other[trial, agent] - memory[trial, agent]) / trial); if (memory[trial + 1, agent] == 0){memory[trial + 1, agent] = 0.01;} if (memory[trial + 1, agent] == 1){memory[trial + 1, agent] = 0.99;} } } } // Calculate memory states for test data for (agent in 1:agents_test){ for (trial in 1:trials){ if (trial == 1) { memory_test[trial, agent] = 0.5; } if (trial &lt; trials){ memory_test[trial + 1, agent] = memory_test[trial, agent] + ((other_test[trial, agent] - memory_test[trial, agent]) / trial); if (memory_test[trial + 1, agent] == 0){memory_test[trial + 1, agent] = 0.01;} if (memory_test[trial + 1, agent] == 1){memory_test[trial + 1, agent] = 0.99;} } } } } // Model definition - trained only on training data model { // Population-level priors target += normal_lpdf(biasM | 0, 1); target += normal_lpdf(tau[1] | 0, .3) - normal_lccdf(0 | 0, .3); target += normal_lpdf(betaM | 0, .3); target += normal_lpdf(tau[2] | 0, .3) - normal_lccdf(0 | 0, .3); target += lkj_corr_cholesky_lpdf(L_u | 2); // Standardized individual parameters target += std_normal_lpdf(to_vector(z_IDs)); // Likelihood for training data only for (agent in 1:agents){ for (trial in 1:trials){ target += bernoulli_logit_lpmf(h[trial, agent] | biasM + IDs[agent, 1] + memory[trial, agent] * (betaM + IDs[agent, 2])); } } } // Calculate log-likelihood for both training and test data generated quantities{ // Prior samples real biasM_prior; real&lt;lower=0&gt; biasSD_prior; real betaM_prior; real&lt;lower=0&gt; betaSD_prior; real bias_prior; real beta_prior; // Posterior predictive samples array[agents] int&lt;lower=0, upper = trials&gt; prior_preds0; array[agents] int&lt;lower=0, upper = trials&gt; prior_preds1; array[agents] int&lt;lower=0, upper = trials&gt; prior_preds2; array[agents] int&lt;lower=0, upper = trials&gt; posterior_preds0; array[agents] int&lt;lower=0, upper = trials&gt; posterior_preds1; array[agents] int&lt;lower=0, upper = trials&gt; posterior_preds2; // Log-likelihood for training data array[trials, agents] real log_lik; // Log-likelihood for test data - crucial for cross-validation array[trials, agents_test] real log_lik_test; // Generate prior samples biasM_prior = normal_rng(0,1); biasSD_prior = normal_lb_rng(0,0.3,0); betaM_prior = normal_rng(0,1); betaSD_prior = normal_lb_rng(0,0.3,0); bias_prior = normal_rng(biasM_prior, biasSD_prior); beta_prior = normal_rng(betaM_prior, betaSD_prior); // Generate predictions for different memory conditions for (i in 1:agents){ prior_preds0[i] = binomial_rng(trials, inv_logit(bias_prior + 0 * beta_prior)); prior_preds1[i] = binomial_rng(trials, inv_logit(bias_prior + 1 * beta_prior)); prior_preds2[i] = binomial_rng(trials, inv_logit(bias_prior + 2 * beta_prior)); posterior_preds0[i] = binomial_rng(trials, inv_logit(biasM + IDs[i,1] + 0 * (betaM + IDs[i,2]))); posterior_preds1[i] = binomial_rng(trials, inv_logit(biasM + IDs[i,1] + 1 * (betaM + IDs[i,2]))); posterior_preds2[i] = binomial_rng(trials, inv_logit(biasM + IDs[i,1] + 2 * (betaM + IDs[i,2]))); // Calculate log-likelihood for training data for (t in 1:trials){ log_lik[t,i] = bernoulli_logit_lpmf(h[t, i] | biasM + IDs[i, 1] + memory[t, i] * (betaM + IDs[i, 2])); } } // Calculate log-likelihood for test data // Note: We use population-level estimates for prediction for (i in 1:agents_test){ for (t in 1:trials){ log_lik_test[t,i] = bernoulli_logit_lpmf(h_test[t,i] | biasM + memory_test[t, i] * betaM); } } } &quot; # Write the model to a file write_stan_file( stan_memory_cv_model, dir = &quot;stan/&quot;, basename = &quot;W7_MultilevelMemory_cv.stan&quot; ) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W7_MultilevelMemory_cv.stan&quot; # Compile the model file &lt;- file.path(&quot;stan/W7_MultilevelMemory_cv.stan&quot;) mod_memory_cv &lt;- cmdstan_model( file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;) ) These CV-ready models extend our original models with additional structures to handle test data and compute separate log-likelihoods for training and test observations. Now, let’s demonstrate how to implement k-fold cross-validation using these models (N.b. we only fit both models to the memory data, I still need to implement the full comparison against biased data as well) # Define file path for saved CV results cv_results_file &lt;- &quot;simmodels/W7_CV_Biased&amp;Memory.RData&quot; # Check if we need to run the CV analysis if (regenerate_simulations || !file.exists(cv_results_file)) { # Load necessary packages for parallelization pacman::p_load(future, furrr) # Set up parallel processing (adjust workers based on your system) plan(multisession, workers = 8) # Split the data into folds (10 folds, grouped by agent) d$fold &lt;- kfold_split_grouped(K = 10, x = d$agent) unique_folds &lt;- unique(d$fold) # Initialize matrices to store log predictive densities # We&#39;ll fill these after parallel processing log_pd_biased_kfold &lt;- matrix(nrow = 1000, ncol = nrow(d)) log_pd_memory_kfold &lt;- matrix(nrow = 1000, ncol = nrow(d)) # Define function to process a single fold process_fold &lt;- function(k) { cat(&quot;Starting processing of fold&quot;, k, &quot;\\n&quot;) # Create training set (all data except fold k) d_train &lt;- d %&gt;% filter(fold != k) # Prepare training data for Stan d_memory1_train &lt;- d_train %&gt;% dplyr::select(agent, memoryChoice) %&gt;% mutate(row = row_number()) %&gt;% pivot_wider(names_from = agent, values_from = memoryChoice) d_memory2_train &lt;- d_train %&gt;% dplyr::select(agent, randomChoice) %&gt;% mutate(row = row_number()) %&gt;% pivot_wider(names_from = agent, values_from = randomChoice) agents_n &lt;- length(unique(d_train$agent)) # Create test set (only fold k) d_test &lt;- d %&gt;% filter(fold == k) d_memory1_test &lt;- d_test %&gt;% dplyr::select(agent, memoryChoice) %&gt;% mutate(row = row_number()) %&gt;% pivot_wider(names_from = agent, values_from = memoryChoice) d_memory2_test &lt;- d_test %&gt;% dplyr::select(agent, randomChoice) %&gt;% mutate(row = row_number()) %&gt;% pivot_wider(names_from = agent, values_from = randomChoice) agents_test_n &lt;- length(unique(d_test$agent)) # Prepare data for Stan model data_memory &lt;- list( trials = trials, agents = agents_n, agents_test = agents_test_n, h = as.matrix(d_memory1_train[, 2:(agents_n + 1)]), other = as.matrix(d_memory2_train[, 2:(agents_n + 1)]), h_test = as.matrix(d_memory1_test[, 2:(agents_test_n + 1)]), other_test = as.matrix(d_memory2_test[, 2:(agents_test_n + 1)]) ) # Fit models on training data and evaluate on test data # Note: removed threads_per_chain parameter fit_biased &lt;- mod_biased_cv$sample( data = data_memory, seed = 123 + k, # Unique seed per fold chains = 1, parallel_chains = 1, threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 1000, refresh = 0, # Suppress progress output in parallel runs max_treedepth = 20, adapt_delta = 0.99 ) fit_memory &lt;- mod_memory_cv$sample( data = data_memory, seed = 456 + k, # Unique seed per fold chains = 1, parallel_chains = 1, threads_per_chain = 1, iter_warmup = 1000, iter_sampling = 1000, refresh = 0, # Suppress progress output in parallel runs max_treedepth = 20, adapt_delta = 0.99 ) # Extract log likelihood for test data points biased_log_lik &lt;- fit_biased$draws(&quot;log_lik_test&quot;, format = &quot;matrix&quot;) memory_log_lik &lt;- fit_memory$draws(&quot;log_lik_test&quot;, format = &quot;matrix&quot;) # Create indices to keep track of which positions in the full matrix these values belong to test_indices &lt;- which(d$fold == k) # Return results for this fold result &lt;- list( fold = k, test_indices = test_indices, biased_log_lik = biased_log_lik, memory_log_lik = memory_log_lik ) cat(&quot;Completed processing of fold&quot;, k, &quot;\\n&quot;) return(result) } # Process all folds in parallel using furrr cat(&quot;Starting parallel processing of all folds\\n&quot;) results &lt;- future_map(unique_folds, process_fold, .options = furrr_options(seed = TRUE)) # Now populate the full matrices with results from each fold for (i in seq_along(results)) { fold_result &lt;- results[[i]] test_indices &lt;- fold_result$test_indices # For each test index, populate the corresponding column in the full matrices for (j in seq_along(test_indices)) { idx &lt;- test_indices[j] log_pd_biased_kfold[, idx] &lt;- fold_result$biased_log_lik[, j] log_pd_memory_kfold[, idx] &lt;- fold_result$memory_log_lik[, j] } } # Save results save(log_pd_biased_kfold, log_pd_memory_kfold, file = cv_results_file) cat(&quot;Generated new cross-validation results and saved to&quot;, cv_results_file, &quot;\\n&quot;) } else { # Load existing results load(cv_results_file) cat(&quot;Loaded existing cross-validation results from&quot;, cv_results_file, &quot;\\n&quot;) } ## Starting parallel processing of all folds ## Starting processing of fold 7 ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 13.8 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 1253.8 seconds. ## Completed processing of fold 7 ## Starting processing of fold 10 ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 12.8 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 2854.1 seconds. ## Completed processing of fold 10 ## Starting processing of fold 8 ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 13.1 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 1117.8 seconds. ## Completed processing of fold 8 ## Starting processing of fold 5 ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 14.4 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 774.3 seconds. ## Completed processing of fold 5 ## Starting processing of fold 4 ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 13.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 987.3 seconds. ## Completed processing of fold 4 ## Starting processing of fold 9 ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 13.8 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 1352.2 seconds. ## Completed processing of fold 9 ## Starting processing of fold 2 ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 13.9 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 724.8 seconds. ## Completed processing of fold 2 ## Starting processing of fold 1 ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 13.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 938.2 seconds. ## Completed processing of fold 1 ## Starting processing of fold 6 ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 13.6 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 756.7 seconds. ## Completed processing of fold 6 ## Starting processing of fold 3 ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 13.5 seconds. ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 finished in 3045.9 seconds. ## Completed processing of fold 3 ## Generated new cross-validation results and saved to simmodels/W7_CV_Biased&amp;Memory.RData # Load cross-validation results cv_results_file &lt;- &quot;simmodels/W7_CV_Biased&amp;Memory.RData&quot; load(cv_results_file) # Calculate ELPD values from cross-validation results elpd_biased_kfold &lt;- elpd(log_pd_biased_kfold) elpd_memory_kfold &lt;- elpd(log_pd_memory_kfold) # Extract ELPD values and standard errors biased_elpd_value &lt;- elpd_biased_kfold$estimates[&quot;elpd&quot;, &quot;Estimate&quot;] memory_elpd_value &lt;- elpd_memory_kfold$estimates[&quot;elpd&quot;, &quot;Estimate&quot;] biased_se &lt;- elpd_biased_kfold$estimates[&quot;elpd&quot;, &quot;SE&quot;] memory_se &lt;- elpd_memory_kfold$estimates[&quot;elpd&quot;, &quot;SE&quot;] # Print values for comparison cat(&quot;K-fold CV results:\\n&quot;) ## K-fold CV results: cat(&quot;Biased model ELPD:&quot;, biased_elpd_value, &quot;±&quot;, biased_se, &quot;\\n&quot;) ## Biased model ELPD: -5543.23 ± 78.8173 cat(&quot;Memory model ELPD:&quot;, memory_elpd_value, &quot;±&quot;, memory_se, &quot;\\n&quot;) ## Memory model ELPD: -4860.888 ± 66.89472 # Create a results table kfold_results &lt;- tibble( model = c(&quot;Biased Agent Model&quot;, &quot;Memory Agent Model&quot;), elpd = c(biased_elpd_value, memory_elpd_value), se = c(biased_se, memory_se) ) # Calculate model weights from ELPD values max_elpd &lt;- max(c(biased_elpd_value, memory_elpd_value)) relative_elpd &lt;- c(biased_elpd_value, memory_elpd_value) - max_elpd model_weights &lt;- exp(relative_elpd) / sum(exp(relative_elpd)) # Add weights to results kfold_results$weight &lt;- model_weights # Display the results table kfold_results ## # A tibble: 2 × 4 ## model elpd se weight ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Biased Agent Model -5543. 78.8 4.60e-297 ## 2 Memory Agent Model -4861. 66.9 1 e+ 0 8.10.2 Comparing LOO and K-Fold Results # Create comparison table of PSIS-LOO vs K-fold CV loo_results &lt;- tibble( model = c(&quot;Biased Agent Model&quot;, &quot;Memory Agent Model&quot;), loo_elpd = c(Loo_biased2biased$estimates[&quot;elpd_loo&quot;, &quot;Estimate&quot;], Loo_memory2biased$estimates[&quot;elpd_loo&quot;, &quot;Estimate&quot;]), loo_se = c(Loo_biased2biased$estimates[&quot;elpd_loo&quot;, &quot;SE&quot;], Loo_memory2biased$estimates[&quot;elpd_loo&quot;, &quot;SE&quot;]), loo_weight = c(weights_biased[1], weights_biased[2]) ) # Combine with k-fold results comparison_results &lt;- left_join(loo_results, kfold_results, by = &quot;model&quot;) %&gt;% rename(kfold_elpd = elpd, kfold_se = se, kfold_weight = weight) # Display comparison comparison_results %&gt;% mutate( loo_elpd = round(loo_elpd, 1), loo_se = round(loo_se, 2), loo_weight = round(loo_weight, 3), kfold_elpd = round(kfold_elpd, 1), kfold_se = round(kfold_se, 2), kfold_weight = round(kfold_weight, 3) ) %&gt;% knitr::kable( caption = &quot;Comparison of PSIS-LOO and K-fold Cross-Validation Results&quot;, col.names = c(&quot;Model&quot;, &quot;LOO ELPD&quot;, &quot;LOO SE&quot;, &quot;LOO Weight&quot;, &quot;K-fold ELPD&quot;, &quot;K-fold SE&quot;, &quot;K-fold Weight&quot;) ) Table 8.1: Comparison of PSIS-LOO and K-fold Cross-Validation Results Model LOO ELPD LOO SE LOO Weight K-fold ELPD K-fold SE K-fold Weight Biased Agent Model -6015.1 58.82 1 -5543.2 78.82 0 Memory Agent Model -6019.2 58.83 0 -4860.9 66.89 1 Funnily enough cross-validation indicates the wrong model. 8.11 Limitations of Model Comparison Approaches While cross-validation and ELPD provide powerful tools for model comparison, it’s important to understand their limitations: Training vs. Transfer Generalization Cross-validation only assesses a model’s ability to generalize to new data from the same distribution (training generalization). It doesn’t evaluate how well models transfer to different contexts or populations (transfer generalization). Model Misspecification All models are wrong, but some are useful. Cross-validation helps identify which wrong model is most useful for prediction, but doesn’t guarantee we’ve captured the true generating process. Limited Data With limited data, cross-validation estimates can have high variance, especially for complex models. K-fold CV with small k can help mitigate this issue. Computational Cost True cross-validation requires refitting models multiple times, which can be prohibitively expensive for complex Bayesian models. PSIS-LOO offers an efficient approximation but may not always be reliable. Parameter vs. Predictive Focus Model comparison based on predictive performance might select different models than if we were focused on accurate parameter estimation. The “best” model depends on your goals. 8.12 Exercises Compare the models on different subsets of the data (e.g., early vs. late trials). Does the preferred model change depending on which portion of the data you use? Experiment with different priors for the models. How sensitive are the model comparison results to prior choices? Implement a different model (e.g., win-stay-lose-shift) and compare it to the biased and memory models. Which performs best? Explore how the amount of data affects model comparison. How many trials do you need to reliably identify the true model? Investigate the relationship between model complexity and predictive performance in this context. Are there systematic patterns in when simpler models are preferred? 8.12.1 Rant on internal vs external validity However, we need to think carefully about what we mean by “out of sample.” There are actually two distinct types of test sets we might consider: internal and external. Internal test sets come from the same data collection effort as our training data - for example, we might randomly set aside 20% of our matching pennies games to test on. While this approach helps us detect overfitting to specific participants or trials, it cannot tell us how well our model generalizes to truly new contexts. Our test set participants were recruited from the same population, played the game under the same conditions, and were influenced by the same experimental setup as our training participants. External test sets, in contrast, come from genuinely different contexts. For our matching pennies model, this might mean testing on games played: In different cultures or age groups Under time pressure versus relaxed conditions For real money versus just for fun Against human opponents versus computer agents In laboratory versus online settings The distinction matters because cognitive models often capture not just universal mental processes, but also specific strategies that people adopt in particular contexts. A model that perfectly predicts behavior in laboratory matching pennies games might fail entirely when applied to high-stakes poker games, even though both involve similar strategic thinking. This raises deeper questions about what kind of generalization we want our models to achieve. Are we trying to build models that capture universal cognitive processes, or are we content with models that work well within specific contexts? The answer affects not just how we evaluate our models, but how we design them in the first place. In practice, truly external test sets are rare in cognitive science - they require additional data collection under different conditions, which is often impractical. This means we must be humble about our claims of generalization. When we talk about a model’s predictive accuracy, we should be clear that we’re usually measuring its ability to generalize within a specific experimental context, not its ability to capture human cognition in all its diversity. This limitation of internal test sets is one reason why cognitive scientists often complement predictive accuracy metrics with other forms of model evaluation, such as testing theoretical predictions on new tasks or examining whether model parameters correlate sensibly with individual differences. These approaches help us build confidence that our models capture meaningful cognitive processes rather than just statistical patterns specific to our experimental setup. *** "],["mixture-models.html", "Chapter 9 Mixture models 9.1 Visualizing Mixture Distributions: Reaction Time Example 9.2 Theoretical Foundation for Mixture Models 9.3 Implementing a Basic Mixture Model 9.4 Parameter Interpretation 9.5 Multilevel Mixture Models 9.6 Comparing Single-Process and Mixture Models 9.7 Conclusion", " Chapter 9 Mixture models When we model human cognition and behavior, we often find ourselves facing a puzzling reality: people don’t always follow a single, consistent strategy. Consider a person playing our matching pennies game - sometimes they might carefully track their opponent’s patterns, other times they might rely on a simple bias toward choosing “heads,” and occasionally they might respond completely randomly when their attention lapses. Traditional cognitive models that assume a single process are ill-equipped to capture this complexity. Mixture models provide an elegant solution to this challenge. Rather than assuming behavior reflects just one cognitive process, mixture models allow us to combine multiple different processes within a unified modeling framework. In the previous chapter, we explored model comparison techniques that help us select between competing cognitive models. Mixture models approach in a sense reframe that very problem - instead of asking “which model is correct?”, they allow us to ask “how much does each model contribute?” This shift acknowledges the possibility that multiple cognitive processes might coexist, either within a single individual or across a population. 9.0.1 Learning Objectives After completing this chapter, you will be able to: Understand how mixture models combine multiple cognitive processes Implement mixture models for different types of behavioral data using Stan Estimate mixture weights and component-specific parameters Evaluate mixture models through posterior predictive checks Compare mixture models to single-process alternatives 9.0.2 Chapter Roadmap In this chapter, we will: Examine reaction time data to visualize how mixture distributions appear in practice Develop a theoretical foundation for mixture modeling Implement a simple mixture model combining biased and random choice processes (thus going back to our matching pennies example) Evaluate and validate this model through posterior checks Extend to multilevel mixture models that capture individual differences Compare mixture models with traditional single-process approaches 9.1 Visualizing Mixture Distributions: Reaction Time Example Before diving into the mathematics and implementation of mixture models, let’s start with a concrete example that visually demonstrates why we need them. Reaction time (RT) data provides a particularly clear window into the mixture of cognitive processes. In many cognitive tasks, participants are asked to respond as quickly as possible while still being accurate. However, attention fluctuates over time. Let’s simulate a scenario where participants sometimes engage in deliberate thinking (producing a log-normal distribution of RTs) and sometimes experience attentional lapses (producing more or less random responses with a uniform distribution of RTs). # Flag to control whether to regenerate simulations regenerate_simulations &lt;- TRUE # Load necessary packages library(tidyverse) library(here) library(posterior) library(cmdstanr) library(bayesplot) library(patchwork) library(loo) set.seed(123) # For reproducible results # Number of observations n_obs &lt;- 500 # Parameters for the two processes # Process 1: Deliberate thinking (log-normal distribution) mu_delib &lt;- 6.2 # Mean of log(RT) for deliberate process (≈ 500ms) sigma_delib &lt;- 0.2 # SD of log(RT) for deliberate process # Process 2: Attentional lapses (uniform distribution) min_lapse &lt;- 200 # Minimum RT during attentional lapses (ms) max_lapse &lt;- 2000 # Maximum RT during attentional lapses (ms) # Mixture weight (proportion of deliberate responses) pi_delib &lt;- 0.8 # 80% deliberate thinking, 20% attentional lapses # Simulate the mixture process &lt;- rbinom(n_obs, 1, pi_delib) # 1 = deliberate, 0 = lapse # Generate RTs from the appropriate distribution based on process rt &lt;- numeric(n_obs) for (i in 1:n_obs) { if (process[i] == 1) { # Deliberate process - log-normal distribution rt[i] &lt;- rlnorm(1, mu_delib, sigma_delib) } else { # Attentional lapse - uniform distribution rt[i] &lt;- runif(1, min_lapse, max_lapse) } } # Combine into a data frame rt_data &lt;- tibble( observation = 1:n_obs, rt = rt, process = ifelse(process == 1, &quot;Deliberate&quot;, &quot;Attentional Lapse&quot;), log_rt = log(rt) ) # Summary statistics by process rt_summary &lt;- rt_data %&gt;% group_by(process) %&gt;% summarize( count = n(), proportion = n()/n_obs, mean_rt = mean(rt), median_rt = median(rt), sd_rt = sd(rt) ) # Display summary knitr::kable(rt_summary, digits = 2) process count proportion mean_rt median_rt sd_rt Attentional Lapse 94 0.19 1076.04 1106.17 552.71 Deliberate 406 0.81 494.98 490.17 92.08 Now, let’s visualize the reaction time distribution: # Create histogram of reaction times with overlaid density curves p1 &lt;- ggplot() + # Histogram of all data geom_histogram(data = rt_data, aes(x = rt, y = ..density..), bins = 30, fill = &quot;gray80&quot;, color = &quot;black&quot;, alpha = 0.5) + # Density curves for each component geom_density(data = rt_data %&gt;% filter(process == &quot;Deliberate&quot;), aes(x = rt, fill = &quot;Deliberate Thinking&quot;), alpha = 0.4) + # We can&#39;t use geom_density for uniform distribution (it would smooth it) # Instead, overlay the theoretical uniform density geom_segment(aes(x = min_lapse, xend = max_lapse, y = 1/(max_lapse-min_lapse), yend = 1/(max_lapse-min_lapse), color = &quot;Attentional Lapse&quot;), linewidth = 1.5, alpha = 0.8) + # Overall density geom_density(data = rt_data, aes(x = rt), color = &quot;black&quot;, linewidth = 1) + # Aesthetics scale_fill_manual(values = c(&quot;Deliberate Thinking&quot; = &quot;blue&quot;), name = &quot;Process&quot;) + scale_color_manual(values = c(&quot;Attentional Lapse&quot; = &quot;red&quot;), name = &quot;Process&quot;) + labs(title = &quot;Reaction Time Distribution&quot;, subtitle = &quot;A mixture of deliberate thinking and attentional lapses&quot;, x = &quot;Reaction Time (ms)&quot;, y = &quot;Density&quot;) + coord_cartesian(xlim = c(0, 2000)) + theme_minimal() + theme(legend.position = &quot;top&quot;) # Also plot the log RT p2 &lt;- ggplot() + geom_histogram(data = rt_data, aes(x = log_rt, y = ..density..), bins = 30, fill = &quot;gray80&quot;, color = &quot;black&quot;, alpha = 0.5) + geom_density(data = rt_data %&gt;% filter(process == &quot;Deliberate&quot;), aes(x = log_rt, fill = &quot;Deliberate Thinking&quot;), alpha = 0.4) + geom_density(data = rt_data, aes(x = log_rt), color = &quot;black&quot;, linewidth = 1) + scale_fill_manual(values = c(&quot;Deliberate Thinking&quot; = &quot;blue&quot;), name = &quot;Process&quot;) + labs(title = &quot;Log Reaction Time Distribution&quot;, subtitle = &quot;Log transformation highlights the mixture components&quot;, x = &quot;Log Reaction Time&quot;, y = &quot;Density&quot;) + theme_minimal() + theme(legend.position = &quot;top&quot;) # Display both plots p1 / p2 In this example, we can clearly see how the overall reaction time distribution (black line) is a combination of two distinct processes: Deliberate Thinking (Blue): A log-normal distribution centered around 500ms, representing focused cognitive processing of the task. Attentional Lapses (Red Line): A uniform distribution spanning from very quick to very slow responses, representing trials where attention has drifted, leading to either impulsive responses or delayed responses due to mind-wandering. The mixture of these processes creates a complex distribution with a prominent peak (from the deliberate process) and extended tails (from the attentional lapses). A standard single-process model assuming only a log-normal distribution would fail to capture these extended tails, leading to poor fit and potentially misleading conclusions about the cognitive processes involved. This is exactly the situation where mixture models excel. They allow us to represent observed data as coming from a weighted combination of different underlying processes. Next, we’ll formalize this intuition and extend it to decision-making models. 9.2 Theoretical Foundation for Mixture Models 9.2.1 The Mixture Model Framework At their core, mixture models represent data as coming from a weighted combination of different “component” distributions or processes. Mathematically, a mixture model can be expressed as: p(y) = π_1p_1(y) + π_2p_2(y) + … + π_kp_k(y) Where: p(y) is the overall probability of observing data point y p_j(y) is the probability of y according to component model j π_j is the weight or mixing proportion of component j (with all π_j summing to 1) Each component distribution p_j(y) can have its own parameters, and the mixing proportions π_j determine how much each component contributes to the overall model. 9.2.2 Mixture Models and Latent Variables An alternative and often useful way to think about mixture models is through latent (unobserved) variables. We can introduce a latent categorical variable z that indicates which component generated each observation. For example, in a two-component mixture: z_i = 1 means observation i came from component 1 z_i=2 means observation i came from component 2 We can then formulate the mixture model as: p(y_i, z_i) = p(z_i)p(y_i∣z_i) Where p(z_i=j)=π_j is the prior probability of component j, and p(y_i∣z_i = j) is the likelihood of y_i under component j. This latent variable perspective is particularly useful for implementing mixture models in Bayesian frameworks like Stan. 9.2.3 Applications in Cognitive Modeling In cognitive modeling, mixture models can represent several important phenomena: Attentional fluctuations: As in our reaction time example, performance may reflect both focused engagement and attentional lapses Strategy switching: Individuals might switch between different strategies over time Dual-process theories: Behavior might arise from multiple cognitive systems (e.g., automatic vs. controlled) Individual differences: Different individuals might use different strategies Exploration vs. exploitation: Some decisions might reflect exploring new options while others exploit known rewards 9.3 Implementing a Basic Mixture Model Now that we understand the theoretical foundation, let’s implement a simple mixture model for choice data. We’ll model behavior as a mixture of two processes: A biased process that chooses option 1, let’s say “right”, with some probability θ A random process that chooses randomly (50/50) between options 1 and 0, or “right” and “left” This could represent, for example, a person who sometimes carefully performs a task but occasionally responds randomly due to attentional lapses. 9.3.1 Simulating Mixed Strategy Data In previous chapters we generated data to use for fitting models. Let’s use that same data but focus on a particular agent who might be mixing strategies: # Load data from a previous chapter d &lt;- read_csv(&quot;simdata/W3_randomnoise.csv&quot;) # Choose data with some bias (0.8) and noise (0.1) dd &lt;- d %&gt;% subset(rate == 0.8 &amp; noise == 0.1) # Prepare data for Stan data &lt;- list( n = 120, h = dd$choice ) # Display summary statistics mean_choice &lt;- mean(dd$choice) cat(&quot;Mean choice (proportion of &#39;right&#39; choices):&quot;, mean_choice, &quot;\\n&quot;) ## Mean choice (proportion of &#39;right&#39; choices): 0.7666667 Let’s visualize this data to see if we can detect patterns suggestive of a mixture: # Calculate running proportion of right choices running_prop &lt;- cumsum(dd$choice) / seq_along(dd$choice) # Plot actual choices over time p1 &lt;- ggplot(dd, aes(x = trial)) + geom_point(aes(y = choice), alpha = 0.5) + geom_hline(yintercept = 0.5, linetype = &quot;dashed&quot;, color = &quot;red&quot;) + labs(title = &quot;Sequence of Choices&quot;, subtitle = &quot;0 = left, 1 = right&quot;, x = &quot;Trial&quot;, y = &quot;Choice&quot;) + theme_minimal() # Plot running average p2 &lt;- ggplot(dd, aes(x = trial)) + geom_line(aes(y = running_prop)) + geom_hline(yintercept = 0.5, linetype = &quot;dashed&quot;, color = &quot;red&quot;) + geom_hline(yintercept = 0.8, linetype = &quot;dashed&quot;, color = &quot;blue&quot;) + labs(title = &quot;Running Proportion of Right Choices&quot;, subtitle = &quot;Red: random choice (0.5), Blue: true bias (0.8)&quot;, x = &quot;Trial&quot;, y = &quot;Proportion Right&quot;) + ylim(0, 1) + theme_minimal() p1 / p2 For binary choice data, it’s harder to visually detect a mixture compared to reaction times. The overall proportion of right choices falls between what we’d expect from random choice (0.5) and fully biased choice with rate 0.8, which is consistent with a mixture of these processes. 9.3.2 Stan Implementation of a Mixture Model Now let’s implement a Stan model that represents choices as coming from a mixture of a biased process and a random process: stan_mixture_model &lt;- &quot; // Mixture model for binary choice data // Combines a biased choice process with a random choice process data { int&lt;lower=1&gt; n; // Number of trials array[n] int h; // Choice data (0/1) } parameters { real bias; // Bias parameter for biased process (logit scale) real noise_logit; // Mixing weight for random process (logit scale) } model { // Priors target += normal_lpdf(bias | 0, 1); // Prior for bias parameter (centered at 0.5 in prob scale) target += normal_lpdf(noise_logit | -1, 1); // Prior for noise proportion (favors lower noise) // Mixture likelihood using log_sum_exp for numerical stability target += log_sum_exp( log(inv_logit(noise_logit)) + // Log probability of random process bernoulli_logit_lpmf(h | 0), // Log likelihood under random process (p=0.5) log1m(inv_logit(noise_logit)) + // Log probability of biased process bernoulli_logit_lpmf(h | bias) // Log likelihood under biased process ); } generated quantities { // Transform parameters to probability scale for easier interpretation real&lt;lower=0, upper=1&gt; noise_p = inv_logit(noise_logit); // Proportion of random choices real&lt;lower=0, upper=1&gt; bias_p = inv_logit(bias); // Bias toward right in biased choices // Predicted distributions vector[n] log_lik; array[n] int pred_component; // Which component generated each prediction (1=random, 0=biased) array[n] int pred_choice; // Predicted choices // Calculate log likelihood for each observation (for model comparison) for (i in 1:n) { log_lik[i] = log_sum_exp( log(noise_p) + bernoulli_logit_lpmf(h[i] | 0), log1m(noise_p) + bernoulli_logit_lpmf(h[i] | bias) ); } // Generate posterior predictions for (i in 1:n) { // First determine which component to use pred_component[i] = bernoulli_rng(noise_p); // Then generate prediction from appropriate component if (pred_component[i] == 1) { // Random component pred_choice[i] = bernoulli_rng(0.5); } else { // Biased component pred_choice[i] = bernoulli_rng(bias_p); } } } &quot; # Write the Stan model to a file write_stan_file( stan_mixture_model, dir = &quot;stan/&quot;, basename = &quot;W8_MixtureSingle.stan&quot; ) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W8_MixtureSingle.stan&quot; # Compile the model file &lt;- file.path(&quot;stan/W8_MixtureSingle.stan&quot;) mod_mixture &lt;- cmdstan_model(file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;)) 9.3.3 Fitting and Evaluating the Mixture Model Now let’s fit the model and evaluate its performance: # File path for saved model results model_file &lt;- &quot;simmodels/W8_singlemixture.RDS&quot; # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(model_file)) { # Fit the model samples &lt;- mod_mixture$sample( data = data, seed = 123, chains = 2, parallel_chains = 2, threads_per_chain = 1, iter_warmup = 2000, iter_sampling = 2000, refresh = 500, max_treedepth = 20, adapt_delta = 0.99, ) # Save the results samples$save_object(file = model_file) cat(&quot;Generated new model fit and saved to&quot;, model_file, &quot;\\n&quot;) } else { # Load existing results samples &lt;- readRDS(model_file) cat(&quot;Loaded existing model fit from&quot;, model_file, &quot;\\n&quot;) } ## Running MCMC with 2 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 Iteration: 1 / 4000 [ 0%] (Warmup) ## Chain 1 Iteration: 500 / 4000 [ 12%] (Warmup) ## Chain 1 Iteration: 1000 / 4000 [ 25%] (Warmup) ## Chain 1 Iteration: 1500 / 4000 [ 37%] (Warmup) ## Chain 1 Iteration: 2000 / 4000 [ 50%] (Warmup) ## Chain 1 Iteration: 2001 / 4000 [ 50%] (Sampling) ## Chain 1 Iteration: 2500 / 4000 [ 62%] (Sampling) ## Chain 1 Iteration: 3000 / 4000 [ 75%] (Sampling) ## Chain 2 Iteration: 1 / 4000 [ 0%] (Warmup) ## Chain 2 Iteration: 500 / 4000 [ 12%] (Warmup) ## Chain 2 Iteration: 1000 / 4000 [ 25%] (Warmup) ## Chain 2 Iteration: 1500 / 4000 [ 37%] (Warmup) ## Chain 2 Iteration: 2000 / 4000 [ 50%] (Warmup) ## Chain 2 Iteration: 2001 / 4000 [ 50%] (Sampling) ## Chain 1 Iteration: 3500 / 4000 [ 87%] (Sampling) ## Chain 1 Iteration: 4000 / 4000 [100%] (Sampling) ## Chain 2 Iteration: 2500 / 4000 [ 62%] (Sampling) ## Chain 2 Iteration: 3000 / 4000 [ 75%] (Sampling) ## Chain 1 finished in 0.3 seconds. ## Chain 2 Iteration: 3500 / 4000 [ 87%] (Sampling) ## Chain 2 Iteration: 4000 / 4000 [100%] (Sampling) ## Chain 2 finished in 0.3 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.5 seconds. ## ## Generated new model fit and saved to simmodels/W8_singlemixture.RDS # Extract summary statistics summary &lt;- samples$summary(c(&quot;bias_p&quot;, &quot;noise_p&quot;)) print(summary) ## # A tibble: 2 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 bias_p 0.756 0.758 0.0381 0.0392 0.692 0.816 1.00 2276. 2058. ## 2 noise_p 0.253 0.224 0.152 0.149 0.0608 0.549 1.00 2298. 1875. 9.3.4 Model Diagnostics Let’s check the model diagnostics to ensure our inference is reliable: # Extract draws for diagnostics draws &lt;- as_draws_df(samples$draws()) # Check for convergence with trace plots p1 &lt;- mcmc_trace(draws, pars = c(&quot;bias_p&quot;, &quot;noise_p&quot;)) + ggtitle(&quot;Trace Plots for Mixture Model Parameters&quot;) + theme_minimal() # Examine parameter distributions p2 &lt;- mcmc_hist(draws, pars = c(&quot;bias_p&quot;, &quot;noise_p&quot;)) + ggtitle(&quot;Posterior Distributions&quot;) + theme_minimal() # Display diagnostics p1 p2 # Check for potential divergences cat(&quot;Number of divergent transitions:&quot;, sum(draws$.divergent), &quot;\\n&quot;) ## Number of divergent transitions: 0 9.3.5 Parameter Recovery Analysis To verify that our model can accurately recover parameters, let’s examine how close our inferred parameters are to the true values used in the simulation: # True parameters (from the simulation) true_bias &lt;- 0.8 true_noise &lt;- 0.1 # Calculate parameter recovery metrics bias_error &lt;- mean(draws$bias_p) - true_bias noise_error &lt;- mean(draws$noise_p) - true_noise cat(&quot;Parameter Recovery Results:\\n&quot;) ## Parameter Recovery Results: cat(&quot;Bias parameter - True:&quot;, true_bias, &quot;Estimated:&quot;, round(mean(draws$bias_p), 3), &quot;Error:&quot;, round(bias_error, 3), &quot;\\n&quot;) ## Bias parameter - True: 0.8 Estimated: 0.756 Error: -0.044 cat(&quot;Noise parameter - True:&quot;, true_noise, &quot;Estimated:&quot;, round(mean(draws$noise_p), 3), &quot;Error:&quot;, round(noise_error, 3), &quot;\\n&quot;) ## Noise parameter - True: 0.1 Estimated: 0.253 Error: 0.153 # Generate prior samples in probability space set.seed(123) n_samples &lt;- nrow(draws) prior_samples &lt;- tibble( bias_prior = inv_logit_scaled(rnorm(n_samples, 0, 1)), noise_prior = inv_logit_scaled(rnorm(n_samples, 0, 1)) ) # Prepare data for plotting recovery_data &lt;- tibble( parameter = rep(c(&quot;Bias&quot;, &quot;Noise&quot;), each = nrow(draws)), posterior = c(draws$bias_p, draws$noise_p), prior = c(prior_samples$bias_prior, prior_samples$noise_prior), true_value = rep(c(true_bias, true_noise), each = nrow(draws)), type = &quot;Posterior&quot; ) prior_data &lt;- tibble( parameter = rep(c(&quot;Bias&quot;, &quot;Noise&quot;), each = nrow(prior_samples)), value = c(prior_samples$bias_prior, prior_samples$noise_prior), true_value = rep(c(true_bias, true_noise), each = nrow(prior_samples)), type = &quot;Prior&quot; ) # Visualize parameter recovery with priors ggplot() + # Add prior densities geom_histogram(data = prior_data, aes(x = value, fill = type), alpha = 0.5, bins = 30) + # Add posterior densities geom_histogram(data = recovery_data, aes(x = posterior, fill = type), alpha = 0.5, bins = 30) + # Add true values geom_vline(data = recovery_data %&gt;% distinct(parameter, true_value), aes(xintercept = true_value), linetype = &quot;dashed&quot;, size = 1, color = &quot;black&quot;) + # Facet by parameter facet_wrap(~parameter, scales = &quot;free&quot;) + # Formatting scale_fill_manual(values = c(&quot;Posterior&quot; = &quot;blue&quot;, &quot;Prior&quot; = &quot;red&quot;), name = &quot;Distribution&quot;) + labs(title = &quot;Parameter Recovery Analysis&quot;, subtitle = &quot;Dashed lines show true values used in simulation&quot;, x = &quot;Parameter Value (probability scale)&quot;, y = &quot;Count&quot;) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) The model has recovered the true parameters reasonably well, providing confidence in our approach. In a real project, we’d want to run this across a broad range of parameter values! 9.3.6 Posterior Predictive Checks # Generate posterior and prior predictive samples n_samples &lt;- 100 # First, generate posterior predictive samples post_pred_samples &lt;- matrix(NA, nrow = n_samples, ncol = data$n) post_component_samples &lt;- matrix(NA, nrow = n_samples, ncol = data$n) # Sample from posterior distributions post_sample_indices &lt;- sample(1:nrow(draws), n_samples, replace = TRUE) for(i in 1:n_samples) { idx &lt;- post_sample_indices[i] noise_p_sample &lt;- draws$noise_p[idx] bias_p_sample &lt;- draws$bias_p[idx] for(j in 1:data$n) { # Determine component (0 = biased, 1 = random) component &lt;- rbinom(1, 1, noise_p_sample) post_component_samples[i,j] &lt;- component if(component == 1) { # Random component post_pred_samples[i,j] &lt;- rbinom(1, 1, 0.5) } else { # Biased component post_pred_samples[i,j] &lt;- rbinom(1, 1, bias_p_sample) } } } # Now, generate prior predictive samples in the same way prior_pred_samples &lt;- matrix(NA, nrow = n_samples, ncol = data$n) prior_component_samples &lt;- matrix(NA, nrow = n_samples, ncol = data$n) # Generate samples from prior distributions prior_noise_samples &lt;- inv_logit_scaled(rnorm(n_samples, 0, 1)) prior_bias_samples &lt;- inv_logit_scaled(rnorm(n_samples, 0, 1)) for(i in 1:n_samples) { noise_p_prior &lt;- prior_noise_samples[i] bias_p_prior &lt;- prior_bias_samples[i] for(j in 1:data$n) { # Determine component (0 = biased, 1 = random) component &lt;- rbinom(1, 1, noise_p_prior) prior_component_samples[i,j] &lt;- component if(component == 1) { # Random component prior_pred_samples[i,j] &lt;- rbinom(1, 1, 0.5) } else { # Biased component prior_pred_samples[i,j] &lt;- rbinom(1, 1, bias_p_prior) } } } # Calculate summary statistics from predictive samples # For posterior post_pred_means &lt;- apply(post_pred_samples, 1, mean) post_pred_run_lengths &lt;- apply(post_pred_samples, 1, function(x) mean(rle(x)$lengths)) # For prior prior_pred_means &lt;- apply(prior_pred_samples, 1, mean) prior_pred_run_lengths &lt;- apply(prior_pred_samples, 1, function(x) mean(rle(x)$lengths)) # Calculate observed statistics obs_mean &lt;- mean(data$h) obs_runs &lt;- mean(rle(data$h)$lengths) # Prepare data for visualization pred_means_data &lt;- tibble( mean = c(post_pred_means, prior_pred_means), type = factor(c(rep(&quot;Posterior&quot;, n_samples), rep(&quot;Prior&quot;, n_samples)), levels = c(&quot;Prior&quot;, &quot;Posterior&quot;)) ) pred_runs_data &lt;- tibble( run_length = c(post_pred_run_lengths, prior_pred_run_lengths), type = factor(c(rep(&quot;Posterior&quot;, n_samples), rep(&quot;Prior&quot;, n_samples)), levels = c(&quot;Prior&quot;, &quot;Posterior&quot;)) ) # Visualize posterior and prior predictive checks p1 &lt;- ggplot() + # Add prior predictive distribution geom_histogram(data = pred_means_data %&gt;% filter(type == &quot;Prior&quot;), aes(x = mean, fill = type), bins = 30, alpha = 0.5) + # Add posterior predictive distribution geom_histogram(data = pred_means_data %&gt;% filter(type == &quot;Posterior&quot;), aes(x = mean, fill = type), bins = 30, alpha = 0.5) + # Add observed statistic geom_vline(xintercept = obs_mean, color = &quot;black&quot;, size = 1, linetype = &quot;dashed&quot;) + scale_fill_manual(values = c(&quot;Prior&quot; = &quot;red&quot;, &quot;Posterior&quot; = &quot;blue&quot;), name = &quot;Distribution&quot;) + labs(title = &quot;Predictive Check: Mean&quot;, subtitle = &quot;Dashed line shows observed data value&quot;, x = &quot;Mean (proportion of right choices)&quot;, y = &quot;Count&quot;) + theme_minimal() p2 &lt;- ggplot() + # Add prior predictive distribution geom_histogram(data = pred_runs_data %&gt;% filter(type == &quot;Prior&quot;), aes(x = run_length, fill = type), bins = 30, alpha = 0.5) + # Add posterior predictive distribution geom_histogram(data = pred_runs_data %&gt;% filter(type == &quot;Posterior&quot;), aes(x = run_length, fill = type), bins = 30, alpha = 0.5) + # Add observed statistic geom_vline(xintercept = obs_runs, color = &quot;black&quot;, size = 1, linetype = &quot;dashed&quot;) + scale_fill_manual(values = c(&quot;Prior&quot; = &quot;red&quot;, &quot;Posterior&quot; = &quot;blue&quot;), name = &quot;Distribution&quot;) + labs(title = &quot;Predictive Check: Run Lengths&quot;, subtitle = &quot;Dashed line shows observed data value&quot;, x = &quot;Mean run length&quot;, y = &quot;Count&quot;) + theme_minimal() # Display the plots side by side with shared legend p1 + p2 + plot_layout(guides = &quot;collect&quot;) &amp; theme(legend.position = &quot;bottom&quot;) Our model appears to capture both the overall proportion of right choices and the distribution of run lengths in the observed data. This suggests the mixture model is adequately representing the data-generating process. 9.4 Parameter Interpretation The mixture model provides valuable insights into the cognitive processes underlying the observed behavior: Bias Parameter (θ): The bias parameter (estimated as approximately r round(mean(draws$bias_p), 2)) represents the probability of choosing the right option when the participant is following the biased process. Noise Parameter (π): The noise parameter (estimated as approximately r round(mean(draws$noise_p), 2)) represents the proportion of choices that come from the random process rather than the biased process. This can be interpreted as the frequency of attentional lapses or exploratory behavior. The mixture model thus decomposes behavior into two distinct processes, providing a more nuanced understanding than a single-process model could offer. This has important cognitive implications: for instance, besides the usual focus on “deliberate” reaction times, individual variations attentional lapses might be affected by the experimental condition, or an underlying diagnosis, thus providing richer information. 9.5 Multilevel Mixture Models Now let’s extend our approach to a multilevel (hierarchical) mixture model that can accommodate individual differences across multiple participants. This allows us to estimate both population-level parameters and individual-specific variations. 9.5.1 Stan Implementation stan_multilevel_mixture_model &lt;- &quot; // Multilevel mixture model for binary choice data // Allows individual differences in both bias and mixture weights functions { real normal_lb_rng(real mu, real sigma, real lb) { real p = normal_cdf(lb | mu, sigma); // cdf for bounds real u = uniform_rng(p, 1); return (sigma * inv_Phi(u)) + mu; // inverse cdf for value } } data { int&lt;lower=1&gt; trials; // Number of trials per agent int&lt;lower=1&gt; agents; // Number of agents array[trials, agents] int h; // Choice data (0/1) } parameters { // Population-level parameters real biasM; // Population mean of bias (logit scale) real noiseM; // Population mean of noise proportion (logit scale) // Population standard deviations vector&lt;lower=0&gt;[2] tau; // SDs for [bias, noise] // Individual z-scores (non-centered parameterization) matrix[2, agents] z_IDs; // Correlation matrix cholesky_factor_corr[2] L_u; } transformed parameters { // Individual parameters (constructed from non-centered parameterization) matrix[agents, 2] IDs; IDs = (diag_pre_multiply(tau, L_u) * z_IDs)&#39;; } model { // Priors for population means target += normal_lpdf(biasM | 0, 1); // Prior for population bias mean target += normal_lpdf(noiseM | -1, 0.5); // Prior for population noise mean (favoring lower noise) // Priors for population SDs (half-normal) target += normal_lpdf(tau[1] | 0, 0.3) - normal_lccdf(0 | 0, 0.3); target += normal_lpdf(tau[2] | 0, 0.3) - normal_lccdf(0 | 0, 0.3); // Prior for correlation matrix target += lkj_corr_cholesky_lpdf(L_u | 2); // Prior for individual z-scores target += std_normal_lpdf(to_vector(z_IDs)); // Likelihood for (i in 1:agents) { target += log_sum_exp( log(inv_logit(noiseM + IDs[i,2])) + // Prob of random process for agent i bernoulli_logit_lpmf(h[,i] | 0), // Likelihood under random process log1m(inv_logit(noiseM + IDs[i,2])) + // Prob of biased process for agent i bernoulli_logit_lpmf(h[,i] | biasM + IDs[i,1]) // Likelihood under biased process ); } } generated quantities { // Prior predictive samples real biasM_prior = normal_rng(0, 1); real&lt;lower=0&gt; biasSD_prior = normal_lb_rng(0, 0.3, 0); real noiseM_prior = normal_rng(-1, 0.5); real&lt;lower=0&gt; noiseSD_prior = normal_lb_rng(0, 0.3, 0); // Transform to probability scale for easier interpretation real&lt;lower=0, upper=1&gt; bias_mean = inv_logit(biasM); real&lt;lower=0, upper=1&gt; noise_mean = inv_logit(noiseM); // Correlation between parameters corr_matrix[2] Omega = multiply_lower_tri_self_transpose(L_u); real bias_noise_corr = Omega[1,2]; // For each agent, generate individual parameters and predictions array[agents] real&lt;lower=0, upper=1&gt; agent_bias; array[agents] real&lt;lower=0, upper=1&gt; agent_noise; array[trials, agents] int pred_component; array[trials, agents] int pred_choice; array[trials, agents] real log_lik; // Calculate individual parameters and generate predictions for (i in 1:agents) { // Individual parameters agent_bias[i] = inv_logit(biasM + IDs[i,1]); agent_noise[i] = inv_logit(noiseM + IDs[i,2]); // Log likelihood calculations for (t in 1:trials) { log_lik[t,i] = log_sum_exp( log(agent_noise[i]) + bernoulli_logit_lpmf(h[t,i] | 0), log1m(agent_noise[i]) + bernoulli_logit_lpmf(h[t,i] | logit(agent_bias[i])) ); } // Generate predictions for (t in 1:trials) { pred_component[t,i] = bernoulli_rng(agent_noise[i]); if (pred_component[t,i] == 1) { // Random component pred_choice[t,i] = bernoulli_rng(0.5); } else { // Biased component pred_choice[t,i] = bernoulli_rng(agent_bias[i]); } } } } &quot; # Write the model to a file write_stan_file( stan_multilevel_mixture_model, dir = &quot;stan/&quot;, basename = &quot;W8_MixtureMultilevel.stan&quot; ) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W8_MixtureMultilevel.stan&quot; # Compile the model file &lt;- file.path(&quot;stan/W8_MixtureMultilevel.stan&quot;) mod_multilevel_mixture &lt;- cmdstan_model( file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;) ) 9.5.2 Preparing Data for the Multilevel Model Now we’ll prepare data from multiple agents for the multilevel model: # Set random seed for reproducibility set.seed(12345) # Simulation parameters agents &lt;- 20 trials &lt;- 120 # Population-level parameters rate_M &lt;- 1.386 # Population mean of bias (~0.8 in probability space) rate_SD &lt;- 0.65 # Population SD of bias noise_M &lt;- -2.2 # Population mean of noise (~0.1 in probability space) noise_SD &lt;- 0.5 # Population SD of noise # Print population parameters in probability space for clarity cat(&quot;Population parameters in probability space:\\n&quot;) ## Population parameters in probability space: cat(&quot;Mean rate:&quot;, round(plogis(rate_M), 2), &quot;\\n&quot;) ## Mean rate: 0.8 cat(&quot;Mean noise:&quot;, round(plogis(noise_M), 2), &quot;\\n&quot;) ## Mean noise: 0.1 # Generate random agent parameters agent_rates &lt;- rnorm(agents, rate_M, rate_SD) agent_noises &lt;- rnorm(agents, noise_M, noise_SD) # Convert to probability space for easier interpretation agent_rates_prob &lt;- plogis(agent_rates) agent_noises_prob &lt;- plogis(agent_noises) # Generate choice data generate_agent_data &lt;- function(agent_id, rate, noise, n_trials) { # Convert parameters to probability space rate_p &lt;- plogis(rate) noise_p &lt;- plogis(noise) # Initialize vectors choices &lt;- numeric(n_trials) components &lt;- numeric(n_trials) # 0 = biased, 1 = random for (t in 1:n_trials) { # Determine which component generates this choice component &lt;- rbinom(1, 1, noise_p) components[t] &lt;- component if (component == 1) { # Random component (0.5 probability) choices[t] &lt;- rbinom(1, 1, 0.5) } else { # Biased component choices[t] &lt;- rbinom(1, 1, rate_p) } } return(tibble( agent = agent_id, trial = 1:n_trials, choice = choices, component = components, true_rate = rate, true_rate_prob = rate_p, true_noise = noise, true_noise_prob = noise_p )) } # Generate data for all agents d &lt;- map_df(1:agents, function(id) { generate_agent_data( agent_id = id, rate = agent_rates[id], noise = agent_noises[id], n_trials = trials ) }) # Create matrix where columns are agents, rows are trials h_matrix &lt;- d %&gt;% dplyr::select(agent, trial, choice) %&gt;% pivot_wider( names_from = agent, values_from = choice ) %&gt;% dplyr::select(-trial) %&gt;% as.matrix() # Summary statistics agent_summary &lt;- d %&gt;% group_by(agent, true_rate_prob, true_noise_prob) %&gt;% summarize( mean_choice = mean(choice), prop_component_1 = mean(component), n_trials = n() ) # Prepare data for Stan data_multilevel &lt;- list( trials = trials, agents = agents, h = h_matrix ) # Verify the dimensions of the matrix print(paste(&quot;Matrix dimensions (trials × agents):&quot;, paste(dim(h_matrix), collapse = &quot; × &quot;))) ## [1] &quot;Matrix dimensions (trials × agents): 120 × 20&quot; # Display summary of agent parameters cat(&quot;\\nAgent parameters summary:\\n&quot;) ## ## Agent parameters summary: print(summary(agent_summary)) ## agent true_rate_prob true_noise_prob mean_choice prop_component_1 ## Min. : 1.00 Min. :0.5509 Min. :0.04604 Min. :0.6083 Min. :0.02500 ## 1st Qu.: 5.75 1st Qu.:0.7596 1st Qu.:0.08461 1st Qu.:0.7375 1st Qu.:0.06458 ## Median :10.50 Median :0.8088 Median :0.12743 Median :0.8083 Median :0.12083 ## Mean :10.50 Mean :0.7952 Mean :0.13278 Mean :0.7771 Mean :0.12500 ## 3rd Qu.:15.25 3rd Qu.:0.8562 3rd Qu.:0.19007 3rd Qu.:0.8187 3rd Qu.:0.16042 ## Max. :20.00 Max. :0.9287 Max. :0.24944 Max. :0.8750 Max. :0.30000 ## n_trials ## Min. :120 ## 1st Qu.:120 ## Median :120 ## Mean :120 ## 3rd Qu.:120 ## Max. :120 # Plot agent-level parameters ggplot(agent_summary, aes(x = true_rate_prob, y = true_noise_prob)) + geom_point(alpha = 0.7) + geom_text(aes(label = agent), hjust = -0.3, vjust = 0) + labs( title = &quot;True Agent Parameters&quot;, subtitle = &quot;Each point represents one agent&quot;, x = &quot;Rate (probability of choosing right when using biased component)&quot;, y = &quot;Noise (probability of using random component)&quot;, size = &quot;Mean Choice&quot; ) + theme_minimal() # Plot choice patterns for a few selected agents selected_agents &lt;- c(1, 5, 10, 15, 20) d_selected &lt;- d %&gt;% filter(agent %in% selected_agents) ggplot(d_selected, aes(x = trial, y = choice, color = factor(agent))) + geom_line(alpha = 0.5) + geom_point(aes(shape = factor(component)), size = 2) + scale_shape_manual(values = c(16, 4), labels = c(&quot;Biased&quot;, &quot;Random&quot;), name = &quot;Component&quot;) + labs( title = &quot;Choice Patterns for Selected Agents&quot;, subtitle = &quot;Circles = biased component, X = random component&quot;, x = &quot;Trial&quot;, y = &quot;Choice (0/1)&quot;, color = &quot;Agent&quot; ) + theme_minimal() + facet_wrap(~agent, ncol = 1) 9.5.3 Fitting and Evaluating the Multilevel Model # File path for saved multilevel model results multilevel_model_file &lt;- &quot;simmodels/W8_multimixture.RDS&quot; # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(multilevel_model_file)) { # Fit the multilevel model multilevel_samples &lt;- mod_multilevel_mixture$sample( data = data_multilevel, seed = 123, chains = 2, parallel_chains = 2, threads_per_chain = 2, iter_warmup = 2000, iter_sampling = 2000, refresh = 500, max_treedepth = 20, adapt_delta = 0.99, ) # Save the results multilevel_samples$save_object(file = multilevel_model_file) cat(&quot;Generated new multilevel model fit and saved to&quot;, multilevel_model_file, &quot;\\n&quot;) } else { # Load existing results multilevel_samples &lt;- readRDS(multilevel_model_file) cat(&quot;Loaded existing multilevel model fit from&quot;, multilevel_model_file, &quot;\\n&quot;) } ## Running MCMC with 2 parallel chains, with 2 thread(s) per chain... ## ## Chain 1 Iteration: 1 / 4000 [ 0%] (Warmup) ## Chain 2 Iteration: 1 / 4000 [ 0%] (Warmup) ## Chain 1 Iteration: 500 / 4000 [ 12%] (Warmup) ## Chain 2 Iteration: 500 / 4000 [ 12%] (Warmup) ## Chain 1 Iteration: 1000 / 4000 [ 25%] (Warmup) ## Chain 2 Iteration: 1000 / 4000 [ 25%] (Warmup) ## Chain 1 Iteration: 1500 / 4000 [ 37%] (Warmup) ## Chain 2 Iteration: 1500 / 4000 [ 37%] (Warmup) ## Chain 1 Iteration: 2000 / 4000 [ 50%] (Warmup) ## Chain 1 Iteration: 2001 / 4000 [ 50%] (Sampling) ## Chain 2 Iteration: 2000 / 4000 [ 50%] (Warmup) ## Chain 2 Iteration: 2001 / 4000 [ 50%] (Sampling) ## Chain 1 Iteration: 2500 / 4000 [ 62%] (Sampling) ## Chain 2 Iteration: 2500 / 4000 [ 62%] (Sampling) ## Chain 1 Iteration: 3000 / 4000 [ 75%] (Sampling) ## Chain 2 Iteration: 3000 / 4000 [ 75%] (Sampling) ## Chain 1 Iteration: 3500 / 4000 [ 87%] (Sampling) ## Chain 2 Iteration: 3500 / 4000 [ 87%] (Sampling) ## Chain 1 Iteration: 4000 / 4000 [100%] (Sampling) ## Chain 1 finished in 12.1 seconds. ## Chain 2 Iteration: 4000 / 4000 [100%] (Sampling) ## Chain 2 finished in 14.0 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 13.0 seconds. ## Total execution time: 14.3 seconds. ## ## Generated new multilevel model fit and saved to simmodels/W8_multimixture.RDS # Summary of population-level parameters summary_ml &lt;- multilevel_samples$summary(c(&quot;bias_mean&quot;, &quot;noise_mean&quot;, &quot;bias_noise_corr&quot;)) print(summary_ml) ## # A tibble: 3 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 bias_mean 0.782 0.783 0.0152 0.0148 0.756 0.805 1.00 1643. 2204. ## 2 noise_mean 0.159 0.153 0.0538 0.0522 0.0823 0.257 1.00 4433. 2085. ## 3 bias_noise_corr -0.00263 0.00248 0.444 0.512 -0.721 0.715 1.00 5316. 2615. 9.5.4 Examining Individual Differences The multilevel mixture model allows us to examine individual differences in both bias and mixture weights: # Extract draws for individual parameters draws_ml &lt;- as_draws_df(multilevel_samples$draws()) # First, we need to extract all the necessary parameters # Population-level parameters biasM &lt;- mean(draws_ml$biasM) noiseM &lt;- mean(draws_ml$noiseM) tau1 &lt;- mean(draws_ml$`tau[1]`) tau2 &lt;- mean(draws_ml$`tau[2]`) # Create a dataframe to store the individual parameters individual_params &lt;- tibble( agent = 1:data_multilevel$agents, bias = numeric(data_multilevel$agents), noise = numeric(data_multilevel$agents) ) # Loop through agents to extract and transform z_IDs to actual parameter values for (i in 1:data_multilevel$agents) { # Extract the z-scores for this agent z1_name &lt;- paste0(&quot;z_IDs[1,&quot;, i, &quot;]&quot;) z2_name &lt;- paste0(&quot;z_IDs[2,&quot;, i, &quot;]&quot;) # Calculate bias and noise based on the multilevel structure # Note: This assumes the model uses z-scores to represent deviations from population means z1 &lt;- mean(draws_ml[[z1_name]]) z2 &lt;- mean(draws_ml[[z2_name]]) # Transform back to original scale individual_params$bias[i] &lt;- plogis(biasM + z1 * tau1) # Convert to probability scale individual_params$noise[i] &lt;- plogis(noiseM + z2 * tau2) # Convert to probability scale } # Get the true values from our generated data frame &#39;d&#39; true_params &lt;- d %&gt;% group_by(agent) %&gt;% summarize( true_rate = first(true_rate), true_rate_prob = first(true_rate_prob), true_noise = first(true_noise), true_noise_prob = first(true_noise_prob) ) # Join with the true parameter values individual_params &lt;- individual_params %&gt;% left_join(true_params, by = &quot;agent&quot;) # Check the result head(individual_params) ## # A tibble: 6 × 7 ## agent bias noise true_rate true_rate_prob true_noise true_noise_prob ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.800 0.151 1.77 0.854 -1.81 0.141 ## 2 2 0.815 0.151 1.85 0.864 -1.47 0.187 ## 3 3 0.758 0.150 1.31 0.788 -2.52 0.0743 ## 4 4 0.800 0.150 1.09 0.749 -2.98 0.0485 ## 5 5 0.840 0.151 1.78 0.856 -3.00 0.0475 ## 6 6 0.709 0.153 0.204 0.551 -1.30 0.215 # Plot individual parameter estimates p1 &lt;- ggplot(individual_params, aes(x = bias, y = noise)) + geom_point(size = 3, alpha = 0.7) + geom_hline(yintercept = mean(draws_ml$noise_mean), linetype = &quot;dashed&quot;) + geom_vline(xintercept = mean(draws_ml$bias_mean), linetype = &quot;dashed&quot;) + labs(title = &quot;Individual Parameter Estimates&quot;, subtitle = &quot;Each point represents one agent&quot;, x = &quot;Bias Parameter (θ)&quot;, y = &quot;Noise Parameter (π)&quot;) + theme_minimal() # Plot correlation between true and estimated parameters p2 &lt;- ggplot(individual_params, aes(x = inv_logit_scaled(true_rate), y = bias)) + geom_point(size = 3, alpha = 0.7) + geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;) + labs(title = &quot;Parameter Recovery: Bias&quot;, subtitle = &quot;True vs. estimated bias parameter&quot;, x = &quot;True Bias&quot;, y = &quot;Estimated Bias&quot;) + theme_minimal() # Display plots p1 p2 9.5.5 Interpreting Parameter Correlations An important advantage of multilevel mixture models is their ability to reveal correlations between parameters across individuals: # Visualize correlation between parameters ggplot() + geom_histogram(aes(x = draws_ml$bias_noise_corr), bins = 30, fill = &quot;steelblue&quot;, alpha = 0.7) + geom_vline(xintercept = 0, linetype = &quot;dashed&quot;) + labs(title = &quot;Correlation Between Bias and Noise Parameters&quot;, subtitle = &quot;Population-level correlation across individuals&quot;, x = &quot;Correlation Coefficient&quot;, y = &quot;Count&quot;) + theme_minimal() The correlation between bias and noise parameters can provide important insights into cognitive processes. For example, a negative correlation might suggest that individuals with stronger biases tend to have fewer random lapses, while a positive correlation could indicate that strong biases are associated with more exploratory behavior. However, in the simulation process we did not include any correlation between the bias and noise parameters, so the correlation we observe here is correctly estimated as centered at 0. 9.6 Comparing Single-Process and Mixture Models Finally, let’s compare our mixture model with a single-process alternative to determine which better captures the observed behavior. 9.6.1 Single-Process Model First, let’s implement a simple single-process model that assumes all choices come from a biased process: # Stan model for single-process biased agent stan_biased_model &lt;- &quot; data { int&lt;lower=1&gt; n; // Number of trials array[n] int h; // Choice data (0/1) } parameters { real bias; // Bias parameter (logit scale) } model { // Prior target += normal_lpdf(bias | 0, 1); // Likelihood (all choices come from biased process) target += bernoulli_logit_lpmf(h | bias); } generated quantities { real&lt;lower=0, upper=1&gt; bias_p = inv_logit(bias); // Bias on probability scale // Log likelihood for model comparison vector[n] log_lik; for (i in 1:n) { log_lik[i] = bernoulli_logit_lpmf(h[i] | bias); } // Posterior predictions array[n] int pred_choice; for (i in 1:n) { pred_choice[i] = bernoulli_rng(bias_p); } } &quot; # Write the model to a file write_stan_file( stan_biased_model, dir = &quot;stan/&quot;, basename = &quot;W8_BiasedSingle.stan&quot; ) ## [1] &quot;/Users/au209589/Dropbox/Teaching/AdvancedCognitiveModeling23_book/stan/W8_BiasedSingle.stan&quot; # Compile the model file &lt;- file.path(&quot;stan/W8_BiasedSingle.stan&quot;) mod_biased &lt;- cmdstan_model( file, cpp_options = list(stan_threads = TRUE), stanc_options = list(&quot;O1&quot;) ) # File path for saved single-process model results biased_model_file &lt;- &quot;simmodels/W8_biased_single.RDS&quot; # Check if we need to rerun the simulation if (regenerate_simulations || !file.exists(biased_model_file)) { # Fit the single-process model biased_samples &lt;- mod_biased$sample( data = data, seed = 123, chains = 2, parallel_chains = 2, threads_per_chain = 2, iter_warmup = 2000, iter_sampling = 2000, refresh = 500, max_treedepth = 20, adapt_delta = 0.99, ) # Save the results biased_samples$save_object(file = biased_model_file) cat(&quot;Generated new single-process model fit and saved to&quot;, biased_model_file, &quot;\\n&quot;) } else { # Load existing results biased_samples &lt;- readRDS(biased_model_file) cat(&quot;Loaded existing single-process model fit from&quot;, biased_model_file, &quot;\\n&quot;) } ## Running MCMC with 2 parallel chains, with 2 thread(s) per chain... ## ## Chain 1 Iteration: 1 / 4000 [ 0%] (Warmup) ## Chain 1 Iteration: 500 / 4000 [ 12%] (Warmup) ## Chain 1 Iteration: 1000 / 4000 [ 25%] (Warmup) ## Chain 1 Iteration: 1500 / 4000 [ 37%] (Warmup) ## Chain 1 Iteration: 2000 / 4000 [ 50%] (Warmup) ## Chain 1 Iteration: 2001 / 4000 [ 50%] (Sampling) ## Chain 1 Iteration: 2500 / 4000 [ 62%] (Sampling) ## Chain 1 Iteration: 3000 / 4000 [ 75%] (Sampling) ## Chain 1 Iteration: 3500 / 4000 [ 87%] (Sampling) ## Chain 1 Iteration: 4000 / 4000 [100%] (Sampling) ## Chain 2 Iteration: 1 / 4000 [ 0%] (Warmup) ## Chain 2 Iteration: 500 / 4000 [ 12%] (Warmup) ## Chain 2 Iteration: 1000 / 4000 [ 25%] (Warmup) ## Chain 2 Iteration: 1500 / 4000 [ 37%] (Warmup) ## Chain 2 Iteration: 2000 / 4000 [ 50%] (Warmup) ## Chain 2 Iteration: 2001 / 4000 [ 50%] (Sampling) ## Chain 2 Iteration: 2500 / 4000 [ 62%] (Sampling) ## Chain 2 Iteration: 3000 / 4000 [ 75%] (Sampling) ## Chain 1 finished in 0.1 seconds. ## Chain 2 Iteration: 3500 / 4000 [ 87%] (Sampling) ## Chain 2 Iteration: 4000 / 4000 [100%] (Sampling) ## Chain 2 finished in 0.2 seconds. ## ## Both chains finished successfully. ## Mean chain execution time: 0.2 seconds. ## Total execution time: 0.4 seconds. ## ## Generated new single-process model fit and saved to simmodels/W8_biased_single.RDS # Summary of single-process model summary_biased &lt;- biased_samples$summary(&quot;bias_p&quot;) print(summary_biased) ## # A tibble: 1 × 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 bias_p 0.754 0.756 0.0373 0.0386 0.692 0.814 1.00 797. 994. 9.6.2 Model Comparison Now let’s compare the single-process model with our mixture model using leave-one-out cross-validation (LOO-CV): # Calculate LOO for both models loo_mixture &lt;- samples$loo() loo_biased &lt;- biased_samples$loo() # Compare models comparison &lt;- loo_compare(loo_mixture, loo_biased) print(comparison) ## elpd_diff se_diff ## model2 0.0 0.0 ## model1 -1.9 1.5 # Calculate model weights weights &lt;- loo_model_weights(list( &quot;Mixture&quot; = loo_mixture, &quot;Single-Process&quot; = loo_biased )) print(weights) ## Method: stacking ## ------ ## weight ## Mixture 0.000 ## Single-Process 1.000 # Visualize model comparison ggplot(tibble( model = names(weights), weight = as.numeric(weights) )) + geom_col(aes(x = model, y = weight, fill = model), alpha = 0.7) + scale_fill_brewer(palette = &quot;Set1&quot;) + labs(title = &quot;Model Comparison via LOO-CV&quot;, subtitle = &quot;Higher weights indicate better predictive performance&quot;, x = NULL, y = &quot;Model Weight&quot;) + theme_minimal() + theme(legend.position = &quot;none&quot;) + geom_text(aes(x = model, y = weight + 0.05, label = scales::percent(weight, accuracy = 0.1))) Based on the model comparison, the single model appears to better capture the data-generating process than the single-process alternative. This might seem counterintuitive, but it highlights that predictive performance is not necessarily the best way of choosing your model. What happens here is that a combination of two binomials can be mathematically reduced to a single binomial (in this case with a lower rate that the biased component). This is why the single-process model performs better in terms of LOO-CV, even though the mixture model is more realistic and provides a richer interpretation of the data. Theory should guide you in this case. 9.6.3 Applications of Mixture Models in Cognitive Science Mixture models have found numerous applications in cognitive science. Here are a few examples that highlight their versatility: Attention and Vigilance: Modeling attentional lapses during sustained attention tasks as a mixture of focused and random responses. Memory: Representing recognition memory as a mixture of more implicit familiarity and more explicit recollection processes, each with distinct characteristics. *Decision Making: Modeling economic choices as combinations of heuristic and deliberative processes, with the proportion varying based on task demands. Learning: Capturing the transition from rule-based to automatic processing during skill acquisition, with the mixture weights shifting over time. Individual Differences: Identifying subgroups of participants who employ qualitatively different strategies to solve the same task. 9.7 Conclusion Mixture models represent a crucial step forward in our cognitive modeling toolkit, allowing us to capture the complexity and variability inherent in human behavior. Through this chapter, we’ve seen how combining multiple cognitive strategies within a single model can provide richer and more realistic accounts of decision-making processes. Several key insights emerge from our exploration of mixture models: Beyond Single-Process Simplifications: Mixture models allow us to move beyond the false choice between oversimplified single-strategy models and intractably complex specifications. By combining a small number of interpretable components, we can capture substantial behavioral complexity while maintaining mathematical and computational tractability. Bayesian Implementation: The Bayesian implementation of mixture models in Stan provides powerful tools for inference. We can estimate not only the parameters of different cognitive strategies but also their relative contributions to behavior and how they might vary across individuals. Model Validation: Mixture models require careful attention to identifiability and validation. Through parameter recovery studies and posterior predictive checks, we’ve seen how to verify that our specifications can reliably recover true parameter values and generate realistic behavioral patterns. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
